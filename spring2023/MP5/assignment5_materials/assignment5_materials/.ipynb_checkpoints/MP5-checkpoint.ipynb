{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44971/607054412.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "/tmp/ipykernel_44971/607054412.py:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 0   score: 0.0   mem len: 122   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 0.0\n",
      "epis: 1   score: 2.0   mem len: 339   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.0\n",
      "epis: 2   score: 3.0   mem len: 567   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.67\n",
      "epis: 3   score: 2.0   mem len: 766   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.75\n",
      "epis: 4   score: 1.0   mem len: 917   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 5   score: 1.0   mem len: 1089   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.5\n",
      "epis: 6   score: 2.0   mem len: 1271   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.57\n",
      "epis: 7   score: 1.0   mem len: 1439   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 8   score: 0.0   mem len: 1562   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 9   score: 0.0   mem len: 1685   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 10   score: 1.0   mem len: 1836   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.18\n",
      "epis: 11   score: 2.0   mem len: 2033   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.25\n",
      "epis: 12   score: 1.0   mem len: 2184   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.23\n",
      "epis: 13   score: 3.0   mem len: 2430   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.36\n",
      "epis: 14   score: 1.0   mem len: 2599   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.33\n",
      "epis: 15   score: 1.0   mem len: 2770   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.31\n",
      "epis: 16   score: 0.0   mem len: 2892   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.24\n",
      "epis: 17   score: 3.0   mem len: 3117   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.33\n",
      "epis: 18   score: 3.0   mem len: 3387   epsilon: 1.0    steps: 270    lr: 0.0001     reward: 1.42\n",
      "epis: 19   score: 1.0   mem len: 3537   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.4\n",
      "epis: 20   score: 1.0   mem len: 3688   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 21   score: 4.0   mem len: 3944   epsilon: 1.0    steps: 256    lr: 0.0001     reward: 1.5\n",
      "epis: 22   score: 1.0   mem len: 4113   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 23   score: 0.0   mem len: 4235   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 24   score: 1.0   mem len: 4386   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 25   score: 1.0   mem len: 4555   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 26   score: 1.0   mem len: 4725   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.37\n",
      "epis: 27   score: 1.0   mem len: 4896   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.36\n",
      "epis: 28   score: 2.0   mem len: 5098   epsilon: 1.0    steps: 202    lr: 0.0001     reward: 1.38\n",
      "epis: 29   score: 1.0   mem len: 5266   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.37\n",
      "epis: 30   score: 0.0   mem len: 5388   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 31   score: 1.0   mem len: 5556   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.31\n",
      "epis: 32   score: 0.0   mem len: 5679   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 33   score: 3.0   mem len: 5925   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.32\n",
      "epis: 34   score: 2.0   mem len: 6125   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.34\n",
      "epis: 35   score: 0.0   mem len: 6248   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 36   score: 0.0   mem len: 6371   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 37   score: 0.0   mem len: 6493   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.24\n",
      "epis: 38   score: 0.0   mem len: 6615   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.21\n",
      "epis: 39   score: 3.0   mem len: 6879   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.25\n",
      "epis: 40   score: 2.0   mem len: 7078   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.27\n",
      "epis: 41   score: 2.0   mem len: 7260   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.29\n",
      "epis: 42   score: 0.0   mem len: 7383   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.26\n",
      "epis: 43   score: 1.0   mem len: 7555   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.25\n",
      "epis: 44   score: 2.0   mem len: 7774   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.27\n",
      "epis: 45   score: 1.0   mem len: 7924   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.26\n",
      "epis: 46   score: 2.0   mem len: 8142   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.28\n",
      "epis: 47   score: 2.0   mem len: 8340   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 48   score: 2.0   mem len: 8537   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.31\n",
      "epis: 49   score: 0.0   mem len: 8660   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 50   score: 4.0   mem len: 8937   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.33\n",
      "epis: 51   score: 2.0   mem len: 9116   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.35\n",
      "epis: 52   score: 4.0   mem len: 9414   epsilon: 1.0    steps: 298    lr: 0.0001     reward: 1.4\n",
      "epis: 53   score: 2.0   mem len: 9629   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.41\n",
      "epis: 54   score: 1.0   mem len: 9798   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 55   score: 0.0   mem len: 9921   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 56   score: 0.0   mem len: 10044   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 57   score: 0.0   mem len: 10167   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 58   score: 1.0   mem len: 10339   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.32\n",
      "epis: 59   score: 2.0   mem len: 10537   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 60   score: 0.0   mem len: 10659   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 61   score: 4.0   mem len: 10935   epsilon: 1.0    steps: 276    lr: 0.0001     reward: 1.35\n",
      "epis: 62   score: 1.0   mem len: 11107   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.35\n",
      "epis: 63   score: 0.0   mem len: 11229   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 64   score: 3.0   mem len: 11496   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.35\n",
      "epis: 65   score: 2.0   mem len: 11694   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 66   score: 0.0   mem len: 11816   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 67   score: 1.0   mem len: 11967   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 68   score: 3.0   mem len: 12229   epsilon: 1.0    steps: 262    lr: 0.0001     reward: 1.36\n",
      "epis: 69   score: 2.0   mem len: 12427   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 70   score: 0.0   mem len: 12550   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 71   score: 1.0   mem len: 12719   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 72   score: 3.0   mem len: 12985   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.37\n",
      "epis: 73   score: 0.0   mem len: 13108   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 74   score: 2.0   mem len: 13306   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 75   score: 0.0   mem len: 13429   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 76   score: 1.0   mem len: 13581   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.34\n",
      "epis: 77   score: 0.0   mem len: 13703   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 78   score: 1.0   mem len: 13871   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.32\n",
      "epis: 79   score: 3.0   mem len: 14118   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.34\n",
      "epis: 80   score: 0.0   mem len: 14241   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 81   score: 0.0   mem len: 14363   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 82   score: 1.0   mem len: 14513   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.3\n",
      "epis: 83   score: 4.0   mem len: 14771   epsilon: 1.0    steps: 258    lr: 0.0001     reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 84   score: 0.0   mem len: 14894   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 85   score: 1.0   mem len: 15045   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.31\n",
      "epis: 86   score: 1.0   mem len: 15197   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.31\n",
      "epis: 87   score: 4.0   mem len: 15494   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.34\n",
      "epis: 88   score: 2.0   mem len: 15713   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.35\n",
      "epis: 89   score: 1.0   mem len: 15864   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 90   score: 1.0   mem len: 16014   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.34\n",
      "epis: 91   score: 1.0   mem len: 16183   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 92   score: 0.0   mem len: 16306   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 93   score: 2.0   mem len: 16524   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.33\n",
      "epis: 94   score: 1.0   mem len: 16675   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 95   score: 3.0   mem len: 16941   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.34\n",
      "epis: 96   score: 3.0   mem len: 17191   epsilon: 1.0    steps: 250    lr: 0.0001     reward: 1.36\n",
      "epis: 97   score: 0.0   mem len: 17314   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 98   score: 1.0   mem len: 17483   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 99   score: 1.0   mem len: 17633   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.34\n",
      "epis: 100   score: 1.0   mem len: 17805   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.35\n",
      "epis: 101   score: 0.0   mem len: 17928   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 102   score: 0.0   mem len: 18050   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 103   score: 1.0   mem len: 18219   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.29\n",
      "epis: 104   score: 2.0   mem len: 18417   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.3\n",
      "epis: 105   score: 0.0   mem len: 18539   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 106   score: 2.0   mem len: 18737   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 107   score: 0.0   mem len: 18860   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 108   score: 4.0   mem len: 19156   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.32\n",
      "epis: 109   score: 0.0   mem len: 19278   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 110   score: 0.0   mem len: 19401   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 111   score: 2.0   mem len: 19623   epsilon: 1.0    steps: 222    lr: 0.0001     reward: 1.31\n",
      "epis: 112   score: 2.0   mem len: 19822   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.32\n",
      "epis: 113   score: 2.0   mem len: 20039   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.31\n",
      "epis: 114   score: 1.0   mem len: 20208   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 115   score: 0.0   mem len: 20331   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.3\n",
      "epis: 116   score: 2.0   mem len: 20549   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 117   score: 1.0   mem len: 20720   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.3\n",
      "epis: 118   score: 2.0   mem len: 20918   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 119   score: 3.0   mem len: 21185   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.31\n",
      "epis: 120   score: 2.0   mem len: 21382   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.32\n",
      "epis: 121   score: 1.0   mem len: 21533   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.29\n",
      "epis: 122   score: 3.0   mem len: 21778   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.31\n",
      "epis: 123   score: 2.0   mem len: 21976   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 124   score: 2.0   mem len: 22191   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.34\n",
      "epis: 125   score: 5.0   mem len: 22499   epsilon: 1.0    steps: 308    lr: 0.0001     reward: 1.38\n",
      "epis: 126   score: 0.0   mem len: 22621   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 127   score: 2.0   mem len: 22819   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 128   score: 1.0   mem len: 22989   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.37\n",
      "epis: 129   score: 3.0   mem len: 23235   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.39\n",
      "epis: 130   score: 1.0   mem len: 23403   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.4\n",
      "epis: 131   score: 1.0   mem len: 23571   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.4\n",
      "epis: 132   score: 0.0   mem len: 23694   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 133   score: 1.0   mem len: 23845   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 134   score: 0.0   mem len: 23967   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 135   score: 0.0   mem len: 24089   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 136   score: 3.0   mem len: 24337   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.39\n",
      "epis: 137   score: 1.0   mem len: 24488   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 138   score: 0.0   mem len: 24610   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 139   score: 1.0   mem len: 24781   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.38\n",
      "epis: 140   score: 4.0   mem len: 25053   epsilon: 1.0    steps: 272    lr: 0.0001     reward: 1.4\n",
      "epis: 141   score: 0.0   mem len: 25175   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 142   score: 1.0   mem len: 25344   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 143   score: 1.0   mem len: 25513   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 144   score: 0.0   mem len: 25636   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 145   score: 1.0   mem len: 25787   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 146   score: 0.0   mem len: 25909   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 147   score: 4.0   mem len: 26225   epsilon: 1.0    steps: 316    lr: 0.0001     reward: 1.37\n",
      "epis: 148   score: 0.0   mem len: 26347   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 149   score: 4.0   mem len: 26627   epsilon: 1.0    steps: 280    lr: 0.0001     reward: 1.39\n",
      "epis: 150   score: 0.0   mem len: 26749   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 151   score: 3.0   mem len: 26996   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.36\n",
      "epis: 152   score: 1.0   mem len: 27147   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 153   score: 2.0   mem len: 27345   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 154   score: 2.0   mem len: 27543   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 155   score: 1.0   mem len: 27712   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 156   score: 1.0   mem len: 27864   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.36\n",
      "epis: 157   score: 4.0   mem len: 28139   epsilon: 1.0    steps: 275    lr: 0.0001     reward: 1.4\n",
      "epis: 158   score: 1.0   mem len: 28310   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.4\n",
      "epis: 159   score: 2.0   mem len: 28527   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.4\n",
      "epis: 160   score: 0.0   mem len: 28650   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 161   score: 0.0   mem len: 28773   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 162   score: 2.0   mem len: 28974   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.37\n",
      "epis: 163   score: 3.0   mem len: 29222   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.4\n",
      "epis: 164   score: 2.0   mem len: 29421   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.39\n",
      "epis: 165   score: 2.0   mem len: 29637   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.39\n",
      "epis: 166   score: 0.0   mem len: 29760   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 167   score: 1.0   mem len: 29929   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 168   score: 2.0   mem len: 30126   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.38\n",
      "epis: 169   score: 2.0   mem len: 30324   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 170   score: 4.0   mem len: 30600   epsilon: 1.0    steps: 276    lr: 0.0001     reward: 1.42\n",
      "epis: 171   score: 0.0   mem len: 30723   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 172   score: 1.0   mem len: 30892   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 173   score: 2.0   mem len: 31110   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 174   score: 0.0   mem len: 31233   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 175   score: 1.0   mem len: 31384   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 176   score: 2.0   mem len: 31602   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 177   score: 1.0   mem len: 31771   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 178   score: 0.0   mem len: 31894   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 179   score: 1.0   mem len: 32045   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 180   score: 0.0   mem len: 32167   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 181   score: 0.0   mem len: 32289   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 182   score: 0.0   mem len: 32412   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 183   score: 3.0   mem len: 32660   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.37\n",
      "epis: 184   score: 2.0   mem len: 32878   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.39\n",
      "epis: 185   score: 4.0   mem len: 33136   epsilon: 1.0    steps: 258    lr: 0.0001     reward: 1.42\n",
      "epis: 186   score: 0.0   mem len: 33259   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 187   score: 1.0   mem len: 33430   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.38\n",
      "epis: 188   score: 0.0   mem len: 33553   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 189   score: 0.0   mem len: 33675   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 190   score: 2.0   mem len: 33875   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.36\n",
      "epis: 191   score: 3.0   mem len: 34143   epsilon: 1.0    steps: 268    lr: 0.0001     reward: 1.38\n",
      "epis: 192   score: 0.0   mem len: 34265   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 193   score: 2.0   mem len: 34445   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.38\n",
      "epis: 194   score: 1.0   mem len: 34614   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 195   score: 1.0   mem len: 34785   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.36\n",
      "epis: 196   score: 0.0   mem len: 34907   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 197   score: 4.0   mem len: 35221   epsilon: 1.0    steps: 314    lr: 0.0001     reward: 1.37\n",
      "epis: 198   score: 1.0   mem len: 35390   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 199   score: 0.0   mem len: 35512   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 200   score: 3.0   mem len: 35761   epsilon: 1.0    steps: 249    lr: 0.0001     reward: 1.38\n",
      "epis: 201   score: 1.0   mem len: 35912   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 202   score: 3.0   mem len: 36138   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.42\n",
      "epis: 203   score: 8.0   mem len: 36601   epsilon: 1.0    steps: 463    lr: 0.0001     reward: 1.49\n",
      "epis: 204   score: 2.0   mem len: 36817   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.49\n",
      "epis: 205   score: 2.0   mem len: 37015   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 206   score: 1.0   mem len: 37186   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.5\n",
      "epis: 207   score: 1.0   mem len: 37355   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.51\n",
      "epis: 208   score: 0.0   mem len: 37477   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 209   score: 0.0   mem len: 37599   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 210   score: 4.0   mem len: 37913   epsilon: 1.0    steps: 314    lr: 0.0001     reward: 1.51\n",
      "epis: 211   score: 3.0   mem len: 38178   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.52\n",
      "epis: 212   score: 1.0   mem len: 38329   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 213   score: 2.0   mem len: 38547   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.51\n",
      "epis: 214   score: 2.0   mem len: 38726   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.52\n",
      "epis: 215   score: 0.0   mem len: 38849   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 216   score: 3.0   mem len: 39076   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.53\n",
      "epis: 217   score: 1.0   mem len: 39245   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 218   score: 0.0   mem len: 39367   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 219   score: 2.0   mem len: 39565   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 220   score: 1.0   mem len: 39736   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.49\n",
      "epis: 221   score: 0.0   mem len: 39858   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 222   score: 1.0   mem len: 40029   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.46\n",
      "epis: 223   score: 1.0   mem len: 40180   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.45\n",
      "epis: 224   score: 0.0   mem len: 40303   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 225   score: 2.0   mem len: 40521   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.4\n",
      "epis: 226   score: 2.0   mem len: 40738   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.42\n",
      "epis: 227   score: 0.0   mem len: 40860   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 228   score: 2.0   mem len: 41077   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.41\n",
      "epis: 229   score: 2.0   mem len: 41274   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.4\n",
      "epis: 230   score: 4.0   mem len: 41534   epsilon: 1.0    steps: 260    lr: 0.0001     reward: 1.43\n",
      "epis: 231   score: 2.0   mem len: 41731   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 232   score: 0.0   mem len: 41853   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 233   score: 0.0   mem len: 41976   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 234   score: 2.0   mem len: 42191   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.45\n",
      "epis: 235   score: 0.0   mem len: 42314   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 236   score: 1.0   mem len: 42486   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.43\n",
      "epis: 237   score: 1.0   mem len: 42655   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 238   score: 0.0   mem len: 42778   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 239   score: 0.0   mem len: 42900   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 240   score: 1.0   mem len: 43051   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 241   score: 0.0   mem len: 43174   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 242   score: 1.0   mem len: 43344   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.39\n",
      "epis: 243   score: 0.0   mem len: 43466   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 244   score: 3.0   mem len: 43713   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.41\n",
      "epis: 245   score: 1.0   mem len: 43883   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.41\n",
      "epis: 246   score: 1.0   mem len: 44051   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.42\n",
      "epis: 247   score: 2.0   mem len: 44249   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.4\n",
      "epis: 248   score: 3.0   mem len: 44496   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 249   score: 1.0   mem len: 44668   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.4\n",
      "epis: 250   score: 0.0   mem len: 44791   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 251   score: 0.0   mem len: 44914   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 252   score: 2.0   mem len: 45112   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 253   score: 0.0   mem len: 45235   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 254   score: 2.0   mem len: 45433   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 255   score: 0.0   mem len: 45556   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 256   score: 1.0   mem len: 45724   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.35\n",
      "epis: 257   score: 0.0   mem len: 45847   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 258   score: 0.0   mem len: 45970   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.3\n",
      "epis: 259   score: 0.0   mem len: 46093   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 260   score: 2.0   mem len: 46291   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.3\n",
      "epis: 261   score: 1.0   mem len: 46463   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.31\n",
      "epis: 262   score: 0.0   mem len: 46586   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 263   score: 1.0   mem len: 46736   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.27\n",
      "epis: 264   score: 4.0   mem len: 47013   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.29\n",
      "epis: 265   score: 1.0   mem len: 47182   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.28\n",
      "epis: 266   score: 4.0   mem len: 47433   epsilon: 1.0    steps: 251    lr: 0.0001     reward: 1.32\n",
      "epis: 267   score: 0.0   mem len: 47556   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 268   score: 3.0   mem len: 47808   epsilon: 1.0    steps: 252    lr: 0.0001     reward: 1.32\n",
      "epis: 269   score: 2.0   mem len: 48025   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.32\n",
      "epis: 270   score: 1.0   mem len: 48175   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.29\n",
      "epis: 271   score: 2.0   mem len: 48372   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.31\n",
      "epis: 272   score: 0.0   mem len: 48494   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 273   score: 0.0   mem len: 48616   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.28\n",
      "epis: 274   score: 0.0   mem len: 48738   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.28\n",
      "epis: 275   score: 2.0   mem len: 48955   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.29\n",
      "epis: 276   score: 1.0   mem len: 49125   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.28\n",
      "epis: 277   score: 0.0   mem len: 49248   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 278   score: 0.0   mem len: 49370   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.27\n",
      "epis: 279   score: 3.0   mem len: 49615   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.29\n",
      "epis: 280   score: 0.0   mem len: 49737   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 281   score: 0.0   mem len: 49860   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 282   score: 3.0   mem len: 50070   epsilon: 1.0    steps: 210    lr: 0.0001     reward: 1.32\n",
      "epis: 283   score: 2.0   mem len: 50285   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.31\n",
      "epis: 284   score: 2.0   mem len: 50482   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.31\n",
      "epis: 285   score: 1.0   mem len: 50654   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.28\n",
      "epis: 286   score: 0.0   mem len: 50777   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 287   score: 1.0   mem len: 50927   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.28\n",
      "epis: 288   score: 2.0   mem len: 51148   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.3\n",
      "epis: 289   score: 0.0   mem len: 51270   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 290   score: 1.0   mem len: 51439   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.29\n",
      "epis: 291   score: 2.0   mem len: 51637   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 292   score: 6.0   mem len: 51976   epsilon: 1.0    steps: 339    lr: 0.0001     reward: 1.34\n",
      "epis: 293   score: 0.0   mem len: 52099   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 294   score: 0.0   mem len: 52221   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 295   score: 2.0   mem len: 52439   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 296   score: 0.0   mem len: 52562   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 297   score: 3.0   mem len: 52790   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.31\n",
      "epis: 298   score: 0.0   mem len: 52912   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 299   score: 1.0   mem len: 53082   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.31\n",
      "epis: 300   score: 0.0   mem len: 53205   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 301   score: 0.0   mem len: 53327   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.27\n",
      "epis: 302   score: 2.0   mem len: 53525   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.26\n",
      "epis: 303   score: 0.0   mem len: 53647   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.18\n",
      "epis: 304   score: 1.0   mem len: 53798   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.17\n",
      "epis: 305   score: 4.0   mem len: 54115   epsilon: 1.0    steps: 317    lr: 0.0001     reward: 1.19\n",
      "epis: 306   score: 5.0   mem len: 54465   epsilon: 1.0    steps: 350    lr: 0.0001     reward: 1.23\n",
      "epis: 307   score: 3.0   mem len: 54730   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.25\n",
      "epis: 308   score: 2.0   mem len: 54914   epsilon: 1.0    steps: 184    lr: 0.0001     reward: 1.27\n",
      "epis: 309   score: 1.0   mem len: 55086   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.28\n",
      "epis: 310   score: 1.0   mem len: 55255   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.25\n",
      "epis: 311   score: 1.0   mem len: 55405   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.23\n",
      "epis: 312   score: 4.0   mem len: 55663   epsilon: 1.0    steps: 258    lr: 0.0001     reward: 1.26\n",
      "epis: 313   score: 1.0   mem len: 55814   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.25\n",
      "epis: 314   score: 1.0   mem len: 55984   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.24\n",
      "epis: 315   score: 2.0   mem len: 56181   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.26\n",
      "epis: 316   score: 5.0   mem len: 56508   epsilon: 1.0    steps: 327    lr: 0.0001     reward: 1.28\n",
      "epis: 317   score: 2.0   mem len: 56724   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.29\n",
      "epis: 318   score: 3.0   mem len: 56971   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.32\n",
      "epis: 319   score: 2.0   mem len: 57169   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.32\n",
      "epis: 320   score: 2.0   mem len: 57367   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 321   score: 4.0   mem len: 57644   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.37\n",
      "epis: 322   score: 2.0   mem len: 57861   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.38\n",
      "epis: 323   score: 1.0   mem len: 58030   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 324   score: 1.0   mem len: 58199   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 325   score: 0.0   mem len: 58322   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 326   score: 3.0   mem len: 58568   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.38\n",
      "epis: 327   score: 2.0   mem len: 58765   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.4\n",
      "epis: 328   score: 2.0   mem len: 58983   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.4\n",
      "epis: 329   score: 1.0   mem len: 59152   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 330   score: 2.0   mem len: 59350   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 331   score: 2.0   mem len: 59568   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 332   score: 2.0   mem len: 59786   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.39\n",
      "epis: 333   score: 3.0   mem len: 60050   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.42\n",
      "epis: 334   score: 3.0   mem len: 60297   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.43\n",
      "epis: 335   score: 0.0   mem len: 60419   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 336   score: 1.0   mem len: 60572   epsilon: 1.0    steps: 153    lr: 0.0001     reward: 1.43\n",
      "epis: 337   score: 2.0   mem len: 60792   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.44\n",
      "epis: 338   score: 0.0   mem len: 60915   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 339   score: 0.0   mem len: 61037   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 340   score: 2.0   mem len: 61235   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 341   score: 1.0   mem len: 61406   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.46\n",
      "epis: 342   score: 1.0   mem len: 61556   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 343   score: 1.0   mem len: 61725   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 344   score: 0.0   mem len: 61848   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 345   score: 0.0   mem len: 61971   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 346   score: 0.0   mem len: 62094   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 347   score: 0.0   mem len: 62217   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 348   score: 4.0   mem len: 62491   epsilon: 1.0    steps: 274    lr: 0.0001     reward: 1.41\n",
      "epis: 349   score: 0.0   mem len: 62614   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 350   score: 2.0   mem len: 62812   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.42\n",
      "epis: 351   score: 2.0   mem len: 63010   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.44\n",
      "epis: 352   score: 2.0   mem len: 63207   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 353   score: 3.0   mem len: 63452   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.47\n",
      "epis: 354   score: 1.0   mem len: 63603   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 355   score: 3.0   mem len: 63853   epsilon: 1.0    steps: 250    lr: 0.0001     reward: 1.49\n",
      "epis: 356   score: 2.0   mem len: 64073   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.5\n",
      "epis: 357   score: 2.0   mem len: 64271   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 358   score: 0.0   mem len: 64394   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 359   score: 0.0   mem len: 64517   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 360   score: 1.0   mem len: 64689   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.51\n",
      "epis: 361   score: 0.0   mem len: 64812   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 362   score: 2.0   mem len: 65029   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.52\n",
      "epis: 363   score: 0.0   mem len: 65152   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 364   score: 1.0   mem len: 65302   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.48\n",
      "epis: 365   score: 1.0   mem len: 65452   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.48\n",
      "epis: 366   score: 1.0   mem len: 65621   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 367   score: 2.0   mem len: 65819   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 368   score: 3.0   mem len: 66062   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.47\n",
      "epis: 369   score: 3.0   mem len: 66293   epsilon: 1.0    steps: 231    lr: 0.0001     reward: 1.48\n",
      "epis: 370   score: 0.0   mem len: 66416   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 371   score: 4.0   mem len: 66711   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.49\n",
      "epis: 372   score: 3.0   mem len: 66937   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.52\n",
      "epis: 373   score: 1.0   mem len: 67088   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 374   score: 4.0   mem len: 67383   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.57\n",
      "epis: 375   score: 1.0   mem len: 67534   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 376   score: 1.0   mem len: 67705   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.56\n",
      "epis: 377   score: 3.0   mem len: 67973   epsilon: 1.0    steps: 268    lr: 0.0001     reward: 1.59\n",
      "epis: 378   score: 2.0   mem len: 68154   epsilon: 1.0    steps: 181    lr: 0.0001     reward: 1.61\n",
      "epis: 379   score: 0.0   mem len: 68276   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 380   score: 1.0   mem len: 68445   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 381   score: 2.0   mem len: 68643   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.61\n",
      "epis: 382   score: 1.0   mem len: 68795   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.59\n",
      "epis: 383   score: 1.0   mem len: 68946   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 384   score: 1.0   mem len: 69118   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 385   score: 1.0   mem len: 69269   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 386   score: 3.0   mem len: 69514   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.6\n",
      "epis: 387   score: 3.0   mem len: 69743   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.62\n",
      "epis: 388   score: 0.0   mem len: 69866   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 389   score: 1.0   mem len: 70036   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.61\n",
      "epis: 390   score: 0.0   mem len: 70159   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 391   score: 0.0   mem len: 70282   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 392   score: 1.0   mem len: 70432   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.53\n",
      "epis: 393   score: 1.0   mem len: 70601   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 394   score: 1.0   mem len: 70751   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.55\n",
      "epis: 395   score: 2.0   mem len: 70969   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.55\n",
      "epis: 396   score: 9.0   mem len: 71465   epsilon: 1.0    steps: 496    lr: 0.0001     reward: 1.64\n",
      "epis: 397   score: 3.0   mem len: 71709   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.64\n",
      "epis: 398   score: 3.0   mem len: 71935   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.67\n",
      "epis: 399   score: 0.0   mem len: 72058   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 400   score: 2.0   mem len: 72280   epsilon: 1.0    steps: 222    lr: 0.0001     reward: 1.68\n",
      "epis: 401   score: 0.0   mem len: 72403   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 402   score: 2.0   mem len: 72600   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.68\n",
      "epis: 403   score: 3.0   mem len: 72847   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.71\n",
      "epis: 404   score: 3.0   mem len: 73093   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.73\n",
      "epis: 405   score: 0.0   mem len: 73215   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.69\n",
      "epis: 406   score: 4.0   mem len: 73495   epsilon: 1.0    steps: 280    lr: 0.0001     reward: 1.68\n",
      "epis: 407   score: 0.0   mem len: 73617   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.65\n",
      "epis: 408   score: 4.0   mem len: 73893   epsilon: 1.0    steps: 276    lr: 0.0001     reward: 1.67\n",
      "epis: 409   score: 2.0   mem len: 74094   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.68\n",
      "epis: 410   score: 1.0   mem len: 74264   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.68\n",
      "epis: 411   score: 4.0   mem len: 74563   epsilon: 1.0    steps: 299    lr: 0.0001     reward: 1.71\n",
      "epis: 412   score: 1.0   mem len: 74713   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 413   score: 2.0   mem len: 74915   epsilon: 1.0    steps: 202    lr: 0.0001     reward: 1.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 414   score: 2.0   mem len: 75112   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 415   score: 1.0   mem len: 75281   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.69\n",
      "epis: 416   score: 2.0   mem len: 75479   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.66\n",
      "epis: 417   score: 3.0   mem len: 75728   epsilon: 1.0    steps: 249    lr: 0.0001     reward: 1.67\n",
      "epis: 418   score: 0.0   mem len: 75850   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 419   score: 0.0   mem len: 75973   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 420   score: 2.0   mem len: 76171   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 421   score: 0.0   mem len: 76294   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 422   score: 0.0   mem len: 76416   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 423   score: 3.0   mem len: 76642   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.58\n",
      "epis: 424   score: 1.0   mem len: 76793   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 425   score: 0.0   mem len: 76916   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 426   score: 1.0   mem len: 77086   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.56\n",
      "epis: 427   score: 5.0   mem len: 77424   epsilon: 1.0    steps: 338    lr: 0.0001     reward: 1.59\n",
      "epis: 428   score: 3.0   mem len: 77689   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.6\n",
      "epis: 429   score: 3.0   mem len: 77934   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.62\n",
      "epis: 430   score: 0.0   mem len: 78057   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 431   score: 2.0   mem len: 78273   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.6\n",
      "epis: 432   score: 1.0   mem len: 78424   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 433   score: 2.0   mem len: 78643   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.58\n",
      "epis: 434   score: 0.0   mem len: 78765   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 435   score: 1.0   mem len: 78934   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 436   score: 3.0   mem len: 79182   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.58\n",
      "epis: 437   score: 4.0   mem len: 79469   epsilon: 1.0    steps: 287    lr: 0.0001     reward: 1.6\n",
      "epis: 438   score: 0.0   mem len: 79592   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 439   score: 2.0   mem len: 79790   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 440   score: 0.0   mem len: 79912   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 441   score: 3.0   mem len: 80141   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.62\n",
      "epis: 442   score: 2.0   mem len: 80359   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.63\n",
      "epis: 443   score: 1.0   mem len: 80531   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.63\n",
      "epis: 444   score: 0.0   mem len: 80653   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 445   score: 0.0   mem len: 80776   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 446   score: 0.0   mem len: 80899   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 447   score: 2.0   mem len: 81117   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.65\n",
      "epis: 448   score: 2.0   mem len: 81334   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.63\n",
      "epis: 449   score: 1.0   mem len: 81504   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.64\n",
      "epis: 450   score: 2.0   mem len: 81724   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.64\n",
      "epis: 451   score: 0.0   mem len: 81847   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 452   score: 0.0   mem len: 81970   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 453   score: 2.0   mem len: 82187   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.59\n",
      "epis: 454   score: 1.0   mem len: 82358   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.59\n",
      "epis: 455   score: 1.0   mem len: 82508   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.57\n",
      "epis: 456   score: 2.0   mem len: 82729   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.57\n",
      "epis: 457   score: 3.0   mem len: 82973   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.58\n",
      "epis: 458   score: 0.0   mem len: 83096   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 459   score: 3.0   mem len: 83358   epsilon: 1.0    steps: 262    lr: 0.0001     reward: 1.61\n",
      "epis: 460   score: 3.0   mem len: 83602   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.63\n",
      "epis: 461   score: 1.0   mem len: 83772   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.64\n",
      "epis: 462   score: 2.0   mem len: 83988   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.64\n",
      "epis: 463   score: 0.0   mem len: 84111   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 464   score: 5.0   mem len: 84413   epsilon: 1.0    steps: 302    lr: 0.0001     reward: 1.68\n",
      "epis: 465   score: 0.0   mem len: 84536   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 466   score: 1.0   mem len: 84706   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.67\n",
      "epis: 467   score: 2.0   mem len: 84888   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.67\n",
      "epis: 468   score: 0.0   mem len: 85010   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 469   score: 3.0   mem len: 85276   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.64\n",
      "epis: 470   score: 1.0   mem len: 85447   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.65\n",
      "epis: 471   score: 3.0   mem len: 85673   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.64\n",
      "epis: 472   score: 1.0   mem len: 85844   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.62\n",
      "epis: 473   score: 1.0   mem len: 86012   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.62\n",
      "epis: 474   score: 0.0   mem len: 86135   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 475   score: 2.0   mem len: 86333   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 476   score: 1.0   mem len: 86502   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 477   score: 1.0   mem len: 86673   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.57\n",
      "epis: 478   score: 2.0   mem len: 86871   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.57\n",
      "epis: 479   score: 4.0   mem len: 87132   epsilon: 1.0    steps: 261    lr: 0.0001     reward: 1.61\n",
      "epis: 480   score: 0.0   mem len: 87255   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 481   score: 2.0   mem len: 87452   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.6\n",
      "epis: 482   score: 0.0   mem len: 87575   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 483   score: 2.0   mem len: 87793   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.6\n",
      "epis: 484   score: 6.0   mem len: 88153   epsilon: 1.0    steps: 360    lr: 0.0001     reward: 1.65\n",
      "epis: 485   score: 3.0   mem len: 88417   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.67\n",
      "epis: 486   score: 0.0   mem len: 88540   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 487   score: 3.0   mem len: 88807   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.64\n",
      "epis: 488   score: 3.0   mem len: 89016   epsilon: 1.0    steps: 209    lr: 0.0001     reward: 1.67\n",
      "epis: 489   score: 0.0   mem len: 89139   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 490   score: 0.0   mem len: 89262   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 491   score: 1.0   mem len: 89432   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.67\n",
      "epis: 492   score: 2.0   mem len: 89630   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.68\n",
      "epis: 493   score: 1.0   mem len: 89800   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.68\n",
      "epis: 494   score: 1.0   mem len: 89951   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.68\n",
      "epis: 495   score: 4.0   mem len: 90224   epsilon: 1.0    steps: 273    lr: 0.0001     reward: 1.7\n",
      "epis: 496   score: 2.0   mem len: 90408   epsilon: 1.0    steps: 184    lr: 0.0001     reward: 1.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 497   score: 3.0   mem len: 90654   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.63\n",
      "epis: 498   score: 1.0   mem len: 90806   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.61\n",
      "epis: 499   score: 2.0   mem len: 91004   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.63\n",
      "epis: 500   score: 5.0   mem len: 91311   epsilon: 1.0    steps: 307    lr: 0.0001     reward: 1.66\n",
      "epis: 501   score: 0.0   mem len: 91434   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 502   score: 2.0   mem len: 91653   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.66\n",
      "epis: 503   score: 3.0   mem len: 91878   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.66\n",
      "epis: 504   score: 1.0   mem len: 92046   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.64\n",
      "epis: 505   score: 2.0   mem len: 92244   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.66\n",
      "epis: 506   score: 0.0   mem len: 92367   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 507   score: 0.0   mem len: 92489   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 508   score: 2.0   mem len: 92687   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 509   score: 5.0   mem len: 93030   epsilon: 1.0    steps: 343    lr: 0.0001     reward: 1.63\n",
      "epis: 510   score: 2.0   mem len: 93228   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.64\n",
      "epis: 511   score: 1.0   mem len: 93397   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 512   score: 3.0   mem len: 93644   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.63\n",
      "epis: 513   score: 9.0   mem len: 94031   epsilon: 1.0    steps: 387    lr: 0.0001     reward: 1.7\n",
      "epis: 514   score: 3.0   mem len: 94279   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.71\n",
      "epis: 515   score: 1.0   mem len: 94448   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.71\n",
      "epis: 516   score: 2.0   mem len: 94646   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 517   score: 1.0   mem len: 94818   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.69\n",
      "epis: 518   score: 1.0   mem len: 94988   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.7\n",
      "epis: 519   score: 4.0   mem len: 95282   epsilon: 1.0    steps: 294    lr: 0.0001     reward: 1.74\n",
      "epis: 520   score: 0.0   mem len: 95404   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.72\n",
      "epis: 521   score: 3.0   mem len: 95669   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.75\n",
      "epis: 522   score: 0.0   mem len: 95791   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.75\n",
      "epis: 523   score: 0.0   mem len: 95913   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.72\n",
      "epis: 524   score: 0.0   mem len: 96035   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.71\n",
      "epis: 525   score: 6.0   mem len: 96358   epsilon: 1.0    steps: 323    lr: 0.0001     reward: 1.77\n",
      "epis: 526   score: 4.0   mem len: 96655   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.8\n",
      "epis: 527   score: 3.0   mem len: 96882   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.78\n",
      "epis: 528   score: 5.0   mem len: 97197   epsilon: 1.0    steps: 315    lr: 0.0001     reward: 1.8\n",
      "epis: 529   score: 3.0   mem len: 97468   epsilon: 1.0    steps: 271    lr: 0.0001     reward: 1.8\n",
      "epis: 530   score: 1.0   mem len: 97639   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.81\n",
      "epis: 531   score: 3.0   mem len: 97865   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.82\n",
      "epis: 532   score: 2.0   mem len: 98065   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.83\n",
      "epis: 533   score: 1.0   mem len: 98235   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.82\n",
      "epis: 534   score: 0.0   mem len: 98358   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 535   score: 2.0   mem len: 98573   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.83\n",
      "epis: 536   score: 1.0   mem len: 98744   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.81\n",
      "epis: 537   score: 1.0   mem len: 98894   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.78\n",
      "epis: 538   score: 5.0   mem len: 99218   epsilon: 1.0    steps: 324    lr: 0.0001     reward: 1.83\n",
      "epis: 539   score: 0.0   mem len: 99341   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.81\n",
      "epis: 540   score: 0.0   mem len: 99464   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.81\n",
      "epis: 541   score: 1.0   mem len: 99615   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.79\n",
      "epis: 542   score: 2.0   mem len: 99833   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/memory.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array(sample)\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent.py:91: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  next_state_values[mask] = self.policy_net(non_final_next_states).max(1)[0].cuda()[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 543   score: 2.0   mem len: 100031   epsilon: 0.9999    steps: 198    lr: 0.0001     reward: 1.8\n",
      "epis: 544   score: 1.0   mem len: 100202   epsilon: 0.9996    steps: 171    lr: 0.0001     reward: 1.81\n",
      "epis: 545   score: 0.0   mem len: 100325   epsilon: 0.9994    steps: 123    lr: 0.0001     reward: 1.81\n",
      "epis: 546   score: 1.0   mem len: 100475   epsilon: 0.9991    steps: 150    lr: 0.0001     reward: 1.82\n",
      "epis: 547   score: 1.0   mem len: 100625   epsilon: 0.9988    steps: 150    lr: 0.0001     reward: 1.81\n",
      "epis: 548   score: 2.0   mem len: 100824   epsilon: 0.9984    steps: 199    lr: 0.0001     reward: 1.81\n",
      "epis: 549   score: 0.0   mem len: 100947   epsilon: 0.9981    steps: 123    lr: 0.0001     reward: 1.8\n",
      "epis: 550   score: 1.0   mem len: 101116   epsilon: 0.9978    steps: 169    lr: 0.0001     reward: 1.79\n",
      "epis: 551   score: 1.0   mem len: 101266   epsilon: 0.9975    steps: 150    lr: 0.0001     reward: 1.8\n",
      "epis: 552   score: 4.0   mem len: 101564   epsilon: 0.9969    steps: 298    lr: 0.0001     reward: 1.84\n",
      "epis: 553   score: 2.0   mem len: 101762   epsilon: 0.9965    steps: 198    lr: 0.0001     reward: 1.84\n",
      "epis: 554   score: 0.0   mem len: 101885   epsilon: 0.9963    steps: 123    lr: 0.0001     reward: 1.83\n",
      "epis: 555   score: 0.0   mem len: 102008   epsilon: 0.996    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 556   score: 5.0   mem len: 102334   epsilon: 0.9954    steps: 326    lr: 0.0001     reward: 1.85\n",
      "epis: 557   score: 2.0   mem len: 102532   epsilon: 0.995    steps: 198    lr: 0.0001     reward: 1.84\n",
      "epis: 558   score: 0.0   mem len: 102654   epsilon: 0.9947    steps: 122    lr: 0.0001     reward: 1.84\n",
      "epis: 559   score: 2.0   mem len: 102852   epsilon: 0.9944    steps: 198    lr: 0.0001     reward: 1.83\n",
      "epis: 560   score: 2.0   mem len: 103073   epsilon: 0.9939    steps: 221    lr: 0.0001     reward: 1.82\n",
      "epis: 561   score: 1.0   mem len: 103242   epsilon: 0.9936    steps: 169    lr: 0.0001     reward: 1.82\n",
      "epis: 562   score: 6.0   mem len: 103599   epsilon: 0.9929    steps: 357    lr: 0.0001     reward: 1.86\n",
      "epis: 563   score: 1.0   mem len: 103767   epsilon: 0.9925    steps: 168    lr: 0.0001     reward: 1.87\n",
      "epis: 564   score: 0.0   mem len: 103890   epsilon: 0.9923    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 565   score: 3.0   mem len: 104134   epsilon: 0.9918    steps: 244    lr: 0.0001     reward: 1.85\n",
      "epis: 566   score: 2.0   mem len: 104353   epsilon: 0.9914    steps: 219    lr: 0.0001     reward: 1.86\n",
      "epis: 567   score: 2.0   mem len: 104551   epsilon: 0.991    steps: 198    lr: 0.0001     reward: 1.86\n",
      "epis: 568   score: 0.0   mem len: 104674   epsilon: 0.9907    steps: 123    lr: 0.0001     reward: 1.86\n",
      "epis: 569   score: 1.0   mem len: 104824   epsilon: 0.9904    steps: 150    lr: 0.0001     reward: 1.84\n",
      "epis: 570   score: 0.0   mem len: 104946   epsilon: 0.9902    steps: 122    lr: 0.0001     reward: 1.83\n",
      "epis: 571   score: 3.0   mem len: 105193   epsilon: 0.9897    steps: 247    lr: 0.0001     reward: 1.83\n",
      "epis: 572   score: 0.0   mem len: 105316   epsilon: 0.9895    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 573   score: 1.0   mem len: 105486   epsilon: 0.9891    steps: 170    lr: 0.0001     reward: 1.82\n",
      "epis: 574   score: 0.0   mem len: 105608   epsilon: 0.9889    steps: 122    lr: 0.0001     reward: 1.82\n",
      "epis: 575   score: 4.0   mem len: 105889   epsilon: 0.9883    steps: 281    lr: 0.0001     reward: 1.84\n",
      "epis: 576   score: 3.0   mem len: 106138   epsilon: 0.9878    steps: 249    lr: 0.0001     reward: 1.86\n",
      "epis: 577   score: 3.0   mem len: 106366   epsilon: 0.9874    steps: 228    lr: 0.0001     reward: 1.88\n",
      "epis: 578   score: 1.0   mem len: 106517   epsilon: 0.9871    steps: 151    lr: 0.0001     reward: 1.87\n",
      "epis: 579   score: 1.0   mem len: 106668   epsilon: 0.9868    steps: 151    lr: 0.0001     reward: 1.84\n",
      "epis: 580   score: 2.0   mem len: 106850   epsilon: 0.9864    steps: 182    lr: 0.0001     reward: 1.86\n",
      "epis: 581   score: 1.0   mem len: 107021   epsilon: 0.9861    steps: 171    lr: 0.0001     reward: 1.85\n",
      "epis: 582   score: 1.0   mem len: 107172   epsilon: 0.9858    steps: 151    lr: 0.0001     reward: 1.86\n",
      "epis: 583   score: 0.0   mem len: 107295   epsilon: 0.9856    steps: 123    lr: 0.0001     reward: 1.84\n",
      "epis: 584   score: 2.0   mem len: 107493   epsilon: 0.9852    steps: 198    lr: 0.0001     reward: 1.8\n",
      "epis: 585   score: 3.0   mem len: 107718   epsilon: 0.9847    steps: 225    lr: 0.0001     reward: 1.8\n",
      "epis: 586   score: 0.0   mem len: 107840   epsilon: 0.9845    steps: 122    lr: 0.0001     reward: 1.8\n",
      "epis: 587   score: 3.0   mem len: 108105   epsilon: 0.984    steps: 265    lr: 0.0001     reward: 1.8\n",
      "epis: 588   score: 1.0   mem len: 108256   epsilon: 0.9837    steps: 151    lr: 0.0001     reward: 1.78\n",
      "epis: 589   score: 1.0   mem len: 108406   epsilon: 0.9834    steps: 150    lr: 0.0001     reward: 1.79\n",
      "epis: 590   score: 2.0   mem len: 108603   epsilon: 0.983    steps: 197    lr: 0.0001     reward: 1.81\n",
      "epis: 591   score: 2.0   mem len: 108801   epsilon: 0.9826    steps: 198    lr: 0.0001     reward: 1.82\n",
      "epis: 592   score: 1.0   mem len: 108953   epsilon: 0.9823    steps: 152    lr: 0.0001     reward: 1.81\n",
      "epis: 593   score: 3.0   mem len: 109202   epsilon: 0.9818    steps: 249    lr: 0.0001     reward: 1.83\n",
      "epis: 594   score: 0.0   mem len: 109325   epsilon: 0.9815    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 595   score: 1.0   mem len: 109476   epsilon: 0.9812    steps: 151    lr: 0.0001     reward: 1.79\n",
      "epis: 596   score: 1.0   mem len: 109645   epsilon: 0.9809    steps: 169    lr: 0.0001     reward: 1.78\n",
      "epis: 597   score: 1.0   mem len: 109814   epsilon: 0.9806    steps: 169    lr: 0.0001     reward: 1.76\n",
      "epis: 598   score: 2.0   mem len: 110012   epsilon: 0.9802    steps: 198    lr: 0.0001     reward: 1.77\n",
      "epis: 599   score: 1.0   mem len: 110184   epsilon: 0.9798    steps: 172    lr: 0.0001     reward: 1.76\n",
      "epis: 600   score: 2.0   mem len: 110382   epsilon: 0.9794    steps: 198    lr: 0.0001     reward: 1.73\n",
      "epis: 601   score: 4.0   mem len: 110645   epsilon: 0.9789    steps: 263    lr: 0.0001     reward: 1.77\n",
      "epis: 602   score: 0.0   mem len: 110767   epsilon: 0.9787    steps: 122    lr: 0.0001     reward: 1.75\n",
      "epis: 603   score: 1.0   mem len: 110936   epsilon: 0.9783    steps: 169    lr: 0.0001     reward: 1.73\n",
      "epis: 604   score: 3.0   mem len: 111164   epsilon: 0.9779    steps: 228    lr: 0.0001     reward: 1.75\n",
      "epis: 605   score: 0.0   mem len: 111286   epsilon: 0.9777    steps: 122    lr: 0.0001     reward: 1.73\n",
      "epis: 606   score: 1.0   mem len: 111457   epsilon: 0.9773    steps: 171    lr: 0.0001     reward: 1.74\n",
      "epis: 607   score: 1.0   mem len: 111628   epsilon: 0.977    steps: 171    lr: 0.0001     reward: 1.75\n",
      "epis: 608   score: 4.0   mem len: 111923   epsilon: 0.9764    steps: 295    lr: 0.0001     reward: 1.77\n",
      "epis: 609   score: 2.0   mem len: 112139   epsilon: 0.976    steps: 216    lr: 0.0001     reward: 1.74\n",
      "epis: 610   score: 1.0   mem len: 112308   epsilon: 0.9756    steps: 169    lr: 0.0001     reward: 1.73\n",
      "epis: 611   score: 1.0   mem len: 112479   epsilon: 0.9753    steps: 171    lr: 0.0001     reward: 1.73\n",
      "epis: 612   score: 5.0   mem len: 112807   epsilon: 0.9746    steps: 328    lr: 0.0001     reward: 1.75\n",
      "epis: 613   score: 1.0   mem len: 112979   epsilon: 0.9743    steps: 172    lr: 0.0001     reward: 1.67\n",
      "epis: 614   score: 3.0   mem len: 113251   epsilon: 0.9738    steps: 272    lr: 0.0001     reward: 1.67\n",
      "epis: 615   score: 2.0   mem len: 113451   epsilon: 0.9734    steps: 200    lr: 0.0001     reward: 1.68\n",
      "epis: 616   score: 2.0   mem len: 113667   epsilon: 0.9729    steps: 216    lr: 0.0001     reward: 1.68\n",
      "epis: 617   score: 1.0   mem len: 113835   epsilon: 0.9726    steps: 168    lr: 0.0001     reward: 1.68\n",
      "epis: 618   score: 0.0   mem len: 113958   epsilon: 0.9724    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 619   score: 0.0   mem len: 114080   epsilon: 0.9721    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 620   score: 2.0   mem len: 114298   epsilon: 0.9717    steps: 218    lr: 0.0001     reward: 1.65\n",
      "epis: 621   score: 3.0   mem len: 114545   epsilon: 0.9712    steps: 247    lr: 0.0001     reward: 1.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 622   score: 0.0   mem len: 114667   epsilon: 0.971    steps: 122    lr: 0.0001     reward: 1.65\n",
      "epis: 623   score: 1.0   mem len: 114818   epsilon: 0.9707    steps: 151    lr: 0.0001     reward: 1.66\n",
      "epis: 624   score: 4.0   mem len: 115112   epsilon: 0.9701    steps: 294    lr: 0.0001     reward: 1.7\n",
      "epis: 625   score: 2.0   mem len: 115294   epsilon: 0.9697    steps: 182    lr: 0.0001     reward: 1.66\n",
      "epis: 626   score: 2.0   mem len: 115476   epsilon: 0.9694    steps: 182    lr: 0.0001     reward: 1.64\n",
      "epis: 627   score: 0.0   mem len: 115599   epsilon: 0.9691    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 628   score: 2.0   mem len: 115799   epsilon: 0.9687    steps: 200    lr: 0.0001     reward: 1.58\n",
      "epis: 629   score: 2.0   mem len: 115979   epsilon: 0.9684    steps: 180    lr: 0.0001     reward: 1.57\n",
      "epis: 630   score: 1.0   mem len: 116151   epsilon: 0.968    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 631   score: 0.0   mem len: 116273   epsilon: 0.9678    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 632   score: 1.0   mem len: 116443   epsilon: 0.9674    steps: 170    lr: 0.0001     reward: 1.53\n",
      "epis: 633   score: 5.0   mem len: 116733   epsilon: 0.9669    steps: 290    lr: 0.0001     reward: 1.57\n",
      "epis: 634   score: 0.0   mem len: 116856   epsilon: 0.9666    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 635   score: 2.0   mem len: 117053   epsilon: 0.9662    steps: 197    lr: 0.0001     reward: 1.57\n",
      "epis: 636   score: 3.0   mem len: 117282   epsilon: 0.9658    steps: 229    lr: 0.0001     reward: 1.59\n",
      "epis: 637   score: 3.0   mem len: 117507   epsilon: 0.9653    steps: 225    lr: 0.0001     reward: 1.61\n",
      "epis: 638   score: 7.0   mem len: 117917   epsilon: 0.9645    steps: 410    lr: 0.0001     reward: 1.63\n",
      "epis: 639   score: 2.0   mem len: 118133   epsilon: 0.9641    steps: 216    lr: 0.0001     reward: 1.65\n",
      "epis: 640   score: 1.0   mem len: 118302   epsilon: 0.9638    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 641   score: 2.0   mem len: 118500   epsilon: 0.9634    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 642   score: 4.0   mem len: 118795   epsilon: 0.9628    steps: 295    lr: 0.0001     reward: 1.69\n",
      "epis: 643   score: 2.0   mem len: 118993   epsilon: 0.9624    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 644   score: 1.0   mem len: 119144   epsilon: 0.9621    steps: 151    lr: 0.0001     reward: 1.69\n",
      "epis: 645   score: 0.0   mem len: 119267   epsilon: 0.9618    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 646   score: 0.0   mem len: 119390   epsilon: 0.9616    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 647   score: 1.0   mem len: 119558   epsilon: 0.9613    steps: 168    lr: 0.0001     reward: 1.68\n",
      "epis: 648   score: 0.0   mem len: 119680   epsilon: 0.961    steps: 122    lr: 0.0001     reward: 1.66\n",
      "epis: 649   score: 0.0   mem len: 119803   epsilon: 0.9608    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 650   score: 1.0   mem len: 119972   epsilon: 0.9605    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 651   score: 1.0   mem len: 120141   epsilon: 0.9601    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 652   score: 1.0   mem len: 120311   epsilon: 0.9598    steps: 170    lr: 0.0001     reward: 1.63\n",
      "epis: 653   score: 1.0   mem len: 120462   epsilon: 0.9595    steps: 151    lr: 0.0001     reward: 1.62\n",
      "epis: 654   score: 4.0   mem len: 120759   epsilon: 0.9589    steps: 297    lr: 0.0001     reward: 1.66\n",
      "epis: 655   score: 1.0   mem len: 120930   epsilon: 0.9586    steps: 171    lr: 0.0001     reward: 1.67\n",
      "epis: 656   score: 6.0   mem len: 121270   epsilon: 0.9579    steps: 340    lr: 0.0001     reward: 1.68\n",
      "epis: 657   score: 1.0   mem len: 121439   epsilon: 0.9575    steps: 169    lr: 0.0001     reward: 1.67\n",
      "epis: 658   score: 1.0   mem len: 121589   epsilon: 0.9573    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 659   score: 1.0   mem len: 121758   epsilon: 0.9569    steps: 169    lr: 0.0001     reward: 1.67\n",
      "epis: 660   score: 2.0   mem len: 121956   epsilon: 0.9565    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 661   score: 1.0   mem len: 122126   epsilon: 0.9562    steps: 170    lr: 0.0001     reward: 1.67\n",
      "epis: 662   score: 2.0   mem len: 122344   epsilon: 0.9558    steps: 218    lr: 0.0001     reward: 1.63\n",
      "epis: 663   score: 0.0   mem len: 122466   epsilon: 0.9555    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 664   score: 0.0   mem len: 122589   epsilon: 0.9553    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 665   score: 0.0   mem len: 122712   epsilon: 0.955    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 666   score: 1.0   mem len: 122881   epsilon: 0.9547    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 667   score: 2.0   mem len: 123082   epsilon: 0.9543    steps: 201    lr: 0.0001     reward: 1.58\n",
      "epis: 668   score: 1.0   mem len: 123251   epsilon: 0.954    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 669   score: 4.0   mem len: 123547   epsilon: 0.9534    steps: 296    lr: 0.0001     reward: 1.62\n",
      "epis: 670   score: 0.0   mem len: 123670   epsilon: 0.9531    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 671   score: 0.0   mem len: 123793   epsilon: 0.9529    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 672   score: 0.0   mem len: 123916   epsilon: 0.9526    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 673   score: 2.0   mem len: 124134   epsilon: 0.9522    steps: 218    lr: 0.0001     reward: 1.6\n",
      "epis: 674   score: 2.0   mem len: 124352   epsilon: 0.9518    steps: 218    lr: 0.0001     reward: 1.62\n",
      "epis: 675   score: 2.0   mem len: 124550   epsilon: 0.9514    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 676   score: 2.0   mem len: 124768   epsilon: 0.951    steps: 218    lr: 0.0001     reward: 1.59\n",
      "epis: 677   score: 0.0   mem len: 124891   epsilon: 0.9507    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 678   score: 3.0   mem len: 125116   epsilon: 0.9503    steps: 225    lr: 0.0001     reward: 1.58\n",
      "epis: 679   score: 1.0   mem len: 125266   epsilon: 0.95    steps: 150    lr: 0.0001     reward: 1.58\n",
      "epis: 680   score: 2.0   mem len: 125464   epsilon: 0.9496    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 681   score: 0.0   mem len: 125586   epsilon: 0.9493    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 682   score: 1.0   mem len: 125756   epsilon: 0.949    steps: 170    lr: 0.0001     reward: 1.57\n",
      "epis: 683   score: 1.0   mem len: 125928   epsilon: 0.9487    steps: 172    lr: 0.0001     reward: 1.58\n",
      "epis: 684   score: 2.0   mem len: 126129   epsilon: 0.9483    steps: 201    lr: 0.0001     reward: 1.58\n",
      "epis: 685   score: 0.0   mem len: 126252   epsilon: 0.948    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 686   score: 1.0   mem len: 126420   epsilon: 0.9477    steps: 168    lr: 0.0001     reward: 1.56\n",
      "epis: 687   score: 3.0   mem len: 126686   epsilon: 0.9472    steps: 266    lr: 0.0001     reward: 1.56\n",
      "epis: 688   score: 1.0   mem len: 126837   epsilon: 0.9469    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 689   score: 0.0   mem len: 126959   epsilon: 0.9466    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 690   score: 2.0   mem len: 127156   epsilon: 0.9462    steps: 197    lr: 0.0001     reward: 1.55\n",
      "epis: 691   score: 2.0   mem len: 127356   epsilon: 0.9458    steps: 200    lr: 0.0001     reward: 1.55\n",
      "epis: 692   score: 1.0   mem len: 127507   epsilon: 0.9455    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 693   score: 0.0   mem len: 127629   epsilon: 0.9453    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 694   score: 2.0   mem len: 127828   epsilon: 0.9449    steps: 199    lr: 0.0001     reward: 1.54\n",
      "epis: 695   score: 1.0   mem len: 127997   epsilon: 0.9446    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 696   score: 1.0   mem len: 128148   epsilon: 0.9443    steps: 151    lr: 0.0001     reward: 1.54\n",
      "epis: 697   score: 0.0   mem len: 128270   epsilon: 0.944    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 698   score: 1.0   mem len: 128441   epsilon: 0.9437    steps: 171    lr: 0.0001     reward: 1.52\n",
      "epis: 699   score: 0.0   mem len: 128564   epsilon: 0.9434    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 700   score: 0.0   mem len: 128687   epsilon: 0.9432    steps: 123    lr: 0.0001     reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 701   score: 2.0   mem len: 128905   epsilon: 0.9428    steps: 218    lr: 0.0001     reward: 1.47\n",
      "epis: 702   score: 2.0   mem len: 129103   epsilon: 0.9424    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 703   score: 0.0   mem len: 129225   epsilon: 0.9421    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 704   score: 2.0   mem len: 129423   epsilon: 0.9417    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 705   score: 2.0   mem len: 129639   epsilon: 0.9413    steps: 216    lr: 0.0001     reward: 1.49\n",
      "epis: 706   score: 3.0   mem len: 129868   epsilon: 0.9409    steps: 229    lr: 0.0001     reward: 1.51\n",
      "epis: 707   score: 0.0   mem len: 129991   epsilon: 0.9406    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 708   score: 2.0   mem len: 130189   epsilon: 0.9402    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 709   score: 1.0   mem len: 130340   epsilon: 0.9399    steps: 151    lr: 0.0001     reward: 1.47\n",
      "epis: 710   score: 1.0   mem len: 130509   epsilon: 0.9396    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 711   score: 1.0   mem len: 130659   epsilon: 0.9393    steps: 150    lr: 0.0001     reward: 1.47\n",
      "epis: 712   score: 2.0   mem len: 130877   epsilon: 0.9389    steps: 218    lr: 0.0001     reward: 1.44\n",
      "epis: 713   score: 0.0   mem len: 131000   epsilon: 0.9386    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 714   score: 3.0   mem len: 131225   epsilon: 0.9382    steps: 225    lr: 0.0001     reward: 1.43\n",
      "epis: 715   score: 2.0   mem len: 131443   epsilon: 0.9377    steps: 218    lr: 0.0001     reward: 1.43\n",
      "epis: 716   score: 1.0   mem len: 131614   epsilon: 0.9374    steps: 171    lr: 0.0001     reward: 1.42\n",
      "epis: 717   score: 1.0   mem len: 131783   epsilon: 0.9371    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 718   score: 0.0   mem len: 131905   epsilon: 0.9368    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 719   score: 0.0   mem len: 132028   epsilon: 0.9366    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 720   score: 3.0   mem len: 132258   epsilon: 0.9361    steps: 230    lr: 0.0001     reward: 1.43\n",
      "epis: 721   score: 5.0   mem len: 132582   epsilon: 0.9355    steps: 324    lr: 0.0001     reward: 1.45\n",
      "epis: 722   score: 1.0   mem len: 132751   epsilon: 0.9352    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 723   score: 2.0   mem len: 132949   epsilon: 0.9348    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 724   score: 2.0   mem len: 133146   epsilon: 0.9344    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 725   score: 0.0   mem len: 133269   epsilon: 0.9341    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 726   score: 0.0   mem len: 133392   epsilon: 0.9339    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 727   score: 2.0   mem len: 133572   epsilon: 0.9335    steps: 180    lr: 0.0001     reward: 1.43\n",
      "epis: 728   score: 0.0   mem len: 133695   epsilon: 0.9333    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 729   score: 0.0   mem len: 133817   epsilon: 0.933    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 730   score: 1.0   mem len: 133986   epsilon: 0.9327    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 731   score: 5.0   mem len: 134277   epsilon: 0.9321    steps: 291    lr: 0.0001     reward: 1.44\n",
      "epis: 732   score: 2.0   mem len: 134463   epsilon: 0.9318    steps: 186    lr: 0.0001     reward: 1.45\n",
      "epis: 733   score: 0.0   mem len: 134586   epsilon: 0.9315    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 734   score: 0.0   mem len: 134709   epsilon: 0.9313    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 735   score: 1.0   mem len: 134878   epsilon: 0.9309    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 736   score: 1.0   mem len: 135049   epsilon: 0.9306    steps: 171    lr: 0.0001     reward: 1.37\n",
      "epis: 737   score: 0.0   mem len: 135171   epsilon: 0.9304    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 738   score: 2.0   mem len: 135369   epsilon: 0.93    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 739   score: 0.0   mem len: 135492   epsilon: 0.9297    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 740   score: 0.0   mem len: 135614   epsilon: 0.9295    steps: 122    lr: 0.0001     reward: 1.26\n",
      "epis: 741   score: 0.0   mem len: 135737   epsilon: 0.9292    steps: 123    lr: 0.0001     reward: 1.24\n",
      "epis: 742   score: 0.0   mem len: 135860   epsilon: 0.929    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 743   score: 2.0   mem len: 136077   epsilon: 0.9286    steps: 217    lr: 0.0001     reward: 1.2\n",
      "epis: 744   score: 1.0   mem len: 136248   epsilon: 0.9282    steps: 171    lr: 0.0001     reward: 1.2\n",
      "epis: 745   score: 0.0   mem len: 136371   epsilon: 0.928    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 746   score: 0.0   mem len: 136494   epsilon: 0.9277    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 747   score: 0.0   mem len: 136617   epsilon: 0.9275    steps: 123    lr: 0.0001     reward: 1.19\n",
      "epis: 748   score: 1.0   mem len: 136786   epsilon: 0.9272    steps: 169    lr: 0.0001     reward: 1.2\n",
      "epis: 749   score: 1.0   mem len: 136954   epsilon: 0.9268    steps: 168    lr: 0.0001     reward: 1.21\n",
      "epis: 750   score: 2.0   mem len: 137153   epsilon: 0.9264    steps: 199    lr: 0.0001     reward: 1.22\n",
      "epis: 751   score: 0.0   mem len: 137276   epsilon: 0.9262    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 752   score: 0.0   mem len: 137399   epsilon: 0.9259    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 753   score: 0.0   mem len: 137522   epsilon: 0.9257    steps: 123    lr: 0.0001     reward: 1.19\n",
      "epis: 754   score: 1.0   mem len: 137693   epsilon: 0.9254    steps: 171    lr: 0.0001     reward: 1.16\n",
      "epis: 755   score: 0.0   mem len: 137816   epsilon: 0.9251    steps: 123    lr: 0.0001     reward: 1.15\n",
      "epis: 756   score: 1.0   mem len: 137966   epsilon: 0.9248    steps: 150    lr: 0.0001     reward: 1.1\n",
      "epis: 757   score: 0.0   mem len: 138089   epsilon: 0.9246    steps: 123    lr: 0.0001     reward: 1.09\n",
      "epis: 758   score: 2.0   mem len: 138269   epsilon: 0.9242    steps: 180    lr: 0.0001     reward: 1.1\n",
      "epis: 759   score: 3.0   mem len: 138495   epsilon: 0.9238    steps: 226    lr: 0.0001     reward: 1.12\n",
      "epis: 760   score: 0.0   mem len: 138617   epsilon: 0.9235    steps: 122    lr: 0.0001     reward: 1.1\n",
      "epis: 761   score: 4.0   mem len: 138932   epsilon: 0.9229    steps: 315    lr: 0.0001     reward: 1.13\n",
      "epis: 762   score: 0.0   mem len: 139055   epsilon: 0.9227    steps: 123    lr: 0.0001     reward: 1.11\n",
      "epis: 763   score: 2.0   mem len: 139275   epsilon: 0.9222    steps: 220    lr: 0.0001     reward: 1.13\n",
      "epis: 764   score: 3.0   mem len: 139507   epsilon: 0.9218    steps: 232    lr: 0.0001     reward: 1.16\n",
      "epis: 765   score: 1.0   mem len: 139658   epsilon: 0.9215    steps: 151    lr: 0.0001     reward: 1.17\n",
      "epis: 766   score: 4.0   mem len: 139953   epsilon: 0.9209    steps: 295    lr: 0.0001     reward: 1.2\n",
      "epis: 767   score: 1.0   mem len: 140103   epsilon: 0.9206    steps: 150    lr: 0.0001     reward: 1.19\n",
      "epis: 768   score: 0.0   mem len: 140226   epsilon: 0.9204    steps: 123    lr: 0.0001     reward: 1.18\n",
      "epis: 769   score: 0.0   mem len: 140349   epsilon: 0.9201    steps: 123    lr: 0.0001     reward: 1.14\n",
      "epis: 770   score: 1.0   mem len: 140518   epsilon: 0.9198    steps: 169    lr: 0.0001     reward: 1.15\n",
      "epis: 771   score: 0.0   mem len: 140640   epsilon: 0.9195    steps: 122    lr: 0.0001     reward: 1.15\n",
      "epis: 772   score: 1.0   mem len: 140809   epsilon: 0.9192    steps: 169    lr: 0.0001     reward: 1.16\n",
      "epis: 773   score: 0.0   mem len: 140932   epsilon: 0.919    steps: 123    lr: 0.0001     reward: 1.14\n",
      "epis: 774   score: 0.0   mem len: 141055   epsilon: 0.9187    steps: 123    lr: 0.0001     reward: 1.12\n",
      "epis: 775   score: 0.0   mem len: 141177   epsilon: 0.9185    steps: 122    lr: 0.0001     reward: 1.1\n",
      "epis: 776   score: 2.0   mem len: 141375   epsilon: 0.9181    steps: 198    lr: 0.0001     reward: 1.1\n",
      "epis: 777   score: 1.0   mem len: 141546   epsilon: 0.9177    steps: 171    lr: 0.0001     reward: 1.11\n",
      "epis: 778   score: 2.0   mem len: 141763   epsilon: 0.9173    steps: 217    lr: 0.0001     reward: 1.1\n",
      "epis: 779   score: 1.0   mem len: 141932   epsilon: 0.917    steps: 169    lr: 0.0001     reward: 1.1\n",
      "epis: 780   score: 0.0   mem len: 142055   epsilon: 0.9167    steps: 123    lr: 0.0001     reward: 1.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 781   score: 1.0   mem len: 142224   epsilon: 0.9164    steps: 169    lr: 0.0001     reward: 1.09\n",
      "epis: 782   score: 0.0   mem len: 142346   epsilon: 0.9162    steps: 122    lr: 0.0001     reward: 1.08\n",
      "epis: 783   score: 0.0   mem len: 142469   epsilon: 0.9159    steps: 123    lr: 0.0001     reward: 1.07\n",
      "epis: 784   score: 0.0   mem len: 142592   epsilon: 0.9157    steps: 123    lr: 0.0001     reward: 1.05\n",
      "epis: 785   score: 4.0   mem len: 142889   epsilon: 0.9151    steps: 297    lr: 0.0001     reward: 1.09\n",
      "epis: 786   score: 0.0   mem len: 143012   epsilon: 0.9148    steps: 123    lr: 0.0001     reward: 1.08\n",
      "epis: 787   score: 2.0   mem len: 143210   epsilon: 0.9144    steps: 198    lr: 0.0001     reward: 1.07\n",
      "epis: 788   score: 2.0   mem len: 143407   epsilon: 0.9141    steps: 197    lr: 0.0001     reward: 1.08\n",
      "epis: 789   score: 0.0   mem len: 143530   epsilon: 0.9138    steps: 123    lr: 0.0001     reward: 1.08\n",
      "epis: 790   score: 2.0   mem len: 143746   epsilon: 0.9134    steps: 216    lr: 0.0001     reward: 1.08\n",
      "epis: 791   score: 2.0   mem len: 143962   epsilon: 0.913    steps: 216    lr: 0.0001     reward: 1.08\n",
      "epis: 792   score: 2.0   mem len: 144162   epsilon: 0.9126    steps: 200    lr: 0.0001     reward: 1.09\n",
      "epis: 793   score: 0.0   mem len: 144284   epsilon: 0.9123    steps: 122    lr: 0.0001     reward: 1.09\n",
      "epis: 794   score: 3.0   mem len: 144549   epsilon: 0.9118    steps: 265    lr: 0.0001     reward: 1.1\n",
      "epis: 795   score: 4.0   mem len: 144823   epsilon: 0.9112    steps: 274    lr: 0.0001     reward: 1.13\n",
      "epis: 796   score: 2.0   mem len: 145042   epsilon: 0.9108    steps: 219    lr: 0.0001     reward: 1.14\n",
      "epis: 797   score: 2.0   mem len: 145240   epsilon: 0.9104    steps: 198    lr: 0.0001     reward: 1.16\n",
      "epis: 798   score: 3.0   mem len: 145507   epsilon: 0.9099    steps: 267    lr: 0.0001     reward: 1.18\n",
      "epis: 799   score: 0.0   mem len: 145630   epsilon: 0.9097    steps: 123    lr: 0.0001     reward: 1.18\n",
      "epis: 800   score: 0.0   mem len: 145752   epsilon: 0.9094    steps: 122    lr: 0.0001     reward: 1.18\n",
      "epis: 801   score: 3.0   mem len: 145982   epsilon: 0.909    steps: 230    lr: 0.0001     reward: 1.19\n",
      "epis: 802   score: 5.0   mem len: 146306   epsilon: 0.9083    steps: 324    lr: 0.0001     reward: 1.22\n",
      "epis: 803   score: 0.0   mem len: 146429   epsilon: 0.9081    steps: 123    lr: 0.0001     reward: 1.22\n",
      "epis: 804   score: 4.0   mem len: 146738   epsilon: 0.9075    steps: 309    lr: 0.0001     reward: 1.24\n",
      "epis: 805   score: 2.0   mem len: 146935   epsilon: 0.9071    steps: 197    lr: 0.0001     reward: 1.24\n",
      "epis: 806   score: 2.0   mem len: 147133   epsilon: 0.9067    steps: 198    lr: 0.0001     reward: 1.23\n",
      "epis: 807   score: 0.0   mem len: 147256   epsilon: 0.9064    steps: 123    lr: 0.0001     reward: 1.23\n",
      "epis: 808   score: 0.0   mem len: 147379   epsilon: 0.9062    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 809   score: 1.0   mem len: 147530   epsilon: 0.9059    steps: 151    lr: 0.0001     reward: 1.21\n",
      "epis: 810   score: 2.0   mem len: 147729   epsilon: 0.9055    steps: 199    lr: 0.0001     reward: 1.22\n",
      "epis: 811   score: 2.0   mem len: 147927   epsilon: 0.9051    steps: 198    lr: 0.0001     reward: 1.23\n",
      "epis: 812   score: 0.0   mem len: 148050   epsilon: 0.9049    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 813   score: 0.0   mem len: 148172   epsilon: 0.9046    steps: 122    lr: 0.0001     reward: 1.21\n",
      "epis: 814   score: 5.0   mem len: 148485   epsilon: 0.904    steps: 313    lr: 0.0001     reward: 1.23\n",
      "epis: 815   score: 3.0   mem len: 148749   epsilon: 0.9035    steps: 264    lr: 0.0001     reward: 1.24\n",
      "epis: 816   score: 3.0   mem len: 148975   epsilon: 0.903    steps: 226    lr: 0.0001     reward: 1.26\n",
      "epis: 817   score: 0.0   mem len: 149098   epsilon: 0.9028    steps: 123    lr: 0.0001     reward: 1.25\n",
      "epis: 818   score: 0.0   mem len: 149221   epsilon: 0.9025    steps: 123    lr: 0.0001     reward: 1.25\n",
      "epis: 819   score: 4.0   mem len: 149488   epsilon: 0.902    steps: 267    lr: 0.0001     reward: 1.29\n",
      "epis: 820   score: 1.0   mem len: 149639   epsilon: 0.9017    steps: 151    lr: 0.0001     reward: 1.27\n",
      "epis: 821   score: 4.0   mem len: 149894   epsilon: 0.9012    steps: 255    lr: 0.0001     reward: 1.26\n",
      "epis: 822   score: 1.0   mem len: 150062   epsilon: 0.9009    steps: 168    lr: 0.0001     reward: 1.26\n",
      "epis: 823   score: 1.0   mem len: 150212   epsilon: 0.9006    steps: 150    lr: 0.0001     reward: 1.25\n",
      "epis: 824   score: 1.0   mem len: 150363   epsilon: 0.9003    steps: 151    lr: 0.0001     reward: 1.24\n",
      "epis: 825   score: 1.0   mem len: 150532   epsilon: 0.8999    steps: 169    lr: 0.0001     reward: 1.25\n",
      "epis: 826   score: 3.0   mem len: 150758   epsilon: 0.8995    steps: 226    lr: 0.0001     reward: 1.28\n",
      "epis: 827   score: 0.0   mem len: 150880   epsilon: 0.8993    steps: 122    lr: 0.0001     reward: 1.26\n",
      "epis: 828   score: 2.0   mem len: 151078   epsilon: 0.8989    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 829   score: 2.0   mem len: 151296   epsilon: 0.8984    steps: 218    lr: 0.0001     reward: 1.3\n",
      "epis: 830   score: 2.0   mem len: 151494   epsilon: 0.898    steps: 198    lr: 0.0001     reward: 1.31\n",
      "epis: 831   score: 2.0   mem len: 151692   epsilon: 0.8976    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 832   score: 0.0   mem len: 151814   epsilon: 0.8974    steps: 122    lr: 0.0001     reward: 1.26\n",
      "epis: 833   score: 2.0   mem len: 152032   epsilon: 0.897    steps: 218    lr: 0.0001     reward: 1.28\n",
      "epis: 834   score: 1.0   mem len: 152183   epsilon: 0.8967    steps: 151    lr: 0.0001     reward: 1.29\n",
      "epis: 835   score: 4.0   mem len: 152480   epsilon: 0.8961    steps: 297    lr: 0.0001     reward: 1.32\n",
      "epis: 836   score: 1.0   mem len: 152650   epsilon: 0.8958    steps: 170    lr: 0.0001     reward: 1.32\n",
      "epis: 837   score: 2.0   mem len: 152868   epsilon: 0.8953    steps: 218    lr: 0.0001     reward: 1.34\n",
      "epis: 838   score: 4.0   mem len: 153144   epsilon: 0.8948    steps: 276    lr: 0.0001     reward: 1.36\n",
      "epis: 839   score: 0.0   mem len: 153267   epsilon: 0.8945    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 840   score: 1.0   mem len: 153436   epsilon: 0.8942    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 841   score: 2.0   mem len: 153654   epsilon: 0.8938    steps: 218    lr: 0.0001     reward: 1.39\n",
      "epis: 842   score: 2.0   mem len: 153872   epsilon: 0.8933    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 843   score: 0.0   mem len: 153995   epsilon: 0.8931    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 844   score: 3.0   mem len: 154221   epsilon: 0.8926    steps: 226    lr: 0.0001     reward: 1.41\n",
      "epis: 845   score: 1.0   mem len: 154372   epsilon: 0.8923    steps: 151    lr: 0.0001     reward: 1.42\n",
      "epis: 846   score: 4.0   mem len: 154688   epsilon: 0.8917    steps: 316    lr: 0.0001     reward: 1.46\n",
      "epis: 847   score: 1.0   mem len: 154857   epsilon: 0.8914    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 848   score: 1.0   mem len: 155029   epsilon: 0.891    steps: 172    lr: 0.0001     reward: 1.47\n",
      "epis: 849   score: 1.0   mem len: 155198   epsilon: 0.8907    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 850   score: 0.0   mem len: 155320   epsilon: 0.8905    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 851   score: 3.0   mem len: 155567   epsilon: 0.89    steps: 247    lr: 0.0001     reward: 1.48\n",
      "epis: 852   score: 2.0   mem len: 155787   epsilon: 0.8895    steps: 220    lr: 0.0001     reward: 1.5\n",
      "epis: 853   score: 3.0   mem len: 156034   epsilon: 0.8891    steps: 247    lr: 0.0001     reward: 1.53\n",
      "epis: 854   score: 1.0   mem len: 156203   epsilon: 0.8887    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 855   score: 4.0   mem len: 156479   epsilon: 0.8882    steps: 276    lr: 0.0001     reward: 1.57\n",
      "epis: 856   score: 2.0   mem len: 156677   epsilon: 0.8878    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 857   score: 2.0   mem len: 156877   epsilon: 0.8874    steps: 200    lr: 0.0001     reward: 1.6\n",
      "epis: 858   score: 3.0   mem len: 157125   epsilon: 0.8869    steps: 248    lr: 0.0001     reward: 1.61\n",
      "epis: 859   score: 2.0   mem len: 157327   epsilon: 0.8865    steps: 202    lr: 0.0001     reward: 1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 860   score: 3.0   mem len: 157574   epsilon: 0.886    steps: 247    lr: 0.0001     reward: 1.63\n",
      "epis: 861   score: 0.0   mem len: 157697   epsilon: 0.8858    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 862   score: 0.0   mem len: 157819   epsilon: 0.8855    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 863   score: 2.0   mem len: 158017   epsilon: 0.8851    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 864   score: 3.0   mem len: 158242   epsilon: 0.8847    steps: 225    lr: 0.0001     reward: 1.59\n",
      "epis: 865   score: 2.0   mem len: 158440   epsilon: 0.8843    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 866   score: 2.0   mem len: 158637   epsilon: 0.8839    steps: 197    lr: 0.0001     reward: 1.58\n",
      "epis: 867   score: 0.0   mem len: 158759   epsilon: 0.8837    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 868   score: 3.0   mem len: 158985   epsilon: 0.8832    steps: 226    lr: 0.0001     reward: 1.6\n",
      "epis: 869   score: 1.0   mem len: 159154   epsilon: 0.8829    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 870   score: 0.0   mem len: 159276   epsilon: 0.8826    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 871   score: 1.0   mem len: 159445   epsilon: 0.8823    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 872   score: 2.0   mem len: 159661   epsilon: 0.8819    steps: 216    lr: 0.0001     reward: 1.62\n",
      "epis: 873   score: 3.0   mem len: 159908   epsilon: 0.8814    steps: 247    lr: 0.0001     reward: 1.65\n",
      "epis: 874   score: 2.0   mem len: 160106   epsilon: 0.881    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 875   score: 3.0   mem len: 160354   epsilon: 0.8805    steps: 248    lr: 0.0001     reward: 1.7\n",
      "epis: 876   score: 1.0   mem len: 160523   epsilon: 0.8802    steps: 169    lr: 0.0001     reward: 1.69\n",
      "epis: 877   score: 1.0   mem len: 160691   epsilon: 0.8798    steps: 168    lr: 0.0001     reward: 1.69\n",
      "epis: 878   score: 1.0   mem len: 160860   epsilon: 0.8795    steps: 169    lr: 0.0001     reward: 1.68\n",
      "epis: 879   score: 1.0   mem len: 161028   epsilon: 0.8792    steps: 168    lr: 0.0001     reward: 1.68\n",
      "epis: 880   score: 2.0   mem len: 161226   epsilon: 0.8788    steps: 198    lr: 0.0001     reward: 1.7\n",
      "epis: 881   score: 2.0   mem len: 161447   epsilon: 0.8783    steps: 221    lr: 0.0001     reward: 1.71\n",
      "epis: 882   score: 1.0   mem len: 161618   epsilon: 0.878    steps: 171    lr: 0.0001     reward: 1.72\n",
      "epis: 883   score: 2.0   mem len: 161836   epsilon: 0.8776    steps: 218    lr: 0.0001     reward: 1.74\n",
      "epis: 884   score: 1.0   mem len: 162005   epsilon: 0.8772    steps: 169    lr: 0.0001     reward: 1.75\n",
      "epis: 885   score: 3.0   mem len: 162270   epsilon: 0.8767    steps: 265    lr: 0.0001     reward: 1.74\n",
      "epis: 886   score: 2.0   mem len: 162468   epsilon: 0.8763    steps: 198    lr: 0.0001     reward: 1.76\n",
      "epis: 887   score: 1.0   mem len: 162619   epsilon: 0.876    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 888   score: 0.0   mem len: 162742   epsilon: 0.8758    steps: 123    lr: 0.0001     reward: 1.73\n",
      "epis: 889   score: 2.0   mem len: 162940   epsilon: 0.8754    steps: 198    lr: 0.0001     reward: 1.75\n",
      "epis: 890   score: 1.0   mem len: 163108   epsilon: 0.875    steps: 168    lr: 0.0001     reward: 1.74\n",
      "epis: 891   score: 1.0   mem len: 163259   epsilon: 0.8747    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 892   score: 2.0   mem len: 163480   epsilon: 0.8743    steps: 221    lr: 0.0001     reward: 1.73\n",
      "epis: 893   score: 2.0   mem len: 163678   epsilon: 0.8739    steps: 198    lr: 0.0001     reward: 1.75\n",
      "epis: 894   score: 1.0   mem len: 163829   epsilon: 0.8736    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 895   score: 2.0   mem len: 164030   epsilon: 0.8732    steps: 201    lr: 0.0001     reward: 1.71\n",
      "epis: 896   score: 2.0   mem len: 164228   epsilon: 0.8728    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 897   score: 2.0   mem len: 164427   epsilon: 0.8724    steps: 199    lr: 0.0001     reward: 1.71\n",
      "epis: 898   score: 0.0   mem len: 164549   epsilon: 0.8722    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 899   score: 0.0   mem len: 164671   epsilon: 0.8719    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 900   score: 3.0   mem len: 164937   epsilon: 0.8714    steps: 266    lr: 0.0001     reward: 1.71\n",
      "epis: 901   score: 3.0   mem len: 165185   epsilon: 0.8709    steps: 248    lr: 0.0001     reward: 1.71\n",
      "epis: 902   score: 3.0   mem len: 165432   epsilon: 0.8704    steps: 247    lr: 0.0001     reward: 1.69\n",
      "epis: 903   score: 0.0   mem len: 165555   epsilon: 0.8702    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 904   score: 0.0   mem len: 165677   epsilon: 0.87    steps: 122    lr: 0.0001     reward: 1.65\n",
      "epis: 905   score: 3.0   mem len: 165926   epsilon: 0.8695    steps: 249    lr: 0.0001     reward: 1.66\n",
      "epis: 906   score: 0.0   mem len: 166048   epsilon: 0.8692    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 907   score: 0.0   mem len: 166170   epsilon: 0.869    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 908   score: 1.0   mem len: 166340   epsilon: 0.8686    steps: 170    lr: 0.0001     reward: 1.65\n",
      "epis: 909   score: 0.0   mem len: 166462   epsilon: 0.8684    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 910   score: 2.0   mem len: 166678   epsilon: 0.868    steps: 216    lr: 0.0001     reward: 1.64\n",
      "epis: 911   score: 1.0   mem len: 166847   epsilon: 0.8676    steps: 169    lr: 0.0001     reward: 1.63\n",
      "epis: 912   score: 2.0   mem len: 167045   epsilon: 0.8672    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 913   score: 3.0   mem len: 167310   epsilon: 0.8667    steps: 265    lr: 0.0001     reward: 1.68\n",
      "epis: 914   score: 0.0   mem len: 167433   epsilon: 0.8665    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 915   score: 1.0   mem len: 167602   epsilon: 0.8661    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 916   score: 0.0   mem len: 167724   epsilon: 0.8659    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 917   score: 1.0   mem len: 167893   epsilon: 0.8656    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 918   score: 3.0   mem len: 168139   epsilon: 0.8651    steps: 246    lr: 0.0001     reward: 1.62\n",
      "epis: 919   score: 4.0   mem len: 168416   epsilon: 0.8645    steps: 277    lr: 0.0001     reward: 1.62\n",
      "epis: 920   score: 0.0   mem len: 168539   epsilon: 0.8643    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 921   score: 2.0   mem len: 168741   epsilon: 0.8639    steps: 202    lr: 0.0001     reward: 1.59\n",
      "epis: 922   score: 1.0   mem len: 168911   epsilon: 0.8636    steps: 170    lr: 0.0001     reward: 1.59\n",
      "epis: 923   score: 1.0   mem len: 169063   epsilon: 0.8633    steps: 152    lr: 0.0001     reward: 1.59\n",
      "epis: 924   score: 0.0   mem len: 169185   epsilon: 0.863    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 925   score: 1.0   mem len: 169353   epsilon: 0.8627    steps: 168    lr: 0.0001     reward: 1.58\n",
      "epis: 926   score: 2.0   mem len: 169573   epsilon: 0.8622    steps: 220    lr: 0.0001     reward: 1.57\n",
      "epis: 927   score: 1.0   mem len: 169723   epsilon: 0.8619    steps: 150    lr: 0.0001     reward: 1.58\n",
      "epis: 928   score: 1.0   mem len: 169874   epsilon: 0.8616    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 929   score: 1.0   mem len: 170025   epsilon: 0.8613    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 930   score: 2.0   mem len: 170223   epsilon: 0.861    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 931   score: 0.0   mem len: 170346   epsilon: 0.8607    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 932   score: 3.0   mem len: 170595   epsilon: 0.8602    steps: 249    lr: 0.0001     reward: 1.57\n",
      "epis: 933   score: 1.0   mem len: 170746   epsilon: 0.8599    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 934   score: 2.0   mem len: 170965   epsilon: 0.8595    steps: 219    lr: 0.0001     reward: 1.57\n",
      "epis: 935   score: 2.0   mem len: 171163   epsilon: 0.8591    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 936   score: 5.0   mem len: 171526   epsilon: 0.8584    steps: 363    lr: 0.0001     reward: 1.59\n",
      "epis: 937   score: 1.0   mem len: 171697   epsilon: 0.858    steps: 171    lr: 0.0001     reward: 1.58\n",
      "epis: 938   score: 1.0   mem len: 171866   epsilon: 0.8577    steps: 169    lr: 0.0001     reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 939   score: 4.0   mem len: 172162   epsilon: 0.8571    steps: 296    lr: 0.0001     reward: 1.59\n",
      "epis: 940   score: 0.0   mem len: 172285   epsilon: 0.8569    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 941   score: 2.0   mem len: 172483   epsilon: 0.8565    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 942   score: 1.0   mem len: 172655   epsilon: 0.8561    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 943   score: 0.0   mem len: 172778   epsilon: 0.8559    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 944   score: 1.0   mem len: 172929   epsilon: 0.8556    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 945   score: 2.0   mem len: 173127   epsilon: 0.8552    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 946   score: 1.0   mem len: 173296   epsilon: 0.8549    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 947   score: 2.0   mem len: 173494   epsilon: 0.8545    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 948   score: 2.0   mem len: 173691   epsilon: 0.8541    steps: 197    lr: 0.0001     reward: 1.55\n",
      "epis: 949   score: 2.0   mem len: 173889   epsilon: 0.8537    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 950   score: 0.0   mem len: 174011   epsilon: 0.8535    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 951   score: 3.0   mem len: 174237   epsilon: 0.853    steps: 226    lr: 0.0001     reward: 1.56\n",
      "epis: 952   score: 3.0   mem len: 174483   epsilon: 0.8525    steps: 246    lr: 0.0001     reward: 1.57\n",
      "epis: 953   score: 2.0   mem len: 174683   epsilon: 0.8521    steps: 200    lr: 0.0001     reward: 1.56\n",
      "epis: 954   score: 0.0   mem len: 174806   epsilon: 0.8519    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 955   score: 0.0   mem len: 174929   epsilon: 0.8516    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 956   score: 2.0   mem len: 175127   epsilon: 0.8512    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 957   score: 1.0   mem len: 175277   epsilon: 0.8509    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 958   score: 1.0   mem len: 175445   epsilon: 0.8506    steps: 168    lr: 0.0001     reward: 1.48\n",
      "epis: 959   score: 0.0   mem len: 175568   epsilon: 0.8504    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 960   score: 0.0   mem len: 175690   epsilon: 0.8501    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 961   score: 1.0   mem len: 175862   epsilon: 0.8498    steps: 172    lr: 0.0001     reward: 1.44\n",
      "epis: 962   score: 2.0   mem len: 176081   epsilon: 0.8494    steps: 219    lr: 0.0001     reward: 1.46\n",
      "epis: 963   score: 4.0   mem len: 176377   epsilon: 0.8488    steps: 296    lr: 0.0001     reward: 1.48\n",
      "epis: 964   score: 4.0   mem len: 176675   epsilon: 0.8482    steps: 298    lr: 0.0001     reward: 1.49\n",
      "epis: 965   score: 0.0   mem len: 176798   epsilon: 0.8479    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 966   score: 3.0   mem len: 177047   epsilon: 0.8474    steps: 249    lr: 0.0001     reward: 1.48\n",
      "epis: 967   score: 0.0   mem len: 177169   epsilon: 0.8472    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 968   score: 1.0   mem len: 177338   epsilon: 0.8469    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 969   score: 0.0   mem len: 177460   epsilon: 0.8466    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 970   score: 0.0   mem len: 177583   epsilon: 0.8464    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 971   score: 2.0   mem len: 177763   epsilon: 0.846    steps: 180    lr: 0.0001     reward: 1.46\n",
      "epis: 972   score: 0.0   mem len: 177886   epsilon: 0.8458    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 973   score: 0.0   mem len: 178008   epsilon: 0.8455    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 974   score: 3.0   mem len: 178255   epsilon: 0.8451    steps: 247    lr: 0.0001     reward: 1.42\n",
      "epis: 975   score: 4.0   mem len: 178529   epsilon: 0.8445    steps: 274    lr: 0.0001     reward: 1.43\n",
      "epis: 976   score: 0.0   mem len: 178652   epsilon: 0.8443    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 977   score: 4.0   mem len: 178929   epsilon: 0.8437    steps: 277    lr: 0.0001     reward: 1.45\n",
      "epis: 978   score: 2.0   mem len: 179127   epsilon: 0.8433    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 979   score: 2.0   mem len: 179324   epsilon: 0.8429    steps: 197    lr: 0.0001     reward: 1.47\n",
      "epis: 980   score: 1.0   mem len: 179475   epsilon: 0.8426    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 981   score: 2.0   mem len: 179673   epsilon: 0.8422    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 982   score: 2.0   mem len: 179854   epsilon: 0.8419    steps: 181    lr: 0.0001     reward: 1.47\n",
      "epis: 983   score: 0.0   mem len: 179977   epsilon: 0.8416    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 984   score: 2.0   mem len: 180175   epsilon: 0.8413    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 985   score: 1.0   mem len: 180326   epsilon: 0.841    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 986   score: 3.0   mem len: 180592   epsilon: 0.8404    steps: 266    lr: 0.0001     reward: 1.45\n",
      "epis: 987   score: 3.0   mem len: 180818   epsilon: 0.84    steps: 226    lr: 0.0001     reward: 1.47\n",
      "epis: 988   score: 4.0   mem len: 181095   epsilon: 0.8394    steps: 277    lr: 0.0001     reward: 1.51\n",
      "epis: 989   score: 1.0   mem len: 181246   epsilon: 0.8391    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 990   score: 2.0   mem len: 181445   epsilon: 0.8387    steps: 199    lr: 0.0001     reward: 1.51\n",
      "epis: 991   score: 6.0   mem len: 181804   epsilon: 0.838    steps: 359    lr: 0.0001     reward: 1.56\n",
      "epis: 992   score: 3.0   mem len: 182069   epsilon: 0.8375    steps: 265    lr: 0.0001     reward: 1.57\n",
      "epis: 993   score: 2.0   mem len: 182288   epsilon: 0.8371    steps: 219    lr: 0.0001     reward: 1.57\n",
      "epis: 994   score: 3.0   mem len: 182534   epsilon: 0.8366    steps: 246    lr: 0.0001     reward: 1.59\n",
      "epis: 995   score: 1.0   mem len: 182706   epsilon: 0.8362    steps: 172    lr: 0.0001     reward: 1.58\n",
      "epis: 996   score: 0.0   mem len: 182829   epsilon: 0.836    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 997   score: 0.0   mem len: 182951   epsilon: 0.8358    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 998   score: 0.0   mem len: 183074   epsilon: 0.8355    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 999   score: 0.0   mem len: 183196   epsilon: 0.8353    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 1000   score: 1.0   mem len: 183365   epsilon: 0.8349    steps: 169    lr: 0.0001     reward: 1.52\n",
      "epis: 1001   score: 1.0   mem len: 183515   epsilon: 0.8346    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 1002   score: 0.0   mem len: 183637   epsilon: 0.8344    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 1003   score: 3.0   mem len: 183903   epsilon: 0.8339    steps: 266    lr: 0.0001     reward: 1.5\n",
      "epis: 1004   score: 3.0   mem len: 184129   epsilon: 0.8334    steps: 226    lr: 0.0001     reward: 1.53\n",
      "epis: 1005   score: 1.0   mem len: 184280   epsilon: 0.8331    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 1006   score: 1.0   mem len: 184450   epsilon: 0.8328    steps: 170    lr: 0.0001     reward: 1.52\n",
      "epis: 1007   score: 0.0   mem len: 184573   epsilon: 0.8325    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 1008   score: 4.0   mem len: 184850   epsilon: 0.832    steps: 277    lr: 0.0001     reward: 1.55\n",
      "epis: 1009   score: 0.0   mem len: 184973   epsilon: 0.8318    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 1010   score: 1.0   mem len: 185142   epsilon: 0.8314    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 1011   score: 3.0   mem len: 185407   epsilon: 0.8309    steps: 265    lr: 0.0001     reward: 1.56\n",
      "epis: 1012   score: 0.0   mem len: 185529   epsilon: 0.8307    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 1013   score: 2.0   mem len: 185745   epsilon: 0.8302    steps: 216    lr: 0.0001     reward: 1.53\n",
      "epis: 1014   score: 2.0   mem len: 185943   epsilon: 0.8298    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 1015   score: 3.0   mem len: 186192   epsilon: 0.8293    steps: 249    lr: 0.0001     reward: 1.57\n",
      "epis: 1016   score: 6.0   mem len: 186584   epsilon: 0.8286    steps: 392    lr: 0.0001     reward: 1.63\n",
      "epis: 1017   score: 3.0   mem len: 186834   epsilon: 0.8281    steps: 250    lr: 0.0001     reward: 1.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1018   score: 1.0   mem len: 187003   epsilon: 0.8277    steps: 169    lr: 0.0001     reward: 1.63\n",
      "epis: 1019   score: 2.0   mem len: 187220   epsilon: 0.8273    steps: 217    lr: 0.0001     reward: 1.61\n",
      "epis: 1020   score: 0.0   mem len: 187343   epsilon: 0.8271    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 1021   score: 2.0   mem len: 187562   epsilon: 0.8266    steps: 219    lr: 0.0001     reward: 1.61\n",
      "epis: 1022   score: 5.0   mem len: 187898   epsilon: 0.826    steps: 336    lr: 0.0001     reward: 1.65\n",
      "epis: 1023   score: 3.0   mem len: 188124   epsilon: 0.8255    steps: 226    lr: 0.0001     reward: 1.67\n",
      "epis: 1024   score: 4.0   mem len: 188441   epsilon: 0.8249    steps: 317    lr: 0.0001     reward: 1.71\n",
      "epis: 1025   score: 0.0   mem len: 188564   epsilon: 0.8246    steps: 123    lr: 0.0001     reward: 1.7\n",
      "epis: 1026   score: 3.0   mem len: 188812   epsilon: 0.8242    steps: 248    lr: 0.0001     reward: 1.71\n",
      "epis: 1027   score: 2.0   mem len: 188994   epsilon: 0.8238    steps: 182    lr: 0.0001     reward: 1.72\n",
      "epis: 1028   score: 3.0   mem len: 189240   epsilon: 0.8233    steps: 246    lr: 0.0001     reward: 1.74\n",
      "epis: 1029   score: 0.0   mem len: 189362   epsilon: 0.8231    steps: 122    lr: 0.0001     reward: 1.73\n",
      "epis: 1030   score: 1.0   mem len: 189513   epsilon: 0.8228    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 1031   score: 0.0   mem len: 189636   epsilon: 0.8225    steps: 123    lr: 0.0001     reward: 1.72\n",
      "epis: 1032   score: 2.0   mem len: 189834   epsilon: 0.8221    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 1033   score: 1.0   mem len: 190003   epsilon: 0.8218    steps: 169    lr: 0.0001     reward: 1.71\n",
      "epis: 1034   score: 1.0   mem len: 190154   epsilon: 0.8215    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 1035   score: 2.0   mem len: 190352   epsilon: 0.8211    steps: 198    lr: 0.0001     reward: 1.7\n",
      "epis: 1036   score: 3.0   mem len: 190580   epsilon: 0.8206    steps: 228    lr: 0.0001     reward: 1.68\n",
      "epis: 1037   score: 2.0   mem len: 190778   epsilon: 0.8203    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 1038   score: 3.0   mem len: 191004   epsilon: 0.8198    steps: 226    lr: 0.0001     reward: 1.71\n",
      "epis: 1039   score: 2.0   mem len: 191202   epsilon: 0.8194    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 1040   score: 1.0   mem len: 191353   epsilon: 0.8191    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 1041   score: 2.0   mem len: 191550   epsilon: 0.8187    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 1042   score: 2.0   mem len: 191748   epsilon: 0.8183    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 1043   score: 0.0   mem len: 191870   epsilon: 0.8181    steps: 122    lr: 0.0001     reward: 1.71\n",
      "epis: 1044   score: 1.0   mem len: 192039   epsilon: 0.8178    steps: 169    lr: 0.0001     reward: 1.71\n",
      "epis: 1045   score: 2.0   mem len: 192237   epsilon: 0.8174    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 1046   score: 1.0   mem len: 192387   epsilon: 0.8171    steps: 150    lr: 0.0001     reward: 1.71\n",
      "epis: 1047   score: 0.0   mem len: 192510   epsilon: 0.8168    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 1048   score: 2.0   mem len: 192707   epsilon: 0.8164    steps: 197    lr: 0.0001     reward: 1.69\n",
      "epis: 1049   score: 1.0   mem len: 192857   epsilon: 0.8161    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 1050   score: 1.0   mem len: 193027   epsilon: 0.8158    steps: 170    lr: 0.0001     reward: 1.69\n",
      "epis: 1051   score: 0.0   mem len: 193149   epsilon: 0.8156    steps: 122    lr: 0.0001     reward: 1.66\n",
      "epis: 1052   score: 5.0   mem len: 193513   epsilon: 0.8148    steps: 364    lr: 0.0001     reward: 1.68\n",
      "epis: 1053   score: 1.0   mem len: 193663   epsilon: 0.8145    steps: 150    lr: 0.0001     reward: 1.67\n",
      "epis: 1054   score: 2.0   mem len: 193861   epsilon: 0.8142    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 1055   score: 2.0   mem len: 194061   epsilon: 0.8138    steps: 200    lr: 0.0001     reward: 1.71\n",
      "epis: 1056   score: 4.0   mem len: 194318   epsilon: 0.8132    steps: 257    lr: 0.0001     reward: 1.73\n",
      "epis: 1057   score: 0.0   mem len: 194440   epsilon: 0.813    steps: 122    lr: 0.0001     reward: 1.72\n",
      "epis: 1058   score: 4.0   mem len: 194736   epsilon: 0.8124    steps: 296    lr: 0.0001     reward: 1.75\n",
      "epis: 1059   score: 2.0   mem len: 194934   epsilon: 0.812    steps: 198    lr: 0.0001     reward: 1.77\n",
      "epis: 1060   score: 0.0   mem len: 195057   epsilon: 0.8118    steps: 123    lr: 0.0001     reward: 1.77\n",
      "epis: 1061   score: 1.0   mem len: 195225   epsilon: 0.8115    steps: 168    lr: 0.0001     reward: 1.77\n",
      "epis: 1062   score: 4.0   mem len: 195500   epsilon: 0.8109    steps: 275    lr: 0.0001     reward: 1.79\n",
      "epis: 1063   score: 3.0   mem len: 195747   epsilon: 0.8104    steps: 247    lr: 0.0001     reward: 1.78\n",
      "epis: 1064   score: 0.0   mem len: 195869   epsilon: 0.8102    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1065   score: 0.0   mem len: 195991   epsilon: 0.8099    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1066   score: 0.0   mem len: 196114   epsilon: 0.8097    steps: 123    lr: 0.0001     reward: 1.71\n",
      "epis: 1067   score: 2.0   mem len: 196312   epsilon: 0.8093    steps: 198    lr: 0.0001     reward: 1.73\n",
      "epis: 1068   score: 2.0   mem len: 196512   epsilon: 0.8089    steps: 200    lr: 0.0001     reward: 1.74\n",
      "epis: 1069   score: 0.0   mem len: 196634   epsilon: 0.8087    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1070   score: 0.0   mem len: 196756   epsilon: 0.8084    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1071   score: 2.0   mem len: 196973   epsilon: 0.808    steps: 217    lr: 0.0001     reward: 1.74\n",
      "epis: 1072   score: 1.0   mem len: 197141   epsilon: 0.8077    steps: 168    lr: 0.0001     reward: 1.75\n",
      "epis: 1073   score: 1.0   mem len: 197310   epsilon: 0.8073    steps: 169    lr: 0.0001     reward: 1.76\n",
      "epis: 1074   score: 1.0   mem len: 197479   epsilon: 0.807    steps: 169    lr: 0.0001     reward: 1.74\n",
      "epis: 1075   score: 2.0   mem len: 197677   epsilon: 0.8066    steps: 198    lr: 0.0001     reward: 1.72\n",
      "epis: 1076   score: 5.0   mem len: 198026   epsilon: 0.8059    steps: 349    lr: 0.0001     reward: 1.77\n",
      "epis: 1077   score: 2.0   mem len: 198245   epsilon: 0.8055    steps: 219    lr: 0.0001     reward: 1.75\n",
      "epis: 1078   score: 4.0   mem len: 198505   epsilon: 0.805    steps: 260    lr: 0.0001     reward: 1.77\n",
      "epis: 1079   score: 4.0   mem len: 198803   epsilon: 0.8044    steps: 298    lr: 0.0001     reward: 1.79\n",
      "epis: 1080   score: 4.0   mem len: 199101   epsilon: 0.8038    steps: 298    lr: 0.0001     reward: 1.82\n",
      "epis: 1081   score: 3.0   mem len: 199347   epsilon: 0.8033    steps: 246    lr: 0.0001     reward: 1.83\n",
      "epis: 1082   score: 3.0   mem len: 199576   epsilon: 0.8028    steps: 229    lr: 0.0001     reward: 1.84\n",
      "epis: 1083   score: 4.0   mem len: 199849   epsilon: 0.8023    steps: 273    lr: 0.0001     reward: 1.88\n",
      "epis: 1084   score: 2.0   mem len: 200047   epsilon: 0.8019    steps: 198    lr: 4e-05     reward: 1.88\n",
      "epis: 1085   score: 2.0   mem len: 200244   epsilon: 0.8015    steps: 197    lr: 4e-05     reward: 1.89\n",
      "epis: 1086   score: 1.0   mem len: 200414   epsilon: 0.8012    steps: 170    lr: 4e-05     reward: 1.87\n",
      "epis: 1087   score: 3.0   mem len: 200661   epsilon: 0.8007    steps: 247    lr: 4e-05     reward: 1.87\n",
      "epis: 1088   score: 1.0   mem len: 200832   epsilon: 0.8004    steps: 171    lr: 4e-05     reward: 1.84\n",
      "epis: 1089   score: 2.0   mem len: 201050   epsilon: 0.7999    steps: 218    lr: 4e-05     reward: 1.85\n",
      "epis: 1090   score: 1.0   mem len: 201219   epsilon: 0.7996    steps: 169    lr: 4e-05     reward: 1.84\n",
      "epis: 1091   score: 2.0   mem len: 201436   epsilon: 0.7992    steps: 217    lr: 4e-05     reward: 1.8\n",
      "epis: 1092   score: 0.0   mem len: 201559   epsilon: 0.7989    steps: 123    lr: 4e-05     reward: 1.77\n",
      "epis: 1093   score: 1.0   mem len: 201728   epsilon: 0.7986    steps: 169    lr: 4e-05     reward: 1.76\n",
      "epis: 1094   score: 0.0   mem len: 201850   epsilon: 0.7983    steps: 122    lr: 4e-05     reward: 1.73\n",
      "epis: 1095   score: 1.0   mem len: 202000   epsilon: 0.798    steps: 150    lr: 4e-05     reward: 1.73\n",
      "epis: 1096   score: 2.0   mem len: 202198   epsilon: 0.7976    steps: 198    lr: 4e-05     reward: 1.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1097   score: 2.0   mem len: 202416   epsilon: 0.7972    steps: 218    lr: 4e-05     reward: 1.77\n",
      "epis: 1098   score: 4.0   mem len: 202691   epsilon: 0.7967    steps: 275    lr: 4e-05     reward: 1.81\n",
      "epis: 1099   score: 5.0   mem len: 203055   epsilon: 0.7959    steps: 364    lr: 4e-05     reward: 1.86\n",
      "epis: 1100   score: 1.0   mem len: 203224   epsilon: 0.7956    steps: 169    lr: 4e-05     reward: 1.86\n",
      "epis: 1101   score: 0.0   mem len: 203346   epsilon: 0.7954    steps: 122    lr: 4e-05     reward: 1.85\n",
      "epis: 1102   score: 3.0   mem len: 203611   epsilon: 0.7948    steps: 265    lr: 4e-05     reward: 1.88\n",
      "epis: 1103   score: 4.0   mem len: 203888   epsilon: 0.7943    steps: 277    lr: 4e-05     reward: 1.89\n",
      "epis: 1104   score: 2.0   mem len: 204068   epsilon: 0.7939    steps: 180    lr: 4e-05     reward: 1.88\n",
      "epis: 1105   score: 1.0   mem len: 204239   epsilon: 0.7936    steps: 171    lr: 4e-05     reward: 1.88\n",
      "epis: 1106   score: 2.0   mem len: 204436   epsilon: 0.7932    steps: 197    lr: 4e-05     reward: 1.89\n",
      "epis: 1107   score: 1.0   mem len: 204605   epsilon: 0.7929    steps: 169    lr: 4e-05     reward: 1.9\n",
      "epis: 1108   score: 3.0   mem len: 204831   epsilon: 0.7924    steps: 226    lr: 4e-05     reward: 1.89\n",
      "epis: 1109   score: 3.0   mem len: 205062   epsilon: 0.792    steps: 231    lr: 4e-05     reward: 1.92\n",
      "epis: 1110   score: 2.0   mem len: 205263   epsilon: 0.7916    steps: 201    lr: 4e-05     reward: 1.93\n",
      "epis: 1111   score: 1.0   mem len: 205414   epsilon: 0.7913    steps: 151    lr: 4e-05     reward: 1.91\n",
      "epis: 1112   score: 0.0   mem len: 205537   epsilon: 0.791    steps: 123    lr: 4e-05     reward: 1.91\n",
      "epis: 1113   score: 4.0   mem len: 205831   epsilon: 0.7905    steps: 294    lr: 4e-05     reward: 1.93\n",
      "epis: 1114   score: 4.0   mem len: 206124   epsilon: 0.7899    steps: 293    lr: 4e-05     reward: 1.95\n",
      "epis: 1115   score: 2.0   mem len: 206321   epsilon: 0.7895    steps: 197    lr: 4e-05     reward: 1.94\n",
      "epis: 1116   score: 2.0   mem len: 206539   epsilon: 0.7891    steps: 218    lr: 4e-05     reward: 1.9\n",
      "epis: 1117   score: 0.0   mem len: 206662   epsilon: 0.7888    steps: 123    lr: 4e-05     reward: 1.87\n",
      "epis: 1118   score: 2.0   mem len: 206859   epsilon: 0.7884    steps: 197    lr: 4e-05     reward: 1.88\n",
      "epis: 1119   score: 3.0   mem len: 207088   epsilon: 0.788    steps: 229    lr: 4e-05     reward: 1.89\n",
      "epis: 1120   score: 4.0   mem len: 207382   epsilon: 0.7874    steps: 294    lr: 4e-05     reward: 1.93\n",
      "epis: 1121   score: 2.0   mem len: 207580   epsilon: 0.787    steps: 198    lr: 4e-05     reward: 1.93\n",
      "epis: 1122   score: 1.0   mem len: 207748   epsilon: 0.7867    steps: 168    lr: 4e-05     reward: 1.89\n",
      "epis: 1123   score: 0.0   mem len: 207871   epsilon: 0.7864    steps: 123    lr: 4e-05     reward: 1.86\n",
      "epis: 1124   score: 2.0   mem len: 208068   epsilon: 0.786    steps: 197    lr: 4e-05     reward: 1.84\n",
      "epis: 1125   score: 1.0   mem len: 208218   epsilon: 0.7857    steps: 150    lr: 4e-05     reward: 1.85\n",
      "epis: 1126   score: 0.0   mem len: 208340   epsilon: 0.7855    steps: 122    lr: 4e-05     reward: 1.82\n",
      "epis: 1127   score: 3.0   mem len: 208567   epsilon: 0.785    steps: 227    lr: 4e-05     reward: 1.83\n",
      "epis: 1128   score: 0.0   mem len: 208690   epsilon: 0.7848    steps: 123    lr: 4e-05     reward: 1.8\n",
      "epis: 1129   score: 1.0   mem len: 208859   epsilon: 0.7845    steps: 169    lr: 4e-05     reward: 1.81\n",
      "epis: 1130   score: 5.0   mem len: 209203   epsilon: 0.7838    steps: 344    lr: 4e-05     reward: 1.85\n",
      "epis: 1131   score: 1.0   mem len: 209354   epsilon: 0.7835    steps: 151    lr: 4e-05     reward: 1.86\n",
      "epis: 1132   score: 3.0   mem len: 209600   epsilon: 0.783    steps: 246    lr: 4e-05     reward: 1.87\n",
      "epis: 1133   score: 4.0   mem len: 209881   epsilon: 0.7824    steps: 281    lr: 4e-05     reward: 1.9\n",
      "epis: 1134   score: 0.0   mem len: 210003   epsilon: 0.7822    steps: 122    lr: 4e-05     reward: 1.89\n",
      "epis: 1135   score: 5.0   mem len: 210349   epsilon: 0.7815    steps: 346    lr: 4e-05     reward: 1.92\n",
      "epis: 1136   score: 0.0   mem len: 210472   epsilon: 0.7813    steps: 123    lr: 4e-05     reward: 1.89\n",
      "epis: 1137   score: 2.0   mem len: 210671   epsilon: 0.7809    steps: 199    lr: 4e-05     reward: 1.89\n",
      "epis: 1138   score: 0.0   mem len: 210794   epsilon: 0.7806    steps: 123    lr: 4e-05     reward: 1.86\n",
      "epis: 1139   score: 4.0   mem len: 211111   epsilon: 0.78    steps: 317    lr: 4e-05     reward: 1.88\n",
      "epis: 1140   score: 3.0   mem len: 211375   epsilon: 0.7795    steps: 264    lr: 4e-05     reward: 1.9\n",
      "epis: 1141   score: 1.0   mem len: 211544   epsilon: 0.7791    steps: 169    lr: 4e-05     reward: 1.89\n",
      "epis: 1142   score: 2.0   mem len: 211742   epsilon: 0.7787    steps: 198    lr: 4e-05     reward: 1.89\n",
      "epis: 1143   score: 1.0   mem len: 211892   epsilon: 0.7785    steps: 150    lr: 4e-05     reward: 1.9\n",
      "epis: 1144   score: 1.0   mem len: 212061   epsilon: 0.7781    steps: 169    lr: 4e-05     reward: 1.9\n",
      "epis: 1145   score: 2.0   mem len: 212259   epsilon: 0.7777    steps: 198    lr: 4e-05     reward: 1.9\n",
      "epis: 1146   score: 2.0   mem len: 212475   epsilon: 0.7773    steps: 216    lr: 4e-05     reward: 1.91\n",
      "epis: 1147   score: 0.0   mem len: 212598   epsilon: 0.7771    steps: 123    lr: 4e-05     reward: 1.91\n",
      "epis: 1148   score: 3.0   mem len: 212844   epsilon: 0.7766    steps: 246    lr: 4e-05     reward: 1.92\n",
      "epis: 1149   score: 4.0   mem len: 213140   epsilon: 0.776    steps: 296    lr: 4e-05     reward: 1.95\n",
      "epis: 1150   score: 1.0   mem len: 213311   epsilon: 0.7756    steps: 171    lr: 4e-05     reward: 1.95\n",
      "epis: 1151   score: 4.0   mem len: 213590   epsilon: 0.7751    steps: 279    lr: 4e-05     reward: 1.99\n",
      "epis: 1152   score: 2.0   mem len: 213771   epsilon: 0.7747    steps: 181    lr: 4e-05     reward: 1.96\n",
      "epis: 1153   score: 5.0   mem len: 214094   epsilon: 0.7741    steps: 323    lr: 4e-05     reward: 2.0\n",
      "epis: 1154   score: 1.0   mem len: 214265   epsilon: 0.7738    steps: 171    lr: 4e-05     reward: 1.99\n",
      "epis: 1155   score: 1.0   mem len: 214416   epsilon: 0.7735    steps: 151    lr: 4e-05     reward: 1.98\n",
      "epis: 1156   score: 2.0   mem len: 214617   epsilon: 0.7731    steps: 201    lr: 4e-05     reward: 1.96\n",
      "epis: 1157   score: 2.0   mem len: 214815   epsilon: 0.7727    steps: 198    lr: 4e-05     reward: 1.98\n",
      "epis: 1158   score: 4.0   mem len: 215111   epsilon: 0.7721    steps: 296    lr: 4e-05     reward: 1.98\n",
      "epis: 1159   score: 2.0   mem len: 215329   epsilon: 0.7716    steps: 218    lr: 4e-05     reward: 1.98\n",
      "epis: 1160   score: 1.0   mem len: 215498   epsilon: 0.7713    steps: 169    lr: 4e-05     reward: 1.99\n",
      "epis: 1161   score: 6.0   mem len: 215844   epsilon: 0.7706    steps: 346    lr: 4e-05     reward: 2.04\n",
      "epis: 1162   score: 2.0   mem len: 216042   epsilon: 0.7702    steps: 198    lr: 4e-05     reward: 2.02\n",
      "epis: 1163   score: 4.0   mem len: 216317   epsilon: 0.7697    steps: 275    lr: 4e-05     reward: 2.03\n",
      "epis: 1164   score: 1.0   mem len: 216468   epsilon: 0.7694    steps: 151    lr: 4e-05     reward: 2.04\n",
      "epis: 1165   score: 2.0   mem len: 216665   epsilon: 0.769    steps: 197    lr: 4e-05     reward: 2.06\n",
      "epis: 1166   score: 0.0   mem len: 216788   epsilon: 0.7688    steps: 123    lr: 4e-05     reward: 2.06\n",
      "epis: 1167   score: 2.0   mem len: 216969   epsilon: 0.7684    steps: 181    lr: 4e-05     reward: 2.06\n",
      "epis: 1168   score: 4.0   mem len: 217227   epsilon: 0.7679    steps: 258    lr: 4e-05     reward: 2.08\n",
      "epis: 1169   score: 3.0   mem len: 217453   epsilon: 0.7674    steps: 226    lr: 4e-05     reward: 2.11\n",
      "epis: 1170   score: 5.0   mem len: 217780   epsilon: 0.7668    steps: 327    lr: 4e-05     reward: 2.16\n",
      "epis: 1171   score: 2.0   mem len: 217977   epsilon: 0.7664    steps: 197    lr: 4e-05     reward: 2.16\n",
      "epis: 1172   score: 2.0   mem len: 218157   epsilon: 0.766    steps: 180    lr: 4e-05     reward: 2.17\n",
      "epis: 1173   score: 3.0   mem len: 218382   epsilon: 0.7656    steps: 225    lr: 4e-05     reward: 2.19\n",
      "epis: 1174   score: 2.0   mem len: 218580   epsilon: 0.7652    steps: 198    lr: 4e-05     reward: 2.2\n",
      "epis: 1175   score: 2.0   mem len: 218798   epsilon: 0.7648    steps: 218    lr: 4e-05     reward: 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1176   score: 0.0   mem len: 218921   epsilon: 0.7645    steps: 123    lr: 4e-05     reward: 2.15\n",
      "epis: 1177   score: 2.0   mem len: 219141   epsilon: 0.7641    steps: 220    lr: 4e-05     reward: 2.15\n",
      "epis: 1178   score: 1.0   mem len: 219292   epsilon: 0.7638    steps: 151    lr: 4e-05     reward: 2.12\n",
      "epis: 1179   score: 1.0   mem len: 219460   epsilon: 0.7635    steps: 168    lr: 4e-05     reward: 2.09\n",
      "epis: 1180   score: 6.0   mem len: 219799   epsilon: 0.7628    steps: 339    lr: 4e-05     reward: 2.11\n",
      "epis: 1181   score: 0.0   mem len: 219922   epsilon: 0.7626    steps: 123    lr: 4e-05     reward: 2.08\n",
      "epis: 1182   score: 3.0   mem len: 220152   epsilon: 0.7621    steps: 230    lr: 4e-05     reward: 2.08\n",
      "epis: 1183   score: 5.0   mem len: 220455   epsilon: 0.7615    steps: 303    lr: 4e-05     reward: 2.09\n",
      "epis: 1184   score: 5.0   mem len: 220766   epsilon: 0.7609    steps: 311    lr: 4e-05     reward: 2.12\n",
      "epis: 1185   score: 1.0   mem len: 220917   epsilon: 0.7606    steps: 151    lr: 4e-05     reward: 2.11\n",
      "epis: 1186   score: 2.0   mem len: 221117   epsilon: 0.7602    steps: 200    lr: 4e-05     reward: 2.12\n",
      "epis: 1187   score: 3.0   mem len: 221362   epsilon: 0.7597    steps: 245    lr: 4e-05     reward: 2.12\n",
      "epis: 1188   score: 3.0   mem len: 221588   epsilon: 0.7593    steps: 226    lr: 4e-05     reward: 2.14\n",
      "epis: 1189   score: 3.0   mem len: 221856   epsilon: 0.7587    steps: 268    lr: 4e-05     reward: 2.15\n",
      "epis: 1190   score: 2.0   mem len: 222054   epsilon: 0.7583    steps: 198    lr: 4e-05     reward: 2.16\n",
      "epis: 1191   score: 2.0   mem len: 222233   epsilon: 0.758    steps: 179    lr: 4e-05     reward: 2.16\n",
      "epis: 1192   score: 0.0   mem len: 222356   epsilon: 0.7577    steps: 123    lr: 4e-05     reward: 2.16\n",
      "epis: 1193   score: 3.0   mem len: 222582   epsilon: 0.7573    steps: 226    lr: 4e-05     reward: 2.18\n",
      "epis: 1194   score: 2.0   mem len: 222802   epsilon: 0.7569    steps: 220    lr: 4e-05     reward: 2.2\n",
      "epis: 1195   score: 3.0   mem len: 223028   epsilon: 0.7564    steps: 226    lr: 4e-05     reward: 2.22\n",
      "epis: 1196   score: 4.0   mem len: 223303   epsilon: 0.7559    steps: 275    lr: 4e-05     reward: 2.24\n",
      "epis: 1197   score: 0.0   mem len: 223426   epsilon: 0.7556    steps: 123    lr: 4e-05     reward: 2.22\n",
      "epis: 1198   score: 1.0   mem len: 223596   epsilon: 0.7553    steps: 170    lr: 4e-05     reward: 2.19\n",
      "epis: 1199   score: 1.0   mem len: 223765   epsilon: 0.7549    steps: 169    lr: 4e-05     reward: 2.15\n",
      "epis: 1200   score: 0.0   mem len: 223888   epsilon: 0.7547    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1201   score: 3.0   mem len: 224135   epsilon: 0.7542    steps: 247    lr: 4e-05     reward: 2.17\n",
      "epis: 1202   score: 0.0   mem len: 224258   epsilon: 0.754    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1203   score: 2.0   mem len: 224438   epsilon: 0.7536    steps: 180    lr: 4e-05     reward: 2.12\n",
      "epis: 1204   score: 3.0   mem len: 224707   epsilon: 0.7531    steps: 269    lr: 4e-05     reward: 2.13\n",
      "epis: 1205   score: 4.0   mem len: 224981   epsilon: 0.7525    steps: 274    lr: 4e-05     reward: 2.16\n",
      "epis: 1206   score: 2.0   mem len: 225179   epsilon: 0.7521    steps: 198    lr: 4e-05     reward: 2.16\n",
      "epis: 1207   score: 1.0   mem len: 225348   epsilon: 0.7518    steps: 169    lr: 4e-05     reward: 2.16\n",
      "epis: 1208   score: 4.0   mem len: 225624   epsilon: 0.7513    steps: 276    lr: 4e-05     reward: 2.17\n",
      "epis: 1209   score: 3.0   mem len: 225870   epsilon: 0.7508    steps: 246    lr: 4e-05     reward: 2.17\n",
      "epis: 1210   score: 0.0   mem len: 225993   epsilon: 0.7505    steps: 123    lr: 4e-05     reward: 2.15\n",
      "epis: 1211   score: 0.0   mem len: 226116   epsilon: 0.7503    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1212   score: 0.0   mem len: 226239   epsilon: 0.75    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1213   score: 3.0   mem len: 226465   epsilon: 0.7496    steps: 226    lr: 4e-05     reward: 2.13\n",
      "epis: 1214   score: 1.0   mem len: 226616   epsilon: 0.7493    steps: 151    lr: 4e-05     reward: 2.1\n",
      "epis: 1215   score: 8.0   mem len: 226957   epsilon: 0.7486    steps: 341    lr: 4e-05     reward: 2.16\n",
      "epis: 1216   score: 2.0   mem len: 227158   epsilon: 0.7482    steps: 201    lr: 4e-05     reward: 2.16\n",
      "epis: 1217   score: 2.0   mem len: 227361   epsilon: 0.7478    steps: 203    lr: 4e-05     reward: 2.18\n",
      "epis: 1218   score: 1.0   mem len: 227512   epsilon: 0.7475    steps: 151    lr: 4e-05     reward: 2.17\n",
      "epis: 1219   score: 3.0   mem len: 227759   epsilon: 0.747    steps: 247    lr: 4e-05     reward: 2.17\n",
      "epis: 1220   score: 4.0   mem len: 228053   epsilon: 0.7465    steps: 294    lr: 4e-05     reward: 2.17\n",
      "epis: 1221   score: 2.0   mem len: 228251   epsilon: 0.7461    steps: 198    lr: 4e-05     reward: 2.17\n",
      "epis: 1222   score: 5.0   mem len: 228560   epsilon: 0.7454    steps: 309    lr: 4e-05     reward: 2.21\n",
      "epis: 1223   score: 3.0   mem len: 228786   epsilon: 0.745    steps: 226    lr: 4e-05     reward: 2.24\n",
      "epis: 1224   score: 2.0   mem len: 228984   epsilon: 0.7446    steps: 198    lr: 4e-05     reward: 2.24\n",
      "epis: 1225   score: 2.0   mem len: 229168   epsilon: 0.7442    steps: 184    lr: 4e-05     reward: 2.25\n",
      "epis: 1226   score: 2.0   mem len: 229366   epsilon: 0.7439    steps: 198    lr: 4e-05     reward: 2.27\n",
      "epis: 1227   score: 3.0   mem len: 229594   epsilon: 0.7434    steps: 228    lr: 4e-05     reward: 2.27\n",
      "epis: 1228   score: 4.0   mem len: 229888   epsilon: 0.7428    steps: 294    lr: 4e-05     reward: 2.31\n",
      "epis: 1229   score: 2.0   mem len: 230088   epsilon: 0.7424    steps: 200    lr: 4e-05     reward: 2.32\n",
      "epis: 1230   score: 3.0   mem len: 230362   epsilon: 0.7419    steps: 274    lr: 4e-05     reward: 2.3\n",
      "epis: 1231   score: 3.0   mem len: 230588   epsilon: 0.7414    steps: 226    lr: 4e-05     reward: 2.32\n",
      "epis: 1232   score: 2.0   mem len: 230769   epsilon: 0.7411    steps: 181    lr: 4e-05     reward: 2.31\n",
      "epis: 1233   score: 2.0   mem len: 230951   epsilon: 0.7407    steps: 182    lr: 4e-05     reward: 2.29\n",
      "epis: 1234   score: 4.0   mem len: 231267   epsilon: 0.7401    steps: 316    lr: 4e-05     reward: 2.33\n",
      "epis: 1235   score: 2.0   mem len: 231485   epsilon: 0.7397    steps: 218    lr: 4e-05     reward: 2.3\n",
      "epis: 1236   score: 2.0   mem len: 231683   epsilon: 0.7393    steps: 198    lr: 4e-05     reward: 2.32\n",
      "epis: 1237   score: 2.0   mem len: 231881   epsilon: 0.7389    steps: 198    lr: 4e-05     reward: 2.32\n",
      "epis: 1238   score: 2.0   mem len: 232096   epsilon: 0.7384    steps: 215    lr: 4e-05     reward: 2.34\n",
      "epis: 1239   score: 1.0   mem len: 232247   epsilon: 0.7381    steps: 151    lr: 4e-05     reward: 2.31\n",
      "epis: 1240   score: 2.0   mem len: 232445   epsilon: 0.7378    steps: 198    lr: 4e-05     reward: 2.3\n",
      "epis: 1241   score: 5.0   mem len: 232746   epsilon: 0.7372    steps: 301    lr: 4e-05     reward: 2.34\n",
      "epis: 1242   score: 2.0   mem len: 232928   epsilon: 0.7368    steps: 182    lr: 4e-05     reward: 2.34\n",
      "epis: 1243   score: 2.0   mem len: 233126   epsilon: 0.7364    steps: 198    lr: 4e-05     reward: 2.35\n",
      "epis: 1244   score: 1.0   mem len: 233277   epsilon: 0.7361    steps: 151    lr: 4e-05     reward: 2.35\n",
      "epis: 1245   score: 2.0   mem len: 233477   epsilon: 0.7357    steps: 200    lr: 4e-05     reward: 2.35\n",
      "epis: 1246   score: 2.0   mem len: 233659   epsilon: 0.7354    steps: 182    lr: 4e-05     reward: 2.35\n",
      "epis: 1247   score: 1.0   mem len: 233810   epsilon: 0.7351    steps: 151    lr: 4e-05     reward: 2.36\n",
      "epis: 1248   score: 3.0   mem len: 234057   epsilon: 0.7346    steps: 247    lr: 4e-05     reward: 2.36\n",
      "epis: 1249   score: 2.0   mem len: 234280   epsilon: 0.7341    steps: 223    lr: 4e-05     reward: 2.34\n",
      "epis: 1250   score: 3.0   mem len: 234527   epsilon: 0.7336    steps: 247    lr: 4e-05     reward: 2.36\n",
      "epis: 1251   score: 3.0   mem len: 234752   epsilon: 0.7332    steps: 225    lr: 4e-05     reward: 2.35\n",
      "epis: 1252   score: 2.0   mem len: 234950   epsilon: 0.7328    steps: 198    lr: 4e-05     reward: 2.35\n",
      "epis: 1253   score: 2.0   mem len: 235150   epsilon: 0.7324    steps: 200    lr: 4e-05     reward: 2.32\n",
      "epis: 1254   score: 1.0   mem len: 235301   epsilon: 0.7321    steps: 151    lr: 4e-05     reward: 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1255   score: 1.0   mem len: 235452   epsilon: 0.7318    steps: 151    lr: 4e-05     reward: 2.32\n",
      "epis: 1256   score: 3.0   mem len: 235678   epsilon: 0.7314    steps: 226    lr: 4e-05     reward: 2.33\n",
      "epis: 1257   score: 2.0   mem len: 235878   epsilon: 0.731    steps: 200    lr: 4e-05     reward: 2.33\n",
      "epis: 1258   score: 5.0   mem len: 236200   epsilon: 0.7303    steps: 322    lr: 4e-05     reward: 2.34\n",
      "epis: 1259   score: 5.0   mem len: 236534   epsilon: 0.7297    steps: 334    lr: 4e-05     reward: 2.37\n",
      "epis: 1260   score: 1.0   mem len: 236685   epsilon: 0.7294    steps: 151    lr: 4e-05     reward: 2.37\n",
      "epis: 1261   score: 3.0   mem len: 236911   epsilon: 0.7289    steps: 226    lr: 4e-05     reward: 2.34\n",
      "epis: 1262   score: 2.0   mem len: 237111   epsilon: 0.7285    steps: 200    lr: 4e-05     reward: 2.34\n",
      "epis: 1263   score: 7.0   mem len: 237527   epsilon: 0.7277    steps: 416    lr: 4e-05     reward: 2.37\n",
      "epis: 1264   score: 3.0   mem len: 237753   epsilon: 0.7272    steps: 226    lr: 4e-05     reward: 2.39\n",
      "epis: 1265   score: 2.0   mem len: 237953   epsilon: 0.7269    steps: 200    lr: 4e-05     reward: 2.39\n",
      "epis: 1266   score: 1.0   mem len: 238122   epsilon: 0.7265    steps: 169    lr: 4e-05     reward: 2.4\n",
      "epis: 1267   score: 1.0   mem len: 238273   epsilon: 0.7262    steps: 151    lr: 4e-05     reward: 2.39\n",
      "epis: 1268   score: 2.0   mem len: 238471   epsilon: 0.7258    steps: 198    lr: 4e-05     reward: 2.37\n",
      "epis: 1269   score: 3.0   mem len: 238717   epsilon: 0.7253    steps: 246    lr: 4e-05     reward: 2.37\n",
      "epis: 1270   score: 3.0   mem len: 238985   epsilon: 0.7248    steps: 268    lr: 4e-05     reward: 2.35\n",
      "epis: 1271   score: 5.0   mem len: 239327   epsilon: 0.7241    steps: 342    lr: 4e-05     reward: 2.38\n",
      "epis: 1272   score: 5.0   mem len: 239636   epsilon: 0.7235    steps: 309    lr: 4e-05     reward: 2.41\n",
      "epis: 1273   score: 0.0   mem len: 239759   epsilon: 0.7233    steps: 123    lr: 4e-05     reward: 2.38\n",
      "epis: 1274   score: 2.0   mem len: 239957   epsilon: 0.7229    steps: 198    lr: 4e-05     reward: 2.38\n",
      "epis: 1275   score: 2.0   mem len: 240155   epsilon: 0.7225    steps: 198    lr: 4e-05     reward: 2.38\n",
      "epis: 1276   score: 2.0   mem len: 240372   epsilon: 0.7221    steps: 217    lr: 4e-05     reward: 2.4\n",
      "epis: 1277   score: 2.0   mem len: 240553   epsilon: 0.7217    steps: 181    lr: 4e-05     reward: 2.4\n",
      "epis: 1278   score: 2.0   mem len: 240754   epsilon: 0.7213    steps: 201    lr: 4e-05     reward: 2.41\n",
      "epis: 1279   score: 1.0   mem len: 240923   epsilon: 0.721    steps: 169    lr: 4e-05     reward: 2.41\n",
      "epis: 1280   score: 4.0   mem len: 241186   epsilon: 0.7204    steps: 263    lr: 4e-05     reward: 2.39\n",
      "epis: 1281   score: 4.0   mem len: 241461   epsilon: 0.7199    steps: 275    lr: 4e-05     reward: 2.43\n",
      "epis: 1282   score: 2.0   mem len: 241659   epsilon: 0.7195    steps: 198    lr: 4e-05     reward: 2.42\n",
      "epis: 1283   score: 3.0   mem len: 241885   epsilon: 0.7191    steps: 226    lr: 4e-05     reward: 2.4\n",
      "epis: 1284   score: 6.0   mem len: 242278   epsilon: 0.7183    steps: 393    lr: 4e-05     reward: 2.41\n",
      "epis: 1285   score: 5.0   mem len: 242584   epsilon: 0.7177    steps: 306    lr: 4e-05     reward: 2.45\n",
      "epis: 1286   score: 1.0   mem len: 242735   epsilon: 0.7174    steps: 151    lr: 4e-05     reward: 2.44\n",
      "epis: 1287   score: 2.0   mem len: 242933   epsilon: 0.717    steps: 198    lr: 4e-05     reward: 2.43\n",
      "epis: 1288   score: 2.0   mem len: 243131   epsilon: 0.7166    steps: 198    lr: 4e-05     reward: 2.42\n",
      "epis: 1289   score: 1.0   mem len: 243282   epsilon: 0.7163    steps: 151    lr: 4e-05     reward: 2.4\n",
      "epis: 1290   score: 4.0   mem len: 243582   epsilon: 0.7157    steps: 300    lr: 4e-05     reward: 2.42\n",
      "epis: 1291   score: 3.0   mem len: 243829   epsilon: 0.7152    steps: 247    lr: 4e-05     reward: 2.43\n",
      "epis: 1292   score: 4.0   mem len: 244105   epsilon: 0.7147    steps: 276    lr: 4e-05     reward: 2.47\n",
      "epis: 1293   score: 1.0   mem len: 244256   epsilon: 0.7144    steps: 151    lr: 4e-05     reward: 2.45\n",
      "epis: 1294   score: 2.0   mem len: 244454   epsilon: 0.714    steps: 198    lr: 4e-05     reward: 2.45\n",
      "epis: 1295   score: 2.0   mem len: 244636   epsilon: 0.7136    steps: 182    lr: 4e-05     reward: 2.44\n",
      "epis: 1296   score: 3.0   mem len: 244865   epsilon: 0.7132    steps: 229    lr: 4e-05     reward: 2.43\n",
      "epis: 1297   score: 3.0   mem len: 245091   epsilon: 0.7127    steps: 226    lr: 4e-05     reward: 2.46\n",
      "epis: 1298   score: 2.0   mem len: 245309   epsilon: 0.7123    steps: 218    lr: 4e-05     reward: 2.47\n",
      "epis: 1299   score: 2.0   mem len: 245528   epsilon: 0.7119    steps: 219    lr: 4e-05     reward: 2.48\n",
      "epis: 1300   score: 1.0   mem len: 245679   epsilon: 0.7116    steps: 151    lr: 4e-05     reward: 2.49\n",
      "epis: 1301   score: 3.0   mem len: 245905   epsilon: 0.7111    steps: 226    lr: 4e-05     reward: 2.49\n",
      "epis: 1302   score: 2.0   mem len: 246102   epsilon: 0.7107    steps: 197    lr: 4e-05     reward: 2.51\n",
      "epis: 1303   score: 0.0   mem len: 246224   epsilon: 0.7105    steps: 122    lr: 4e-05     reward: 2.49\n",
      "epis: 1304   score: 2.0   mem len: 246421   epsilon: 0.7101    steps: 197    lr: 4e-05     reward: 2.48\n",
      "epis: 1305   score: 2.0   mem len: 246637   epsilon: 0.7097    steps: 216    lr: 4e-05     reward: 2.46\n",
      "epis: 1306   score: 3.0   mem len: 246887   epsilon: 0.7092    steps: 250    lr: 4e-05     reward: 2.47\n",
      "epis: 1307   score: 3.0   mem len: 247130   epsilon: 0.7087    steps: 243    lr: 4e-05     reward: 2.49\n",
      "epis: 1308   score: 6.0   mem len: 247464   epsilon: 0.708    steps: 334    lr: 4e-05     reward: 2.51\n",
      "epis: 1309   score: 4.0   mem len: 247758   epsilon: 0.7074    steps: 294    lr: 4e-05     reward: 2.52\n",
      "epis: 1310   score: 2.0   mem len: 247956   epsilon: 0.707    steps: 198    lr: 4e-05     reward: 2.54\n",
      "epis: 1311   score: 3.0   mem len: 248226   epsilon: 0.7065    steps: 270    lr: 4e-05     reward: 2.57\n",
      "epis: 1312   score: 5.0   mem len: 248550   epsilon: 0.7059    steps: 324    lr: 4e-05     reward: 2.62\n",
      "epis: 1313   score: 9.0   mem len: 248876   epsilon: 0.7052    steps: 326    lr: 4e-05     reward: 2.68\n",
      "epis: 1314   score: 0.0   mem len: 248998   epsilon: 0.705    steps: 122    lr: 4e-05     reward: 2.67\n",
      "epis: 1315   score: 2.0   mem len: 249216   epsilon: 0.7046    steps: 218    lr: 4e-05     reward: 2.61\n",
      "epis: 1316   score: 3.0   mem len: 249484   epsilon: 0.704    steps: 268    lr: 4e-05     reward: 2.62\n",
      "epis: 1317   score: 1.0   mem len: 249634   epsilon: 0.7037    steps: 150    lr: 4e-05     reward: 2.61\n",
      "epis: 1318   score: 4.0   mem len: 249908   epsilon: 0.7032    steps: 274    lr: 4e-05     reward: 2.64\n",
      "epis: 1319   score: 1.0   mem len: 250058   epsilon: 0.7029    steps: 150    lr: 4e-05     reward: 2.62\n",
      "epis: 1320   score: 1.0   mem len: 250227   epsilon: 0.7025    steps: 169    lr: 4e-05     reward: 2.59\n",
      "epis: 1321   score: 5.0   mem len: 250569   epsilon: 0.7019    steps: 342    lr: 4e-05     reward: 2.62\n",
      "epis: 1322   score: 2.0   mem len: 250788   epsilon: 0.7014    steps: 219    lr: 4e-05     reward: 2.59\n",
      "epis: 1323   score: 3.0   mem len: 251034   epsilon: 0.701    steps: 246    lr: 4e-05     reward: 2.59\n",
      "epis: 1324   score: 4.0   mem len: 251352   epsilon: 0.7003    steps: 318    lr: 4e-05     reward: 2.61\n",
      "epis: 1325   score: 2.0   mem len: 251570   epsilon: 0.6999    steps: 218    lr: 4e-05     reward: 2.61\n",
      "epis: 1326   score: 2.0   mem len: 251787   epsilon: 0.6995    steps: 217    lr: 4e-05     reward: 2.61\n",
      "epis: 1327   score: 2.0   mem len: 252005   epsilon: 0.699    steps: 218    lr: 4e-05     reward: 2.6\n",
      "epis: 1328   score: 2.0   mem len: 252185   epsilon: 0.6987    steps: 180    lr: 4e-05     reward: 2.58\n",
      "epis: 1329   score: 0.0   mem len: 252308   epsilon: 0.6984    steps: 123    lr: 4e-05     reward: 2.56\n",
      "epis: 1330   score: 1.0   mem len: 252476   epsilon: 0.6981    steps: 168    lr: 4e-05     reward: 2.54\n",
      "epis: 1331   score: 2.0   mem len: 252674   epsilon: 0.6977    steps: 198    lr: 4e-05     reward: 2.53\n",
      "epis: 1332   score: 1.0   mem len: 252843   epsilon: 0.6974    steps: 169    lr: 4e-05     reward: 2.52\n",
      "epis: 1333   score: 6.0   mem len: 253193   epsilon: 0.6967    steps: 350    lr: 4e-05     reward: 2.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1334   score: 5.0   mem len: 253539   epsilon: 0.696    steps: 346    lr: 4e-05     reward: 2.57\n",
      "epis: 1335   score: 2.0   mem len: 253736   epsilon: 0.6956    steps: 197    lr: 4e-05     reward: 2.57\n",
      "epis: 1336   score: 5.0   mem len: 254017   epsilon: 0.695    steps: 281    lr: 4e-05     reward: 2.6\n",
      "epis: 1337   score: 2.0   mem len: 254215   epsilon: 0.6947    steps: 198    lr: 4e-05     reward: 2.6\n",
      "epis: 1338   score: 5.0   mem len: 254540   epsilon: 0.694    steps: 325    lr: 4e-05     reward: 2.63\n",
      "epis: 1339   score: 6.0   mem len: 254935   epsilon: 0.6932    steps: 395    lr: 4e-05     reward: 2.68\n",
      "epis: 1340   score: 3.0   mem len: 255182   epsilon: 0.6927    steps: 247    lr: 4e-05     reward: 2.69\n",
      "epis: 1341   score: 1.0   mem len: 255333   epsilon: 0.6924    steps: 151    lr: 4e-05     reward: 2.65\n",
      "epis: 1342   score: 4.0   mem len: 255649   epsilon: 0.6918    steps: 316    lr: 4e-05     reward: 2.67\n",
      "epis: 1343   score: 3.0   mem len: 255875   epsilon: 0.6914    steps: 226    lr: 4e-05     reward: 2.68\n",
      "epis: 1344   score: 3.0   mem len: 256101   epsilon: 0.6909    steps: 226    lr: 4e-05     reward: 2.7\n",
      "epis: 1345   score: 3.0   mem len: 256364   epsilon: 0.6904    steps: 263    lr: 4e-05     reward: 2.71\n",
      "epis: 1346   score: 2.0   mem len: 256545   epsilon: 0.69    steps: 181    lr: 4e-05     reward: 2.71\n",
      "epis: 1347   score: 1.0   mem len: 256716   epsilon: 0.6897    steps: 171    lr: 4e-05     reward: 2.71\n",
      "epis: 1348   score: 2.0   mem len: 256936   epsilon: 0.6893    steps: 220    lr: 4e-05     reward: 2.7\n",
      "epis: 1349   score: 1.0   mem len: 257087   epsilon: 0.689    steps: 151    lr: 4e-05     reward: 2.69\n",
      "epis: 1350   score: 6.0   mem len: 257441   epsilon: 0.6883    steps: 354    lr: 4e-05     reward: 2.72\n",
      "epis: 1351   score: 4.0   mem len: 257744   epsilon: 0.6877    steps: 303    lr: 4e-05     reward: 2.73\n",
      "epis: 1352   score: 3.0   mem len: 257972   epsilon: 0.6872    steps: 228    lr: 4e-05     reward: 2.74\n",
      "epis: 1353   score: 3.0   mem len: 258198   epsilon: 0.6868    steps: 226    lr: 4e-05     reward: 2.75\n",
      "epis: 1354   score: 2.0   mem len: 258380   epsilon: 0.6864    steps: 182    lr: 4e-05     reward: 2.76\n",
      "epis: 1355   score: 4.0   mem len: 258677   epsilon: 0.6858    steps: 297    lr: 4e-05     reward: 2.79\n",
      "epis: 1356   score: 2.0   mem len: 258875   epsilon: 0.6854    steps: 198    lr: 4e-05     reward: 2.78\n",
      "epis: 1357   score: 1.0   mem len: 259047   epsilon: 0.6851    steps: 172    lr: 4e-05     reward: 2.77\n",
      "epis: 1358   score: 1.0   mem len: 259219   epsilon: 0.6847    steps: 172    lr: 4e-05     reward: 2.73\n",
      "epis: 1359   score: 4.0   mem len: 259493   epsilon: 0.6842    steps: 274    lr: 4e-05     reward: 2.72\n",
      "epis: 1360   score: 2.0   mem len: 259691   epsilon: 0.6838    steps: 198    lr: 4e-05     reward: 2.73\n",
      "epis: 1361   score: 1.0   mem len: 259842   epsilon: 0.6835    steps: 151    lr: 4e-05     reward: 2.71\n",
      "epis: 1362   score: 2.0   mem len: 260060   epsilon: 0.6831    steps: 218    lr: 4e-05     reward: 2.71\n",
      "epis: 1363   score: 1.0   mem len: 260211   epsilon: 0.6828    steps: 151    lr: 4e-05     reward: 2.65\n",
      "epis: 1364   score: 3.0   mem len: 260457   epsilon: 0.6823    steps: 246    lr: 4e-05     reward: 2.65\n",
      "epis: 1365   score: 2.0   mem len: 260639   epsilon: 0.6819    steps: 182    lr: 4e-05     reward: 2.65\n",
      "epis: 1366   score: 7.0   mem len: 261014   epsilon: 0.6812    steps: 375    lr: 4e-05     reward: 2.71\n",
      "epis: 1367   score: 3.0   mem len: 261243   epsilon: 0.6807    steps: 229    lr: 4e-05     reward: 2.73\n",
      "epis: 1368   score: 2.0   mem len: 261441   epsilon: 0.6803    steps: 198    lr: 4e-05     reward: 2.73\n",
      "epis: 1369   score: 3.0   mem len: 261688   epsilon: 0.6799    steps: 247    lr: 4e-05     reward: 2.73\n",
      "epis: 1370   score: 2.0   mem len: 261870   epsilon: 0.6795    steps: 182    lr: 4e-05     reward: 2.72\n",
      "epis: 1371   score: 2.0   mem len: 262086   epsilon: 0.6791    steps: 216    lr: 4e-05     reward: 2.69\n",
      "epis: 1372   score: 1.0   mem len: 262256   epsilon: 0.6787    steps: 170    lr: 4e-05     reward: 2.65\n",
      "epis: 1373   score: 2.0   mem len: 262454   epsilon: 0.6783    steps: 198    lr: 4e-05     reward: 2.67\n",
      "epis: 1374   score: 5.0   mem len: 262779   epsilon: 0.6777    steps: 325    lr: 4e-05     reward: 2.7\n",
      "epis: 1375   score: 4.0   mem len: 263054   epsilon: 0.6772    steps: 275    lr: 4e-05     reward: 2.72\n",
      "epis: 1376   score: 2.0   mem len: 263254   epsilon: 0.6768    steps: 200    lr: 4e-05     reward: 2.72\n",
      "epis: 1377   score: 3.0   mem len: 263499   epsilon: 0.6763    steps: 245    lr: 4e-05     reward: 2.73\n",
      "epis: 1378   score: 3.0   mem len: 263742   epsilon: 0.6758    steps: 243    lr: 4e-05     reward: 2.74\n",
      "epis: 1379   score: 3.0   mem len: 263989   epsilon: 0.6753    steps: 247    lr: 4e-05     reward: 2.76\n",
      "epis: 1380   score: 2.0   mem len: 264187   epsilon: 0.6749    steps: 198    lr: 4e-05     reward: 2.74\n",
      "epis: 1381   score: 3.0   mem len: 264433   epsilon: 0.6744    steps: 246    lr: 4e-05     reward: 2.73\n",
      "epis: 1382   score: 5.0   mem len: 264757   epsilon: 0.6738    steps: 324    lr: 4e-05     reward: 2.76\n",
      "epis: 1383   score: 2.0   mem len: 264938   epsilon: 0.6734    steps: 181    lr: 4e-05     reward: 2.75\n",
      "epis: 1384   score: 3.0   mem len: 265185   epsilon: 0.6729    steps: 247    lr: 4e-05     reward: 2.72\n",
      "epis: 1385   score: 2.0   mem len: 265367   epsilon: 0.6726    steps: 182    lr: 4e-05     reward: 2.69\n",
      "epis: 1386   score: 2.0   mem len: 265549   epsilon: 0.6722    steps: 182    lr: 4e-05     reward: 2.7\n",
      "epis: 1387   score: 2.0   mem len: 265747   epsilon: 0.6718    steps: 198    lr: 4e-05     reward: 2.7\n",
      "epis: 1388   score: 3.0   mem len: 265973   epsilon: 0.6714    steps: 226    lr: 4e-05     reward: 2.71\n",
      "epis: 1389   score: 2.0   mem len: 266171   epsilon: 0.671    steps: 198    lr: 4e-05     reward: 2.72\n",
      "epis: 1390   score: 4.0   mem len: 266471   epsilon: 0.6704    steps: 300    lr: 4e-05     reward: 2.72\n",
      "epis: 1391   score: 1.0   mem len: 266622   epsilon: 0.6701    steps: 151    lr: 4e-05     reward: 2.7\n",
      "epis: 1392   score: 2.0   mem len: 266820   epsilon: 0.6697    steps: 198    lr: 4e-05     reward: 2.68\n",
      "epis: 1393   score: 2.0   mem len: 267018   epsilon: 0.6693    steps: 198    lr: 4e-05     reward: 2.69\n",
      "epis: 1394   score: 2.0   mem len: 267215   epsilon: 0.6689    steps: 197    lr: 4e-05     reward: 2.69\n",
      "epis: 1395   score: 2.0   mem len: 267412   epsilon: 0.6685    steps: 197    lr: 4e-05     reward: 2.69\n",
      "epis: 1396   score: 5.0   mem len: 267709   epsilon: 0.6679    steps: 297    lr: 4e-05     reward: 2.71\n",
      "epis: 1397   score: 2.0   mem len: 267907   epsilon: 0.6675    steps: 198    lr: 4e-05     reward: 2.7\n",
      "epis: 1398   score: 4.0   mem len: 268203   epsilon: 0.667    steps: 296    lr: 4e-05     reward: 2.72\n",
      "epis: 1399   score: 7.0   mem len: 268575   epsilon: 0.6662    steps: 372    lr: 4e-05     reward: 2.77\n",
      "epis: 1400   score: 3.0   mem len: 268803   epsilon: 0.6658    steps: 228    lr: 4e-05     reward: 2.79\n",
      "epis: 1401   score: 2.0   mem len: 269001   epsilon: 0.6654    steps: 198    lr: 4e-05     reward: 2.78\n",
      "epis: 1402   score: 4.0   mem len: 269276   epsilon: 0.6648    steps: 275    lr: 4e-05     reward: 2.8\n",
      "epis: 1403   score: 11.0   mem len: 269711   epsilon: 0.664    steps: 435    lr: 4e-05     reward: 2.91\n",
      "epis: 1404   score: 3.0   mem len: 269978   epsilon: 0.6634    steps: 267    lr: 4e-05     reward: 2.92\n",
      "epis: 1405   score: 6.0   mem len: 270351   epsilon: 0.6627    steps: 373    lr: 4e-05     reward: 2.96\n",
      "epis: 1406   score: 4.0   mem len: 270629   epsilon: 0.6622    steps: 278    lr: 4e-05     reward: 2.97\n",
      "epis: 1407   score: 3.0   mem len: 270860   epsilon: 0.6617    steps: 231    lr: 4e-05     reward: 2.97\n",
      "epis: 1408   score: 4.0   mem len: 271174   epsilon: 0.6611    steps: 314    lr: 4e-05     reward: 2.95\n",
      "epis: 1409   score: 3.0   mem len: 271421   epsilon: 0.6606    steps: 247    lr: 4e-05     reward: 2.94\n",
      "epis: 1410   score: 3.0   mem len: 271672   epsilon: 0.6601    steps: 251    lr: 4e-05     reward: 2.95\n",
      "epis: 1411   score: 3.0   mem len: 271942   epsilon: 0.6596    steps: 270    lr: 4e-05     reward: 2.95\n",
      "epis: 1412   score: 4.0   mem len: 272240   epsilon: 0.659    steps: 298    lr: 4e-05     reward: 2.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1413   score: 1.0   mem len: 272390   epsilon: 0.6587    steps: 150    lr: 4e-05     reward: 2.86\n",
      "epis: 1414   score: 4.0   mem len: 272666   epsilon: 0.6581    steps: 276    lr: 4e-05     reward: 2.9\n",
      "epis: 1415   score: 4.0   mem len: 272941   epsilon: 0.6576    steps: 275    lr: 4e-05     reward: 2.92\n",
      "epis: 1416   score: 3.0   mem len: 273191   epsilon: 0.6571    steps: 250    lr: 4e-05     reward: 2.92\n",
      "epis: 1417   score: 2.0   mem len: 273370   epsilon: 0.6567    steps: 179    lr: 4e-05     reward: 2.93\n",
      "epis: 1418   score: 2.0   mem len: 273568   epsilon: 0.6563    steps: 198    lr: 4e-05     reward: 2.91\n",
      "epis: 1419   score: 2.0   mem len: 273749   epsilon: 0.656    steps: 181    lr: 4e-05     reward: 2.92\n",
      "epis: 1420   score: 0.0   mem len: 273872   epsilon: 0.6557    steps: 123    lr: 4e-05     reward: 2.91\n",
      "epis: 1421   score: 4.0   mem len: 274148   epsilon: 0.6552    steps: 276    lr: 4e-05     reward: 2.9\n",
      "epis: 1422   score: 4.0   mem len: 274403   epsilon: 0.6547    steps: 255    lr: 4e-05     reward: 2.92\n",
      "epis: 1423   score: 3.0   mem len: 274648   epsilon: 0.6542    steps: 245    lr: 4e-05     reward: 2.92\n",
      "epis: 1424   score: 4.0   mem len: 274945   epsilon: 0.6536    steps: 297    lr: 4e-05     reward: 2.92\n",
      "epis: 1425   score: 4.0   mem len: 275222   epsilon: 0.6531    steps: 277    lr: 4e-05     reward: 2.94\n",
      "epis: 1426   score: 3.0   mem len: 275448   epsilon: 0.6526    steps: 226    lr: 4e-05     reward: 2.95\n",
      "epis: 1427   score: 3.0   mem len: 275694   epsilon: 0.6521    steps: 246    lr: 4e-05     reward: 2.96\n",
      "epis: 1428   score: 2.0   mem len: 275875   epsilon: 0.6518    steps: 181    lr: 4e-05     reward: 2.96\n",
      "epis: 1429   score: 6.0   mem len: 276231   epsilon: 0.6511    steps: 356    lr: 4e-05     reward: 3.02\n",
      "epis: 1430   score: 4.0   mem len: 276489   epsilon: 0.6505    steps: 258    lr: 4e-05     reward: 3.05\n",
      "epis: 1431   score: 5.0   mem len: 276793   epsilon: 0.6499    steps: 304    lr: 4e-05     reward: 3.08\n",
      "epis: 1432   score: 2.0   mem len: 276991   epsilon: 0.6496    steps: 198    lr: 4e-05     reward: 3.09\n",
      "epis: 1433   score: 3.0   mem len: 277217   epsilon: 0.6491    steps: 226    lr: 4e-05     reward: 3.06\n",
      "epis: 1434   score: 4.0   mem len: 277492   epsilon: 0.6486    steps: 275    lr: 4e-05     reward: 3.05\n",
      "epis: 1435   score: 2.0   mem len: 277690   epsilon: 0.6482    steps: 198    lr: 4e-05     reward: 3.05\n",
      "epis: 1436   score: 2.0   mem len: 277890   epsilon: 0.6478    steps: 200    lr: 4e-05     reward: 3.02\n",
      "epis: 1437   score: 6.0   mem len: 278263   epsilon: 0.647    steps: 373    lr: 4e-05     reward: 3.06\n",
      "epis: 1438   score: 1.0   mem len: 278432   epsilon: 0.6467    steps: 169    lr: 4e-05     reward: 3.02\n",
      "epis: 1439   score: 5.0   mem len: 278728   epsilon: 0.6461    steps: 296    lr: 4e-05     reward: 3.01\n",
      "epis: 1440   score: 5.0   mem len: 279052   epsilon: 0.6455    steps: 324    lr: 4e-05     reward: 3.03\n",
      "epis: 1441   score: 6.0   mem len: 279425   epsilon: 0.6447    steps: 373    lr: 4e-05     reward: 3.08\n",
      "epis: 1442   score: 6.0   mem len: 279759   epsilon: 0.6441    steps: 334    lr: 4e-05     reward: 3.1\n",
      "epis: 1443   score: 7.0   mem len: 280149   epsilon: 0.6433    steps: 390    lr: 4e-05     reward: 3.14\n",
      "epis: 1444   score: 0.0   mem len: 280272   epsilon: 0.6431    steps: 123    lr: 4e-05     reward: 3.11\n",
      "epis: 1445   score: 3.0   mem len: 280519   epsilon: 0.6426    steps: 247    lr: 4e-05     reward: 3.11\n",
      "epis: 1446   score: 3.0   mem len: 280744   epsilon: 0.6421    steps: 225    lr: 4e-05     reward: 3.12\n",
      "epis: 1447   score: 0.0   mem len: 280866   epsilon: 0.6419    steps: 122    lr: 4e-05     reward: 3.11\n",
      "epis: 1448   score: 3.0   mem len: 281092   epsilon: 0.6414    steps: 226    lr: 4e-05     reward: 3.12\n",
      "epis: 1449   score: 4.0   mem len: 281389   epsilon: 0.6408    steps: 297    lr: 4e-05     reward: 3.15\n",
      "epis: 1450   score: 5.0   mem len: 281718   epsilon: 0.6402    steps: 329    lr: 4e-05     reward: 3.14\n",
      "epis: 1451   score: 0.0   mem len: 281840   epsilon: 0.64    steps: 122    lr: 4e-05     reward: 3.1\n",
      "epis: 1452   score: 3.0   mem len: 282066   epsilon: 0.6395    steps: 226    lr: 4e-05     reward: 3.1\n",
      "epis: 1453   score: 2.0   mem len: 282266   epsilon: 0.6391    steps: 200    lr: 4e-05     reward: 3.09\n",
      "epis: 1454   score: 4.0   mem len: 282525   epsilon: 0.6386    steps: 259    lr: 4e-05     reward: 3.11\n",
      "epis: 1455   score: 3.0   mem len: 282754   epsilon: 0.6381    steps: 229    lr: 4e-05     reward: 3.1\n",
      "epis: 1456   score: 4.0   mem len: 283049   epsilon: 0.6376    steps: 295    lr: 4e-05     reward: 3.12\n",
      "epis: 1457   score: 5.0   mem len: 283393   epsilon: 0.6369    steps: 344    lr: 4e-05     reward: 3.16\n",
      "epis: 1458   score: 3.0   mem len: 283622   epsilon: 0.6364    steps: 229    lr: 4e-05     reward: 3.18\n",
      "epis: 1459   score: 1.0   mem len: 283773   epsilon: 0.6361    steps: 151    lr: 4e-05     reward: 3.15\n",
      "epis: 1460   score: 3.0   mem len: 284021   epsilon: 0.6356    steps: 248    lr: 4e-05     reward: 3.16\n",
      "epis: 1461   score: 4.0   mem len: 284316   epsilon: 0.6351    steps: 295    lr: 4e-05     reward: 3.19\n",
      "epis: 1462   score: 3.0   mem len: 284547   epsilon: 0.6346    steps: 231    lr: 4e-05     reward: 3.2\n",
      "epis: 1463   score: 3.0   mem len: 284776   epsilon: 0.6341    steps: 229    lr: 4e-05     reward: 3.22\n",
      "epis: 1464   score: 8.0   mem len: 285174   epsilon: 0.6334    steps: 398    lr: 4e-05     reward: 3.27\n",
      "epis: 1465   score: 2.0   mem len: 285374   epsilon: 0.633    steps: 200    lr: 4e-05     reward: 3.27\n",
      "epis: 1466   score: 3.0   mem len: 285600   epsilon: 0.6325    steps: 226    lr: 4e-05     reward: 3.23\n",
      "epis: 1467   score: 0.0   mem len: 285723   epsilon: 0.6323    steps: 123    lr: 4e-05     reward: 3.2\n",
      "epis: 1468   score: 3.0   mem len: 285949   epsilon: 0.6318    steps: 226    lr: 4e-05     reward: 3.21\n",
      "epis: 1469   score: 0.0   mem len: 286072   epsilon: 0.6316    steps: 123    lr: 4e-05     reward: 3.18\n",
      "epis: 1470   score: 2.0   mem len: 286270   epsilon: 0.6312    steps: 198    lr: 4e-05     reward: 3.18\n",
      "epis: 1471   score: 3.0   mem len: 286495   epsilon: 0.6307    steps: 225    lr: 4e-05     reward: 3.19\n",
      "epis: 1472   score: 2.0   mem len: 286692   epsilon: 0.6303    steps: 197    lr: 4e-05     reward: 3.2\n",
      "epis: 1473   score: 2.0   mem len: 286872   epsilon: 0.63    steps: 180    lr: 4e-05     reward: 3.2\n",
      "epis: 1474   score: 3.0   mem len: 287102   epsilon: 0.6295    steps: 230    lr: 4e-05     reward: 3.18\n",
      "epis: 1475   score: 6.0   mem len: 287465   epsilon: 0.6288    steps: 363    lr: 4e-05     reward: 3.2\n",
      "epis: 1476   score: 6.0   mem len: 287805   epsilon: 0.6281    steps: 340    lr: 4e-05     reward: 3.24\n",
      "epis: 1477   score: 3.0   mem len: 288036   epsilon: 0.6277    steps: 231    lr: 4e-05     reward: 3.24\n",
      "epis: 1478   score: 5.0   mem len: 288381   epsilon: 0.627    steps: 345    lr: 4e-05     reward: 3.26\n",
      "epis: 1479   score: 4.0   mem len: 288638   epsilon: 0.6265    steps: 257    lr: 4e-05     reward: 3.27\n",
      "epis: 1480   score: 3.0   mem len: 288863   epsilon: 0.626    steps: 225    lr: 4e-05     reward: 3.28\n",
      "epis: 1481   score: 4.0   mem len: 289138   epsilon: 0.6255    steps: 275    lr: 4e-05     reward: 3.29\n",
      "epis: 1482   score: 2.0   mem len: 289320   epsilon: 0.6251    steps: 182    lr: 4e-05     reward: 3.26\n",
      "epis: 1483   score: 2.0   mem len: 289517   epsilon: 0.6248    steps: 197    lr: 4e-05     reward: 3.26\n",
      "epis: 1484   score: 2.0   mem len: 289715   epsilon: 0.6244    steps: 198    lr: 4e-05     reward: 3.25\n",
      "epis: 1485   score: 0.0   mem len: 289838   epsilon: 0.6241    steps: 123    lr: 4e-05     reward: 3.23\n",
      "epis: 1486   score: 4.0   mem len: 290134   epsilon: 0.6235    steps: 296    lr: 4e-05     reward: 3.25\n",
      "epis: 1487   score: 2.0   mem len: 290334   epsilon: 0.6231    steps: 200    lr: 4e-05     reward: 3.25\n",
      "epis: 1488   score: 3.0   mem len: 290560   epsilon: 0.6227    steps: 226    lr: 4e-05     reward: 3.25\n",
      "epis: 1489   score: 2.0   mem len: 290757   epsilon: 0.6223    steps: 197    lr: 4e-05     reward: 3.25\n",
      "epis: 1490   score: 6.0   mem len: 291114   epsilon: 0.6216    steps: 357    lr: 4e-05     reward: 3.27\n",
      "epis: 1491   score: 5.0   mem len: 291429   epsilon: 0.621    steps: 315    lr: 4e-05     reward: 3.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1492   score: 3.0   mem len: 291655   epsilon: 0.6205    steps: 226    lr: 4e-05     reward: 3.32\n",
      "epis: 1493   score: 4.0   mem len: 291935   epsilon: 0.62    steps: 280    lr: 4e-05     reward: 3.34\n",
      "epis: 1494   score: 3.0   mem len: 292160   epsilon: 0.6195    steps: 225    lr: 4e-05     reward: 3.35\n",
      "epis: 1495   score: 4.0   mem len: 292453   epsilon: 0.6189    steps: 293    lr: 4e-05     reward: 3.37\n",
      "epis: 1496   score: 3.0   mem len: 292682   epsilon: 0.6185    steps: 229    lr: 4e-05     reward: 3.35\n",
      "epis: 1497   score: 2.0   mem len: 292880   epsilon: 0.6181    steps: 198    lr: 4e-05     reward: 3.35\n",
      "epis: 1498   score: 3.0   mem len: 293106   epsilon: 0.6176    steps: 226    lr: 4e-05     reward: 3.34\n",
      "epis: 1499   score: 3.0   mem len: 293332   epsilon: 0.6172    steps: 226    lr: 4e-05     reward: 3.3\n",
      "epis: 1500   score: 5.0   mem len: 293620   epsilon: 0.6166    steps: 288    lr: 4e-05     reward: 3.32\n",
      "epis: 1501   score: 1.0   mem len: 293771   epsilon: 0.6163    steps: 151    lr: 4e-05     reward: 3.31\n",
      "epis: 1502   score: 2.0   mem len: 293952   epsilon: 0.616    steps: 181    lr: 4e-05     reward: 3.29\n",
      "epis: 1503   score: 4.0   mem len: 294248   epsilon: 0.6154    steps: 296    lr: 4e-05     reward: 3.22\n",
      "epis: 1504   score: 4.0   mem len: 294523   epsilon: 0.6148    steps: 275    lr: 4e-05     reward: 3.23\n",
      "epis: 1505   score: 3.0   mem len: 294768   epsilon: 0.6144    steps: 245    lr: 4e-05     reward: 3.2\n",
      "epis: 1506   score: 1.0   mem len: 294918   epsilon: 0.6141    steps: 150    lr: 4e-05     reward: 3.17\n",
      "epis: 1507   score: 4.0   mem len: 295193   epsilon: 0.6135    steps: 275    lr: 4e-05     reward: 3.18\n",
      "epis: 1508   score: 4.0   mem len: 295451   epsilon: 0.613    steps: 258    lr: 4e-05     reward: 3.18\n",
      "epis: 1509   score: 2.0   mem len: 295648   epsilon: 0.6126    steps: 197    lr: 4e-05     reward: 3.17\n",
      "epis: 1510   score: 5.0   mem len: 295992   epsilon: 0.6119    steps: 344    lr: 4e-05     reward: 3.19\n",
      "epis: 1511   score: 3.0   mem len: 296224   epsilon: 0.6115    steps: 232    lr: 4e-05     reward: 3.19\n",
      "epis: 1512   score: 5.0   mem len: 296547   epsilon: 0.6108    steps: 323    lr: 4e-05     reward: 3.2\n",
      "epis: 1513   score: 3.0   mem len: 296773   epsilon: 0.6104    steps: 226    lr: 4e-05     reward: 3.22\n",
      "epis: 1514   score: 1.0   mem len: 296924   epsilon: 0.6101    steps: 151    lr: 4e-05     reward: 3.19\n",
      "epis: 1515   score: 3.0   mem len: 297136   epsilon: 0.6097    steps: 212    lr: 4e-05     reward: 3.18\n",
      "epis: 1516   score: 8.0   mem len: 297444   epsilon: 0.6091    steps: 308    lr: 4e-05     reward: 3.23\n",
      "epis: 1517   score: 5.0   mem len: 297759   epsilon: 0.6084    steps: 315    lr: 4e-05     reward: 3.26\n",
      "epis: 1518   score: 2.0   mem len: 297941   epsilon: 0.6081    steps: 182    lr: 4e-05     reward: 3.26\n",
      "epis: 1519   score: 2.0   mem len: 298122   epsilon: 0.6077    steps: 181    lr: 4e-05     reward: 3.26\n",
      "epis: 1520   score: 3.0   mem len: 298347   epsilon: 0.6073    steps: 225    lr: 4e-05     reward: 3.29\n",
      "epis: 1521   score: 1.0   mem len: 298497   epsilon: 0.607    steps: 150    lr: 4e-05     reward: 3.26\n",
      "epis: 1522   score: 3.0   mem len: 298743   epsilon: 0.6065    steps: 246    lr: 4e-05     reward: 3.25\n",
      "epis: 1523   score: 3.0   mem len: 298969   epsilon: 0.606    steps: 226    lr: 4e-05     reward: 3.25\n",
      "epis: 1524   score: 1.0   mem len: 299140   epsilon: 0.6057    steps: 171    lr: 4e-05     reward: 3.22\n",
      "epis: 1525   score: 3.0   mem len: 299386   epsilon: 0.6052    steps: 246    lr: 4e-05     reward: 3.21\n",
      "epis: 1526   score: 3.0   mem len: 299614   epsilon: 0.6048    steps: 228    lr: 4e-05     reward: 3.21\n",
      "epis: 1527   score: 4.0   mem len: 299889   epsilon: 0.6042    steps: 275    lr: 4e-05     reward: 3.22\n",
      "epis: 1528   score: 3.0   mem len: 300115   epsilon: 0.6038    steps: 226    lr: 1.6e-05     reward: 3.23\n",
      "epis: 1529   score: 4.0   mem len: 300412   epsilon: 0.6032    steps: 297    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1530   score: 2.0   mem len: 300594   epsilon: 0.6028    steps: 182    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1531   score: 4.0   mem len: 300869   epsilon: 0.6023    steps: 275    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1532   score: 4.0   mem len: 301164   epsilon: 0.6017    steps: 295    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1533   score: 4.0   mem len: 301459   epsilon: 0.6011    steps: 295    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1534   score: 3.0   mem len: 301685   epsilon: 0.6007    steps: 226    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1535   score: 1.0   mem len: 301854   epsilon: 0.6003    steps: 169    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1536   score: 4.0   mem len: 302152   epsilon: 0.5997    steps: 298    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1537   score: 5.0   mem len: 302455   epsilon: 0.5991    steps: 303    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1538   score: 3.0   mem len: 302701   epsilon: 0.5987    steps: 246    lr: 1.6e-05     reward: 3.22\n",
      "epis: 1539   score: 3.0   mem len: 302927   epsilon: 0.5982    steps: 226    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1540   score: 2.0   mem len: 303109   epsilon: 0.5978    steps: 182    lr: 1.6e-05     reward: 3.17\n",
      "epis: 1541   score: 4.0   mem len: 303397   epsilon: 0.5973    steps: 288    lr: 1.6e-05     reward: 3.15\n",
      "epis: 1542   score: 1.0   mem len: 303548   epsilon: 0.597    steps: 151    lr: 1.6e-05     reward: 3.1\n",
      "epis: 1543   score: 3.0   mem len: 303773   epsilon: 0.5965    steps: 225    lr: 1.6e-05     reward: 3.06\n",
      "epis: 1544   score: 2.0   mem len: 303973   epsilon: 0.5961    steps: 200    lr: 1.6e-05     reward: 3.08\n",
      "epis: 1545   score: 3.0   mem len: 304217   epsilon: 0.5956    steps: 244    lr: 1.6e-05     reward: 3.08\n",
      "epis: 1546   score: 4.0   mem len: 304492   epsilon: 0.5951    steps: 275    lr: 1.6e-05     reward: 3.09\n",
      "epis: 1547   score: 4.0   mem len: 304767   epsilon: 0.5946    steps: 275    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1548   score: 3.0   mem len: 304994   epsilon: 0.5941    steps: 227    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1549   score: 4.0   mem len: 305288   epsilon: 0.5935    steps: 294    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1550   score: 2.0   mem len: 305470   epsilon: 0.5932    steps: 182    lr: 1.6e-05     reward: 3.1\n",
      "epis: 1551   score: 4.0   mem len: 305745   epsilon: 0.5926    steps: 275    lr: 1.6e-05     reward: 3.14\n",
      "epis: 1552   score: 2.0   mem len: 305925   epsilon: 0.5923    steps: 180    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1553   score: 2.0   mem len: 306123   epsilon: 0.5919    steps: 198    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1554   score: 4.0   mem len: 306397   epsilon: 0.5913    steps: 274    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1555   score: 6.0   mem len: 306740   epsilon: 0.5907    steps: 343    lr: 1.6e-05     reward: 3.16\n",
      "epis: 1556   score: 4.0   mem len: 307035   epsilon: 0.5901    steps: 295    lr: 1.6e-05     reward: 3.16\n",
      "epis: 1557   score: 4.0   mem len: 307331   epsilon: 0.5895    steps: 296    lr: 1.6e-05     reward: 3.15\n",
      "epis: 1558   score: 5.0   mem len: 307652   epsilon: 0.5888    steps: 321    lr: 1.6e-05     reward: 3.17\n",
      "epis: 1559   score: 4.0   mem len: 307927   epsilon: 0.5883    steps: 275    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1560   score: 2.0   mem len: 308125   epsilon: 0.5879    steps: 198    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1561   score: 4.0   mem len: 308400   epsilon: 0.5874    steps: 275    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1562   score: 5.0   mem len: 308688   epsilon: 0.5868    steps: 288    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1563   score: 4.0   mem len: 308966   epsilon: 0.5862    steps: 278    lr: 1.6e-05     reward: 3.22\n",
      "epis: 1564   score: 4.0   mem len: 309241   epsilon: 0.5857    steps: 275    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1565   score: 2.0   mem len: 309423   epsilon: 0.5853    steps: 182    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1566   score: 3.0   mem len: 309649   epsilon: 0.5849    steps: 226    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1567   score: 3.0   mem len: 309875   epsilon: 0.5844    steps: 226    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1568   score: 2.0   mem len: 310076   epsilon: 0.584    steps: 201    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1569   score: 3.0   mem len: 310342   epsilon: 0.5835    steps: 266    lr: 1.6e-05     reward: 3.23\n",
      "epis: 1570   score: 5.0   mem len: 310631   epsilon: 0.5829    steps: 289    lr: 1.6e-05     reward: 3.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1571   score: 2.0   mem len: 310811   epsilon: 0.5826    steps: 180    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1572   score: 5.0   mem len: 311127   epsilon: 0.582    steps: 316    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1573   score: 2.0   mem len: 311309   epsilon: 0.5816    steps: 182    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1574   score: 4.0   mem len: 311603   epsilon: 0.581    steps: 294    lr: 1.6e-05     reward: 3.29\n",
      "epis: 1575   score: 3.0   mem len: 311816   epsilon: 0.5806    steps: 213    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1576   score: 5.0   mem len: 312120   epsilon: 0.58    steps: 304    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1577   score: 5.0   mem len: 312446   epsilon: 0.5794    steps: 326    lr: 1.6e-05     reward: 3.27\n",
      "epis: 1578   score: 4.0   mem len: 312721   epsilon: 0.5788    steps: 275    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1579   score: 3.0   mem len: 312947   epsilon: 0.5784    steps: 226    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1580   score: 6.0   mem len: 313310   epsilon: 0.5776    steps: 363    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1581   score: 2.0   mem len: 313508   epsilon: 0.5773    steps: 198    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1582   score: 1.0   mem len: 313658   epsilon: 0.577    steps: 150    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1583   score: 4.0   mem len: 313933   epsilon: 0.5764    steps: 275    lr: 1.6e-05     reward: 3.27\n",
      "epis: 1584   score: 5.0   mem len: 314240   epsilon: 0.5758    steps: 307    lr: 1.6e-05     reward: 3.3\n",
      "epis: 1585   score: 1.0   mem len: 314390   epsilon: 0.5755    steps: 150    lr: 1.6e-05     reward: 3.31\n",
      "epis: 1586   score: 5.0   mem len: 314704   epsilon: 0.5749    steps: 314    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1587   score: 3.0   mem len: 314930   epsilon: 0.5744    steps: 226    lr: 1.6e-05     reward: 3.33\n",
      "epis: 1588   score: 2.0   mem len: 315111   epsilon: 0.5741    steps: 181    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1589   score: 2.0   mem len: 315291   epsilon: 0.5737    steps: 180    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1590   score: 2.0   mem len: 315492   epsilon: 0.5733    steps: 201    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1591   score: 5.0   mem len: 315818   epsilon: 0.5727    steps: 326    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1592   score: 3.0   mem len: 316031   epsilon: 0.5723    steps: 213    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1593   score: 5.0   mem len: 316339   epsilon: 0.5716    steps: 308    lr: 1.6e-05     reward: 3.29\n",
      "epis: 1594   score: 1.0   mem len: 316490   epsilon: 0.5713    steps: 151    lr: 1.6e-05     reward: 3.27\n",
      "epis: 1595   score: 3.0   mem len: 316720   epsilon: 0.5709    steps: 230    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1596   score: 2.0   mem len: 316938   epsilon: 0.5705    steps: 218    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1597   score: 2.0   mem len: 317120   epsilon: 0.5701    steps: 182    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1598   score: 3.0   mem len: 317346   epsilon: 0.5697    steps: 226    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1599   score: 3.0   mem len: 317590   epsilon: 0.5692    steps: 244    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1600   score: 5.0   mem len: 317894   epsilon: 0.5686    steps: 304    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1601   score: 4.0   mem len: 318169   epsilon: 0.568    steps: 275    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1602   score: 4.0   mem len: 318450   epsilon: 0.5675    steps: 281    lr: 1.6e-05     reward: 3.3\n",
      "epis: 1603   score: 6.0   mem len: 318823   epsilon: 0.5667    steps: 373    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1604   score: 5.0   mem len: 319144   epsilon: 0.5661    steps: 321    lr: 1.6e-05     reward: 3.33\n",
      "epis: 1605   score: 4.0   mem len: 319425   epsilon: 0.5655    steps: 281    lr: 1.6e-05     reward: 3.34\n",
      "epis: 1606   score: 3.0   mem len: 319656   epsilon: 0.5651    steps: 231    lr: 1.6e-05     reward: 3.36\n",
      "epis: 1607   score: 11.0   mem len: 320080   epsilon: 0.5642    steps: 424    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1608   score: 3.0   mem len: 320330   epsilon: 0.5637    steps: 250    lr: 1.6e-05     reward: 3.42\n",
      "epis: 1609   score: 4.0   mem len: 320608   epsilon: 0.5632    steps: 278    lr: 1.6e-05     reward: 3.44\n",
      "epis: 1610   score: 4.0   mem len: 320882   epsilon: 0.5627    steps: 274    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1611   score: 4.0   mem len: 321158   epsilon: 0.5621    steps: 276    lr: 1.6e-05     reward: 3.44\n",
      "epis: 1612   score: 1.0   mem len: 321309   epsilon: 0.5618    steps: 151    lr: 1.6e-05     reward: 3.4\n",
      "epis: 1613   score: 6.0   mem len: 321660   epsilon: 0.5611    steps: 351    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1614   score: 6.0   mem len: 322015   epsilon: 0.5604    steps: 355    lr: 1.6e-05     reward: 3.48\n",
      "epis: 1615   score: 4.0   mem len: 322290   epsilon: 0.5599    steps: 275    lr: 1.6e-05     reward: 3.49\n",
      "epis: 1616   score: 5.0   mem len: 322578   epsilon: 0.5593    steps: 288    lr: 1.6e-05     reward: 3.46\n",
      "epis: 1617   score: 2.0   mem len: 322759   epsilon: 0.5589    steps: 181    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1618   score: 3.0   mem len: 322989   epsilon: 0.5585    steps: 230    lr: 1.6e-05     reward: 3.44\n",
      "epis: 1619   score: 5.0   mem len: 323295   epsilon: 0.5579    steps: 306    lr: 1.6e-05     reward: 3.47\n",
      "epis: 1620   score: 6.0   mem len: 323644   epsilon: 0.5572    steps: 349    lr: 1.6e-05     reward: 3.5\n",
      "epis: 1621   score: 5.0   mem len: 323969   epsilon: 0.5565    steps: 325    lr: 1.6e-05     reward: 3.54\n",
      "epis: 1622   score: 7.0   mem len: 324369   epsilon: 0.5557    steps: 400    lr: 1.6e-05     reward: 3.58\n",
      "epis: 1623   score: 4.0   mem len: 324663   epsilon: 0.5552    steps: 294    lr: 1.6e-05     reward: 3.59\n",
      "epis: 1624   score: 6.0   mem len: 325018   epsilon: 0.5545    steps: 355    lr: 1.6e-05     reward: 3.64\n",
      "epis: 1625   score: 4.0   mem len: 325273   epsilon: 0.554    steps: 255    lr: 1.6e-05     reward: 3.65\n",
      "epis: 1626   score: 5.0   mem len: 325582   epsilon: 0.5533    steps: 309    lr: 1.6e-05     reward: 3.67\n",
      "epis: 1627   score: 4.0   mem len: 325873   epsilon: 0.5528    steps: 291    lr: 1.6e-05     reward: 3.67\n",
      "epis: 1628   score: 4.0   mem len: 326168   epsilon: 0.5522    steps: 295    lr: 1.6e-05     reward: 3.68\n",
      "epis: 1629   score: 5.0   mem len: 326471   epsilon: 0.5516    steps: 303    lr: 1.6e-05     reward: 3.69\n",
      "epis: 1630   score: 2.0   mem len: 326653   epsilon: 0.5512    steps: 182    lr: 1.6e-05     reward: 3.69\n",
      "epis: 1631   score: 8.0   mem len: 327125   epsilon: 0.5503    steps: 472    lr: 1.6e-05     reward: 3.73\n",
      "epis: 1632   score: 5.0   mem len: 327406   epsilon: 0.5497    steps: 281    lr: 1.6e-05     reward: 3.74\n",
      "epis: 1633   score: 2.0   mem len: 327606   epsilon: 0.5493    steps: 200    lr: 1.6e-05     reward: 3.72\n",
      "epis: 1634   score: 7.0   mem len: 328013   epsilon: 0.5485    steps: 407    lr: 1.6e-05     reward: 3.76\n",
      "epis: 1635   score: 6.0   mem len: 328390   epsilon: 0.5478    steps: 377    lr: 1.6e-05     reward: 3.81\n",
      "epis: 1636   score: 9.0   mem len: 328853   epsilon: 0.5469    steps: 463    lr: 1.6e-05     reward: 3.86\n",
      "epis: 1637   score: 5.0   mem len: 329160   epsilon: 0.5463    steps: 307    lr: 1.6e-05     reward: 3.86\n",
      "epis: 1638   score: 4.0   mem len: 329425   epsilon: 0.5457    steps: 265    lr: 1.6e-05     reward: 3.87\n",
      "epis: 1639   score: 4.0   mem len: 329721   epsilon: 0.5452    steps: 296    lr: 1.6e-05     reward: 3.88\n",
      "epis: 1640   score: 4.0   mem len: 329995   epsilon: 0.5446    steps: 274    lr: 1.6e-05     reward: 3.9\n",
      "epis: 1641   score: 4.0   mem len: 330269   epsilon: 0.5441    steps: 274    lr: 1.6e-05     reward: 3.9\n",
      "epis: 1642   score: 8.0   mem len: 330693   epsilon: 0.5432    steps: 424    lr: 1.6e-05     reward: 3.97\n",
      "epis: 1643   score: 8.0   mem len: 331160   epsilon: 0.5423    steps: 467    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1644   score: 5.0   mem len: 331473   epsilon: 0.5417    steps: 313    lr: 1.6e-05     reward: 4.05\n",
      "epis: 1645   score: 2.0   mem len: 331696   epsilon: 0.5412    steps: 223    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1646   score: 4.0   mem len: 331972   epsilon: 0.5407    steps: 276    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1647   score: 4.0   mem len: 332230   epsilon: 0.5402    steps: 258    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1648   score: 3.0   mem len: 332462   epsilon: 0.5397    steps: 232    lr: 1.6e-05     reward: 4.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1649   score: 3.0   mem len: 332692   epsilon: 0.5393    steps: 230    lr: 1.6e-05     reward: 4.03\n",
      "epis: 1650   score: 3.0   mem len: 332904   epsilon: 0.5388    steps: 212    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1651   score: 3.0   mem len: 333114   epsilon: 0.5384    steps: 210    lr: 1.6e-05     reward: 4.03\n",
      "epis: 1652   score: 3.0   mem len: 333348   epsilon: 0.538    steps: 234    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1653   score: 5.0   mem len: 333672   epsilon: 0.5373    steps: 324    lr: 1.6e-05     reward: 4.07\n",
      "epis: 1654   score: 1.0   mem len: 333841   epsilon: 0.537    steps: 169    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1655   score: 4.0   mem len: 334116   epsilon: 0.5364    steps: 275    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1656   score: 3.0   mem len: 334385   epsilon: 0.5359    steps: 269    lr: 1.6e-05     reward: 4.01\n",
      "epis: 1657   score: 6.0   mem len: 334697   epsilon: 0.5353    steps: 312    lr: 1.6e-05     reward: 4.03\n",
      "epis: 1658   score: 4.0   mem len: 334952   epsilon: 0.5348    steps: 255    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1659   score: 4.0   mem len: 335204   epsilon: 0.5343    steps: 252    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1660   score: 1.0   mem len: 335355   epsilon: 0.534    steps: 151    lr: 1.6e-05     reward: 4.01\n",
      "epis: 1661   score: 2.0   mem len: 335536   epsilon: 0.5336    steps: 181    lr: 1.6e-05     reward: 3.99\n",
      "epis: 1662   score: 3.0   mem len: 335762   epsilon: 0.5332    steps: 226    lr: 1.6e-05     reward: 3.97\n",
      "epis: 1663   score: 6.0   mem len: 336123   epsilon: 0.5325    steps: 361    lr: 1.6e-05     reward: 3.99\n",
      "epis: 1664   score: 5.0   mem len: 336416   epsilon: 0.5319    steps: 293    lr: 1.6e-05     reward: 4.0\n",
      "epis: 1665   score: 3.0   mem len: 336647   epsilon: 0.5314    steps: 231    lr: 1.6e-05     reward: 4.01\n",
      "epis: 1666   score: 2.0   mem len: 336845   epsilon: 0.531    steps: 198    lr: 1.6e-05     reward: 4.0\n",
      "epis: 1667   score: 3.0   mem len: 337058   epsilon: 0.5306    steps: 213    lr: 1.6e-05     reward: 4.0\n",
      "epis: 1668   score: 4.0   mem len: 337334   epsilon: 0.5301    steps: 276    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1669   score: 8.0   mem len: 337748   epsilon: 0.5293    steps: 414    lr: 1.6e-05     reward: 4.07\n",
      "epis: 1670   score: 5.0   mem len: 338072   epsilon: 0.5286    steps: 324    lr: 1.6e-05     reward: 4.07\n",
      "epis: 1671   score: 6.0   mem len: 338425   epsilon: 0.5279    steps: 353    lr: 1.6e-05     reward: 4.11\n",
      "epis: 1672   score: 5.0   mem len: 338751   epsilon: 0.5273    steps: 326    lr: 1.6e-05     reward: 4.11\n",
      "epis: 1673   score: 6.0   mem len: 339105   epsilon: 0.5266    steps: 354    lr: 1.6e-05     reward: 4.15\n",
      "epis: 1674   score: 5.0   mem len: 339429   epsilon: 0.5259    steps: 324    lr: 1.6e-05     reward: 4.16\n",
      "epis: 1675   score: 6.0   mem len: 339792   epsilon: 0.5252    steps: 363    lr: 1.6e-05     reward: 4.19\n",
      "epis: 1676   score: 1.0   mem len: 339942   epsilon: 0.5249    steps: 150    lr: 1.6e-05     reward: 4.15\n",
      "epis: 1677   score: 4.0   mem len: 340216   epsilon: 0.5244    steps: 274    lr: 1.6e-05     reward: 4.14\n",
      "epis: 1678   score: 5.0   mem len: 340508   epsilon: 0.5238    steps: 292    lr: 1.6e-05     reward: 4.15\n",
      "epis: 1679   score: 7.0   mem len: 340919   epsilon: 0.523    steps: 411    lr: 1.6e-05     reward: 4.19\n",
      "epis: 1680   score: 6.0   mem len: 341241   epsilon: 0.5223    steps: 322    lr: 1.6e-05     reward: 4.19\n",
      "epis: 1681   score: 5.0   mem len: 341522   epsilon: 0.5218    steps: 281    lr: 1.6e-05     reward: 4.22\n",
      "epis: 1682   score: 3.0   mem len: 341770   epsilon: 0.5213    steps: 248    lr: 1.6e-05     reward: 4.24\n",
      "epis: 1683   score: 3.0   mem len: 342016   epsilon: 0.5208    steps: 246    lr: 1.6e-05     reward: 4.23\n",
      "epis: 1684   score: 4.0   mem len: 342290   epsilon: 0.5203    steps: 274    lr: 1.6e-05     reward: 4.22\n",
      "epis: 1685   score: 4.0   mem len: 342547   epsilon: 0.5198    steps: 257    lr: 1.6e-05     reward: 4.25\n",
      "epis: 1686   score: 4.0   mem len: 342822   epsilon: 0.5192    steps: 275    lr: 1.6e-05     reward: 4.24\n",
      "epis: 1687   score: 8.0   mem len: 343222   epsilon: 0.5184    steps: 400    lr: 1.6e-05     reward: 4.29\n",
      "epis: 1688   score: 4.0   mem len: 343484   epsilon: 0.5179    steps: 262    lr: 1.6e-05     reward: 4.31\n",
      "epis: 1689   score: 7.0   mem len: 343878   epsilon: 0.5171    steps: 394    lr: 1.6e-05     reward: 4.36\n",
      "epis: 1690   score: 3.0   mem len: 344103   epsilon: 0.5167    steps: 225    lr: 1.6e-05     reward: 4.37\n",
      "epis: 1691   score: 5.0   mem len: 344428   epsilon: 0.516    steps: 325    lr: 1.6e-05     reward: 4.37\n",
      "epis: 1692   score: 5.0   mem len: 344754   epsilon: 0.5154    steps: 326    lr: 1.6e-05     reward: 4.39\n",
      "epis: 1693   score: 6.0   mem len: 345127   epsilon: 0.5146    steps: 373    lr: 1.6e-05     reward: 4.4\n",
      "epis: 1694   score: 3.0   mem len: 345395   epsilon: 0.5141    steps: 268    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1695   score: 3.0   mem len: 345621   epsilon: 0.5137    steps: 226    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1696   score: 3.0   mem len: 345850   epsilon: 0.5132    steps: 229    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1697   score: 2.0   mem len: 346050   epsilon: 0.5128    steps: 200    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1698   score: 6.0   mem len: 346387   epsilon: 0.5122    steps: 337    lr: 1.6e-05     reward: 4.46\n",
      "epis: 1699   score: 5.0   mem len: 346678   epsilon: 0.5116    steps: 291    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1700   score: 7.0   mem len: 347076   epsilon: 0.5108    steps: 398    lr: 1.6e-05     reward: 4.5\n",
      "epis: 1701   score: 5.0   mem len: 347401   epsilon: 0.5101    steps: 325    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1702   score: 4.0   mem len: 347656   epsilon: 0.5096    steps: 255    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1703   score: 3.0   mem len: 347884   epsilon: 0.5092    steps: 228    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1704   score: 4.0   mem len: 348150   epsilon: 0.5087    steps: 266    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1705   score: 4.0   mem len: 348431   epsilon: 0.5081    steps: 281    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1706   score: 4.0   mem len: 348689   epsilon: 0.5076    steps: 258    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1707   score: 5.0   mem len: 348980   epsilon: 0.507    steps: 291    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1708   score: 4.0   mem len: 349293   epsilon: 0.5064    steps: 313    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1709   score: 3.0   mem len: 349524   epsilon: 0.5059    steps: 231    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1710   score: 6.0   mem len: 349882   epsilon: 0.5052    steps: 358    lr: 1.6e-05     reward: 4.44\n",
      "epis: 1711   score: 5.0   mem len: 350190   epsilon: 0.5046    steps: 308    lr: 1.6e-05     reward: 4.45\n",
      "epis: 1712   score: 3.0   mem len: 350436   epsilon: 0.5041    steps: 246    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1713   score: 3.0   mem len: 350665   epsilon: 0.5037    steps: 229    lr: 1.6e-05     reward: 4.44\n",
      "epis: 1714   score: 12.0   mem len: 351151   epsilon: 0.5027    steps: 486    lr: 1.6e-05     reward: 4.5\n",
      "epis: 1715   score: 5.0   mem len: 351439   epsilon: 0.5021    steps: 288    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1716   score: 7.0   mem len: 351864   epsilon: 0.5013    steps: 425    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1717   score: 3.0   mem len: 352089   epsilon: 0.5009    steps: 225    lr: 1.6e-05     reward: 4.54\n",
      "epis: 1718   score: 8.0   mem len: 352510   epsilon: 0.5    steps: 421    lr: 1.6e-05     reward: 4.59\n",
      "epis: 1719   score: 2.0   mem len: 352710   epsilon: 0.4996    steps: 200    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1720   score: 3.0   mem len: 352938   epsilon: 0.4992    steps: 228    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1721   score: 3.0   mem len: 353168   epsilon: 0.4987    steps: 230    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1722   score: 12.0   mem len: 353756   epsilon: 0.4976    steps: 588    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1723   score: 4.0   mem len: 354035   epsilon: 0.497    steps: 279    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1724   score: 5.0   mem len: 354365   epsilon: 0.4964    steps: 330    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1725   score: 7.0   mem len: 354769   epsilon: 0.4956    steps: 404    lr: 1.6e-05     reward: 4.58\n",
      "epis: 1726   score: 2.0   mem len: 354951   epsilon: 0.4952    steps: 182    lr: 1.6e-05     reward: 4.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1727   score: 4.0   mem len: 355226   epsilon: 0.4947    steps: 275    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1728   score: 1.0   mem len: 355394   epsilon: 0.4943    steps: 168    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1729   score: 4.0   mem len: 355669   epsilon: 0.4938    steps: 275    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1730   score: 2.0   mem len: 355851   epsilon: 0.4934    steps: 182    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1731   score: 5.0   mem len: 356139   epsilon: 0.4928    steps: 288    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1732   score: 7.0   mem len: 356480   epsilon: 0.4922    steps: 341    lr: 1.6e-05     reward: 4.5\n",
      "epis: 1733   score: 12.0   mem len: 356918   epsilon: 0.4913    steps: 438    lr: 1.6e-05     reward: 4.6\n",
      "epis: 1734   score: 3.0   mem len: 357150   epsilon: 0.4908    steps: 232    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1735   score: 5.0   mem len: 357431   epsilon: 0.4903    steps: 281    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1736   score: 5.0   mem len: 357710   epsilon: 0.4897    steps: 279    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1737   score: 7.0   mem len: 358115   epsilon: 0.4889    steps: 405    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1738   score: 4.0   mem len: 358356   epsilon: 0.4885    steps: 241    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1739   score: 3.0   mem len: 358569   epsilon: 0.488    steps: 213    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1740   score: 4.0   mem len: 358865   epsilon: 0.4874    steps: 296    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1741   score: 4.0   mem len: 359165   epsilon: 0.4869    steps: 300    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1742   score: 4.0   mem len: 359404   epsilon: 0.4864    steps: 239    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1743   score: 3.0   mem len: 359615   epsilon: 0.486    steps: 211    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1744   score: 3.0   mem len: 359843   epsilon: 0.4855    steps: 228    lr: 1.6e-05     reward: 4.41\n",
      "epis: 1745   score: 3.0   mem len: 360074   epsilon: 0.4851    steps: 231    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1746   score: 4.0   mem len: 360350   epsilon: 0.4845    steps: 276    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1747   score: 5.0   mem len: 360630   epsilon: 0.484    steps: 280    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1748   score: 6.0   mem len: 360985   epsilon: 0.4832    steps: 355    lr: 1.6e-05     reward: 4.46\n",
      "epis: 1749   score: 4.0   mem len: 361250   epsilon: 0.4827    steps: 265    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1750   score: 8.0   mem len: 361691   epsilon: 0.4818    steps: 441    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1751   score: 6.0   mem len: 362036   epsilon: 0.4812    steps: 345    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1752   score: 6.0   mem len: 362356   epsilon: 0.4805    steps: 320    lr: 1.6e-05     reward: 4.58\n",
      "epis: 1753   score: 6.0   mem len: 362726   epsilon: 0.4798    steps: 370    lr: 1.6e-05     reward: 4.59\n",
      "epis: 1754   score: 3.0   mem len: 362972   epsilon: 0.4793    steps: 246    lr: 1.6e-05     reward: 4.61\n",
      "epis: 1755   score: 6.0   mem len: 363307   epsilon: 0.4787    steps: 335    lr: 1.6e-05     reward: 4.63\n",
      "epis: 1756   score: 2.0   mem len: 363489   epsilon: 0.4783    steps: 182    lr: 1.6e-05     reward: 4.62\n",
      "epis: 1757   score: 4.0   mem len: 363748   epsilon: 0.4778    steps: 259    lr: 1.6e-05     reward: 4.6\n",
      "epis: 1758   score: 4.0   mem len: 364028   epsilon: 0.4772    steps: 280    lr: 1.6e-05     reward: 4.6\n",
      "epis: 1759   score: 3.0   mem len: 364241   epsilon: 0.4768    steps: 213    lr: 1.6e-05     reward: 4.59\n",
      "epis: 1760   score: 6.0   mem len: 364578   epsilon: 0.4761    steps: 337    lr: 1.6e-05     reward: 4.64\n",
      "epis: 1761   score: 4.0   mem len: 364857   epsilon: 0.4756    steps: 279    lr: 1.6e-05     reward: 4.66\n",
      "epis: 1762   score: 4.0   mem len: 365131   epsilon: 0.475    steps: 274    lr: 1.6e-05     reward: 4.67\n",
      "epis: 1763   score: 6.0   mem len: 365468   epsilon: 0.4744    steps: 337    lr: 1.6e-05     reward: 4.67\n",
      "epis: 1764   score: 6.0   mem len: 365792   epsilon: 0.4737    steps: 324    lr: 1.6e-05     reward: 4.68\n",
      "epis: 1765   score: 5.0   mem len: 366074   epsilon: 0.4732    steps: 282    lr: 1.6e-05     reward: 4.7\n",
      "epis: 1766   score: 5.0   mem len: 366402   epsilon: 0.4725    steps: 328    lr: 1.6e-05     reward: 4.73\n",
      "epis: 1767   score: 8.0   mem len: 366807   epsilon: 0.4717    steps: 405    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1768   score: 3.0   mem len: 367020   epsilon: 0.4713    steps: 213    lr: 1.6e-05     reward: 4.77\n",
      "epis: 1769   score: 3.0   mem len: 367231   epsilon: 0.4709    steps: 211    lr: 1.6e-05     reward: 4.72\n",
      "epis: 1770   score: 3.0   mem len: 367456   epsilon: 0.4704    steps: 225    lr: 1.6e-05     reward: 4.7\n",
      "epis: 1771   score: 7.0   mem len: 367868   epsilon: 0.4696    steps: 412    lr: 1.6e-05     reward: 4.71\n",
      "epis: 1772   score: 7.0   mem len: 368114   epsilon: 0.4691    steps: 246    lr: 1.6e-05     reward: 4.73\n",
      "epis: 1773   score: 4.0   mem len: 368373   epsilon: 0.4686    steps: 259    lr: 1.6e-05     reward: 4.71\n",
      "epis: 1774   score: 4.0   mem len: 368652   epsilon: 0.4681    steps: 279    lr: 1.6e-05     reward: 4.7\n",
      "epis: 1775   score: 5.0   mem len: 368957   epsilon: 0.4675    steps: 305    lr: 1.6e-05     reward: 4.69\n",
      "epis: 1776   score: 4.0   mem len: 369232   epsilon: 0.4669    steps: 275    lr: 1.6e-05     reward: 4.72\n",
      "epis: 1777   score: 6.0   mem len: 369585   epsilon: 0.4662    steps: 353    lr: 1.6e-05     reward: 4.74\n",
      "epis: 1778   score: 8.0   mem len: 370005   epsilon: 0.4654    steps: 420    lr: 1.6e-05     reward: 4.77\n",
      "epis: 1779   score: 2.0   mem len: 370223   epsilon: 0.465    steps: 218    lr: 1.6e-05     reward: 4.72\n",
      "epis: 1780   score: 3.0   mem len: 370470   epsilon: 0.4645    steps: 247    lr: 1.6e-05     reward: 4.69\n",
      "epis: 1781   score: 9.0   mem len: 370777   epsilon: 0.4639    steps: 307    lr: 1.6e-05     reward: 4.73\n",
      "epis: 1782   score: 6.0   mem len: 371130   epsilon: 0.4632    steps: 353    lr: 1.6e-05     reward: 4.76\n",
      "epis: 1783   score: 7.0   mem len: 371537   epsilon: 0.4624    steps: 407    lr: 1.6e-05     reward: 4.8\n",
      "epis: 1784   score: 8.0   mem len: 371960   epsilon: 0.4615    steps: 423    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1785   score: 8.0   mem len: 372396   epsilon: 0.4607    steps: 436    lr: 1.6e-05     reward: 4.88\n",
      "epis: 1786   score: 2.0   mem len: 372596   epsilon: 0.4603    steps: 200    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1787   score: 6.0   mem len: 372939   epsilon: 0.4596    steps: 343    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1788   score: 3.0   mem len: 373150   epsilon: 0.4592    steps: 211    lr: 1.6e-05     reward: 4.83\n",
      "epis: 1789   score: 2.0   mem len: 373332   epsilon: 0.4588    steps: 182    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1790   score: 3.0   mem len: 373557   epsilon: 0.4584    steps: 225    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1791   score: 4.0   mem len: 373857   epsilon: 0.4578    steps: 300    lr: 1.6e-05     reward: 4.77\n",
      "epis: 1792   score: 7.0   mem len: 374248   epsilon: 0.457    steps: 391    lr: 1.6e-05     reward: 4.79\n",
      "epis: 1793   score: 5.0   mem len: 374592   epsilon: 0.4563    steps: 344    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1794   score: 5.0   mem len: 374919   epsilon: 0.4557    steps: 327    lr: 1.6e-05     reward: 4.8\n",
      "epis: 1795   score: 6.0   mem len: 375252   epsilon: 0.455    steps: 333    lr: 1.6e-05     reward: 4.83\n",
      "epis: 1796   score: 7.0   mem len: 375660   epsilon: 0.4542    steps: 408    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1797   score: 4.0   mem len: 375936   epsilon: 0.4536    steps: 276    lr: 1.6e-05     reward: 4.89\n",
      "epis: 1798   score: 4.0   mem len: 376233   epsilon: 0.4531    steps: 297    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1799   score: 4.0   mem len: 376512   epsilon: 0.4525    steps: 279    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1800   score: 4.0   mem len: 376771   epsilon: 0.452    steps: 259    lr: 1.6e-05     reward: 4.83\n",
      "epis: 1801   score: 4.0   mem len: 377030   epsilon: 0.4515    steps: 259    lr: 1.6e-05     reward: 4.82\n",
      "epis: 1802   score: 4.0   mem len: 377272   epsilon: 0.451    steps: 242    lr: 1.6e-05     reward: 4.82\n",
      "epis: 1803   score: 3.0   mem len: 377485   epsilon: 0.4506    steps: 213    lr: 1.6e-05     reward: 4.82\n",
      "epis: 1804   score: 7.0   mem len: 377881   epsilon: 0.4498    steps: 396    lr: 1.6e-05     reward: 4.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1805   score: 4.0   mem len: 378158   epsilon: 0.4492    steps: 277    lr: 1.6e-05     reward: 4.85\n",
      "epis: 1806   score: 5.0   mem len: 378482   epsilon: 0.4486    steps: 324    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1807   score: 3.0   mem len: 378708   epsilon: 0.4482    steps: 226    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1808   score: 6.0   mem len: 379040   epsilon: 0.4475    steps: 332    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1809   score: 4.0   mem len: 379322   epsilon: 0.4469    steps: 282    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1810   score: 5.0   mem len: 379664   epsilon: 0.4463    steps: 342    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1811   score: 3.0   mem len: 379873   epsilon: 0.4458    steps: 209    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1812   score: 7.0   mem len: 380244   epsilon: 0.4451    steps: 371    lr: 1.6e-05     reward: 4.88\n",
      "epis: 1813   score: 7.0   mem len: 380623   epsilon: 0.4444    steps: 379    lr: 1.6e-05     reward: 4.92\n",
      "epis: 1814   score: 6.0   mem len: 380996   epsilon: 0.4436    steps: 373    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1815   score: 3.0   mem len: 381222   epsilon: 0.4432    steps: 226    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1816   score: 9.0   mem len: 381670   epsilon: 0.4423    steps: 448    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1817   score: 7.0   mem len: 382044   epsilon: 0.4416    steps: 374    lr: 1.6e-05     reward: 4.9\n",
      "epis: 1818   score: 4.0   mem len: 382318   epsilon: 0.441    steps: 274    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1819   score: 6.0   mem len: 382657   epsilon: 0.4403    steps: 339    lr: 1.6e-05     reward: 4.9\n",
      "epis: 1820   score: 8.0   mem len: 383101   epsilon: 0.4395    steps: 444    lr: 1.6e-05     reward: 4.95\n",
      "epis: 1821   score: 7.0   mem len: 383502   epsilon: 0.4387    steps: 401    lr: 1.6e-05     reward: 4.99\n",
      "epis: 1822   score: 3.0   mem len: 383750   epsilon: 0.4382    steps: 248    lr: 1.6e-05     reward: 4.9\n",
      "epis: 1823   score: 3.0   mem len: 383963   epsilon: 0.4378    steps: 213    lr: 1.6e-05     reward: 4.89\n",
      "epis: 1824   score: 3.0   mem len: 384193   epsilon: 0.4373    steps: 230    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1825   score: 7.0   mem len: 384584   epsilon: 0.4365    steps: 391    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1826   score: 7.0   mem len: 385007   epsilon: 0.4357    steps: 423    lr: 1.6e-05     reward: 4.92\n",
      "epis: 1827   score: 9.0   mem len: 385463   epsilon: 0.4348    steps: 456    lr: 1.6e-05     reward: 4.97\n",
      "epis: 1828   score: 7.0   mem len: 385837   epsilon: 0.434    steps: 374    lr: 1.6e-05     reward: 5.03\n",
      "epis: 1829   score: 6.0   mem len: 386192   epsilon: 0.4333    steps: 355    lr: 1.6e-05     reward: 5.05\n",
      "epis: 1830   score: 4.0   mem len: 386467   epsilon: 0.4328    steps: 275    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1831   score: 8.0   mem len: 386745   epsilon: 0.4322    steps: 278    lr: 1.6e-05     reward: 5.1\n",
      "epis: 1832   score: 7.0   mem len: 387183   epsilon: 0.4314    steps: 438    lr: 1.6e-05     reward: 5.1\n",
      "epis: 1833   score: 7.0   mem len: 387581   epsilon: 0.4306    steps: 398    lr: 1.6e-05     reward: 5.05\n",
      "epis: 1834   score: 8.0   mem len: 388049   epsilon: 0.4297    steps: 468    lr: 1.6e-05     reward: 5.1\n",
      "epis: 1835   score: 3.0   mem len: 388262   epsilon: 0.4292    steps: 213    lr: 1.6e-05     reward: 5.08\n",
      "epis: 1836   score: 2.0   mem len: 388462   epsilon: 0.4288    steps: 200    lr: 1.6e-05     reward: 5.05\n",
      "epis: 1837   score: 2.0   mem len: 388660   epsilon: 0.4285    steps: 198    lr: 1.6e-05     reward: 5.0\n",
      "epis: 1838   score: 6.0   mem len: 389005   epsilon: 0.4278    steps: 345    lr: 1.6e-05     reward: 5.02\n",
      "epis: 1839   score: 7.0   mem len: 389385   epsilon: 0.427    steps: 380    lr: 1.6e-05     reward: 5.06\n",
      "epis: 1840   score: 3.0   mem len: 389613   epsilon: 0.4266    steps: 228    lr: 1.6e-05     reward: 5.05\n",
      "epis: 1841   score: 8.0   mem len: 390057   epsilon: 0.4257    steps: 444    lr: 1.6e-05     reward: 5.09\n",
      "epis: 1842   score: 2.0   mem len: 390254   epsilon: 0.4253    steps: 197    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1843   score: 8.0   mem len: 390713   epsilon: 0.4244    steps: 459    lr: 1.6e-05     reward: 5.12\n",
      "epis: 1844   score: 5.0   mem len: 391037   epsilon: 0.4237    steps: 324    lr: 1.6e-05     reward: 5.14\n",
      "epis: 1845   score: 4.0   mem len: 391314   epsilon: 0.4232    steps: 277    lr: 1.6e-05     reward: 5.15\n",
      "epis: 1846   score: 8.0   mem len: 391762   epsilon: 0.4223    steps: 448    lr: 1.6e-05     reward: 5.19\n",
      "epis: 1847   score: 3.0   mem len: 391991   epsilon: 0.4219    steps: 229    lr: 1.6e-05     reward: 5.17\n",
      "epis: 1848   score: 4.0   mem len: 392269   epsilon: 0.4213    steps: 278    lr: 1.6e-05     reward: 5.15\n",
      "epis: 1849   score: 6.0   mem len: 392623   epsilon: 0.4206    steps: 354    lr: 1.6e-05     reward: 5.17\n",
      "epis: 1850   score: 2.0   mem len: 392805   epsilon: 0.4202    steps: 182    lr: 1.6e-05     reward: 5.11\n",
      "epis: 1851   score: 6.0   mem len: 393159   epsilon: 0.4195    steps: 354    lr: 1.6e-05     reward: 5.11\n",
      "epis: 1852   score: 2.0   mem len: 393358   epsilon: 0.4191    steps: 199    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1853   score: 3.0   mem len: 393589   epsilon: 0.4187    steps: 231    lr: 1.6e-05     reward: 5.04\n",
      "epis: 1854   score: 5.0   mem len: 393897   epsilon: 0.4181    steps: 308    lr: 1.6e-05     reward: 5.06\n",
      "epis: 1855   score: 3.0   mem len: 394126   epsilon: 0.4176    steps: 229    lr: 1.6e-05     reward: 5.03\n",
      "epis: 1856   score: 4.0   mem len: 394424   epsilon: 0.417    steps: 298    lr: 1.6e-05     reward: 5.05\n",
      "epis: 1857   score: 6.0   mem len: 394797   epsilon: 0.4163    steps: 373    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1858   score: 3.0   mem len: 395042   epsilon: 0.4158    steps: 245    lr: 1.6e-05     reward: 5.06\n",
      "epis: 1859   score: 5.0   mem len: 395364   epsilon: 0.4152    steps: 322    lr: 1.6e-05     reward: 5.08\n",
      "epis: 1860   score: 7.0   mem len: 395744   epsilon: 0.4144    steps: 380    lr: 1.6e-05     reward: 5.09\n",
      "epis: 1861   score: 5.0   mem len: 396050   epsilon: 0.4138    steps: 306    lr: 1.6e-05     reward: 5.1\n",
      "epis: 1862   score: 4.0   mem len: 396326   epsilon: 0.4133    steps: 276    lr: 1.6e-05     reward: 5.1\n",
      "epis: 1863   score: 6.0   mem len: 396699   epsilon: 0.4125    steps: 373    lr: 1.6e-05     reward: 5.1\n",
      "epis: 1864   score: 3.0   mem len: 396912   epsilon: 0.4121    steps: 213    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1865   score: 4.0   mem len: 397172   epsilon: 0.4116    steps: 260    lr: 1.6e-05     reward: 5.06\n",
      "epis: 1866   score: 6.0   mem len: 397531   epsilon: 0.4109    steps: 359    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1867   score: 5.0   mem len: 397860   epsilon: 0.4102    steps: 329    lr: 1.6e-05     reward: 5.04\n",
      "epis: 1868   score: 5.0   mem len: 398202   epsilon: 0.4096    steps: 342    lr: 1.6e-05     reward: 5.06\n",
      "epis: 1869   score: 4.0   mem len: 398446   epsilon: 0.4091    steps: 244    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1870   score: 4.0   mem len: 398743   epsilon: 0.4085    steps: 297    lr: 1.6e-05     reward: 5.08\n",
      "epis: 1871   score: 6.0   mem len: 399119   epsilon: 0.4077    steps: 376    lr: 1.6e-05     reward: 5.07\n",
      "epis: 1872   score: 4.0   mem len: 399381   epsilon: 0.4072    steps: 262    lr: 1.6e-05     reward: 5.04\n",
      "epis: 1873   score: 3.0   mem len: 399609   epsilon: 0.4068    steps: 228    lr: 1.6e-05     reward: 5.03\n",
      "epis: 1874   score: 7.0   mem len: 400013   epsilon: 0.406    steps: 404    lr: 6.4e-06     reward: 5.06\n",
      "epis: 1875   score: 4.0   mem len: 400275   epsilon: 0.4055    steps: 262    lr: 6.4e-06     reward: 5.05\n",
      "epis: 1876   score: 6.0   mem len: 400618   epsilon: 0.4048    steps: 343    lr: 6.4e-06     reward: 5.07\n",
      "epis: 1877   score: 5.0   mem len: 400960   epsilon: 0.4041    steps: 342    lr: 6.4e-06     reward: 5.06\n",
      "epis: 1878   score: 12.0   mem len: 401398   epsilon: 0.4032    steps: 438    lr: 6.4e-06     reward: 5.1\n",
      "epis: 1879   score: 5.0   mem len: 401680   epsilon: 0.4027    steps: 282    lr: 6.4e-06     reward: 5.13\n",
      "epis: 1880   score: 5.0   mem len: 401990   epsilon: 0.4021    steps: 310    lr: 6.4e-06     reward: 5.15\n",
      "epis: 1881   score: 4.0   mem len: 402286   epsilon: 0.4015    steps: 296    lr: 6.4e-06     reward: 5.1\n",
      "epis: 1882   score: 4.0   mem len: 402562   epsilon: 0.4009    steps: 276    lr: 6.4e-06     reward: 5.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1883   score: 4.0   mem len: 402824   epsilon: 0.4004    steps: 262    lr: 6.4e-06     reward: 5.05\n",
      "epis: 1884   score: 6.0   mem len: 403183   epsilon: 0.3997    steps: 359    lr: 6.4e-06     reward: 5.03\n",
      "epis: 1885   score: 6.0   mem len: 403553   epsilon: 0.399    steps: 370    lr: 6.4e-06     reward: 5.01\n",
      "epis: 1886   score: 6.0   mem len: 403932   epsilon: 0.3982    steps: 379    lr: 6.4e-06     reward: 5.05\n",
      "epis: 1887   score: 5.0   mem len: 404213   epsilon: 0.3977    steps: 281    lr: 6.4e-06     reward: 5.04\n",
      "epis: 1888   score: 3.0   mem len: 404462   epsilon: 0.3972    steps: 249    lr: 6.4e-06     reward: 5.04\n",
      "epis: 1889   score: 10.0   mem len: 404953   epsilon: 0.3962    steps: 491    lr: 6.4e-06     reward: 5.12\n",
      "epis: 1890   score: 9.0   mem len: 405405   epsilon: 0.3953    steps: 452    lr: 6.4e-06     reward: 5.18\n",
      "epis: 1891   score: 5.0   mem len: 405727   epsilon: 0.3947    steps: 322    lr: 6.4e-06     reward: 5.19\n",
      "epis: 1892   score: 10.0   mem len: 406248   epsilon: 0.3936    steps: 521    lr: 6.4e-06     reward: 5.22\n",
      "epis: 1893   score: 6.0   mem len: 406603   epsilon: 0.3929    steps: 355    lr: 6.4e-06     reward: 5.23\n",
      "epis: 1894   score: 3.0   mem len: 406816   epsilon: 0.3925    steps: 213    lr: 6.4e-06     reward: 5.21\n",
      "epis: 1895   score: 5.0   mem len: 407124   epsilon: 0.3919    steps: 308    lr: 6.4e-06     reward: 5.2\n",
      "epis: 1896   score: 7.0   mem len: 407527   epsilon: 0.3911    steps: 403    lr: 6.4e-06     reward: 5.2\n",
      "epis: 1897   score: 7.0   mem len: 407906   epsilon: 0.3903    steps: 379    lr: 6.4e-06     reward: 5.23\n",
      "epis: 1898   score: 7.0   mem len: 408303   epsilon: 0.3896    steps: 397    lr: 6.4e-06     reward: 5.26\n",
      "epis: 1899   score: 6.0   mem len: 408644   epsilon: 0.3889    steps: 341    lr: 6.4e-06     reward: 5.28\n",
      "epis: 1900   score: 4.0   mem len: 408885   epsilon: 0.3884    steps: 241    lr: 6.4e-06     reward: 5.28\n",
      "epis: 1901   score: 5.0   mem len: 409192   epsilon: 0.3878    steps: 307    lr: 6.4e-06     reward: 5.29\n",
      "epis: 1902   score: 3.0   mem len: 409422   epsilon: 0.3873    steps: 230    lr: 6.4e-06     reward: 5.28\n",
      "epis: 1903   score: 6.0   mem len: 409773   epsilon: 0.3866    steps: 351    lr: 6.4e-06     reward: 5.31\n",
      "epis: 1904   score: 3.0   mem len: 410002   epsilon: 0.3862    steps: 229    lr: 6.4e-06     reward: 5.27\n",
      "epis: 1905   score: 3.0   mem len: 410230   epsilon: 0.3857    steps: 228    lr: 6.4e-06     reward: 5.26\n",
      "epis: 1906   score: 7.0   mem len: 410622   epsilon: 0.385    steps: 392    lr: 6.4e-06     reward: 5.28\n",
      "epis: 1907   score: 5.0   mem len: 410965   epsilon: 0.3843    steps: 343    lr: 6.4e-06     reward: 5.3\n",
      "epis: 1908   score: 4.0   mem len: 411227   epsilon: 0.3838    steps: 262    lr: 6.4e-06     reward: 5.28\n",
      "epis: 1909   score: 3.0   mem len: 411474   epsilon: 0.3833    steps: 247    lr: 6.4e-06     reward: 5.27\n",
      "epis: 1910   score: 3.0   mem len: 411702   epsilon: 0.3828    steps: 228    lr: 6.4e-06     reward: 5.25\n",
      "epis: 1911   score: 4.0   mem len: 411959   epsilon: 0.3823    steps: 257    lr: 6.4e-06     reward: 5.26\n",
      "epis: 1912   score: 6.0   mem len: 412282   epsilon: 0.3817    steps: 323    lr: 6.4e-06     reward: 5.25\n",
      "epis: 1913   score: 3.0   mem len: 412493   epsilon: 0.3813    steps: 211    lr: 6.4e-06     reward: 5.21\n",
      "epis: 1914   score: 13.0   mem len: 412990   epsilon: 0.3803    steps: 497    lr: 6.4e-06     reward: 5.28\n",
      "epis: 1915   score: 4.0   mem len: 413267   epsilon: 0.3797    steps: 277    lr: 6.4e-06     reward: 5.29\n",
      "epis: 1916   score: 3.0   mem len: 413480   epsilon: 0.3793    steps: 213    lr: 6.4e-06     reward: 5.23\n",
      "epis: 1917   score: 3.0   mem len: 413709   epsilon: 0.3789    steps: 229    lr: 6.4e-06     reward: 5.19\n",
      "epis: 1918   score: 5.0   mem len: 414016   epsilon: 0.3782    steps: 307    lr: 6.4e-06     reward: 5.2\n",
      "epis: 1919   score: 3.0   mem len: 414247   epsilon: 0.3778    steps: 231    lr: 6.4e-06     reward: 5.17\n",
      "epis: 1920   score: 3.0   mem len: 414474   epsilon: 0.3773    steps: 227    lr: 6.4e-06     reward: 5.12\n",
      "epis: 1921   score: 5.0   mem len: 414780   epsilon: 0.3767    steps: 306    lr: 6.4e-06     reward: 5.1\n",
      "epis: 1922   score: 4.0   mem len: 415042   epsilon: 0.3762    steps: 262    lr: 6.4e-06     reward: 5.11\n",
      "epis: 1923   score: 3.0   mem len: 415274   epsilon: 0.3758    steps: 232    lr: 6.4e-06     reward: 5.11\n",
      "epis: 1924   score: 4.0   mem len: 415555   epsilon: 0.3752    steps: 281    lr: 6.4e-06     reward: 5.12\n",
      "epis: 1925   score: 6.0   mem len: 415914   epsilon: 0.3745    steps: 359    lr: 6.4e-06     reward: 5.11\n",
      "epis: 1926   score: 7.0   mem len: 416290   epsilon: 0.3737    steps: 376    lr: 6.4e-06     reward: 5.11\n",
      "epis: 1927   score: 6.0   mem len: 416647   epsilon: 0.373    steps: 357    lr: 6.4e-06     reward: 5.08\n",
      "epis: 1928   score: 3.0   mem len: 416858   epsilon: 0.3726    steps: 211    lr: 6.4e-06     reward: 5.04\n",
      "epis: 1929   score: 7.0   mem len: 417219   epsilon: 0.3719    steps: 361    lr: 6.4e-06     reward: 5.05\n",
      "epis: 1930   score: 5.0   mem len: 417560   epsilon: 0.3712    steps: 341    lr: 6.4e-06     reward: 5.06\n",
      "epis: 1931   score: 9.0   mem len: 418037   epsilon: 0.3703    steps: 477    lr: 6.4e-06     reward: 5.07\n",
      "epis: 1932   score: 5.0   mem len: 418340   epsilon: 0.3697    steps: 303    lr: 6.4e-06     reward: 5.05\n",
      "epis: 1933   score: 14.0   mem len: 418894   epsilon: 0.3686    steps: 554    lr: 6.4e-06     reward: 5.12\n",
      "epis: 1934   score: 6.0   mem len: 419231   epsilon: 0.3679    steps: 337    lr: 6.4e-06     reward: 5.1\n",
      "epis: 1935   score: 9.0   mem len: 419668   epsilon: 0.3671    steps: 437    lr: 6.4e-06     reward: 5.16\n",
      "epis: 1936   score: 6.0   mem len: 420005   epsilon: 0.3664    steps: 337    lr: 6.4e-06     reward: 5.2\n",
      "epis: 1937   score: 3.0   mem len: 420235   epsilon: 0.3659    steps: 230    lr: 6.4e-06     reward: 5.21\n",
      "epis: 1938   score: 11.0   mem len: 420652   epsilon: 0.3651    steps: 417    lr: 6.4e-06     reward: 5.26\n",
      "epis: 1939   score: 6.0   mem len: 420986   epsilon: 0.3644    steps: 334    lr: 6.4e-06     reward: 5.25\n",
      "epis: 1940   score: 4.0   mem len: 421266   epsilon: 0.3639    steps: 280    lr: 6.4e-06     reward: 5.26\n",
      "epis: 1941   score: 7.0   mem len: 421646   epsilon: 0.3631    steps: 380    lr: 6.4e-06     reward: 5.25\n",
      "epis: 1942   score: 6.0   mem len: 422022   epsilon: 0.3624    steps: 376    lr: 6.4e-06     reward: 5.29\n",
      "epis: 1943   score: 8.0   mem len: 422469   epsilon: 0.3615    steps: 447    lr: 6.4e-06     reward: 5.29\n",
      "epis: 1944   score: 4.0   mem len: 422728   epsilon: 0.361    steps: 259    lr: 6.4e-06     reward: 5.28\n",
      "epis: 1945   score: 5.0   mem len: 423052   epsilon: 0.3604    steps: 324    lr: 6.4e-06     reward: 5.29\n",
      "epis: 1946   score: 4.0   mem len: 423331   epsilon: 0.3598    steps: 279    lr: 6.4e-06     reward: 5.25\n",
      "epis: 1947   score: 3.0   mem len: 423544   epsilon: 0.3594    steps: 213    lr: 6.4e-06     reward: 5.25\n",
      "epis: 1948   score: 10.0   mem len: 424028   epsilon: 0.3584    steps: 484    lr: 6.4e-06     reward: 5.31\n",
      "epis: 1949   score: 5.0   mem len: 424358   epsilon: 0.3578    steps: 330    lr: 6.4e-06     reward: 5.3\n",
      "epis: 1950   score: 7.0   mem len: 424753   epsilon: 0.357    steps: 395    lr: 6.4e-06     reward: 5.35\n",
      "epis: 1951   score: 6.0   mem len: 425127   epsilon: 0.3562    steps: 374    lr: 6.4e-06     reward: 5.35\n",
      "epis: 1952   score: 11.0   mem len: 425640   epsilon: 0.3552    steps: 513    lr: 6.4e-06     reward: 5.44\n",
      "epis: 1953   score: 3.0   mem len: 425870   epsilon: 0.3548    steps: 230    lr: 6.4e-06     reward: 5.44\n",
      "epis: 1954   score: 4.0   mem len: 426150   epsilon: 0.3542    steps: 280    lr: 6.4e-06     reward: 5.43\n",
      "epis: 1955   score: 9.0   mem len: 426596   epsilon: 0.3533    steps: 446    lr: 6.4e-06     reward: 5.49\n",
      "epis: 1956   score: 5.0   mem len: 426886   epsilon: 0.3528    steps: 290    lr: 6.4e-06     reward: 5.5\n",
      "epis: 1957   score: 5.0   mem len: 427209   epsilon: 0.3521    steps: 323    lr: 6.4e-06     reward: 5.49\n",
      "epis: 1958   score: 8.0   mem len: 427595   epsilon: 0.3514    steps: 386    lr: 6.4e-06     reward: 5.54\n",
      "epis: 1959   score: 4.0   mem len: 427854   epsilon: 0.3508    steps: 259    lr: 6.4e-06     reward: 5.53\n",
      "epis: 1960   score: 5.0   mem len: 428181   epsilon: 0.3502    steps: 327    lr: 6.4e-06     reward: 5.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1961   score: 8.0   mem len: 428614   epsilon: 0.3493    steps: 433    lr: 6.4e-06     reward: 5.54\n",
      "epis: 1962   score: 7.0   mem len: 428973   epsilon: 0.3486    steps: 359    lr: 6.4e-06     reward: 5.57\n",
      "epis: 1963   score: 7.0   mem len: 429375   epsilon: 0.3478    steps: 402    lr: 6.4e-06     reward: 5.58\n",
      "epis: 1964   score: 6.0   mem len: 429748   epsilon: 0.3471    steps: 373    lr: 6.4e-06     reward: 5.61\n",
      "epis: 1965   score: 3.0   mem len: 429977   epsilon: 0.3466    steps: 229    lr: 6.4e-06     reward: 5.6\n",
      "epis: 1966   score: 7.0   mem len: 430372   epsilon: 0.3459    steps: 395    lr: 6.4e-06     reward: 5.61\n",
      "epis: 1967   score: 8.0   mem len: 430751   epsilon: 0.3451    steps: 379    lr: 6.4e-06     reward: 5.64\n",
      "epis: 1968   score: 7.0   mem len: 431152   epsilon: 0.3443    steps: 401    lr: 6.4e-06     reward: 5.66\n",
      "epis: 1969   score: 4.0   mem len: 431413   epsilon: 0.3438    steps: 261    lr: 6.4e-06     reward: 5.66\n",
      "epis: 1970   score: 6.0   mem len: 431800   epsilon: 0.343    steps: 387    lr: 6.4e-06     reward: 5.68\n",
      "epis: 1971   score: 7.0   mem len: 432209   epsilon: 0.3422    steps: 409    lr: 6.4e-06     reward: 5.69\n",
      "epis: 1972   score: 5.0   mem len: 432492   epsilon: 0.3417    steps: 283    lr: 6.4e-06     reward: 5.7\n",
      "epis: 1973   score: 5.0   mem len: 432803   epsilon: 0.341    steps: 311    lr: 6.4e-06     reward: 5.72\n",
      "epis: 1974   score: 6.0   mem len: 433129   epsilon: 0.3404    steps: 326    lr: 6.4e-06     reward: 5.71\n",
      "epis: 1975   score: 6.0   mem len: 433509   epsilon: 0.3397    steps: 380    lr: 6.4e-06     reward: 5.73\n",
      "epis: 1976   score: 6.0   mem len: 433862   epsilon: 0.339    steps: 353    lr: 6.4e-06     reward: 5.73\n",
      "epis: 1977   score: 7.0   mem len: 434268   epsilon: 0.3381    steps: 406    lr: 6.4e-06     reward: 5.75\n",
      "epis: 1978   score: 8.0   mem len: 434687   epsilon: 0.3373    steps: 419    lr: 6.4e-06     reward: 5.71\n",
      "epis: 1979   score: 4.0   mem len: 434946   epsilon: 0.3368    steps: 259    lr: 6.4e-06     reward: 5.7\n",
      "epis: 1980   score: 3.0   mem len: 435159   epsilon: 0.3364    steps: 213    lr: 6.4e-06     reward: 5.68\n",
      "epis: 1981   score: 7.0   mem len: 435549   epsilon: 0.3356    steps: 390    lr: 6.4e-06     reward: 5.71\n",
      "epis: 1982   score: 4.0   mem len: 435790   epsilon: 0.3351    steps: 241    lr: 6.4e-06     reward: 5.71\n",
      "epis: 1983   score: 7.0   mem len: 436194   epsilon: 0.3343    steps: 404    lr: 6.4e-06     reward: 5.74\n",
      "epis: 1984   score: 8.0   mem len: 436582   epsilon: 0.3336    steps: 388    lr: 6.4e-06     reward: 5.76\n",
      "epis: 1985   score: 8.0   mem len: 436955   epsilon: 0.3328    steps: 373    lr: 6.4e-06     reward: 5.78\n",
      "epis: 1986   score: 4.0   mem len: 437231   epsilon: 0.3323    steps: 276    lr: 6.4e-06     reward: 5.76\n",
      "epis: 1987   score: 7.0   mem len: 437607   epsilon: 0.3315    steps: 376    lr: 6.4e-06     reward: 5.78\n",
      "epis: 1988   score: 8.0   mem len: 438041   epsilon: 0.3307    steps: 434    lr: 6.4e-06     reward: 5.83\n",
      "epis: 1989   score: 6.0   mem len: 438432   epsilon: 0.3299    steps: 391    lr: 6.4e-06     reward: 5.79\n",
      "epis: 1990   score: 13.0   mem len: 439052   epsilon: 0.3287    steps: 620    lr: 6.4e-06     reward: 5.83\n",
      "epis: 1991   score: 7.0   mem len: 439451   epsilon: 0.3279    steps: 399    lr: 6.4e-06     reward: 5.85\n",
      "epis: 1992   score: 10.0   mem len: 439972   epsilon: 0.3269    steps: 521    lr: 6.4e-06     reward: 5.85\n",
      "epis: 1993   score: 5.0   mem len: 440276   epsilon: 0.3263    steps: 304    lr: 6.4e-06     reward: 5.84\n",
      "epis: 1994   score: 5.0   mem len: 440567   epsilon: 0.3257    steps: 291    lr: 6.4e-06     reward: 5.86\n",
      "epis: 1995   score: 5.0   mem len: 440873   epsilon: 0.3251    steps: 306    lr: 6.4e-06     reward: 5.86\n",
      "epis: 1996   score: 7.0   mem len: 441232   epsilon: 0.3244    steps: 359    lr: 6.4e-06     reward: 5.86\n",
      "epis: 1997   score: 4.0   mem len: 441474   epsilon: 0.3239    steps: 242    lr: 6.4e-06     reward: 5.83\n",
      "epis: 1998   score: 11.0   mem len: 441885   epsilon: 0.3231    steps: 411    lr: 6.4e-06     reward: 5.87\n",
      "epis: 1999   score: 6.0   mem len: 442257   epsilon: 0.3223    steps: 372    lr: 6.4e-06     reward: 5.87\n",
      "epis: 2000   score: 6.0   mem len: 442600   epsilon: 0.3217    steps: 343    lr: 6.4e-06     reward: 5.89\n",
      "epis: 2001   score: 7.0   mem len: 442980   epsilon: 0.3209    steps: 380    lr: 6.4e-06     reward: 5.91\n",
      "epis: 2002   score: 6.0   mem len: 443352   epsilon: 0.3202    steps: 372    lr: 6.4e-06     reward: 5.94\n",
      "epis: 2003   score: 7.0   mem len: 443735   epsilon: 0.3194    steps: 383    lr: 6.4e-06     reward: 5.95\n",
      "epis: 2004   score: 3.0   mem len: 443962   epsilon: 0.319    steps: 227    lr: 6.4e-06     reward: 5.95\n",
      "epis: 2005   score: 4.0   mem len: 444221   epsilon: 0.3184    steps: 259    lr: 6.4e-06     reward: 5.96\n",
      "epis: 2006   score: 8.0   mem len: 444648   epsilon: 0.3176    steps: 427    lr: 6.4e-06     reward: 5.97\n",
      "epis: 2007   score: 11.0   mem len: 445091   epsilon: 0.3167    steps: 443    lr: 6.4e-06     reward: 6.03\n",
      "epis: 2008   score: 7.0   mem len: 445481   epsilon: 0.3159    steps: 390    lr: 6.4e-06     reward: 6.06\n",
      "epis: 2009   score: 3.0   mem len: 445694   epsilon: 0.3155    steps: 213    lr: 6.4e-06     reward: 6.06\n",
      "epis: 2010   score: 3.0   mem len: 445925   epsilon: 0.3151    steps: 231    lr: 6.4e-06     reward: 6.06\n",
      "epis: 2011   score: 7.0   mem len: 446308   epsilon: 0.3143    steps: 383    lr: 6.4e-06     reward: 6.09\n",
      "epis: 2012   score: 7.0   mem len: 446694   epsilon: 0.3135    steps: 386    lr: 6.4e-06     reward: 6.1\n",
      "epis: 2013   score: 7.0   mem len: 447066   epsilon: 0.3128    steps: 372    lr: 6.4e-06     reward: 6.14\n",
      "epis: 2014   score: 3.0   mem len: 447291   epsilon: 0.3124    steps: 225    lr: 6.4e-06     reward: 6.04\n",
      "epis: 2015   score: 3.0   mem len: 447502   epsilon: 0.3119    steps: 211    lr: 6.4e-06     reward: 6.03\n",
      "epis: 2016   score: 2.0   mem len: 447701   epsilon: 0.3116    steps: 199    lr: 6.4e-06     reward: 6.02\n",
      "epis: 2017   score: 3.0   mem len: 447913   epsilon: 0.3111    steps: 212    lr: 6.4e-06     reward: 6.02\n",
      "epis: 2018   score: 5.0   mem len: 448204   epsilon: 0.3106    steps: 291    lr: 6.4e-06     reward: 6.02\n",
      "epis: 2019   score: 6.0   mem len: 448534   epsilon: 0.3099    steps: 330    lr: 6.4e-06     reward: 6.05\n",
      "epis: 2020   score: 7.0   mem len: 448908   epsilon: 0.3092    steps: 374    lr: 6.4e-06     reward: 6.09\n",
      "epis: 2021   score: 3.0   mem len: 449156   epsilon: 0.3087    steps: 248    lr: 6.4e-06     reward: 6.07\n",
      "epis: 2022   score: 4.0   mem len: 449399   epsilon: 0.3082    steps: 243    lr: 6.4e-06     reward: 6.07\n",
      "epis: 2023   score: 2.0   mem len: 449580   epsilon: 0.3078    steps: 181    lr: 6.4e-06     reward: 6.06\n",
      "epis: 2024   score: 3.0   mem len: 449809   epsilon: 0.3074    steps: 229    lr: 6.4e-06     reward: 6.05\n",
      "epis: 2025   score: 5.0   mem len: 450115   epsilon: 0.3068    steps: 306    lr: 6.4e-06     reward: 6.04\n",
      "epis: 2026   score: 8.0   mem len: 450493   epsilon: 0.306    steps: 378    lr: 6.4e-06     reward: 6.05\n",
      "epis: 2027   score: 4.0   mem len: 450751   epsilon: 0.3055    steps: 258    lr: 6.4e-06     reward: 6.03\n",
      "epis: 2028   score: 10.0   mem len: 451194   epsilon: 0.3046    steps: 443    lr: 6.4e-06     reward: 6.1\n",
      "epis: 2029   score: 4.0   mem len: 451474   epsilon: 0.3041    steps: 280    lr: 6.4e-06     reward: 6.07\n",
      "epis: 2030   score: 7.0   mem len: 451859   epsilon: 0.3033    steps: 385    lr: 6.4e-06     reward: 6.09\n",
      "epis: 2031   score: 9.0   mem len: 452336   epsilon: 0.3024    steps: 477    lr: 6.4e-06     reward: 6.09\n",
      "epis: 2032   score: 5.0   mem len: 452644   epsilon: 0.3018    steps: 308    lr: 6.4e-06     reward: 6.09\n",
      "epis: 2033   score: 3.0   mem len: 452857   epsilon: 0.3013    steps: 213    lr: 6.4e-06     reward: 5.98\n",
      "epis: 2034   score: 7.0   mem len: 453224   epsilon: 0.3006    steps: 367    lr: 6.4e-06     reward: 5.99\n",
      "epis: 2035   score: 7.0   mem len: 453647   epsilon: 0.2998    steps: 423    lr: 6.4e-06     reward: 5.97\n",
      "epis: 2036   score: 4.0   mem len: 453925   epsilon: 0.2992    steps: 278    lr: 6.4e-06     reward: 5.95\n",
      "epis: 2037   score: 6.0   mem len: 454280   epsilon: 0.2985    steps: 355    lr: 6.4e-06     reward: 5.98\n",
      "epis: 2038   score: 5.0   mem len: 454623   epsilon: 0.2978    steps: 343    lr: 6.4e-06     reward: 5.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2039   score: 6.0   mem len: 454997   epsilon: 0.2971    steps: 374    lr: 6.4e-06     reward: 5.92\n",
      "epis: 2040   score: 4.0   mem len: 455255   epsilon: 0.2966    steps: 258    lr: 6.4e-06     reward: 5.92\n",
      "epis: 2041   score: 6.0   mem len: 455589   epsilon: 0.2959    steps: 334    lr: 6.4e-06     reward: 5.91\n",
      "epis: 2042   score: 9.0   mem len: 456047   epsilon: 0.295    steps: 458    lr: 6.4e-06     reward: 5.94\n",
      "epis: 2043   score: 4.0   mem len: 456308   epsilon: 0.2945    steps: 261    lr: 6.4e-06     reward: 5.9\n",
      "epis: 2044   score: 5.0   mem len: 456614   epsilon: 0.2939    steps: 306    lr: 6.4e-06     reward: 5.91\n",
      "epis: 2045   score: 4.0   mem len: 456856   epsilon: 0.2934    steps: 242    lr: 6.4e-06     reward: 5.9\n",
      "epis: 2046   score: 8.0   mem len: 457278   epsilon: 0.2926    steps: 422    lr: 6.4e-06     reward: 5.94\n",
      "epis: 2047   score: 3.0   mem len: 457491   epsilon: 0.2922    steps: 213    lr: 6.4e-06     reward: 5.94\n",
      "epis: 2048   score: 4.0   mem len: 457750   epsilon: 0.2917    steps: 259    lr: 6.4e-06     reward: 5.88\n",
      "epis: 2049   score: 5.0   mem len: 458073   epsilon: 0.291    steps: 323    lr: 6.4e-06     reward: 5.88\n",
      "epis: 2050   score: 5.0   mem len: 458361   epsilon: 0.2904    steps: 288    lr: 6.4e-06     reward: 5.86\n",
      "epis: 2051   score: 8.0   mem len: 458817   epsilon: 0.2895    steps: 456    lr: 6.4e-06     reward: 5.88\n",
      "epis: 2052   score: 4.0   mem len: 459078   epsilon: 0.289    steps: 261    lr: 6.4e-06     reward: 5.81\n",
      "epis: 2053   score: 5.0   mem len: 459406   epsilon: 0.2884    steps: 328    lr: 6.4e-06     reward: 5.83\n",
      "epis: 2054   score: 6.0   mem len: 459754   epsilon: 0.2877    steps: 348    lr: 6.4e-06     reward: 5.85\n",
      "epis: 2055   score: 3.0   mem len: 459982   epsilon: 0.2872    steps: 228    lr: 6.4e-06     reward: 5.79\n",
      "epis: 2056   score: 5.0   mem len: 460288   epsilon: 0.2866    steps: 306    lr: 6.4e-06     reward: 5.79\n",
      "epis: 2057   score: 4.0   mem len: 460550   epsilon: 0.2861    steps: 262    lr: 6.4e-06     reward: 5.78\n",
      "epis: 2058   score: 3.0   mem len: 460763   epsilon: 0.2857    steps: 213    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2059   score: 4.0   mem len: 461027   epsilon: 0.2852    steps: 264    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2060   score: 5.0   mem len: 461312   epsilon: 0.2846    steps: 285    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2061   score: 9.0   mem len: 461798   epsilon: 0.2836    steps: 486    lr: 6.4e-06     reward: 5.74\n",
      "epis: 2062   score: 3.0   mem len: 462027   epsilon: 0.2832    steps: 229    lr: 6.4e-06     reward: 5.7\n",
      "epis: 2063   score: 10.0   mem len: 462418   epsilon: 0.2824    steps: 391    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2064   score: 8.0   mem len: 462862   epsilon: 0.2815    steps: 444    lr: 6.4e-06     reward: 5.75\n",
      "epis: 2065   score: 5.0   mem len: 463171   epsilon: 0.2809    steps: 309    lr: 6.4e-06     reward: 5.77\n",
      "epis: 2066   score: 8.0   mem len: 463598   epsilon: 0.2801    steps: 427    lr: 6.4e-06     reward: 5.78\n",
      "epis: 2067   score: 4.0   mem len: 463893   epsilon: 0.2795    steps: 295    lr: 6.4e-06     reward: 5.74\n",
      "epis: 2068   score: 9.0   mem len: 464372   epsilon: 0.2785    steps: 479    lr: 6.4e-06     reward: 5.76\n",
      "epis: 2069   score: 9.0   mem len: 464847   epsilon: 0.2776    steps: 475    lr: 6.4e-06     reward: 5.81\n",
      "epis: 2070   score: 5.0   mem len: 465172   epsilon: 0.277    steps: 325    lr: 6.4e-06     reward: 5.8\n",
      "epis: 2071   score: 5.0   mem len: 465463   epsilon: 0.2764    steps: 291    lr: 6.4e-06     reward: 5.78\n",
      "epis: 2072   score: 2.0   mem len: 465663   epsilon: 0.276    steps: 200    lr: 6.4e-06     reward: 5.75\n",
      "epis: 2073   score: 3.0   mem len: 465895   epsilon: 0.2755    steps: 232    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2074   score: 10.0   mem len: 466388   epsilon: 0.2745    steps: 493    lr: 6.4e-06     reward: 5.77\n",
      "epis: 2075   score: 10.0   mem len: 466920   epsilon: 0.2735    steps: 532    lr: 6.4e-06     reward: 5.81\n",
      "epis: 2076   score: 6.0   mem len: 467267   epsilon: 0.2728    steps: 347    lr: 6.4e-06     reward: 5.81\n",
      "epis: 2077   score: 8.0   mem len: 467566   epsilon: 0.2722    steps: 299    lr: 6.4e-06     reward: 5.82\n",
      "epis: 2078   score: 7.0   mem len: 467964   epsilon: 0.2714    steps: 398    lr: 6.4e-06     reward: 5.81\n",
      "epis: 2079   score: 6.0   mem len: 468299   epsilon: 0.2708    steps: 335    lr: 6.4e-06     reward: 5.83\n",
      "epis: 2080   score: 2.0   mem len: 468481   epsilon: 0.2704    steps: 182    lr: 6.4e-06     reward: 5.82\n",
      "epis: 2081   score: 7.0   mem len: 468855   epsilon: 0.2697    steps: 374    lr: 6.4e-06     reward: 5.82\n",
      "epis: 2082   score: 6.0   mem len: 469227   epsilon: 0.2689    steps: 372    lr: 6.4e-06     reward: 5.84\n",
      "epis: 2083   score: 6.0   mem len: 469581   epsilon: 0.2682    steps: 354    lr: 6.4e-06     reward: 5.83\n",
      "epis: 2084   score: 6.0   mem len: 469955   epsilon: 0.2675    steps: 374    lr: 6.4e-06     reward: 5.81\n",
      "epis: 2085   score: 3.0   mem len: 470168   epsilon: 0.2671    steps: 213    lr: 6.4e-06     reward: 5.76\n",
      "epis: 2086   score: 7.0   mem len: 470551   epsilon: 0.2663    steps: 383    lr: 6.4e-06     reward: 5.79\n",
      "epis: 2087   score: 5.0   mem len: 470878   epsilon: 0.2657    steps: 327    lr: 6.4e-06     reward: 5.77\n",
      "epis: 2088   score: 5.0   mem len: 471186   epsilon: 0.265    steps: 308    lr: 6.4e-06     reward: 5.74\n",
      "epis: 2089   score: 9.0   mem len: 471669   epsilon: 0.2641    steps: 483    lr: 6.4e-06     reward: 5.77\n",
      "epis: 2090   score: 6.0   mem len: 472022   epsilon: 0.2634    steps: 353    lr: 6.4e-06     reward: 5.7\n",
      "epis: 2091   score: 6.0   mem len: 472358   epsilon: 0.2627    steps: 336    lr: 6.4e-06     reward: 5.69\n",
      "epis: 2092   score: 7.0   mem len: 472764   epsilon: 0.2619    steps: 406    lr: 6.4e-06     reward: 5.66\n",
      "epis: 2093   score: 6.0   mem len: 473133   epsilon: 0.2612    steps: 369    lr: 6.4e-06     reward: 5.67\n",
      "epis: 2094   score: 11.0   mem len: 473726   epsilon: 0.26    steps: 593    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2095   score: 4.0   mem len: 473968   epsilon: 0.2595    steps: 242    lr: 6.4e-06     reward: 5.72\n",
      "epis: 2096   score: 4.0   mem len: 474247   epsilon: 0.259    steps: 279    lr: 6.4e-06     reward: 5.69\n",
      "epis: 2097   score: 5.0   mem len: 474575   epsilon: 0.2583    steps: 328    lr: 6.4e-06     reward: 5.7\n",
      "epis: 2098   score: 4.0   mem len: 474817   epsilon: 0.2579    steps: 242    lr: 6.4e-06     reward: 5.63\n",
      "epis: 2099   score: 17.0   mem len: 475368   epsilon: 0.2568    steps: 551    lr: 6.4e-06     reward: 5.74\n",
      "epis: 2100   score: 11.0   mem len: 475891   epsilon: 0.2557    steps: 523    lr: 6.4e-06     reward: 5.79\n",
      "epis: 2101   score: 5.0   mem len: 476185   epsilon: 0.2552    steps: 294    lr: 6.4e-06     reward: 5.77\n",
      "epis: 2102   score: 4.0   mem len: 476427   epsilon: 0.2547    steps: 242    lr: 6.4e-06     reward: 5.75\n",
      "epis: 2103   score: 4.0   mem len: 476684   epsilon: 0.2542    steps: 257    lr: 6.4e-06     reward: 5.72\n",
      "epis: 2104   score: 4.0   mem len: 476958   epsilon: 0.2536    steps: 274    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2105   score: 4.0   mem len: 477233   epsilon: 0.2531    steps: 275    lr: 6.4e-06     reward: 5.73\n",
      "epis: 2106   score: 9.0   mem len: 477660   epsilon: 0.2522    steps: 427    lr: 6.4e-06     reward: 5.74\n",
      "epis: 2107   score: 9.0   mem len: 478118   epsilon: 0.2513    steps: 458    lr: 6.4e-06     reward: 5.72\n",
      "epis: 2108   score: 10.0   mem len: 478610   epsilon: 0.2504    steps: 492    lr: 6.4e-06     reward: 5.75\n",
      "epis: 2109   score: 6.0   mem len: 478981   epsilon: 0.2496    steps: 371    lr: 6.4e-06     reward: 5.78\n",
      "epis: 2110   score: 5.0   mem len: 479307   epsilon: 0.249    steps: 326    lr: 6.4e-06     reward: 5.8\n",
      "epis: 2111   score: 4.0   mem len: 479569   epsilon: 0.2485    steps: 262    lr: 6.4e-06     reward: 5.77\n",
      "epis: 2112   score: 7.0   mem len: 479994   epsilon: 0.2476    steps: 425    lr: 6.4e-06     reward: 5.77\n",
      "epis: 2113   score: 6.0   mem len: 480353   epsilon: 0.2469    steps: 359    lr: 6.4e-06     reward: 5.76\n",
      "epis: 2114   score: 7.0   mem len: 480742   epsilon: 0.2461    steps: 389    lr: 6.4e-06     reward: 5.8\n",
      "epis: 2115   score: 4.0   mem len: 480999   epsilon: 0.2456    steps: 257    lr: 6.4e-06     reward: 5.81\n",
      "epis: 2116   score: 9.0   mem len: 481501   epsilon: 0.2446    steps: 502    lr: 6.4e-06     reward: 5.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2117   score: 6.0   mem len: 481845   epsilon: 0.2439    steps: 344    lr: 6.4e-06     reward: 5.91\n",
      "epis: 2118   score: 7.0   mem len: 482249   epsilon: 0.2431    steps: 404    lr: 6.4e-06     reward: 5.93\n",
      "epis: 2119   score: 7.0   mem len: 482641   epsilon: 0.2424    steps: 392    lr: 6.4e-06     reward: 5.94\n",
      "epis: 2120   score: 4.0   mem len: 482900   epsilon: 0.2419    steps: 259    lr: 6.4e-06     reward: 5.91\n",
      "epis: 2121   score: 3.0   mem len: 483129   epsilon: 0.2414    steps: 229    lr: 6.4e-06     reward: 5.91\n",
      "epis: 2122   score: 5.0   mem len: 483440   epsilon: 0.2408    steps: 311    lr: 6.4e-06     reward: 5.92\n",
      "epis: 2123   score: 4.0   mem len: 483720   epsilon: 0.2402    steps: 280    lr: 6.4e-06     reward: 5.94\n",
      "epis: 2124   score: 4.0   mem len: 483999   epsilon: 0.2397    steps: 279    lr: 6.4e-06     reward: 5.95\n",
      "epis: 2125   score: 10.0   mem len: 484523   epsilon: 0.2386    steps: 524    lr: 6.4e-06     reward: 6.0\n",
      "epis: 2126   score: 5.0   mem len: 484867   epsilon: 0.238    steps: 344    lr: 6.4e-06     reward: 5.97\n",
      "epis: 2127   score: 3.0   mem len: 485097   epsilon: 0.2375    steps: 230    lr: 6.4e-06     reward: 5.96\n",
      "epis: 2128   score: 4.0   mem len: 485357   epsilon: 0.237    steps: 260    lr: 6.4e-06     reward: 5.9\n",
      "epis: 2129   score: 6.0   mem len: 485698   epsilon: 0.2363    steps: 341    lr: 6.4e-06     reward: 5.92\n",
      "epis: 2130   score: 5.0   mem len: 485989   epsilon: 0.2357    steps: 291    lr: 6.4e-06     reward: 5.9\n",
      "epis: 2131   score: 3.0   mem len: 486236   epsilon: 0.2353    steps: 247    lr: 6.4e-06     reward: 5.84\n",
      "epis: 2132   score: 6.0   mem len: 486579   epsilon: 0.2346    steps: 343    lr: 6.4e-06     reward: 5.85\n",
      "epis: 2133   score: 7.0   mem len: 486952   epsilon: 0.2338    steps: 373    lr: 6.4e-06     reward: 5.89\n",
      "epis: 2134   score: 7.0   mem len: 487314   epsilon: 0.2331    steps: 362    lr: 6.4e-06     reward: 5.89\n",
      "epis: 2135   score: 4.0   mem len: 487576   epsilon: 0.2326    steps: 262    lr: 6.4e-06     reward: 5.86\n",
      "epis: 2136   score: 5.0   mem len: 487883   epsilon: 0.232    steps: 307    lr: 6.4e-06     reward: 5.87\n",
      "epis: 2137   score: 12.0   mem len: 488376   epsilon: 0.231    steps: 493    lr: 6.4e-06     reward: 5.93\n",
      "epis: 2138   score: 4.0   mem len: 488655   epsilon: 0.2305    steps: 279    lr: 6.4e-06     reward: 5.92\n",
      "epis: 2139   score: 11.0   mem len: 489162   epsilon: 0.2295    steps: 507    lr: 6.4e-06     reward: 5.97\n",
      "epis: 2140   score: 5.0   mem len: 489488   epsilon: 0.2288    steps: 326    lr: 6.4e-06     reward: 5.98\n",
      "epis: 2141   score: 4.0   mem len: 489763   epsilon: 0.2283    steps: 275    lr: 6.4e-06     reward: 5.96\n",
      "epis: 2142   score: 5.0   mem len: 490033   epsilon: 0.2277    steps: 270    lr: 6.4e-06     reward: 5.92\n",
      "epis: 2143   score: 12.0   mem len: 490437   epsilon: 0.2269    steps: 404    lr: 6.4e-06     reward: 6.0\n",
      "epis: 2144   score: 10.0   mem len: 490881   epsilon: 0.2261    steps: 444    lr: 6.4e-06     reward: 6.05\n",
      "epis: 2145   score: 13.0   mem len: 491359   epsilon: 0.2251    steps: 478    lr: 6.4e-06     reward: 6.14\n",
      "epis: 2146   score: 6.0   mem len: 491698   epsilon: 0.2244    steps: 339    lr: 6.4e-06     reward: 6.12\n",
      "epis: 2147   score: 4.0   mem len: 491996   epsilon: 0.2238    steps: 298    lr: 6.4e-06     reward: 6.13\n",
      "epis: 2148   score: 11.0   mem len: 492430   epsilon: 0.223    steps: 434    lr: 6.4e-06     reward: 6.2\n",
      "epis: 2149   score: 7.0   mem len: 492842   epsilon: 0.2222    steps: 412    lr: 6.4e-06     reward: 6.22\n",
      "epis: 2150   score: 9.0   mem len: 493296   epsilon: 0.2213    steps: 454    lr: 6.4e-06     reward: 6.26\n",
      "epis: 2151   score: 9.0   mem len: 493785   epsilon: 0.2203    steps: 489    lr: 6.4e-06     reward: 6.27\n",
      "epis: 2152   score: 8.0   mem len: 494207   epsilon: 0.2195    steps: 422    lr: 6.4e-06     reward: 6.31\n",
      "epis: 2153   score: 7.0   mem len: 494573   epsilon: 0.2187    steps: 366    lr: 6.4e-06     reward: 6.33\n",
      "epis: 2154   score: 6.0   mem len: 494950   epsilon: 0.218    steps: 377    lr: 6.4e-06     reward: 6.33\n",
      "epis: 2155   score: 7.0   mem len: 495332   epsilon: 0.2172    steps: 382    lr: 6.4e-06     reward: 6.37\n",
      "epis: 2156   score: 3.0   mem len: 495563   epsilon: 0.2168    steps: 231    lr: 6.4e-06     reward: 6.35\n",
      "epis: 2157   score: 4.0   mem len: 495805   epsilon: 0.2163    steps: 242    lr: 6.4e-06     reward: 6.35\n",
      "epis: 2158   score: 8.0   mem len: 496211   epsilon: 0.2155    steps: 406    lr: 6.4e-06     reward: 6.4\n",
      "epis: 2159   score: 10.0   mem len: 496688   epsilon: 0.2146    steps: 477    lr: 6.4e-06     reward: 6.46\n",
      "epis: 2160   score: 4.0   mem len: 496947   epsilon: 0.214    steps: 259    lr: 6.4e-06     reward: 6.45\n",
      "epis: 2161   score: 5.0   mem len: 497242   epsilon: 0.2135    steps: 295    lr: 6.4e-06     reward: 6.41\n",
      "epis: 2162   score: 7.0   mem len: 497629   epsilon: 0.2127    steps: 387    lr: 6.4e-06     reward: 6.45\n",
      "epis: 2163   score: 4.0   mem len: 497907   epsilon: 0.2121    steps: 278    lr: 6.4e-06     reward: 6.39\n",
      "epis: 2164   score: 9.0   mem len: 498377   epsilon: 0.2112    steps: 470    lr: 6.4e-06     reward: 6.4\n",
      "epis: 2165   score: 6.0   mem len: 498720   epsilon: 0.2105    steps: 343    lr: 6.4e-06     reward: 6.41\n",
      "epis: 2166   score: 5.0   mem len: 499029   epsilon: 0.2099    steps: 309    lr: 6.4e-06     reward: 6.38\n",
      "epis: 2167   score: 6.0   mem len: 499371   epsilon: 0.2092    steps: 342    lr: 6.4e-06     reward: 6.4\n",
      "epis: 2168   score: 3.0   mem len: 499602   epsilon: 0.2088    steps: 231    lr: 6.4e-06     reward: 6.34\n",
      "epis: 2169   score: 7.0   mem len: 499991   epsilon: 0.208    steps: 389    lr: 6.4e-06     reward: 6.32\n",
      "epis: 2170   score: 7.0   mem len: 500369   epsilon: 0.2073    steps: 378    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2171   score: 4.0   mem len: 500644   epsilon: 0.2067    steps: 275    lr: 2.6e-06     reward: 6.33\n",
      "epis: 2172   score: 5.0   mem len: 500955   epsilon: 0.2061    steps: 311    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2173   score: 8.0   mem len: 501334   epsilon: 0.2054    steps: 379    lr: 2.6e-06     reward: 6.41\n",
      "epis: 2174   score: 4.0   mem len: 501597   epsilon: 0.2048    steps: 263    lr: 2.6e-06     reward: 6.35\n",
      "epis: 2175   score: 7.0   mem len: 502019   epsilon: 0.204    steps: 422    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2176   score: 5.0   mem len: 502330   epsilon: 0.2034    steps: 311    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2177   score: 6.0   mem len: 502660   epsilon: 0.2027    steps: 330    lr: 2.6e-06     reward: 6.29\n",
      "epis: 2178   score: 5.0   mem len: 502969   epsilon: 0.2021    steps: 309    lr: 2.6e-06     reward: 6.27\n",
      "epis: 2179   score: 4.0   mem len: 503248   epsilon: 0.2016    steps: 279    lr: 2.6e-06     reward: 6.25\n",
      "epis: 2180   score: 6.0   mem len: 503619   epsilon: 0.2008    steps: 371    lr: 2.6e-06     reward: 6.29\n",
      "epis: 2181   score: 10.0   mem len: 504080   epsilon: 0.1999    steps: 461    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2182   score: 4.0   mem len: 504339   epsilon: 0.1994    steps: 259    lr: 2.6e-06     reward: 6.3\n",
      "epis: 2183   score: 7.0   mem len: 504742   epsilon: 0.1986    steps: 403    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2184   score: 5.0   mem len: 505071   epsilon: 0.198    steps: 329    lr: 2.6e-06     reward: 6.3\n",
      "epis: 2185   score: 9.0   mem len: 505525   epsilon: 0.1971    steps: 454    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2186   score: 5.0   mem len: 505853   epsilon: 0.1964    steps: 328    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2187   score: 9.0   mem len: 506320   epsilon: 0.1955    steps: 467    lr: 2.6e-06     reward: 6.38\n",
      "epis: 2188   score: 4.0   mem len: 506562   epsilon: 0.195    steps: 242    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2189   score: 5.0   mem len: 506890   epsilon: 0.1944    steps: 328    lr: 2.6e-06     reward: 6.33\n",
      "epis: 2190   score: 5.0   mem len: 507216   epsilon: 0.1937    steps: 326    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2191   score: 10.0   mem len: 507701   epsilon: 0.1928    steps: 485    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2192   score: 7.0   mem len: 508086   epsilon: 0.192    steps: 385    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2193   score: 7.0   mem len: 508459   epsilon: 0.1912    steps: 373    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2194   score: 8.0   mem len: 508914   epsilon: 0.1903    steps: 455    lr: 2.6e-06     reward: 6.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2195   score: 5.0   mem len: 509224   epsilon: 0.1897    steps: 310    lr: 2.6e-06     reward: 6.35\n",
      "epis: 2196   score: 15.0   mem len: 509657   epsilon: 0.1889    steps: 433    lr: 2.6e-06     reward: 6.46\n",
      "epis: 2197   score: 6.0   mem len: 510048   epsilon: 0.1881    steps: 391    lr: 2.6e-06     reward: 6.47\n",
      "epis: 2198   score: 6.0   mem len: 510381   epsilon: 0.1874    steps: 333    lr: 2.6e-06     reward: 6.49\n",
      "epis: 2199   score: 5.0   mem len: 510687   epsilon: 0.1868    steps: 306    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2200   score: 6.0   mem len: 511060   epsilon: 0.1861    steps: 373    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2201   score: 8.0   mem len: 511537   epsilon: 0.1852    steps: 477    lr: 2.6e-06     reward: 6.35\n",
      "epis: 2202   score: 5.0   mem len: 511828   epsilon: 0.1846    steps: 291    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2203   score: 5.0   mem len: 512157   epsilon: 0.1839    steps: 329    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2204   score: 15.0   mem len: 512705   epsilon: 0.1828    steps: 548    lr: 2.6e-06     reward: 6.48\n",
      "epis: 2205   score: 5.0   mem len: 513034   epsilon: 0.1822    steps: 329    lr: 2.6e-06     reward: 6.49\n",
      "epis: 2206   score: 5.0   mem len: 513344   epsilon: 0.1816    steps: 310    lr: 2.6e-06     reward: 6.45\n",
      "epis: 2207   score: 9.0   mem len: 513801   epsilon: 0.1807    steps: 457    lr: 2.6e-06     reward: 6.45\n",
      "epis: 2208   score: 4.0   mem len: 514041   epsilon: 0.1802    steps: 240    lr: 2.6e-06     reward: 6.39\n",
      "epis: 2209   score: 5.0   mem len: 514384   epsilon: 0.1795    steps: 343    lr: 2.6e-06     reward: 6.38\n",
      "epis: 2210   score: 4.0   mem len: 514644   epsilon: 0.179    steps: 260    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2211   score: 4.0   mem len: 514906   epsilon: 0.1785    steps: 262    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2212   score: 4.0   mem len: 515166   epsilon: 0.178    steps: 260    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2213   score: 5.0   mem len: 515494   epsilon: 0.1773    steps: 328    lr: 2.6e-06     reward: 6.33\n",
      "epis: 2214   score: 3.0   mem len: 515724   epsilon: 0.1769    steps: 230    lr: 2.6e-06     reward: 6.29\n",
      "epis: 2215   score: 7.0   mem len: 516109   epsilon: 0.1761    steps: 385    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2216   score: 8.0   mem len: 516501   epsilon: 0.1753    steps: 392    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2217   score: 3.0   mem len: 516712   epsilon: 0.1749    steps: 211    lr: 2.6e-06     reward: 6.28\n",
      "epis: 2218   score: 3.0   mem len: 516925   epsilon: 0.1745    steps: 213    lr: 2.6e-06     reward: 6.24\n",
      "epis: 2219   score: 4.0   mem len: 517185   epsilon: 0.174    steps: 260    lr: 2.6e-06     reward: 6.21\n",
      "epis: 2220   score: 14.0   mem len: 517703   epsilon: 0.1729    steps: 518    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2221   score: 9.0   mem len: 518130   epsilon: 0.1721    steps: 427    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2222   score: 5.0   mem len: 518420   epsilon: 0.1715    steps: 290    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2223   score: 6.0   mem len: 518744   epsilon: 0.1709    steps: 324    lr: 2.6e-06     reward: 6.39\n",
      "epis: 2224   score: 5.0   mem len: 519055   epsilon: 0.1703    steps: 311    lr: 2.6e-06     reward: 6.4\n",
      "epis: 2225   score: 10.0   mem len: 519579   epsilon: 0.1692    steps: 524    lr: 2.6e-06     reward: 6.4\n",
      "epis: 2226   score: 3.0   mem len: 519792   epsilon: 0.1688    steps: 213    lr: 2.6e-06     reward: 6.38\n",
      "epis: 2227   score: 4.0   mem len: 520066   epsilon: 0.1683    steps: 274    lr: 2.6e-06     reward: 6.39\n",
      "epis: 2228   score: 7.0   mem len: 520473   epsilon: 0.1675    steps: 407    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2229   score: 4.0   mem len: 520717   epsilon: 0.167    steps: 244    lr: 2.6e-06     reward: 6.4\n",
      "epis: 2230   score: 6.0   mem len: 521057   epsilon: 0.1663    steps: 340    lr: 2.6e-06     reward: 6.41\n",
      "epis: 2231   score: 5.0   mem len: 521351   epsilon: 0.1657    steps: 294    lr: 2.6e-06     reward: 6.43\n",
      "epis: 2232   score: 7.0   mem len: 521755   epsilon: 0.1649    steps: 404    lr: 2.6e-06     reward: 6.44\n",
      "epis: 2233   score: 3.0   mem len: 521985   epsilon: 0.1645    steps: 230    lr: 2.6e-06     reward: 6.4\n",
      "epis: 2234   score: 7.0   mem len: 522354   epsilon: 0.1637    steps: 369    lr: 2.6e-06     reward: 6.4\n",
      "epis: 2235   score: 9.0   mem len: 522794   epsilon: 0.1629    steps: 440    lr: 2.6e-06     reward: 6.45\n",
      "epis: 2236   score: 9.0   mem len: 523265   epsilon: 0.1619    steps: 471    lr: 2.6e-06     reward: 6.49\n",
      "epis: 2237   score: 5.0   mem len: 523557   epsilon: 0.1614    steps: 292    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2238   score: 5.0   mem len: 523868   epsilon: 0.1607    steps: 311    lr: 2.6e-06     reward: 6.43\n",
      "epis: 2239   score: 10.0   mem len: 524362   epsilon: 0.1598    steps: 494    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2240   score: 5.0   mem len: 524671   epsilon: 0.1591    steps: 309    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2241   score: 4.0   mem len: 524948   epsilon: 0.1586    steps: 277    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2242   score: 3.0   mem len: 525180   epsilon: 0.1581    steps: 232    lr: 2.6e-06     reward: 6.4\n",
      "epis: 2243   score: 6.0   mem len: 525523   epsilon: 0.1575    steps: 343    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2244   score: 7.0   mem len: 525935   epsilon: 0.1566    steps: 412    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2245   score: 5.0   mem len: 526261   epsilon: 0.156    steps: 326    lr: 2.6e-06     reward: 6.23\n",
      "epis: 2246   score: 9.0   mem len: 526719   epsilon: 0.1551    steps: 458    lr: 2.6e-06     reward: 6.26\n",
      "epis: 2247   score: 3.0   mem len: 526948   epsilon: 0.1546    steps: 229    lr: 2.6e-06     reward: 6.25\n",
      "epis: 2248   score: 12.0   mem len: 527506   epsilon: 0.1535    steps: 558    lr: 2.6e-06     reward: 6.26\n",
      "epis: 2249   score: 5.0   mem len: 527813   epsilon: 0.1529    steps: 307    lr: 2.6e-06     reward: 6.24\n",
      "epis: 2250   score: 6.0   mem len: 528169   epsilon: 0.1522    steps: 356    lr: 2.6e-06     reward: 6.21\n",
      "epis: 2251   score: 4.0   mem len: 528449   epsilon: 0.1517    steps: 280    lr: 2.6e-06     reward: 6.16\n",
      "epis: 2252   score: 11.0   mem len: 529034   epsilon: 0.1505    steps: 585    lr: 2.6e-06     reward: 6.19\n",
      "epis: 2253   score: 9.0   mem len: 529539   epsilon: 0.1495    steps: 505    lr: 2.6e-06     reward: 6.21\n",
      "epis: 2254   score: 9.0   mem len: 530010   epsilon: 0.1486    steps: 471    lr: 2.6e-06     reward: 6.24\n",
      "epis: 2255   score: 5.0   mem len: 530320   epsilon: 0.148    steps: 310    lr: 2.6e-06     reward: 6.22\n",
      "epis: 2256   score: 6.0   mem len: 530677   epsilon: 0.1473    steps: 357    lr: 2.6e-06     reward: 6.25\n",
      "epis: 2257   score: 7.0   mem len: 531029   epsilon: 0.1466    steps: 352    lr: 2.6e-06     reward: 6.28\n",
      "epis: 2258   score: 11.0   mem len: 531570   epsilon: 0.1455    steps: 541    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2259   score: 6.0   mem len: 531909   epsilon: 0.1448    steps: 339    lr: 2.6e-06     reward: 6.27\n",
      "epis: 2260   score: 7.0   mem len: 532282   epsilon: 0.1441    steps: 373    lr: 2.6e-06     reward: 6.3\n",
      "epis: 2261   score: 7.0   mem len: 532702   epsilon: 0.1432    steps: 420    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2262   score: 10.0   mem len: 533096   epsilon: 0.1425    steps: 394    lr: 2.6e-06     reward: 6.35\n",
      "epis: 2263   score: 6.0   mem len: 533438   epsilon: 0.1418    steps: 342    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2264   score: 6.0   mem len: 533796   epsilon: 0.1411    steps: 358    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2265   score: 5.0   mem len: 534085   epsilon: 0.1405    steps: 289    lr: 2.6e-06     reward: 6.33\n",
      "epis: 2266   score: 6.0   mem len: 534443   epsilon: 0.1398    steps: 358    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2267   score: 5.0   mem len: 534751   epsilon: 0.1392    steps: 308    lr: 2.6e-06     reward: 6.33\n",
      "epis: 2268   score: 4.0   mem len: 534993   epsilon: 0.1387    steps: 242    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2269   score: 7.0   mem len: 535381   epsilon: 0.1379    steps: 388    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2270   score: 5.0   mem len: 535711   epsilon: 0.1373    steps: 330    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2271   score: 8.0   mem len: 536169   epsilon: 0.1364    steps: 458    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2272   score: 5.0   mem len: 536459   epsilon: 0.1358    steps: 290    lr: 2.6e-06     reward: 6.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2273   score: 4.0   mem len: 536700   epsilon: 0.1353    steps: 241    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2274   score: 5.0   mem len: 537028   epsilon: 0.1347    steps: 328    lr: 2.6e-06     reward: 6.33\n",
      "epis: 2275   score: 4.0   mem len: 537285   epsilon: 0.1342    steps: 257    lr: 2.6e-06     reward: 6.3\n",
      "epis: 2276   score: 3.0   mem len: 537498   epsilon: 0.1338    steps: 213    lr: 2.6e-06     reward: 6.28\n",
      "epis: 2277   score: 4.0   mem len: 537757   epsilon: 0.1332    steps: 259    lr: 2.6e-06     reward: 6.26\n",
      "epis: 2278   score: 5.0   mem len: 538048   epsilon: 0.1327    steps: 291    lr: 2.6e-06     reward: 6.26\n",
      "epis: 2279   score: 9.0   mem len: 538521   epsilon: 0.1317    steps: 473    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2280   score: 6.0   mem len: 538863   epsilon: 0.131    steps: 342    lr: 2.6e-06     reward: 6.31\n",
      "epis: 2281   score: 4.0   mem len: 539120   epsilon: 0.1305    steps: 257    lr: 2.6e-06     reward: 6.25\n",
      "epis: 2282   score: 4.0   mem len: 539362   epsilon: 0.1301    steps: 242    lr: 2.6e-06     reward: 6.25\n",
      "epis: 2283   score: 5.0   mem len: 539671   epsilon: 0.1294    steps: 309    lr: 2.6e-06     reward: 6.23\n",
      "epis: 2284   score: 11.0   mem len: 540215   epsilon: 0.1284    steps: 544    lr: 2.6e-06     reward: 6.29\n",
      "epis: 2285   score: 8.0   mem len: 540677   epsilon: 0.1275    steps: 462    lr: 2.6e-06     reward: 6.28\n",
      "epis: 2286   score: 9.0   mem len: 541130   epsilon: 0.1266    steps: 453    lr: 2.6e-06     reward: 6.32\n",
      "epis: 2287   score: 6.0   mem len: 541444   epsilon: 0.1259    steps: 314    lr: 2.6e-06     reward: 6.29\n",
      "epis: 2288   score: 8.0   mem len: 541869   epsilon: 0.1251    steps: 425    lr: 2.6e-06     reward: 6.33\n",
      "epis: 2289   score: 8.0   mem len: 542290   epsilon: 0.1243    steps: 421    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2290   score: 3.0   mem len: 542520   epsilon: 0.1238    steps: 230    lr: 2.6e-06     reward: 6.34\n",
      "epis: 2291   score: 11.0   mem len: 543017   epsilon: 0.1228    steps: 497    lr: 2.6e-06     reward: 6.35\n",
      "epis: 2292   score: 11.0   mem len: 543572   epsilon: 0.1217    steps: 555    lr: 2.6e-06     reward: 6.39\n",
      "epis: 2293   score: 5.0   mem len: 543913   epsilon: 0.1211    steps: 341    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2294   score: 10.0   mem len: 544407   epsilon: 0.1201    steps: 494    lr: 2.6e-06     reward: 6.39\n",
      "epis: 2295   score: 11.0   mem len: 544801   epsilon: 0.1193    steps: 394    lr: 2.6e-06     reward: 6.45\n",
      "epis: 2296   score: 6.0   mem len: 545163   epsilon: 0.1186    steps: 362    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2297   score: 7.0   mem len: 545544   epsilon: 0.1178    steps: 381    lr: 2.6e-06     reward: 6.37\n",
      "epis: 2298   score: 11.0   mem len: 546088   epsilon: 0.1167    steps: 544    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2299   score: 6.0   mem len: 546420   epsilon: 0.1161    steps: 332    lr: 2.6e-06     reward: 6.43\n",
      "epis: 2300   score: 8.0   mem len: 546859   epsilon: 0.1152    steps: 439    lr: 2.6e-06     reward: 6.45\n",
      "epis: 2301   score: 4.0   mem len: 547118   epsilon: 0.1147    steps: 259    lr: 2.6e-06     reward: 6.41\n",
      "epis: 2302   score: 6.0   mem len: 547473   epsilon: 0.114    steps: 355    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2303   score: 9.0   mem len: 547946   epsilon: 0.1131    steps: 473    lr: 2.6e-06     reward: 6.46\n",
      "epis: 2304   score: 5.0   mem len: 548273   epsilon: 0.1124    steps: 327    lr: 2.6e-06     reward: 6.36\n",
      "epis: 2305   score: 4.0   mem len: 548532   epsilon: 0.1119    steps: 259    lr: 2.6e-06     reward: 6.35\n",
      "epis: 2306   score: 16.0   mem len: 549117   epsilon: 0.1107    steps: 585    lr: 2.6e-06     reward: 6.46\n",
      "epis: 2307   score: 5.0   mem len: 549460   epsilon: 0.1101    steps: 343    lr: 2.6e-06     reward: 6.42\n",
      "epis: 2308   score: 10.0   mem len: 549930   epsilon: 0.1091    steps: 470    lr: 2.6e-06     reward: 6.48\n",
      "epis: 2309   score: 9.0   mem len: 550384   epsilon: 0.1082    steps: 454    lr: 2.6e-06     reward: 6.52\n",
      "epis: 2310   score: 9.0   mem len: 550886   epsilon: 0.1072    steps: 502    lr: 2.6e-06     reward: 6.57\n",
      "epis: 2311   score: 5.0   mem len: 551189   epsilon: 0.1066    steps: 303    lr: 2.6e-06     reward: 6.58\n",
      "epis: 2312   score: 5.0   mem len: 551470   epsilon: 0.1061    steps: 281    lr: 2.6e-06     reward: 6.59\n",
      "epis: 2313   score: 4.0   mem len: 551732   epsilon: 0.1056    steps: 262    lr: 2.6e-06     reward: 6.58\n",
      "epis: 2314   score: 5.0   mem len: 552043   epsilon: 0.105    steps: 311    lr: 2.6e-06     reward: 6.6\n",
      "epis: 2315   score: 10.0   mem len: 552414   epsilon: 0.1042    steps: 371    lr: 2.6e-06     reward: 6.63\n",
      "epis: 2316   score: 8.0   mem len: 552872   epsilon: 0.1033    steps: 458    lr: 2.6e-06     reward: 6.63\n",
      "epis: 2317   score: 9.0   mem len: 553324   epsilon: 0.1024    steps: 452    lr: 2.6e-06     reward: 6.69\n",
      "epis: 2318   score: 11.0   mem len: 553799   epsilon: 0.1015    steps: 475    lr: 2.6e-06     reward: 6.77\n",
      "epis: 2319   score: 5.0   mem len: 554107   epsilon: 0.1009    steps: 308    lr: 2.6e-06     reward: 6.78\n",
      "epis: 2320   score: 6.0   mem len: 554469   epsilon: 0.1001    steps: 362    lr: 2.6e-06     reward: 6.7\n",
      "epis: 2321   score: 5.0   mem len: 554757   epsilon: 0.0996    steps: 288    lr: 2.6e-06     reward: 6.66\n",
      "epis: 2322   score: 8.0   mem len: 555192   epsilon: 0.0987    steps: 435    lr: 2.6e-06     reward: 6.69\n",
      "epis: 2323   score: 10.0   mem len: 555727   epsilon: 0.0977    steps: 535    lr: 2.6e-06     reward: 6.73\n",
      "epis: 2324   score: 7.0   mem len: 556135   epsilon: 0.0969    steps: 408    lr: 2.6e-06     reward: 6.75\n",
      "epis: 2325   score: 12.0   mem len: 556687   epsilon: 0.0958    steps: 552    lr: 2.6e-06     reward: 6.77\n",
      "epis: 2326   score: 6.0   mem len: 557042   epsilon: 0.0951    steps: 355    lr: 2.6e-06     reward: 6.8\n",
      "epis: 2327   score: 5.0   mem len: 557317   epsilon: 0.0945    steps: 275    lr: 2.6e-06     reward: 6.81\n",
      "epis: 2328   score: 7.0   mem len: 557662   epsilon: 0.0938    steps: 345    lr: 2.6e-06     reward: 6.81\n",
      "epis: 2329   score: 8.0   mem len: 558070   epsilon: 0.093    steps: 408    lr: 2.6e-06     reward: 6.85\n",
      "epis: 2330   score: 11.0   mem len: 558598   epsilon: 0.092    steps: 528    lr: 2.6e-06     reward: 6.9\n",
      "epis: 2331   score: 10.0   mem len: 559121   epsilon: 0.0909    steps: 523    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2332   score: 4.0   mem len: 559383   epsilon: 0.0904    steps: 262    lr: 2.6e-06     reward: 6.92\n",
      "epis: 2333   score: 6.0   mem len: 559690   epsilon: 0.0898    steps: 307    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2334   score: 9.0   mem len: 560090   epsilon: 0.089    steps: 400    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2335   score: 13.0   mem len: 560702   epsilon: 0.0878    steps: 612    lr: 2.6e-06     reward: 7.01\n",
      "epis: 2336   score: 3.0   mem len: 560932   epsilon: 0.0874    steps: 230    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2337   score: 5.0   mem len: 561223   epsilon: 0.0868    steps: 291    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2338   score: 6.0   mem len: 561595   epsilon: 0.086    steps: 372    lr: 2.6e-06     reward: 6.96\n",
      "epis: 2339   score: 6.0   mem len: 561951   epsilon: 0.0853    steps: 356    lr: 2.6e-06     reward: 6.92\n",
      "epis: 2340   score: 6.0   mem len: 562309   epsilon: 0.0846    steps: 358    lr: 2.6e-06     reward: 6.93\n",
      "epis: 2341   score: 6.0   mem len: 562649   epsilon: 0.084    steps: 340    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2342   score: 5.0   mem len: 562952   epsilon: 0.0834    steps: 303    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2343   score: 4.0   mem len: 563214   epsilon: 0.0828    steps: 262    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2344   score: 9.0   mem len: 563685   epsilon: 0.0819    steps: 471    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2345   score: 12.0   mem len: 564293   epsilon: 0.0807    steps: 608    lr: 2.6e-06     reward: 7.04\n",
      "epis: 2346   score: 4.0   mem len: 564554   epsilon: 0.0802    steps: 261    lr: 2.6e-06     reward: 6.99\n",
      "epis: 2347   score: 10.0   mem len: 564952   epsilon: 0.0794    steps: 398    lr: 2.6e-06     reward: 7.06\n",
      "epis: 2348   score: 6.0   mem len: 565283   epsilon: 0.0787    steps: 331    lr: 2.6e-06     reward: 7.0\n",
      "epis: 2349   score: 6.0   mem len: 565640   epsilon: 0.078    steps: 357    lr: 2.6e-06     reward: 7.01\n",
      "epis: 2350   score: 8.0   mem len: 566057   epsilon: 0.0772    steps: 417    lr: 2.6e-06     reward: 7.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2351   score: 9.0   mem len: 566503   epsilon: 0.0763    steps: 446    lr: 2.6e-06     reward: 7.08\n",
      "epis: 2352   score: 6.0   mem len: 566810   epsilon: 0.0757    steps: 307    lr: 2.6e-06     reward: 7.03\n",
      "epis: 2353   score: 11.0   mem len: 567401   epsilon: 0.0745    steps: 591    lr: 2.6e-06     reward: 7.05\n",
      "epis: 2354   score: 5.0   mem len: 567690   epsilon: 0.074    steps: 289    lr: 2.6e-06     reward: 7.01\n",
      "epis: 2355   score: 9.0   mem len: 568161   epsilon: 0.073    steps: 471    lr: 2.6e-06     reward: 7.05\n",
      "epis: 2356   score: 5.0   mem len: 568472   epsilon: 0.0724    steps: 311    lr: 2.6e-06     reward: 7.04\n",
      "epis: 2357   score: 4.0   mem len: 568749   epsilon: 0.0719    steps: 277    lr: 2.6e-06     reward: 7.01\n",
      "epis: 2358   score: 5.0   mem len: 569075   epsilon: 0.0712    steps: 326    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2359   score: 5.0   mem len: 569366   epsilon: 0.0707    steps: 291    lr: 2.6e-06     reward: 6.94\n",
      "epis: 2360   score: 5.0   mem len: 569671   epsilon: 0.07    steps: 305    lr: 2.6e-06     reward: 6.92\n",
      "epis: 2361   score: 5.0   mem len: 570022   epsilon: 0.0694    steps: 351    lr: 2.6e-06     reward: 6.9\n",
      "epis: 2362   score: 6.0   mem len: 570369   epsilon: 0.0687    steps: 347    lr: 2.6e-06     reward: 6.86\n",
      "epis: 2363   score: 6.0   mem len: 570681   epsilon: 0.068    steps: 312    lr: 2.6e-06     reward: 6.86\n",
      "epis: 2364   score: 8.0   mem len: 571103   epsilon: 0.0672    steps: 422    lr: 2.6e-06     reward: 6.88\n",
      "epis: 2365   score: 8.0   mem len: 571486   epsilon: 0.0665    steps: 383    lr: 2.6e-06     reward: 6.91\n",
      "epis: 2366   score: 5.0   mem len: 571794   epsilon: 0.0658    steps: 308    lr: 2.6e-06     reward: 6.9\n",
      "epis: 2367   score: 9.0   mem len: 572265   epsilon: 0.0649    steps: 471    lr: 2.6e-06     reward: 6.94\n",
      "epis: 2368   score: 7.0   mem len: 572647   epsilon: 0.0642    steps: 382    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2369   score: 5.0   mem len: 572972   epsilon: 0.0635    steps: 325    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2370   score: 7.0   mem len: 573378   epsilon: 0.0627    steps: 406    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2371   score: 8.0   mem len: 573810   epsilon: 0.0619    steps: 432    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2372   score: 11.0   mem len: 574224   epsilon: 0.061    steps: 414    lr: 2.6e-06     reward: 7.03\n",
      "epis: 2373   score: 4.0   mem len: 574503   epsilon: 0.0605    steps: 279    lr: 2.6e-06     reward: 7.03\n",
      "epis: 2374   score: 6.0   mem len: 574840   epsilon: 0.0598    steps: 337    lr: 2.6e-06     reward: 7.04\n",
      "epis: 2375   score: 12.0   mem len: 575435   epsilon: 0.0586    steps: 595    lr: 2.6e-06     reward: 7.12\n",
      "epis: 2376   score: 5.0   mem len: 575743   epsilon: 0.058    steps: 308    lr: 2.6e-06     reward: 7.14\n",
      "epis: 2377   score: 8.0   mem len: 576163   epsilon: 0.0572    steps: 420    lr: 2.6e-06     reward: 7.18\n",
      "epis: 2378   score: 9.0   mem len: 576634   epsilon: 0.0563    steps: 471    lr: 2.6e-06     reward: 7.22\n",
      "epis: 2379   score: 8.0   mem len: 577035   epsilon: 0.0555    steps: 401    lr: 2.6e-06     reward: 7.21\n",
      "epis: 2380   score: 4.0   mem len: 577277   epsilon: 0.055    steps: 242    lr: 2.6e-06     reward: 7.19\n",
      "epis: 2381   score: 6.0   mem len: 577659   epsilon: 0.0542    steps: 382    lr: 2.6e-06     reward: 7.21\n",
      "epis: 2382   score: 5.0   mem len: 577966   epsilon: 0.0536    steps: 307    lr: 2.6e-06     reward: 7.22\n",
      "epis: 2383   score: 4.0   mem len: 578223   epsilon: 0.0531    steps: 257    lr: 2.6e-06     reward: 7.21\n",
      "epis: 2384   score: 9.0   mem len: 578675   epsilon: 0.0522    steps: 452    lr: 2.6e-06     reward: 7.19\n",
      "epis: 2385   score: 6.0   mem len: 579032   epsilon: 0.0515    steps: 357    lr: 2.6e-06     reward: 7.17\n",
      "epis: 2386   score: 12.0   mem len: 579665   epsilon: 0.0503    steps: 633    lr: 2.6e-06     reward: 7.2\n",
      "epis: 2387   score: 4.0   mem len: 579907   epsilon: 0.0498    steps: 242    lr: 2.6e-06     reward: 7.18\n",
      "epis: 2388   score: 5.0   mem len: 580188   epsilon: 0.0492    steps: 281    lr: 2.6e-06     reward: 7.15\n",
      "epis: 2389   score: 5.0   mem len: 580483   epsilon: 0.0486    steps: 295    lr: 2.6e-06     reward: 7.12\n",
      "epis: 2390   score: 8.0   mem len: 580889   epsilon: 0.0478    steps: 406    lr: 2.6e-06     reward: 7.17\n",
      "epis: 2391   score: 7.0   mem len: 581279   epsilon: 0.0471    steps: 390    lr: 2.6e-06     reward: 7.13\n",
      "epis: 2392   score: 4.0   mem len: 581541   epsilon: 0.0465    steps: 262    lr: 2.6e-06     reward: 7.06\n",
      "epis: 2393   score: 7.0   mem len: 581934   epsilon: 0.0458    steps: 393    lr: 2.6e-06     reward: 7.08\n",
      "epis: 2394   score: 7.0   mem len: 582298   epsilon: 0.045    steps: 364    lr: 2.6e-06     reward: 7.05\n",
      "epis: 2395   score: 5.0   mem len: 582579   epsilon: 0.0445    steps: 281    lr: 2.6e-06     reward: 6.99\n",
      "epis: 2396   score: 7.0   mem len: 582957   epsilon: 0.0437    steps: 378    lr: 2.6e-06     reward: 7.0\n",
      "epis: 2397   score: 10.0   mem len: 583444   epsilon: 0.0428    steps: 487    lr: 2.6e-06     reward: 7.03\n",
      "epis: 2398   score: 5.0   mem len: 583735   epsilon: 0.0422    steps: 291    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2399   score: 11.0   mem len: 584308   epsilon: 0.0411    steps: 573    lr: 2.6e-06     reward: 7.02\n",
      "epis: 2400   score: 7.0   mem len: 584698   epsilon: 0.0403    steps: 390    lr: 2.6e-06     reward: 7.01\n",
      "epis: 2401   score: 7.0   mem len: 585082   epsilon: 0.0395    steps: 384    lr: 2.6e-06     reward: 7.04\n",
      "epis: 2402   score: 5.0   mem len: 585393   epsilon: 0.0389    steps: 311    lr: 2.6e-06     reward: 7.03\n",
      "epis: 2403   score: 7.0   mem len: 585785   epsilon: 0.0381    steps: 392    lr: 2.6e-06     reward: 7.01\n",
      "epis: 2404   score: 6.0   mem len: 586142   epsilon: 0.0374    steps: 357    lr: 2.6e-06     reward: 7.02\n",
      "epis: 2405   score: 4.0   mem len: 586402   epsilon: 0.0369    steps: 260    lr: 2.6e-06     reward: 7.02\n",
      "epis: 2406   score: 11.0   mem len: 586953   epsilon: 0.0358    steps: 551    lr: 2.6e-06     reward: 6.97\n",
      "epis: 2407   score: 7.0   mem len: 587357   epsilon: 0.035    steps: 404    lr: 2.6e-06     reward: 6.99\n",
      "epis: 2408   score: 6.0   mem len: 587708   epsilon: 0.0343    steps: 351    lr: 2.6e-06     reward: 6.95\n",
      "epis: 2409   score: 5.0   mem len: 588016   epsilon: 0.0337    steps: 308    lr: 2.6e-06     reward: 6.91\n",
      "epis: 2410   score: 5.0   mem len: 588322   epsilon: 0.0331    steps: 306    lr: 2.6e-06     reward: 6.87\n",
      "epis: 2411   score: 10.0   mem len: 588829   epsilon: 0.0321    steps: 507    lr: 2.6e-06     reward: 6.92\n",
      "epis: 2412   score: 3.0   mem len: 589061   epsilon: 0.0317    steps: 232    lr: 2.6e-06     reward: 6.9\n",
      "epis: 2413   score: 6.0   mem len: 589391   epsilon: 0.031    steps: 330    lr: 2.6e-06     reward: 6.92\n",
      "epis: 2414   score: 7.0   mem len: 589760   epsilon: 0.0303    steps: 369    lr: 2.6e-06     reward: 6.94\n",
      "epis: 2415   score: 10.0   mem len: 590262   epsilon: 0.0293    steps: 502    lr: 2.6e-06     reward: 6.94\n",
      "epis: 2416   score: 6.0   mem len: 590602   epsilon: 0.0286    steps: 340    lr: 2.6e-06     reward: 6.92\n",
      "epis: 2417   score: 8.0   mem len: 591031   epsilon: 0.0278    steps: 429    lr: 2.6e-06     reward: 6.91\n",
      "epis: 2418   score: 11.0   mem len: 591586   epsilon: 0.0267    steps: 555    lr: 2.6e-06     reward: 6.91\n",
      "epis: 2419   score: 3.0   mem len: 591799   epsilon: 0.0262    steps: 213    lr: 2.6e-06     reward: 6.89\n",
      "epis: 2420   score: 4.0   mem len: 592074   epsilon: 0.0257    steps: 275    lr: 2.6e-06     reward: 6.87\n",
      "epis: 2421   score: 5.0   mem len: 592355   epsilon: 0.0251    steps: 281    lr: 2.6e-06     reward: 6.87\n",
      "epis: 2422   score: 10.0   mem len: 592854   epsilon: 0.0241    steps: 499    lr: 2.6e-06     reward: 6.89\n",
      "epis: 2423   score: 5.0   mem len: 593182   epsilon: 0.0235    steps: 328    lr: 2.6e-06     reward: 6.84\n",
      "epis: 2424   score: 7.0   mem len: 593577   epsilon: 0.0227    steps: 395    lr: 2.6e-06     reward: 6.84\n",
      "epis: 2425   score: 7.0   mem len: 593983   epsilon: 0.0219    steps: 406    lr: 2.6e-06     reward: 6.79\n",
      "epis: 2426   score: 5.0   mem len: 594311   epsilon: 0.0213    steps: 328    lr: 2.6e-06     reward: 6.78\n",
      "epis: 2427   score: 5.0   mem len: 594592   epsilon: 0.0207    steps: 281    lr: 2.6e-06     reward: 6.78\n",
      "epis: 2428   score: 6.0   mem len: 594946   epsilon: 0.02    steps: 354    lr: 2.6e-06     reward: 6.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2429   score: 13.0   mem len: 595594   epsilon: 0.0187    steps: 648    lr: 2.6e-06     reward: 6.82\n",
      "epis: 2430   score: 6.0   mem len: 595970   epsilon: 0.018    steps: 376    lr: 2.6e-06     reward: 6.77\n",
      "epis: 2431   score: 4.0   mem len: 596212   epsilon: 0.0175    steps: 242    lr: 2.6e-06     reward: 6.71\n",
      "epis: 2432   score: 5.0   mem len: 596493   epsilon: 0.0169    steps: 281    lr: 2.6e-06     reward: 6.72\n",
      "epis: 2433   score: 5.0   mem len: 596774   epsilon: 0.0164    steps: 281    lr: 2.6e-06     reward: 6.71\n",
      "epis: 2434   score: 7.0   mem len: 597131   epsilon: 0.0157    steps: 357    lr: 2.6e-06     reward: 6.69\n",
      "epis: 2435   score: 9.0   mem len: 597584   epsilon: 0.0148    steps: 453    lr: 2.6e-06     reward: 6.65\n",
      "epis: 2436   score: 9.0   mem len: 598061   epsilon: 0.0138    steps: 477    lr: 2.6e-06     reward: 6.71\n",
      "epis: 2437   score: 4.0   mem len: 598302   epsilon: 0.0134    steps: 241    lr: 2.6e-06     reward: 6.7\n",
      "epis: 2438   score: 8.0   mem len: 598725   epsilon: 0.0125    steps: 423    lr: 2.6e-06     reward: 6.72\n",
      "epis: 2439   score: 7.0   mem len: 599130   epsilon: 0.0117    steps: 405    lr: 2.6e-06     reward: 6.73\n",
      "epis: 2440   score: 7.0   mem len: 599517   epsilon: 0.011    steps: 387    lr: 2.6e-06     reward: 6.74\n",
      "epis: 2441   score: 4.0   mem len: 599779   epsilon: 0.0104    steps: 262    lr: 2.6e-06     reward: 6.72\n",
      "epis: 2442   score: 8.0   mem len: 600171   epsilon: 0.01    steps: 392    lr: 1e-06     reward: 6.75\n",
      "epis: 2443   score: 7.0   mem len: 600540   epsilon: 0.01    steps: 369    lr: 1e-06     reward: 6.78\n",
      "epis: 2444   score: 3.0   mem len: 600770   epsilon: 0.01    steps: 230    lr: 1e-06     reward: 6.72\n",
      "epis: 2445   score: 9.0   mem len: 601215   epsilon: 0.01    steps: 445    lr: 1e-06     reward: 6.69\n",
      "epis: 2446   score: 6.0   mem len: 601573   epsilon: 0.01    steps: 358    lr: 1e-06     reward: 6.71\n",
      "epis: 2447   score: 10.0   mem len: 602089   epsilon: 0.01    steps: 516    lr: 1e-06     reward: 6.71\n",
      "epis: 2448   score: 9.0   mem len: 602543   epsilon: 0.01    steps: 454    lr: 1e-06     reward: 6.74\n",
      "epis: 2449   score: 9.0   mem len: 603018   epsilon: 0.01    steps: 475    lr: 1e-06     reward: 6.77\n",
      "epis: 2450   score: 12.0   mem len: 603463   epsilon: 0.01    steps: 445    lr: 1e-06     reward: 6.81\n",
      "epis: 2451   score: 9.0   mem len: 603907   epsilon: 0.01    steps: 444    lr: 1e-06     reward: 6.81\n",
      "epis: 2452   score: 4.0   mem len: 604182   epsilon: 0.01    steps: 275    lr: 1e-06     reward: 6.79\n",
      "epis: 2453   score: 5.0   mem len: 604474   epsilon: 0.01    steps: 292    lr: 1e-06     reward: 6.73\n",
      "epis: 2454   score: 7.0   mem len: 604853   epsilon: 0.01    steps: 379    lr: 1e-06     reward: 6.75\n",
      "epis: 2455   score: 7.0   mem len: 605230   epsilon: 0.01    steps: 377    lr: 1e-06     reward: 6.73\n",
      "epis: 2456   score: 9.0   mem len: 605674   epsilon: 0.01    steps: 444    lr: 1e-06     reward: 6.77\n",
      "epis: 2457   score: 5.0   mem len: 605955   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.78\n",
      "epis: 2458   score: 7.0   mem len: 606313   epsilon: 0.01    steps: 358    lr: 1e-06     reward: 6.8\n",
      "epis: 2459   score: 8.0   mem len: 606741   epsilon: 0.01    steps: 428    lr: 1e-06     reward: 6.83\n",
      "epis: 2460   score: 5.0   mem len: 607022   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.83\n",
      "epis: 2461   score: 6.0   mem len: 607352   epsilon: 0.01    steps: 330    lr: 1e-06     reward: 6.84\n",
      "epis: 2462   score: 5.0   mem len: 607633   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.83\n",
      "epis: 2463   score: 5.0   mem len: 607925   epsilon: 0.01    steps: 292    lr: 1e-06     reward: 6.82\n",
      "epis: 2464   score: 11.0   mem len: 608424   epsilon: 0.01    steps: 499    lr: 1e-06     reward: 6.85\n",
      "epis: 2465   score: 6.0   mem len: 608763   epsilon: 0.01    steps: 339    lr: 1e-06     reward: 6.83\n",
      "epis: 2466   score: 5.0   mem len: 609044   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.83\n",
      "epis: 2467   score: 7.0   mem len: 609418   epsilon: 0.01    steps: 374    lr: 1e-06     reward: 6.81\n",
      "epis: 2468   score: 9.0   mem len: 609892   epsilon: 0.01    steps: 474    lr: 1e-06     reward: 6.83\n",
      "epis: 2469   score: 9.0   mem len: 610363   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 6.87\n",
      "epis: 2470   score: 7.0   mem len: 610722   epsilon: 0.01    steps: 359    lr: 1e-06     reward: 6.87\n",
      "epis: 2471   score: 13.0   mem len: 611214   epsilon: 0.01    steps: 492    lr: 1e-06     reward: 6.92\n",
      "epis: 2472   score: 7.0   mem len: 611557   epsilon: 0.01    steps: 343    lr: 1e-06     reward: 6.88\n",
      "epis: 2473   score: 7.0   mem len: 611934   epsilon: 0.01    steps: 377    lr: 1e-06     reward: 6.91\n",
      "epis: 2474   score: 5.0   mem len: 612215   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.9\n",
      "epis: 2475   score: 8.0   mem len: 612651   epsilon: 0.01    steps: 436    lr: 1e-06     reward: 6.86\n",
      "epis: 2476   score: 7.0   mem len: 613039   epsilon: 0.01    steps: 388    lr: 1e-06     reward: 6.88\n",
      "epis: 2477   score: 9.0   mem len: 613510   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 6.89\n",
      "epis: 2478   score: 7.0   mem len: 613869   epsilon: 0.01    steps: 359    lr: 1e-06     reward: 6.87\n",
      "epis: 2479   score: 6.0   mem len: 614199   epsilon: 0.01    steps: 330    lr: 1e-06     reward: 6.85\n",
      "epis: 2480   score: 6.0   mem len: 614556   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 6.87\n",
      "epis: 2481   score: 6.0   mem len: 614886   epsilon: 0.01    steps: 330    lr: 1e-06     reward: 6.87\n",
      "epis: 2482   score: 5.0   mem len: 615186   epsilon: 0.01    steps: 300    lr: 1e-06     reward: 6.87\n",
      "epis: 2483   score: 5.0   mem len: 615467   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.88\n",
      "epis: 2484   score: 4.0   mem len: 615745   epsilon: 0.01    steps: 278    lr: 1e-06     reward: 6.83\n",
      "epis: 2485   score: 6.0   mem len: 616086   epsilon: 0.01    steps: 341    lr: 1e-06     reward: 6.83\n",
      "epis: 2486   score: 8.0   mem len: 616522   epsilon: 0.01    steps: 436    lr: 1e-06     reward: 6.79\n",
      "epis: 2487   score: 8.0   mem len: 616928   epsilon: 0.01    steps: 406    lr: 1e-06     reward: 6.83\n",
      "epis: 2488   score: 12.0   mem len: 617373   epsilon: 0.01    steps: 445    lr: 1e-06     reward: 6.9\n",
      "epis: 2489   score: 10.0   mem len: 617859   epsilon: 0.01    steps: 486    lr: 1e-06     reward: 6.95\n",
      "epis: 2490   score: 4.0   mem len: 618121   epsilon: 0.01    steps: 262    lr: 1e-06     reward: 6.91\n",
      "epis: 2491   score: 9.0   mem len: 618590   epsilon: 0.01    steps: 469    lr: 1e-06     reward: 6.93\n",
      "epis: 2492   score: 5.0   mem len: 618871   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.94\n",
      "epis: 2493   score: 5.0   mem len: 619198   epsilon: 0.01    steps: 327    lr: 1e-06     reward: 6.92\n",
      "epis: 2494   score: 5.0   mem len: 619528   epsilon: 0.01    steps: 330    lr: 1e-06     reward: 6.9\n",
      "epis: 2495   score: 5.0   mem len: 619861   epsilon: 0.01    steps: 333    lr: 1e-06     reward: 6.9\n",
      "epis: 2496   score: 8.0   mem len: 620284   epsilon: 0.01    steps: 423    lr: 1e-06     reward: 6.91\n",
      "epis: 2497   score: 8.0   mem len: 620726   epsilon: 0.01    steps: 442    lr: 1e-06     reward: 6.89\n",
      "epis: 2498   score: 10.0   mem len: 621197   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 6.94\n",
      "epis: 2499   score: 5.0   mem len: 621478   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.88\n",
      "epis: 2500   score: 7.0   mem len: 621853   epsilon: 0.01    steps: 375    lr: 1e-06     reward: 6.88\n",
      "epis: 2501   score: 5.0   mem len: 622134   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.86\n",
      "epis: 2502   score: 5.0   mem len: 622415   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.86\n",
      "epis: 2503   score: 9.0   mem len: 622888   epsilon: 0.01    steps: 473    lr: 1e-06     reward: 6.88\n",
      "epis: 2504   score: 5.0   mem len: 623169   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.87\n",
      "epis: 2505   score: 8.0   mem len: 623576   epsilon: 0.01    steps: 407    lr: 1e-06     reward: 6.91\n",
      "epis: 2506   score: 5.0   mem len: 623857   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.85\n",
      "epis: 2507   score: 5.0   mem len: 624138   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.83\n",
      "epis: 2508   score: 7.0   mem len: 624516   epsilon: 0.01    steps: 378    lr: 1e-06     reward: 6.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2509   score: 10.0   mem len: 625018   epsilon: 0.01    steps: 502    lr: 1e-06     reward: 6.89\n",
      "epis: 2510   score: 8.0   mem len: 625469   epsilon: 0.01    steps: 451    lr: 1e-06     reward: 6.92\n",
      "epis: 2511   score: 8.0   mem len: 625910   epsilon: 0.01    steps: 441    lr: 1e-06     reward: 6.9\n",
      "epis: 2512   score: 5.0   mem len: 626191   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 6.92\n",
      "epis: 2513   score: 6.0   mem len: 626548   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 6.92\n",
      "epis: 2514   score: 7.0   mem len: 626938   epsilon: 0.01    steps: 390    lr: 1e-06     reward: 6.92\n",
      "epis: 2515   score: 9.0   mem len: 627372   epsilon: 0.01    steps: 434    lr: 1e-06     reward: 6.91\n",
      "epis: 2516   score: 6.0   mem len: 627684   epsilon: 0.01    steps: 312    lr: 1e-06     reward: 6.91\n",
      "epis: 2517   score: 9.0   mem len: 628124   epsilon: 0.01    steps: 440    lr: 1e-06     reward: 6.92\n",
      "epis: 2518   score: 6.0   mem len: 628472   epsilon: 0.01    steps: 348    lr: 1e-06     reward: 6.87\n",
      "epis: 2519   score: 9.0   mem len: 628922   epsilon: 0.01    steps: 450    lr: 1e-06     reward: 6.93\n",
      "epis: 2520   score: 10.0   mem len: 629423   epsilon: 0.01    steps: 501    lr: 1e-06     reward: 6.99\n",
      "epis: 2521   score: 9.0   mem len: 629900   epsilon: 0.01    steps: 477    lr: 1e-06     reward: 7.03\n",
      "epis: 2522   score: 4.0   mem len: 630162   epsilon: 0.01    steps: 262    lr: 1e-06     reward: 6.97\n",
      "epis: 2523   score: 4.0   mem len: 630404   epsilon: 0.01    steps: 242    lr: 1e-06     reward: 6.96\n",
      "epis: 2524   score: 9.0   mem len: 630878   epsilon: 0.01    steps: 474    lr: 1e-06     reward: 6.98\n",
      "epis: 2525   score: 9.0   mem len: 631349   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 7.0\n",
      "epis: 2526   score: 5.0   mem len: 631630   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 7.0\n",
      "epis: 2527   score: 7.0   mem len: 632036   epsilon: 0.01    steps: 406    lr: 1e-06     reward: 7.02\n",
      "epis: 2528   score: 8.0   mem len: 632464   epsilon: 0.01    steps: 428    lr: 1e-06     reward: 7.04\n",
      "epis: 2529   score: 6.0   mem len: 632792   epsilon: 0.01    steps: 328    lr: 1e-06     reward: 6.97\n",
      "epis: 2530   score: 11.0   mem len: 633341   epsilon: 0.01    steps: 549    lr: 1e-06     reward: 7.02\n",
      "epis: 2531   score: 8.0   mem len: 633796   epsilon: 0.01    steps: 455    lr: 1e-06     reward: 7.06\n",
      "epis: 2532   score: 9.0   mem len: 634267   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 7.1\n",
      "epis: 2533   score: 6.0   mem len: 634625   epsilon: 0.01    steps: 358    lr: 1e-06     reward: 7.11\n",
      "epis: 2534   score: 6.0   mem len: 634977   epsilon: 0.01    steps: 352    lr: 1e-06     reward: 7.1\n",
      "epis: 2535   score: 9.0   mem len: 635451   epsilon: 0.01    steps: 474    lr: 1e-06     reward: 7.1\n",
      "epis: 2536   score: 5.0   mem len: 635758   epsilon: 0.01    steps: 307    lr: 1e-06     reward: 7.06\n",
      "epis: 2537   score: 7.0   mem len: 636164   epsilon: 0.01    steps: 406    lr: 1e-06     reward: 7.09\n",
      "epis: 2538   score: 6.0   mem len: 636527   epsilon: 0.01    steps: 363    lr: 1e-06     reward: 7.07\n",
      "epis: 2539   score: 8.0   mem len: 636964   epsilon: 0.01    steps: 437    lr: 1e-06     reward: 7.08\n",
      "epis: 2540   score: 5.0   mem len: 637264   epsilon: 0.01    steps: 300    lr: 1e-06     reward: 7.06\n",
      "epis: 2541   score: 9.0   mem len: 637734   epsilon: 0.01    steps: 470    lr: 1e-06     reward: 7.11\n",
      "epis: 2542   score: 10.0   mem len: 638220   epsilon: 0.01    steps: 486    lr: 1e-06     reward: 7.13\n",
      "epis: 2543   score: 9.0   mem len: 638693   epsilon: 0.01    steps: 473    lr: 1e-06     reward: 7.15\n",
      "epis: 2544   score: 8.0   mem len: 639122   epsilon: 0.01    steps: 429    lr: 1e-06     reward: 7.2\n",
      "epis: 2545   score: 6.0   mem len: 639470   epsilon: 0.01    steps: 348    lr: 1e-06     reward: 7.17\n",
      "epis: 2546   score: 9.0   mem len: 639910   epsilon: 0.01    steps: 440    lr: 1e-06     reward: 7.2\n",
      "epis: 2547   score: 6.0   mem len: 640250   epsilon: 0.01    steps: 340    lr: 1e-06     reward: 7.16\n",
      "epis: 2548   score: 10.0   mem len: 640752   epsilon: 0.01    steps: 502    lr: 1e-06     reward: 7.17\n",
      "epis: 2549   score: 8.0   mem len: 641176   epsilon: 0.01    steps: 424    lr: 1e-06     reward: 7.16\n",
      "epis: 2550   score: 8.0   mem len: 641586   epsilon: 0.01    steps: 410    lr: 1e-06     reward: 7.12\n",
      "epis: 2551   score: 8.0   mem len: 642002   epsilon: 0.01    steps: 416    lr: 1e-06     reward: 7.11\n",
      "epis: 2552   score: 6.0   mem len: 642306   epsilon: 0.01    steps: 304    lr: 1e-06     reward: 7.13\n",
      "epis: 2553   score: 10.0   mem len: 642806   epsilon: 0.01    steps: 500    lr: 1e-06     reward: 7.18\n",
      "epis: 2554   score: 7.0   mem len: 643181   epsilon: 0.01    steps: 375    lr: 1e-06     reward: 7.18\n",
      "epis: 2555   score: 5.0   mem len: 643462   epsilon: 0.01    steps: 281    lr: 1e-06     reward: 7.16\n",
      "epis: 2556   score: 7.0   mem len: 643821   epsilon: 0.01    steps: 359    lr: 1e-06     reward: 7.14\n",
      "epis: 2557   score: 9.0   mem len: 644294   epsilon: 0.01    steps: 473    lr: 1e-06     reward: 7.18\n",
      "epis: 2558   score: 4.0   mem len: 644536   epsilon: 0.01    steps: 242    lr: 1e-06     reward: 7.15\n",
      "epis: 2559   score: 7.0   mem len: 644895   epsilon: 0.01    steps: 359    lr: 1e-06     reward: 7.14\n",
      "epis: 2560   score: 6.0   mem len: 645243   epsilon: 0.01    steps: 348    lr: 1e-06     reward: 7.15\n",
      "epis: 2561   score: 11.0   mem len: 645782   epsilon: 0.01    steps: 539    lr: 1e-06     reward: 7.2\n",
      "epis: 2562   score: 6.0   mem len: 646112   epsilon: 0.01    steps: 330    lr: 1e-06     reward: 7.21\n",
      "epis: 2563   score: 9.0   mem len: 646585   epsilon: 0.01    steps: 473    lr: 1e-06     reward: 7.25\n",
      "epis: 2564   score: 11.0   mem len: 647082   epsilon: 0.01    steps: 497    lr: 1e-06     reward: 7.25\n",
      "epis: 2565   score: 5.0   mem len: 647382   epsilon: 0.01    steps: 300    lr: 1e-06     reward: 7.24\n",
      "epis: 2566   score: 6.0   mem len: 647730   epsilon: 0.01    steps: 348    lr: 1e-06     reward: 7.25\n",
      "epis: 2567   score: 8.0   mem len: 648164   epsilon: 0.01    steps: 434    lr: 1e-06     reward: 7.26\n",
      "epis: 2568   score: 5.0   mem len: 648455   epsilon: 0.01    steps: 291    lr: 1e-06     reward: 7.22\n",
      "epis: 2569   score: 7.0   mem len: 648814   epsilon: 0.01    steps: 359    lr: 1e-06     reward: 7.2\n",
      "epis: 2570   score: 6.0   mem len: 649174   epsilon: 0.01    steps: 360    lr: 1e-06     reward: 7.19\n",
      "epis: 2571   score: 11.0   mem len: 649736   epsilon: 0.01    steps: 562    lr: 1e-06     reward: 7.17\n",
      "epis: 2572   score: 6.0   mem len: 650095   epsilon: 0.01    steps: 359    lr: 1e-06     reward: 7.16\n",
      "epis: 2573   score: 7.0   mem len: 650453   epsilon: 0.01    steps: 358    lr: 1e-06     reward: 7.16\n",
      "epis: 2574   score: 5.0   mem len: 650761   epsilon: 0.01    steps: 308    lr: 1e-06     reward: 7.16\n",
      "epis: 2575   score: 7.0   mem len: 651112   epsilon: 0.01    steps: 351    lr: 1e-06     reward: 7.15\n",
      "epis: 2576   score: 6.0   mem len: 651487   epsilon: 0.01    steps: 375    lr: 1e-06     reward: 7.14\n",
      "epis: 2577   score: 6.0   mem len: 651830   epsilon: 0.01    steps: 343    lr: 1e-06     reward: 7.11\n",
      "epis: 2578   score: 6.0   mem len: 652187   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 7.1\n",
      "epis: 2579   score: 7.0   mem len: 652564   epsilon: 0.01    steps: 377    lr: 1e-06     reward: 7.11\n",
      "epis: 2580   score: 9.0   mem len: 653011   epsilon: 0.01    steps: 447    lr: 1e-06     reward: 7.14\n",
      "epis: 2581   score: 3.0   mem len: 653241   epsilon: 0.01    steps: 230    lr: 1e-06     reward: 7.11\n",
      "epis: 2582   score: 6.0   mem len: 653577   epsilon: 0.01    steps: 336    lr: 1e-06     reward: 7.12\n",
      "epis: 2583   score: 7.0   mem len: 653929   epsilon: 0.01    steps: 352    lr: 1e-06     reward: 7.14\n",
      "epis: 2584   score: 6.0   mem len: 654305   epsilon: 0.01    steps: 376    lr: 1e-06     reward: 7.16\n",
      "epis: 2585   score: 8.0   mem len: 654691   epsilon: 0.01    steps: 386    lr: 1e-06     reward: 7.18\n",
      "epis: 2586   score: 12.0   mem len: 655271   epsilon: 0.01    steps: 580    lr: 1e-06     reward: 7.22\n",
      "epis: 2587   score: 9.0   mem len: 655727   epsilon: 0.01    steps: 456    lr: 1e-06     reward: 7.23\n",
      "epis: 2588   score: 9.0   mem len: 656197   epsilon: 0.01    steps: 470    lr: 1e-06     reward: 7.2\n",
      "epis: 2589   score: 6.0   mem len: 656569   epsilon: 0.01    steps: 372    lr: 1e-06     reward: 7.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2590   score: 8.0   mem len: 657024   epsilon: 0.01    steps: 455    lr: 1e-06     reward: 7.2\n",
      "epis: 2591   score: 10.0   mem len: 657497   epsilon: 0.01    steps: 473    lr: 1e-06     reward: 7.21\n",
      "epis: 2592   score: 7.0   mem len: 657874   epsilon: 0.01    steps: 377    lr: 1e-06     reward: 7.23\n",
      "epis: 2593   score: 7.0   mem len: 658251   epsilon: 0.01    steps: 377    lr: 1e-06     reward: 7.25\n",
      "epis: 2594   score: 9.0   mem len: 658715   epsilon: 0.01    steps: 464    lr: 1e-06     reward: 7.29\n",
      "epis: 2595   score: 8.0   mem len: 659115   epsilon: 0.01    steps: 400    lr: 1e-06     reward: 7.32\n",
      "epis: 2596   score: 7.0   mem len: 659506   epsilon: 0.01    steps: 391    lr: 1e-06     reward: 7.31\n",
      "epis: 2597   score: 9.0   mem len: 659977   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 7.32\n",
      "epis: 2598   score: 6.0   mem len: 660305   epsilon: 0.01    steps: 328    lr: 1e-06     reward: 7.28\n",
      "epis: 2599   score: 12.0   mem len: 660729   epsilon: 0.01    steps: 424    lr: 1e-06     reward: 7.35\n",
      "epis: 2600   score: 10.0   mem len: 661206   epsilon: 0.01    steps: 477    lr: 1e-06     reward: 7.38\n",
      "epis: 2601   score: 10.0   mem len: 661688   epsilon: 0.01    steps: 482    lr: 1e-06     reward: 7.43\n",
      "epis: 2602   score: 9.0   mem len: 662139   epsilon: 0.01    steps: 451    lr: 1e-06     reward: 7.47\n",
      "epis: 2603   score: 9.0   mem len: 662610   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 7.47\n",
      "epis: 2604   score: 10.0   mem len: 663135   epsilon: 0.01    steps: 525    lr: 1e-06     reward: 7.52\n",
      "epis: 2605   score: 6.0   mem len: 663492   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 7.5\n",
      "epis: 2606   score: 10.0   mem len: 663961   epsilon: 0.01    steps: 469    lr: 1e-06     reward: 7.55\n",
      "epis: 2607   score: 7.0   mem len: 664353   epsilon: 0.01    steps: 392    lr: 1e-06     reward: 7.57\n",
      "epis: 2608   score: 9.0   mem len: 664797   epsilon: 0.01    steps: 444    lr: 1e-06     reward: 7.59\n",
      "epis: 2609   score: 5.0   mem len: 665097   epsilon: 0.01    steps: 300    lr: 1e-06     reward: 7.54\n",
      "epis: 2610   score: 6.0   mem len: 665425   epsilon: 0.01    steps: 328    lr: 1e-06     reward: 7.52\n",
      "epis: 2611   score: 7.0   mem len: 665802   epsilon: 0.01    steps: 377    lr: 1e-06     reward: 7.51\n",
      "epis: 2612   score: 10.0   mem len: 666304   epsilon: 0.01    steps: 502    lr: 1e-06     reward: 7.56\n",
      "epis: 2613   score: 6.0   mem len: 666664   epsilon: 0.01    steps: 360    lr: 1e-06     reward: 7.56\n",
      "epis: 2614   score: 8.0   mem len: 667072   epsilon: 0.01    steps: 408    lr: 1e-06     reward: 7.57\n",
      "epis: 2615   score: 11.0   mem len: 667650   epsilon: 0.01    steps: 578    lr: 1e-06     reward: 7.59\n",
      "epis: 2616   score: 7.0   mem len: 668028   epsilon: 0.01    steps: 378    lr: 1e-06     reward: 7.6\n",
      "epis: 2617   score: 12.0   mem len: 668597   epsilon: 0.01    steps: 569    lr: 1e-06     reward: 7.63\n",
      "epis: 2618   score: 11.0   mem len: 669139   epsilon: 0.01    steps: 542    lr: 1e-06     reward: 7.68\n",
      "epis: 2619   score: 7.0   mem len: 669547   epsilon: 0.01    steps: 408    lr: 1e-06     reward: 7.66\n",
      "epis: 2620   score: 7.0   mem len: 669911   epsilon: 0.01    steps: 364    lr: 1e-06     reward: 7.63\n",
      "epis: 2621   score: 8.0   mem len: 670320   epsilon: 0.01    steps: 409    lr: 1e-06     reward: 7.62\n",
      "epis: 2622   score: 9.0   mem len: 670791   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 7.67\n",
      "epis: 2623   score: 5.0   mem len: 671117   epsilon: 0.01    steps: 326    lr: 1e-06     reward: 7.68\n",
      "epis: 2624   score: 8.0   mem len: 671584   epsilon: 0.01    steps: 467    lr: 1e-06     reward: 7.67\n",
      "epis: 2625   score: 10.0   mem len: 672077   epsilon: 0.01    steps: 493    lr: 1e-06     reward: 7.68\n",
      "epis: 2626   score: 6.0   mem len: 672432   epsilon: 0.01    steps: 355    lr: 1e-06     reward: 7.69\n",
      "epis: 2627   score: 6.0   mem len: 672793   epsilon: 0.01    steps: 361    lr: 1e-06     reward: 7.68\n",
      "epis: 2628   score: 7.0   mem len: 673237   epsilon: 0.01    steps: 444    lr: 1e-06     reward: 7.67\n",
      "epis: 2629   score: 7.0   mem len: 673625   epsilon: 0.01    steps: 388    lr: 1e-06     reward: 7.68\n",
      "epis: 2630   score: 7.0   mem len: 674033   epsilon: 0.01    steps: 408    lr: 1e-06     reward: 7.64\n",
      "epis: 2631   score: 7.0   mem len: 674410   epsilon: 0.01    steps: 377    lr: 1e-06     reward: 7.63\n",
      "epis: 2632   score: 12.0   mem len: 675001   epsilon: 0.01    steps: 591    lr: 1e-06     reward: 7.66\n",
      "epis: 2633   score: 9.0   mem len: 675454   epsilon: 0.01    steps: 453    lr: 1e-06     reward: 7.69\n",
      "epis: 2634   score: 6.0   mem len: 675811   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 7.69\n",
      "epis: 2635   score: 10.0   mem len: 676294   epsilon: 0.01    steps: 483    lr: 1e-06     reward: 7.7\n",
      "epis: 2636   score: 6.0   mem len: 676622   epsilon: 0.01    steps: 328    lr: 1e-06     reward: 7.71\n",
      "epis: 2637   score: 6.0   mem len: 676979   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 7.7\n",
      "epis: 2638   score: 7.0   mem len: 677338   epsilon: 0.01    steps: 359    lr: 1e-06     reward: 7.71\n",
      "epis: 2639   score: 5.0   mem len: 677666   epsilon: 0.01    steps: 328    lr: 1e-06     reward: 7.68\n",
      "epis: 2640   score: 9.0   mem len: 678156   epsilon: 0.01    steps: 490    lr: 1e-06     reward: 7.72\n",
      "epis: 2641   score: 11.0   mem len: 678730   epsilon: 0.01    steps: 574    lr: 1e-06     reward: 7.74\n",
      "epis: 2642   score: 4.0   mem len: 679009   epsilon: 0.01    steps: 279    lr: 1e-06     reward: 7.68\n",
      "epis: 2643   score: 4.0   mem len: 679269   epsilon: 0.01    steps: 260    lr: 1e-06     reward: 7.63\n",
      "epis: 2644   score: 6.0   mem len: 679626   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 7.61\n",
      "epis: 2645   score: 7.0   mem len: 679967   epsilon: 0.01    steps: 341    lr: 1e-06     reward: 7.62\n",
      "epis: 2646   score: 7.0   mem len: 680343   epsilon: 0.01    steps: 376    lr: 1e-06     reward: 7.6\n",
      "epis: 2647   score: 11.0   mem len: 680869   epsilon: 0.01    steps: 526    lr: 1e-06     reward: 7.65\n",
      "epis: 2648   score: 4.0   mem len: 681129   epsilon: 0.01    steps: 260    lr: 1e-06     reward: 7.59\n",
      "epis: 2649   score: 5.0   mem len: 681419   epsilon: 0.01    steps: 290    lr: 1e-06     reward: 7.56\n",
      "epis: 2650   score: 8.0   mem len: 681829   epsilon: 0.01    steps: 410    lr: 1e-06     reward: 7.56\n",
      "epis: 2651   score: 5.0   mem len: 682154   epsilon: 0.01    steps: 325    lr: 1e-06     reward: 7.53\n",
      "epis: 2652   score: 8.0   mem len: 682581   epsilon: 0.01    steps: 427    lr: 1e-06     reward: 7.55\n",
      "epis: 2653   score: 9.0   mem len: 683052   epsilon: 0.01    steps: 471    lr: 1e-06     reward: 7.54\n",
      "epis: 2654   score: 8.0   mem len: 683497   epsilon: 0.01    steps: 445    lr: 1e-06     reward: 7.55\n",
      "epis: 2655   score: 8.0   mem len: 683934   epsilon: 0.01    steps: 437    lr: 1e-06     reward: 7.58\n",
      "epis: 2656   score: 11.0   mem len: 684449   epsilon: 0.01    steps: 515    lr: 1e-06     reward: 7.62\n",
      "epis: 2657   score: 6.0   mem len: 684790   epsilon: 0.01    steps: 341    lr: 1e-06     reward: 7.59\n",
      "epis: 2658   score: 9.0   mem len: 685243   epsilon: 0.01    steps: 453    lr: 1e-06     reward: 7.64\n",
      "epis: 2659   score: 9.0   mem len: 685697   epsilon: 0.01    steps: 454    lr: 1e-06     reward: 7.66\n",
      "epis: 2660   score: 6.0   mem len: 686040   epsilon: 0.01    steps: 343    lr: 1e-06     reward: 7.66\n",
      "epis: 2661   score: 8.0   mem len: 686418   epsilon: 0.01    steps: 378    lr: 1e-06     reward: 7.63\n",
      "epis: 2662   score: 7.0   mem len: 686811   epsilon: 0.01    steps: 393    lr: 1e-06     reward: 7.64\n",
      "epis: 2663   score: 6.0   mem len: 687149   epsilon: 0.01    steps: 338    lr: 1e-06     reward: 7.61\n",
      "epis: 2664   score: 13.0   mem len: 687638   epsilon: 0.01    steps: 489    lr: 1e-06     reward: 7.63\n",
      "epis: 2665   score: 7.0   mem len: 688061   epsilon: 0.01    steps: 423    lr: 1e-06     reward: 7.65\n",
      "epis: 2666   score: 7.0   mem len: 688486   epsilon: 0.01    steps: 425    lr: 1e-06     reward: 7.66\n",
      "epis: 2667   score: 8.0   mem len: 688917   epsilon: 0.01    steps: 431    lr: 1e-06     reward: 7.66\n",
      "epis: 2668   score: 6.0   mem len: 689247   epsilon: 0.01    steps: 330    lr: 1e-06     reward: 7.67\n",
      "epis: 2669   score: 6.0   mem len: 689604   epsilon: 0.01    steps: 357    lr: 1e-06     reward: 7.66\n",
      "epis: 2670   score: 6.0   mem len: 689952   epsilon: 0.01    steps: 348    lr: 1e-06     reward: 7.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2671   score: 9.0   mem len: 690396   epsilon: 0.01    steps: 444    lr: 1e-06     reward: 7.64\n",
      "epis: 2672   score: 9.0   mem len: 690837   epsilon: 0.01    steps: 441    lr: 1e-06     reward: 7.67\n",
      "epis: 2673   score: 12.0   mem len: 691261   epsilon: 0.01    steps: 424    lr: 1e-06     reward: 7.72\n",
      "epis: 2674   score: 9.0   mem len: 691707   epsilon: 0.01    steps: 446    lr: 1e-06     reward: 7.76\n",
      "epis: 2675   score: 9.0   mem len: 692120   epsilon: 0.01    steps: 413    lr: 1e-06     reward: 7.78\n",
      "epis: 2676   score: 12.0   mem len: 692679   epsilon: 0.01    steps: 559    lr: 1e-06     reward: 7.84\n",
      "epis: 2677   score: 8.0   mem len: 693125   epsilon: 0.01    steps: 446    lr: 1e-06     reward: 7.86\n",
      "epis: 2678   score: 8.0   mem len: 693530   epsilon: 0.01    steps: 405    lr: 1e-06     reward: 7.88\n",
      "epis: 2679   score: 5.0   mem len: 693825   epsilon: 0.01    steps: 295    lr: 1e-06     reward: 7.86\n",
      "epis: 2680   score: 11.0   mem len: 694377   epsilon: 0.01    steps: 552    lr: 1e-06     reward: 7.88\n",
      "epis: 2681   score: 10.0   mem len: 694887   epsilon: 0.01    steps: 510    lr: 1e-06     reward: 7.95\n",
      "epis: 2682   score: 10.0   mem len: 695356   epsilon: 0.01    steps: 469    lr: 1e-06     reward: 7.99\n",
      "epis: 2683   score: 9.0   mem len: 695830   epsilon: 0.01    steps: 474    lr: 1e-06     reward: 8.01\n",
      "epis: 2684   score: 10.0   mem len: 696328   epsilon: 0.01    steps: 498    lr: 1e-06     reward: 8.05\n",
      "epis: 2685   score: 4.0   mem len: 696608   epsilon: 0.01    steps: 280    lr: 1e-06     reward: 8.01\n",
      "epis: 2686   score: 9.0   mem len: 697051   epsilon: 0.01    steps: 443    lr: 1e-06     reward: 7.98\n",
      "epis: 2687   score: 5.0   mem len: 697373   epsilon: 0.01    steps: 322    lr: 1e-06     reward: 7.94\n",
      "epis: 2688   score: 8.0   mem len: 697818   epsilon: 0.01    steps: 445    lr: 1e-06     reward: 7.93\n",
      "epis: 2689   score: 17.0   mem len: 698446   epsilon: 0.01    steps: 628    lr: 1e-06     reward: 8.04\n",
      "epis: 2690   score: 7.0   mem len: 698873   epsilon: 0.01    steps: 427    lr: 1e-06     reward: 8.03\n",
      "epis: 2691   score: 6.0   mem len: 699231   epsilon: 0.01    steps: 358    lr: 1e-06     reward: 7.99\n",
      "epis: 2692   score: 4.0   mem len: 699492   epsilon: 0.01    steps: 261    lr: 1e-06     reward: 7.96\n",
      "epis: 2693   score: 8.0   mem len: 699948   epsilon: 0.01    steps: 456    lr: 1e-06     reward: 7.97\n",
      "epis: 2694   score: 11.0   mem len: 700373   epsilon: 0.01    steps: 425    lr: 4e-07     reward: 7.99\n",
      "epis: 2695   score: 6.0   mem len: 700731   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 7.97\n",
      "epis: 2696   score: 13.0   mem len: 701350   epsilon: 0.01    steps: 619    lr: 4e-07     reward: 8.03\n",
      "epis: 2697   score: 9.0   mem len: 701797   epsilon: 0.01    steps: 447    lr: 4e-07     reward: 8.03\n",
      "epis: 2698   score: 6.0   mem len: 702125   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 8.03\n",
      "epis: 2699   score: 6.0   mem len: 702453   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.97\n",
      "epis: 2700   score: 6.0   mem len: 702781   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.93\n",
      "epis: 2701   score: 5.0   mem len: 703091   epsilon: 0.01    steps: 310    lr: 4e-07     reward: 7.88\n",
      "epis: 2702   score: 8.0   mem len: 703496   epsilon: 0.01    steps: 405    lr: 4e-07     reward: 7.87\n",
      "epis: 2703   score: 5.0   mem len: 703825   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 7.83\n",
      "epis: 2704   score: 5.0   mem len: 704151   epsilon: 0.01    steps: 326    lr: 4e-07     reward: 7.78\n",
      "epis: 2705   score: 9.0   mem len: 704570   epsilon: 0.01    steps: 419    lr: 4e-07     reward: 7.81\n",
      "epis: 2706   score: 6.0   mem len: 704928   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 7.77\n",
      "epis: 2707   score: 5.0   mem len: 705257   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 7.75\n",
      "epis: 2708   score: 6.0   mem len: 705585   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.72\n",
      "epis: 2709   score: 6.0   mem len: 705943   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 7.73\n",
      "epis: 2710   score: 7.0   mem len: 706301   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 7.74\n",
      "epis: 2711   score: 6.0   mem len: 706658   epsilon: 0.01    steps: 357    lr: 4e-07     reward: 7.73\n",
      "epis: 2712   score: 8.0   mem len: 707058   epsilon: 0.01    steps: 400    lr: 4e-07     reward: 7.71\n",
      "epis: 2713   score: 6.0   mem len: 707419   epsilon: 0.01    steps: 361    lr: 4e-07     reward: 7.71\n",
      "epis: 2714   score: 6.0   mem len: 707780   epsilon: 0.01    steps: 361    lr: 4e-07     reward: 7.69\n",
      "epis: 2715   score: 7.0   mem len: 708144   epsilon: 0.01    steps: 364    lr: 4e-07     reward: 7.65\n",
      "epis: 2716   score: 9.0   mem len: 708569   epsilon: 0.01    steps: 425    lr: 4e-07     reward: 7.67\n",
      "epis: 2717   score: 6.0   mem len: 708927   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 7.61\n",
      "epis: 2718   score: 6.0   mem len: 709280   epsilon: 0.01    steps: 353    lr: 4e-07     reward: 7.56\n",
      "epis: 2719   score: 7.0   mem len: 709664   epsilon: 0.01    steps: 384    lr: 4e-07     reward: 7.56\n",
      "epis: 2720   score: 8.0   mem len: 710064   epsilon: 0.01    steps: 400    lr: 4e-07     reward: 7.57\n",
      "epis: 2721   score: 6.0   mem len: 710392   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.55\n",
      "epis: 2722   score: 5.0   mem len: 710702   epsilon: 0.01    steps: 310    lr: 4e-07     reward: 7.51\n",
      "epis: 2723   score: 6.0   mem len: 711030   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.52\n",
      "epis: 2724   score: 11.0   mem len: 711557   epsilon: 0.01    steps: 527    lr: 4e-07     reward: 7.55\n",
      "epis: 2725   score: 8.0   mem len: 711957   epsilon: 0.01    steps: 400    lr: 4e-07     reward: 7.53\n",
      "epis: 2726   score: 7.0   mem len: 712321   epsilon: 0.01    steps: 364    lr: 4e-07     reward: 7.54\n",
      "epis: 2727   score: 7.0   mem len: 712698   epsilon: 0.01    steps: 377    lr: 4e-07     reward: 7.55\n",
      "epis: 2728   score: 6.0   mem len: 713026   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.54\n",
      "epis: 2729   score: 6.0   mem len: 713354   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.53\n",
      "epis: 2730   score: 6.0   mem len: 713682   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.52\n",
      "epis: 2731   score: 10.0   mem len: 714188   epsilon: 0.01    steps: 506    lr: 4e-07     reward: 7.55\n",
      "epis: 2732   score: 6.0   mem len: 714516   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.49\n",
      "epis: 2733   score: 6.0   mem len: 714844   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.46\n",
      "epis: 2734   score: 7.0   mem len: 715173   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 7.47\n",
      "epis: 2735   score: 12.0   mem len: 715613   epsilon: 0.01    steps: 440    lr: 4e-07     reward: 7.49\n",
      "epis: 2736   score: 6.0   mem len: 715941   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.49\n",
      "epis: 2737   score: 7.0   mem len: 716329   epsilon: 0.01    steps: 388    lr: 4e-07     reward: 7.5\n",
      "epis: 2738   score: 14.0   mem len: 716861   epsilon: 0.01    steps: 532    lr: 4e-07     reward: 7.57\n",
      "epis: 2739   score: 16.0   mem len: 717512   epsilon: 0.01    steps: 651    lr: 4e-07     reward: 7.68\n",
      "epis: 2740   score: 5.0   mem len: 717834   epsilon: 0.01    steps: 322    lr: 4e-07     reward: 7.64\n",
      "epis: 2741   score: 11.0   mem len: 718369   epsilon: 0.01    steps: 535    lr: 4e-07     reward: 7.64\n",
      "epis: 2742   score: 8.0   mem len: 718769   epsilon: 0.01    steps: 400    lr: 4e-07     reward: 7.68\n",
      "epis: 2743   score: 5.0   mem len: 719098   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 7.69\n",
      "epis: 2744   score: 8.0   mem len: 719533   epsilon: 0.01    steps: 435    lr: 4e-07     reward: 7.71\n",
      "epis: 2745   score: 10.0   mem len: 720035   epsilon: 0.01    steps: 502    lr: 4e-07     reward: 7.74\n",
      "epis: 2746   score: 11.0   mem len: 720568   epsilon: 0.01    steps: 533    lr: 4e-07     reward: 7.78\n",
      "epis: 2747   score: 6.0   mem len: 720942   epsilon: 0.01    steps: 374    lr: 4e-07     reward: 7.73\n",
      "epis: 2748   score: 6.0   mem len: 721270   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.75\n",
      "epis: 2749   score: 3.0   mem len: 721483   epsilon: 0.01    steps: 213    lr: 4e-07     reward: 7.73\n",
      "epis: 2750   score: 10.0   mem len: 721971   epsilon: 0.01    steps: 488    lr: 4e-07     reward: 7.75\n",
      "epis: 2751   score: 5.0   mem len: 722297   epsilon: 0.01    steps: 326    lr: 4e-07     reward: 7.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2752   score: 11.0   mem len: 722842   epsilon: 0.01    steps: 545    lr: 4e-07     reward: 7.78\n",
      "epis: 2753   score: 6.0   mem len: 723170   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.75\n",
      "epis: 2754   score: 9.0   mem len: 723624   epsilon: 0.01    steps: 454    lr: 4e-07     reward: 7.76\n",
      "epis: 2755   score: 14.0   mem len: 724182   epsilon: 0.01    steps: 558    lr: 4e-07     reward: 7.82\n",
      "epis: 2756   score: 5.0   mem len: 724509   epsilon: 0.01    steps: 327    lr: 4e-07     reward: 7.76\n",
      "epis: 2757   score: 9.0   mem len: 725003   epsilon: 0.01    steps: 494    lr: 4e-07     reward: 7.79\n",
      "epis: 2758   score: 9.0   mem len: 725490   epsilon: 0.01    steps: 487    lr: 4e-07     reward: 7.79\n",
      "epis: 2759   score: 4.0   mem len: 725750   epsilon: 0.01    steps: 260    lr: 4e-07     reward: 7.74\n",
      "epis: 2760   score: 8.0   mem len: 726195   epsilon: 0.01    steps: 445    lr: 4e-07     reward: 7.76\n",
      "epis: 2761   score: 6.0   mem len: 726523   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.74\n",
      "epis: 2762   score: 11.0   mem len: 727027   epsilon: 0.01    steps: 504    lr: 4e-07     reward: 7.78\n",
      "epis: 2763   score: 5.0   mem len: 727353   epsilon: 0.01    steps: 326    lr: 4e-07     reward: 7.77\n",
      "epis: 2764   score: 12.0   mem len: 727842   epsilon: 0.01    steps: 489    lr: 4e-07     reward: 7.76\n",
      "epis: 2765   score: 6.0   mem len: 728170   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.75\n",
      "epis: 2766   score: 8.0   mem len: 728595   epsilon: 0.01    steps: 425    lr: 4e-07     reward: 7.76\n",
      "epis: 2767   score: 5.0   mem len: 728921   epsilon: 0.01    steps: 326    lr: 4e-07     reward: 7.73\n",
      "epis: 2768   score: 8.0   mem len: 729321   epsilon: 0.01    steps: 400    lr: 4e-07     reward: 7.75\n",
      "epis: 2769   score: 6.0   mem len: 729649   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.75\n",
      "epis: 2770   score: 7.0   mem len: 730078   epsilon: 0.01    steps: 429    lr: 4e-07     reward: 7.76\n",
      "epis: 2771   score: 10.0   mem len: 730523   epsilon: 0.01    steps: 445    lr: 4e-07     reward: 7.77\n",
      "epis: 2772   score: 6.0   mem len: 730851   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.74\n",
      "epis: 2773   score: 6.0   mem len: 731212   epsilon: 0.01    steps: 361    lr: 4e-07     reward: 7.68\n",
      "epis: 2774   score: 10.0   mem len: 731733   epsilon: 0.01    steps: 521    lr: 4e-07     reward: 7.69\n",
      "epis: 2775   score: 6.0   mem len: 732061   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.66\n",
      "epis: 2776   score: 5.0   mem len: 732367   epsilon: 0.01    steps: 306    lr: 4e-07     reward: 7.59\n",
      "epis: 2777   score: 10.0   mem len: 732846   epsilon: 0.01    steps: 479    lr: 4e-07     reward: 7.61\n",
      "epis: 2778   score: 6.0   mem len: 733174   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.59\n",
      "epis: 2779   score: 6.0   mem len: 733502   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.6\n",
      "epis: 2780   score: 6.0   mem len: 733830   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.55\n",
      "epis: 2781   score: 6.0   mem len: 734187   epsilon: 0.01    steps: 357    lr: 4e-07     reward: 7.51\n",
      "epis: 2782   score: 7.0   mem len: 734563   epsilon: 0.01    steps: 376    lr: 4e-07     reward: 7.48\n",
      "epis: 2783   score: 6.0   mem len: 734904   epsilon: 0.01    steps: 341    lr: 4e-07     reward: 7.45\n",
      "epis: 2784   score: 14.0   mem len: 735465   epsilon: 0.01    steps: 561    lr: 4e-07     reward: 7.49\n",
      "epis: 2785   score: 10.0   mem len: 735944   epsilon: 0.01    steps: 479    lr: 4e-07     reward: 7.55\n",
      "epis: 2786   score: 8.0   mem len: 736310   epsilon: 0.01    steps: 366    lr: 4e-07     reward: 7.54\n",
      "epis: 2787   score: 6.0   mem len: 736651   epsilon: 0.01    steps: 341    lr: 4e-07     reward: 7.55\n",
      "epis: 2788   score: 6.0   mem len: 736979   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.53\n",
      "epis: 2789   score: 6.0   mem len: 737307   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.42\n",
      "epis: 2790   score: 6.0   mem len: 737646   epsilon: 0.01    steps: 339    lr: 4e-07     reward: 7.41\n",
      "epis: 2791   score: 10.0   mem len: 738148   epsilon: 0.01    steps: 502    lr: 4e-07     reward: 7.45\n",
      "epis: 2792   score: 7.0   mem len: 738538   epsilon: 0.01    steps: 390    lr: 4e-07     reward: 7.48\n",
      "epis: 2793   score: 15.0   mem len: 739108   epsilon: 0.01    steps: 570    lr: 4e-07     reward: 7.55\n",
      "epis: 2794   score: 12.0   mem len: 739543   epsilon: 0.01    steps: 435    lr: 4e-07     reward: 7.56\n",
      "epis: 2795   score: 11.0   mem len: 740034   epsilon: 0.01    steps: 491    lr: 4e-07     reward: 7.61\n",
      "epis: 2796   score: 10.0   mem len: 740508   epsilon: 0.01    steps: 474    lr: 4e-07     reward: 7.58\n",
      "epis: 2797   score: 5.0   mem len: 740837   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 7.54\n",
      "epis: 2798   score: 8.0   mem len: 741282   epsilon: 0.01    steps: 445    lr: 4e-07     reward: 7.56\n",
      "epis: 2799   score: 6.0   mem len: 741610   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.56\n",
      "epis: 2800   score: 10.0   mem len: 742095   epsilon: 0.01    steps: 485    lr: 4e-07     reward: 7.6\n",
      "epis: 2801   score: 5.0   mem len: 742424   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 7.6\n",
      "epis: 2802   score: 11.0   mem len: 742892   epsilon: 0.01    steps: 468    lr: 4e-07     reward: 7.63\n",
      "epis: 2803   score: 9.0   mem len: 743385   epsilon: 0.01    steps: 493    lr: 4e-07     reward: 7.67\n",
      "epis: 2804   score: 9.0   mem len: 743858   epsilon: 0.01    steps: 473    lr: 4e-07     reward: 7.71\n",
      "epis: 2805   score: 5.0   mem len: 744168   epsilon: 0.01    steps: 310    lr: 4e-07     reward: 7.67\n",
      "epis: 2806   score: 5.0   mem len: 744479   epsilon: 0.01    steps: 311    lr: 4e-07     reward: 7.66\n",
      "epis: 2807   score: 5.0   mem len: 744772   epsilon: 0.01    steps: 293    lr: 4e-07     reward: 7.66\n",
      "epis: 2808   score: 7.0   mem len: 745108   epsilon: 0.01    steps: 336    lr: 4e-07     reward: 7.67\n",
      "epis: 2809   score: 7.0   mem len: 745514   epsilon: 0.01    steps: 406    lr: 4e-07     reward: 7.68\n",
      "epis: 2810   score: 8.0   mem len: 745923   epsilon: 0.01    steps: 409    lr: 4e-07     reward: 7.69\n",
      "epis: 2811   score: 12.0   mem len: 746398   epsilon: 0.01    steps: 475    lr: 4e-07     reward: 7.75\n",
      "epis: 2812   score: 4.0   mem len: 746642   epsilon: 0.01    steps: 244    lr: 4e-07     reward: 7.71\n",
      "epis: 2813   score: 7.0   mem len: 747048   epsilon: 0.01    steps: 406    lr: 4e-07     reward: 7.72\n",
      "epis: 2814   score: 9.0   mem len: 747501   epsilon: 0.01    steps: 453    lr: 4e-07     reward: 7.75\n",
      "epis: 2815   score: 6.0   mem len: 747829   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.74\n",
      "epis: 2816   score: 4.0   mem len: 748108   epsilon: 0.01    steps: 279    lr: 4e-07     reward: 7.69\n",
      "epis: 2817   score: 4.0   mem len: 748387   epsilon: 0.01    steps: 279    lr: 4e-07     reward: 7.67\n",
      "epis: 2818   score: 8.0   mem len: 748762   epsilon: 0.01    steps: 375    lr: 4e-07     reward: 7.69\n",
      "epis: 2819   score: 6.0   mem len: 749119   epsilon: 0.01    steps: 357    lr: 4e-07     reward: 7.68\n",
      "epis: 2820   score: 6.0   mem len: 749447   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.66\n",
      "epis: 2821   score: 5.0   mem len: 749715   epsilon: 0.01    steps: 268    lr: 4e-07     reward: 7.65\n",
      "epis: 2822   score: 11.0   mem len: 750206   epsilon: 0.01    steps: 491    lr: 4e-07     reward: 7.71\n",
      "epis: 2823   score: 12.0   mem len: 750775   epsilon: 0.01    steps: 569    lr: 4e-07     reward: 7.77\n",
      "epis: 2824   score: 6.0   mem len: 751103   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.72\n",
      "epis: 2825   score: 7.0   mem len: 751480   epsilon: 0.01    steps: 377    lr: 4e-07     reward: 7.71\n",
      "epis: 2826   score: 6.0   mem len: 751808   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.7\n",
      "epis: 2827   score: 6.0   mem len: 752136   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.69\n",
      "epis: 2828   score: 8.0   mem len: 752538   epsilon: 0.01    steps: 402    lr: 4e-07     reward: 7.71\n",
      "epis: 2829   score: 9.0   mem len: 753013   epsilon: 0.01    steps: 475    lr: 4e-07     reward: 7.74\n",
      "epis: 2830   score: 5.0   mem len: 753341   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.73\n",
      "epis: 2831   score: 7.0   mem len: 753765   epsilon: 0.01    steps: 424    lr: 4e-07     reward: 7.7\n",
      "epis: 2832   score: 9.0   mem len: 754097   epsilon: 0.01    steps: 332    lr: 4e-07     reward: 7.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2833   score: 6.0   mem len: 754425   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.73\n",
      "epis: 2834   score: 7.0   mem len: 754813   epsilon: 0.01    steps: 388    lr: 4e-07     reward: 7.73\n",
      "epis: 2835   score: 6.0   mem len: 755141   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.67\n",
      "epis: 2836   score: 9.0   mem len: 755448   epsilon: 0.01    steps: 307    lr: 4e-07     reward: 7.7\n",
      "epis: 2837   score: 10.0   mem len: 755900   epsilon: 0.01    steps: 452    lr: 4e-07     reward: 7.73\n",
      "epis: 2838   score: 7.0   mem len: 756288   epsilon: 0.01    steps: 388    lr: 4e-07     reward: 7.66\n",
      "epis: 2839   score: 7.0   mem len: 756648   epsilon: 0.01    steps: 360    lr: 4e-07     reward: 7.57\n",
      "epis: 2840   score: 5.0   mem len: 756975   epsilon: 0.01    steps: 327    lr: 4e-07     reward: 7.57\n",
      "epis: 2841   score: 6.0   mem len: 757333   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 7.52\n",
      "epis: 2842   score: 12.0   mem len: 757868   epsilon: 0.01    steps: 535    lr: 4e-07     reward: 7.56\n",
      "epis: 2843   score: 12.0   mem len: 758444   epsilon: 0.01    steps: 576    lr: 4e-07     reward: 7.63\n",
      "epis: 2844   score: 3.0   mem len: 758657   epsilon: 0.01    steps: 213    lr: 4e-07     reward: 7.58\n",
      "epis: 2845   score: 10.0   mem len: 759109   epsilon: 0.01    steps: 452    lr: 4e-07     reward: 7.58\n",
      "epis: 2846   score: 4.0   mem len: 759371   epsilon: 0.01    steps: 262    lr: 4e-07     reward: 7.51\n",
      "epis: 2847   score: 6.0   mem len: 759710   epsilon: 0.01    steps: 339    lr: 4e-07     reward: 7.51\n",
      "epis: 2848   score: 5.0   mem len: 760021   epsilon: 0.01    steps: 311    lr: 4e-07     reward: 7.5\n",
      "epis: 2849   score: 7.0   mem len: 760362   epsilon: 0.01    steps: 341    lr: 4e-07     reward: 7.54\n",
      "epis: 2850   score: 7.0   mem len: 760720   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 7.51\n",
      "epis: 2851   score: 5.0   mem len: 761031   epsilon: 0.01    steps: 311    lr: 4e-07     reward: 7.51\n",
      "epis: 2852   score: 6.0   mem len: 761352   epsilon: 0.01    steps: 321    lr: 4e-07     reward: 7.46\n",
      "epis: 2853   score: 6.0   mem len: 761680   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.46\n",
      "epis: 2854   score: 6.0   mem len: 762008   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.43\n",
      "epis: 2855   score: 6.0   mem len: 762336   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.35\n",
      "epis: 2856   score: 6.0   mem len: 762691   epsilon: 0.01    steps: 355    lr: 4e-07     reward: 7.36\n",
      "epis: 2857   score: 5.0   mem len: 763017   epsilon: 0.01    steps: 326    lr: 4e-07     reward: 7.32\n",
      "epis: 2858   score: 6.0   mem len: 763345   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.29\n",
      "epis: 2859   score: 10.0   mem len: 763813   epsilon: 0.01    steps: 468    lr: 4e-07     reward: 7.35\n",
      "epis: 2860   score: 6.0   mem len: 764141   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.33\n",
      "epis: 2861   score: 6.0   mem len: 764469   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.33\n",
      "epis: 2862   score: 6.0   mem len: 764797   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.28\n",
      "epis: 2863   score: 7.0   mem len: 765174   epsilon: 0.01    steps: 377    lr: 4e-07     reward: 7.3\n",
      "epis: 2864   score: 14.0   mem len: 765754   epsilon: 0.01    steps: 580    lr: 4e-07     reward: 7.32\n",
      "epis: 2865   score: 4.0   mem len: 766033   epsilon: 0.01    steps: 279    lr: 4e-07     reward: 7.3\n",
      "epis: 2866   score: 6.0   mem len: 766361   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.28\n",
      "epis: 2867   score: 9.0   mem len: 766774   epsilon: 0.01    steps: 413    lr: 4e-07     reward: 7.32\n",
      "epis: 2868   score: 6.0   mem len: 767111   epsilon: 0.01    steps: 337    lr: 4e-07     reward: 7.3\n",
      "epis: 2869   score: 6.0   mem len: 767439   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.3\n",
      "epis: 2870   score: 6.0   mem len: 767767   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.29\n",
      "epis: 2871   score: 6.0   mem len: 768095   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.25\n",
      "epis: 2872   score: 6.0   mem len: 768423   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.25\n",
      "epis: 2873   score: 6.0   mem len: 768751   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.25\n",
      "epis: 2874   score: 5.0   mem len: 769080   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 7.2\n",
      "epis: 2875   score: 9.0   mem len: 769412   epsilon: 0.01    steps: 332    lr: 4e-07     reward: 7.23\n",
      "epis: 2876   score: 6.0   mem len: 769732   epsilon: 0.01    steps: 320    lr: 4e-07     reward: 7.24\n",
      "epis: 2877   score: 9.0   mem len: 770225   epsilon: 0.01    steps: 493    lr: 4e-07     reward: 7.23\n",
      "epis: 2878   score: 5.0   mem len: 770550   epsilon: 0.01    steps: 325    lr: 4e-07     reward: 7.22\n",
      "epis: 2879   score: 7.0   mem len: 770949   epsilon: 0.01    steps: 399    lr: 4e-07     reward: 7.23\n",
      "epis: 2880   score: 11.0   mem len: 771516   epsilon: 0.01    steps: 567    lr: 4e-07     reward: 7.28\n",
      "epis: 2881   score: 12.0   mem len: 772085   epsilon: 0.01    steps: 569    lr: 4e-07     reward: 7.34\n",
      "epis: 2882   score: 6.0   mem len: 772413   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.33\n",
      "epis: 2883   score: 5.0   mem len: 772735   epsilon: 0.01    steps: 322    lr: 4e-07     reward: 7.32\n",
      "epis: 2884   score: 3.0   mem len: 772948   epsilon: 0.01    steps: 213    lr: 4e-07     reward: 7.21\n",
      "epis: 2885   score: 6.0   mem len: 773276   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.17\n",
      "epis: 2886   score: 6.0   mem len: 773604   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.15\n",
      "epis: 2887   score: 6.0   mem len: 773932   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.15\n",
      "epis: 2888   score: 6.0   mem len: 774260   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.15\n",
      "epis: 2889   score: 6.0   mem len: 774637   epsilon: 0.01    steps: 377    lr: 4e-07     reward: 7.15\n",
      "epis: 2890   score: 6.0   mem len: 774965   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.15\n",
      "epis: 2891   score: 9.0   mem len: 775457   epsilon: 0.01    steps: 492    lr: 4e-07     reward: 7.14\n",
      "epis: 2892   score: 4.0   mem len: 775735   epsilon: 0.01    steps: 278    lr: 4e-07     reward: 7.11\n",
      "epis: 2893   score: 6.0   mem len: 776063   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.02\n",
      "epis: 2894   score: 3.0   mem len: 776276   epsilon: 0.01    steps: 213    lr: 4e-07     reward: 6.93\n",
      "epis: 2895   score: 6.0   mem len: 776604   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.88\n",
      "epis: 2896   score: 6.0   mem len: 776932   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.84\n",
      "epis: 2897   score: 11.0   mem len: 777484   epsilon: 0.01    steps: 552    lr: 4e-07     reward: 6.9\n",
      "epis: 2898   score: 6.0   mem len: 777812   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.88\n",
      "epis: 2899   score: 10.0   mem len: 778280   epsilon: 0.01    steps: 468    lr: 4e-07     reward: 6.92\n",
      "epis: 2900   score: 6.0   mem len: 778608   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.88\n",
      "epis: 2901   score: 5.0   mem len: 778881   epsilon: 0.01    steps: 273    lr: 4e-07     reward: 6.88\n",
      "epis: 2902   score: 10.0   mem len: 779382   epsilon: 0.01    steps: 501    lr: 4e-07     reward: 6.87\n",
      "epis: 2903   score: 11.0   mem len: 779934   epsilon: 0.01    steps: 552    lr: 4e-07     reward: 6.89\n",
      "epis: 2904   score: 6.0   mem len: 780262   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.86\n",
      "epis: 2905   score: 10.0   mem len: 780765   epsilon: 0.01    steps: 503    lr: 4e-07     reward: 6.91\n",
      "epis: 2906   score: 6.0   mem len: 781093   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.92\n",
      "epis: 2907   score: 9.0   mem len: 781571   epsilon: 0.01    steps: 478    lr: 4e-07     reward: 6.96\n",
      "epis: 2908   score: 6.0   mem len: 781899   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.95\n",
      "epis: 2909   score: 6.0   mem len: 782227   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.94\n",
      "epis: 2910   score: 6.0   mem len: 782555   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.92\n",
      "epis: 2911   score: 6.0   mem len: 782883   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.86\n",
      "epis: 2912   score: 6.0   mem len: 783211   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.88\n",
      "epis: 2913   score: 8.0   mem len: 783614   epsilon: 0.01    steps: 403    lr: 4e-07     reward: 6.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2914   score: 7.0   mem len: 784009   epsilon: 0.01    steps: 395    lr: 4e-07     reward: 6.87\n",
      "epis: 2915   score: 10.0   mem len: 784536   epsilon: 0.01    steps: 527    lr: 4e-07     reward: 6.91\n",
      "epis: 2916   score: 8.0   mem len: 784934   epsilon: 0.01    steps: 398    lr: 4e-07     reward: 6.95\n",
      "epis: 2917   score: 7.0   mem len: 785292   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 6.98\n",
      "epis: 2918   score: 6.0   mem len: 785620   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.96\n",
      "epis: 2919   score: 13.0   mem len: 786133   epsilon: 0.01    steps: 513    lr: 4e-07     reward: 7.03\n",
      "epis: 2920   score: 7.0   mem len: 786515   epsilon: 0.01    steps: 382    lr: 4e-07     reward: 7.04\n",
      "epis: 2921   score: 8.0   mem len: 786976   epsilon: 0.01    steps: 461    lr: 4e-07     reward: 7.07\n",
      "epis: 2922   score: 5.0   mem len: 787302   epsilon: 0.01    steps: 326    lr: 4e-07     reward: 7.01\n",
      "epis: 2923   score: 9.0   mem len: 787765   epsilon: 0.01    steps: 463    lr: 4e-07     reward: 6.98\n",
      "epis: 2924   score: 9.0   mem len: 788097   epsilon: 0.01    steps: 332    lr: 4e-07     reward: 7.01\n",
      "epis: 2925   score: 6.0   mem len: 788425   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 7.0\n",
      "epis: 2926   score: 4.0   mem len: 788703   epsilon: 0.01    steps: 278    lr: 4e-07     reward: 6.98\n",
      "epis: 2927   score: 6.0   mem len: 789031   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.98\n",
      "epis: 2928   score: 4.0   mem len: 789291   epsilon: 0.01    steps: 260    lr: 4e-07     reward: 6.94\n",
      "epis: 2929   score: 7.0   mem len: 789649   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 6.92\n",
      "epis: 2930   score: 7.0   mem len: 790007   epsilon: 0.01    steps: 358    lr: 4e-07     reward: 6.94\n",
      "epis: 2931   score: 11.0   mem len: 790552   epsilon: 0.01    steps: 545    lr: 4e-07     reward: 6.98\n",
      "epis: 2932   score: 5.0   mem len: 790881   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 6.94\n",
      "epis: 2933   score: 10.0   mem len: 791349   epsilon: 0.01    steps: 468    lr: 4e-07     reward: 6.98\n",
      "epis: 2934   score: 7.0   mem len: 791726   epsilon: 0.01    steps: 377    lr: 4e-07     reward: 6.98\n",
      "epis: 2935   score: 6.0   mem len: 792054   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.98\n",
      "epis: 2936   score: 5.0   mem len: 792383   epsilon: 0.01    steps: 329    lr: 4e-07     reward: 6.94\n",
      "epis: 2937   score: 5.0   mem len: 792664   epsilon: 0.01    steps: 281    lr: 4e-07     reward: 6.89\n",
      "epis: 2938   score: 8.0   mem len: 793067   epsilon: 0.01    steps: 403    lr: 4e-07     reward: 6.9\n",
      "epis: 2939   score: 9.0   mem len: 793511   epsilon: 0.01    steps: 444    lr: 4e-07     reward: 6.92\n",
      "epis: 2940   score: 5.0   mem len: 793822   epsilon: 0.01    steps: 311    lr: 4e-07     reward: 6.92\n",
      "epis: 2941   score: 9.0   mem len: 794266   epsilon: 0.01    steps: 444    lr: 4e-07     reward: 6.95\n",
      "epis: 2942   score: 11.0   mem len: 794768   epsilon: 0.01    steps: 502    lr: 4e-07     reward: 6.94\n",
      "epis: 2943   score: 6.0   mem len: 795096   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.88\n",
      "epis: 2944   score: 11.0   mem len: 795648   epsilon: 0.01    steps: 552    lr: 4e-07     reward: 6.96\n",
      "epis: 2945   score: 6.0   mem len: 795976   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.92\n",
      "epis: 2946   score: 6.0   mem len: 796304   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.94\n",
      "epis: 2947   score: 6.0   mem len: 796632   epsilon: 0.01    steps: 328    lr: 4e-07     reward: 6.94\n",
      "epis: 2948   score: 6.0   mem len: 796971   epsilon: 0.01    steps: 339    lr: 4e-07     reward: 6.95\n",
      "epis: 2949   score: 14.0   mem len: 797586   epsilon: 0.01    steps: 615    lr: 4e-07     reward: 7.02\n",
      "epis: 2950   score: 7.0   mem len: 797961   epsilon: 0.01    steps: 375    lr: 4e-07     reward: 7.02\n",
      "epis: 2951   score: 8.0   mem len: 798387   epsilon: 0.01    steps: 426    lr: 4e-07     reward: 7.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44971/607054412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episodes vs Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./save_graph/breakout_dqn.png\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save graph for training visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# every episode, plot the play time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3275\u001b[0m                         ax.patch._cm_set(facecolor='none', edgecolor='none'))\n\u001b[1;32m   3276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3277\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2339\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2202\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2203\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2205\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 **kwargs)\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mDECORATORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \"\"\"\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    464\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    468\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    407\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3074\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3075\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   3076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3105\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3107\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3108\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dash_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgbFace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mcant_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"epis:\", e, \"  score:\", score, \"  mem len:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 4), \"   steps:\", step,\n",
    "                  \"   lr:\", round(agent.optimizer.param_groups[0]['lr'], 7), \"    reward:\", round(np.mean(evaluation_reward), 2))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
