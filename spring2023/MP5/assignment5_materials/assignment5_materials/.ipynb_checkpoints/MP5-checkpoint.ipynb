{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37847/2096004702.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "/tmp/ipykernel_37847/2096004702.py:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 0   score: 0.0   mem len: 123   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 0.0\n",
      "epis: 1   score: 2.0   mem len: 305   epsilon: 0.7    steps: 182    lr: 0.0001     reward: 1.0\n",
      "epis: 2   score: 2.0   mem len: 521   epsilon: 0.7    steps: 216    lr: 0.0001     reward: 1.33\n",
      "epis: 3   score: 0.0   mem len: 644   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.0\n",
      "epis: 4   score: 3.0   mem len: 894   epsilon: 0.7    steps: 250    lr: 0.0001     reward: 1.4\n",
      "epis: 5   score: 1.0   mem len: 1045   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 6   score: 1.0   mem len: 1196   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.29\n",
      "epis: 7   score: 0.0   mem len: 1318   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.12\n",
      "epis: 8   score: 5.0   mem len: 1644   epsilon: 0.7    steps: 326    lr: 0.0001     reward: 1.56\n",
      "epis: 9   score: 4.0   mem len: 1904   epsilon: 0.7    steps: 260    lr: 0.0001     reward: 1.8\n",
      "epis: 10   score: 1.0   mem len: 2055   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 11   score: 1.0   mem len: 2205   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.67\n",
      "epis: 12   score: 1.0   mem len: 2356   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.62\n",
      "epis: 13   score: 0.0   mem len: 2478   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 14   score: 2.0   mem len: 2694   epsilon: 0.7    steps: 216    lr: 0.0001     reward: 1.53\n",
      "epis: 15   score: 1.0   mem len: 2845   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 16   score: 2.0   mem len: 3066   epsilon: 0.7    steps: 221    lr: 0.0001     reward: 1.53\n",
      "epis: 17   score: 3.0   mem len: 3331   epsilon: 0.7    steps: 265    lr: 0.0001     reward: 1.61\n",
      "epis: 18   score: 3.0   mem len: 3540   epsilon: 0.7    steps: 209    lr: 0.0001     reward: 1.68\n",
      "epis: 19   score: 2.0   mem len: 3741   epsilon: 0.7    steps: 201    lr: 0.0001     reward: 1.7\n",
      "epis: 20   score: 1.0   mem len: 3891   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.67\n",
      "epis: 21   score: 4.0   mem len: 4186   epsilon: 0.7    steps: 295    lr: 0.0001     reward: 1.77\n",
      "epis: 22   score: 1.0   mem len: 4356   epsilon: 0.7    steps: 170    lr: 0.0001     reward: 1.74\n",
      "epis: 23   score: 1.0   mem len: 4507   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.71\n",
      "epis: 24   score: 3.0   mem len: 4737   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.76\n",
      "epis: 25   score: 1.0   mem len: 4888   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 26   score: 1.0   mem len: 5039   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 27   score: 3.0   mem len: 5288   epsilon: 0.7    steps: 249    lr: 0.0001     reward: 1.75\n",
      "epis: 28   score: 2.0   mem len: 5488   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.76\n",
      "epis: 29   score: 0.0   mem len: 5610   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.7\n",
      "epis: 30   score: 2.0   mem len: 5812   epsilon: 0.7    steps: 202    lr: 0.0001     reward: 1.71\n",
      "epis: 31   score: 1.0   mem len: 5962   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.69\n",
      "epis: 32   score: 1.0   mem len: 6113   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.67\n",
      "epis: 33   score: 3.0   mem len: 6343   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.71\n",
      "epis: 34   score: 4.0   mem len: 6663   epsilon: 0.7    steps: 320    lr: 0.0001     reward: 1.77\n",
      "epis: 35   score: 1.0   mem len: 6813   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.75\n",
      "epis: 36   score: 1.0   mem len: 6964   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 37   score: 1.0   mem len: 7114   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.71\n",
      "epis: 38   score: 2.0   mem len: 7294   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.72\n",
      "epis: 39   score: 1.0   mem len: 7445   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 40   score: 3.0   mem len: 7675   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.73\n",
      "epis: 41   score: 1.0   mem len: 7825   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.71\n",
      "epis: 42   score: 3.0   mem len: 8054   epsilon: 0.7    steps: 229    lr: 0.0001     reward: 1.74\n",
      "epis: 43   score: 0.0   mem len: 8177   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.7\n",
      "epis: 44   score: 1.0   mem len: 8328   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.69\n",
      "epis: 45   score: 1.0   mem len: 8479   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.67\n",
      "epis: 46   score: 2.0   mem len: 8696   epsilon: 0.7    steps: 217    lr: 0.0001     reward: 1.68\n",
      "epis: 47   score: 0.0   mem len: 8819   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 48   score: 2.0   mem len: 9016   epsilon: 0.7    steps: 197    lr: 0.0001     reward: 1.65\n",
      "epis: 49   score: 1.0   mem len: 9167   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.64\n",
      "epis: 50   score: 1.0   mem len: 9317   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.63\n",
      "epis: 51   score: 2.0   mem len: 9515   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.63\n",
      "epis: 52   score: 4.0   mem len: 9763   epsilon: 0.7    steps: 248    lr: 0.0001     reward: 1.68\n",
      "epis: 53   score: 1.0   mem len: 9914   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.67\n",
      "epis: 54   score: 3.0   mem len: 10143   epsilon: 0.7    steps: 229    lr: 0.0001     reward: 1.69\n",
      "epis: 55   score: 2.0   mem len: 10343   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.7\n",
      "epis: 56   score: 2.0   mem len: 10544   epsilon: 0.7    steps: 201    lr: 0.0001     reward: 1.7\n",
      "epis: 57   score: 1.0   mem len: 10695   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.69\n",
      "epis: 58   score: 3.0   mem len: 10923   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 1.71\n",
      "epis: 59   score: 4.0   mem len: 11181   epsilon: 0.7    steps: 258    lr: 0.0001     reward: 1.75\n",
      "epis: 60   score: 1.0   mem len: 11331   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.74\n",
      "epis: 61   score: 1.0   mem len: 11482   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 62   score: 0.0   mem len: 11605   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.7\n",
      "epis: 63   score: 3.0   mem len: 11818   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.72\n",
      "epis: 64   score: 2.0   mem len: 11998   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.72\n",
      "epis: 65   score: 2.0   mem len: 12198   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.73\n",
      "epis: 66   score: 1.0   mem len: 12349   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 67   score: 1.0   mem len: 12499   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.71\n",
      "epis: 68   score: 3.0   mem len: 12729   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.72\n",
      "epis: 69   score: 0.0   mem len: 12851   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.7\n",
      "epis: 70   score: 1.0   mem len: 13002   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.69\n",
      "epis: 71   score: 1.0   mem len: 13153   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.68\n",
      "epis: 72   score: 3.0   mem len: 13364   epsilon: 0.7    steps: 211    lr: 0.0001     reward: 1.7\n",
      "epis: 73   score: 0.0   mem len: 13487   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 74   score: 1.0   mem len: 13638   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.67\n",
      "epis: 75   score: 0.0   mem len: 13760   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 76   score: 3.0   mem len: 13988   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 1.66\n",
      "epis: 77   score: 3.0   mem len: 14234   epsilon: 0.7    steps: 246    lr: 0.0001     reward: 1.68\n",
      "epis: 78   score: 3.0   mem len: 14445   epsilon: 0.7    steps: 211    lr: 0.0001     reward: 1.7\n",
      "epis: 79   score: 2.0   mem len: 14642   epsilon: 0.7    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 80   score: 3.0   mem len: 14852   epsilon: 0.7    steps: 210    lr: 0.0001     reward: 1.72\n",
      "epis: 81   score: 1.0   mem len: 15003   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.71\n",
      "epis: 82   score: 3.0   mem len: 15216   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.72\n",
      "epis: 83   score: 1.0   mem len: 15367   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 84   score: 3.0   mem len: 15577   epsilon: 0.7    steps: 210    lr: 0.0001     reward: 1.73\n",
      "epis: 85   score: 1.0   mem len: 15728   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 86   score: 3.0   mem len: 15958   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.74\n",
      "epis: 87   score: 1.0   mem len: 16109   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 88   score: 3.0   mem len: 16338   epsilon: 0.7    steps: 229    lr: 0.0001     reward: 1.74\n",
      "epis: 89   score: 2.0   mem len: 16518   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.74\n",
      "epis: 90   score: 3.0   mem len: 16768   epsilon: 0.7    steps: 250    lr: 0.0001     reward: 1.76\n",
      "epis: 91   score: 2.0   mem len: 16968   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.76\n",
      "epis: 92   score: 1.0   mem len: 17119   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 93   score: 4.0   mem len: 17412   epsilon: 0.7    steps: 293    lr: 0.0001     reward: 1.78\n",
      "epis: 94   score: 1.0   mem len: 17563   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.77\n",
      "epis: 95   score: 4.0   mem len: 17840   epsilon: 0.7    steps: 277    lr: 0.0001     reward: 1.79\n",
      "epis: 96   score: 1.0   mem len: 17991   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.78\n",
      "epis: 97   score: 1.0   mem len: 18142   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.78\n",
      "epis: 98   score: 1.0   mem len: 18311   epsilon: 0.7    steps: 169    lr: 0.0001     reward: 1.77\n",
      "epis: 99   score: 0.0   mem len: 18433   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.75\n",
      "epis: 100   score: 3.0   mem len: 18661   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 1.78\n",
      "epis: 101   score: 0.0   mem len: 18784   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.76\n",
      "epis: 102   score: 1.0   mem len: 18935   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 103   score: 1.0   mem len: 19086   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.76\n",
      "epis: 104   score: 2.0   mem len: 19266   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.75\n",
      "epis: 105   score: 1.0   mem len: 19417   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 106   score: 0.0   mem len: 19540   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.74\n",
      "epis: 107   score: 1.0   mem len: 19690   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.75\n",
      "epis: 108   score: 1.0   mem len: 19841   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.71\n",
      "epis: 109   score: 0.0   mem len: 19964   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 110   score: 2.0   mem len: 20145   epsilon: 0.7    steps: 181    lr: 0.0001     reward: 1.68\n",
      "epis: 111   score: 2.0   mem len: 20343   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 112   score: 4.0   mem len: 20618   epsilon: 0.7    steps: 275    lr: 0.0001     reward: 1.72\n",
      "epis: 113   score: 1.0   mem len: 20787   epsilon: 0.7    steps: 169    lr: 0.0001     reward: 1.73\n",
      "epis: 114   score: 0.0   mem len: 20910   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.71\n",
      "epis: 115   score: 1.0   mem len: 21061   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.71\n",
      "epis: 116   score: 3.0   mem len: 21272   epsilon: 0.7    steps: 211    lr: 0.0001     reward: 1.72\n",
      "epis: 117   score: 1.0   mem len: 21423   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 118   score: 3.0   mem len: 21669   epsilon: 0.7    steps: 246    lr: 0.0001     reward: 1.7\n",
      "epis: 119   score: 2.0   mem len: 21866   epsilon: 0.7    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 120   score: 0.0   mem len: 21989   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 121   score: 1.0   mem len: 22139   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.66\n",
      "epis: 122   score: 3.0   mem len: 22349   epsilon: 0.7    steps: 210    lr: 0.0001     reward: 1.68\n",
      "epis: 123   score: 5.0   mem len: 22661   epsilon: 0.7    steps: 312    lr: 0.0001     reward: 1.72\n",
      "epis: 124   score: 5.0   mem len: 22969   epsilon: 0.7    steps: 308    lr: 0.0001     reward: 1.74\n",
      "epis: 125   score: 0.0   mem len: 23092   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.73\n",
      "epis: 126   score: 2.0   mem len: 23294   epsilon: 0.7    steps: 202    lr: 0.0001     reward: 1.74\n",
      "epis: 127   score: 4.0   mem len: 23570   epsilon: 0.7    steps: 276    lr: 0.0001     reward: 1.75\n",
      "epis: 128   score: 4.0   mem len: 23829   epsilon: 0.7    steps: 259    lr: 0.0001     reward: 1.77\n",
      "epis: 129   score: 2.0   mem len: 24028   epsilon: 0.7    steps: 199    lr: 0.0001     reward: 1.79\n",
      "epis: 130   score: 3.0   mem len: 24257   epsilon: 0.7    steps: 229    lr: 0.0001     reward: 1.8\n",
      "epis: 131   score: 1.0   mem len: 24408   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.8\n",
      "epis: 132   score: 0.0   mem len: 24531   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.79\n",
      "epis: 133   score: 1.0   mem len: 24682   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.77\n",
      "epis: 134   score: 0.0   mem len: 24804   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.73\n",
      "epis: 135   score: 1.0   mem len: 24954   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.73\n",
      "epis: 136   score: 1.0   mem len: 25105   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 137   score: 1.0   mem len: 25256   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 138   score: 1.0   mem len: 25407   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 139   score: 2.0   mem len: 25625   epsilon: 0.7    steps: 218    lr: 0.0001     reward: 1.73\n",
      "epis: 140   score: 3.0   mem len: 25835   epsilon: 0.7    steps: 210    lr: 0.0001     reward: 1.73\n",
      "epis: 141   score: 0.0   mem len: 25958   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.72\n",
      "epis: 142   score: 1.0   mem len: 26109   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 143   score: 2.0   mem len: 26290   epsilon: 0.7    steps: 181    lr: 0.0001     reward: 1.72\n",
      "epis: 144   score: 0.0   mem len: 26413   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.71\n",
      "epis: 145   score: 1.0   mem len: 26564   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.71\n",
      "epis: 146   score: 1.0   mem len: 26714   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.7\n",
      "epis: 147   score: 0.0   mem len: 26836   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.7\n",
      "epis: 148   score: 3.0   mem len: 27046   epsilon: 0.7    steps: 210    lr: 0.0001     reward: 1.71\n",
      "epis: 149   score: 1.0   mem len: 27197   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.71\n",
      "epis: 150   score: 2.0   mem len: 27418   epsilon: 0.7    steps: 221    lr: 0.0001     reward: 1.72\n",
      "epis: 151   score: 3.0   mem len: 27631   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.73\n",
      "epis: 152   score: 1.0   mem len: 27782   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 153   score: 1.0   mem len: 27933   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 154   score: 1.0   mem len: 28083   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 155   score: 4.0   mem len: 28362   epsilon: 0.7    steps: 279    lr: 0.0001     reward: 1.7\n",
      "epis: 156   score: 2.0   mem len: 28562   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.7\n",
      "epis: 157   score: 3.0   mem len: 28810   epsilon: 0.7    steps: 248    lr: 0.0001     reward: 1.72\n",
      "epis: 158   score: 4.0   mem len: 29105   epsilon: 0.7    steps: 295    lr: 0.0001     reward: 1.73\n",
      "epis: 159   score: 4.0   mem len: 29388   epsilon: 0.7    steps: 283    lr: 0.0001     reward: 1.73\n",
      "epis: 160   score: 2.0   mem len: 29605   epsilon: 0.7    steps: 217    lr: 0.0001     reward: 1.74\n",
      "epis: 161   score: 1.0   mem len: 29756   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.74\n",
      "epis: 162   score: 1.0   mem len: 29907   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 163   score: 1.0   mem len: 30058   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 164   score: 3.0   mem len: 30307   epsilon: 0.7    steps: 249    lr: 0.0001     reward: 1.74\n",
      "epis: 165   score: 3.0   mem len: 30538   epsilon: 0.7    steps: 231    lr: 0.0001     reward: 1.75\n",
      "epis: 166   score: 0.0   mem len: 30661   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 167   score: 1.0   mem len: 30811   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.74\n",
      "epis: 168   score: 0.0   mem len: 30934   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.71\n",
      "epis: 169   score: 1.0   mem len: 31085   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 170   score: 1.0   mem len: 31236   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 171   score: 1.0   mem len: 31387   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 172   score: 4.0   mem len: 31661   epsilon: 0.7    steps: 274    lr: 0.0001     reward: 1.73\n",
      "epis: 173   score: 3.0   mem len: 31907   epsilon: 0.7    steps: 246    lr: 0.0001     reward: 1.76\n",
      "epis: 174   score: 3.0   mem len: 32120   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.78\n",
      "epis: 175   score: 0.0   mem len: 32243   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.78\n",
      "epis: 176   score: 3.0   mem len: 32456   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.78\n",
      "epis: 177   score: 0.0   mem len: 32579   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.75\n",
      "epis: 178   score: 2.0   mem len: 32759   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.74\n",
      "epis: 179   score: 9.0   mem len: 33128   epsilon: 0.7    steps: 369    lr: 0.0001     reward: 1.81\n",
      "epis: 180   score: 3.0   mem len: 33338   epsilon: 0.7    steps: 210    lr: 0.0001     reward: 1.81\n",
      "epis: 181   score: 3.0   mem len: 33568   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.83\n",
      "epis: 182   score: 3.0   mem len: 33798   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.83\n",
      "epis: 183   score: 7.0   mem len: 34171   epsilon: 0.7    steps: 373    lr: 0.0001     reward: 1.89\n",
      "epis: 184   score: 2.0   mem len: 34368   epsilon: 0.7    steps: 197    lr: 0.0001     reward: 1.88\n",
      "epis: 185   score: 1.0   mem len: 34519   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.88\n",
      "epis: 186   score: 4.0   mem len: 34778   epsilon: 0.7    steps: 259    lr: 0.0001     reward: 1.89\n",
      "epis: 187   score: 1.0   mem len: 34947   epsilon: 0.7    steps: 169    lr: 0.0001     reward: 1.89\n",
      "epis: 188   score: 4.0   mem len: 35240   epsilon: 0.7    steps: 293    lr: 0.0001     reward: 1.9\n",
      "epis: 189   score: 2.0   mem len: 35438   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.9\n",
      "epis: 190   score: 0.0   mem len: 35561   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.87\n",
      "epis: 191   score: 1.0   mem len: 35712   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.86\n",
      "epis: 192   score: 4.0   mem len: 35970   epsilon: 0.7    steps: 258    lr: 0.0001     reward: 1.89\n",
      "epis: 193   score: 0.0   mem len: 36093   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.85\n",
      "epis: 194   score: 0.0   mem len: 36215   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.84\n",
      "epis: 195   score: 1.0   mem len: 36366   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.81\n",
      "epis: 196   score: 0.0   mem len: 36489   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.8\n",
      "epis: 197   score: 0.0   mem len: 36612   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.79\n",
      "epis: 198   score: 3.0   mem len: 36842   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.81\n",
      "epis: 199   score: 1.0   mem len: 36992   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.82\n",
      "epis: 200   score: 1.0   mem len: 37143   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.8\n",
      "epis: 201   score: 1.0   mem len: 37294   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.81\n",
      "epis: 202   score: 2.0   mem len: 37494   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.82\n",
      "epis: 203   score: 2.0   mem len: 37691   epsilon: 0.7    steps: 197    lr: 0.0001     reward: 1.83\n",
      "epis: 204   score: 0.0   mem len: 37814   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.81\n",
      "epis: 205   score: 4.0   mem len: 38073   epsilon: 0.7    steps: 259    lr: 0.0001     reward: 1.84\n",
      "epis: 206   score: 3.0   mem len: 38303   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.87\n",
      "epis: 207   score: 3.0   mem len: 38516   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.89\n",
      "epis: 208   score: 1.0   mem len: 38667   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.89\n",
      "epis: 209   score: 1.0   mem len: 38818   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.9\n",
      "epis: 210   score: 3.0   mem len: 39048   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.91\n",
      "epis: 211   score: 2.0   mem len: 39248   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.91\n",
      "epis: 212   score: 1.0   mem len: 39399   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.88\n",
      "epis: 213   score: 0.0   mem len: 39522   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.87\n",
      "epis: 214   score: 1.0   mem len: 39673   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.88\n",
      "epis: 215   score: 1.0   mem len: 39823   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.88\n",
      "epis: 216   score: 1.0   mem len: 39974   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.86\n",
      "epis: 217   score: 3.0   mem len: 40187   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.88\n",
      "epis: 218   score: 5.0   mem len: 40494   epsilon: 0.7    steps: 307    lr: 0.0001     reward: 1.9\n",
      "epis: 219   score: 1.0   mem len: 40645   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.89\n",
      "epis: 220   score: 5.0   mem len: 40975   epsilon: 0.7    steps: 330    lr: 0.0001     reward: 1.94\n",
      "epis: 221   score: 2.0   mem len: 41173   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.95\n",
      "epis: 222   score: 2.0   mem len: 41373   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.94\n",
      "epis: 223   score: 2.0   mem len: 41553   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.91\n",
      "epis: 224   score: 2.0   mem len: 41751   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.88\n",
      "epis: 225   score: 3.0   mem len: 41979   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 1.91\n",
      "epis: 226   score: 3.0   mem len: 42224   epsilon: 0.7    steps: 245    lr: 0.0001     reward: 1.92\n",
      "epis: 227   score: 2.0   mem len: 42422   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.9\n",
      "epis: 228   score: 1.0   mem len: 42572   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.87\n",
      "epis: 229   score: 1.0   mem len: 42723   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.86\n",
      "epis: 230   score: 3.0   mem len: 42953   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.86\n",
      "epis: 231   score: 6.0   mem len: 43307   epsilon: 0.7    steps: 354    lr: 0.0001     reward: 1.91\n",
      "epis: 232   score: 2.0   mem len: 43507   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.93\n",
      "epis: 233   score: 2.0   mem len: 43687   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.94\n",
      "epis: 234   score: 3.0   mem len: 43916   epsilon: 0.7    steps: 229    lr: 0.0001     reward: 1.97\n",
      "epis: 235   score: 1.0   mem len: 44067   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.97\n",
      "epis: 236   score: 2.0   mem len: 44286   epsilon: 0.7    steps: 219    lr: 0.0001     reward: 1.98\n",
      "epis: 237   score: 1.0   mem len: 44437   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.98\n",
      "epis: 238   score: 4.0   mem len: 44714   epsilon: 0.7    steps: 277    lr: 0.0001     reward: 2.01\n",
      "epis: 239   score: 2.0   mem len: 44896   epsilon: 0.7    steps: 182    lr: 0.0001     reward: 2.01\n",
      "epis: 240   score: 1.0   mem len: 45067   epsilon: 0.7    steps: 171    lr: 0.0001     reward: 1.99\n",
      "epis: 241   score: 2.0   mem len: 45247   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 2.01\n",
      "epis: 242   score: 1.0   mem len: 45398   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.01\n",
      "epis: 243   score: 2.0   mem len: 45598   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 2.01\n",
      "epis: 244   score: 0.0   mem len: 45721   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.01\n",
      "epis: 245   score: 4.0   mem len: 46007   epsilon: 0.7    steps: 286    lr: 0.0001     reward: 2.04\n",
      "epis: 246   score: 1.0   mem len: 46158   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.04\n",
      "epis: 247   score: 3.0   mem len: 46422   epsilon: 0.7    steps: 264    lr: 0.0001     reward: 2.07\n",
      "epis: 248   score: 4.0   mem len: 46708   epsilon: 0.7    steps: 286    lr: 0.0001     reward: 2.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 249   score: 3.0   mem len: 46976   epsilon: 0.7    steps: 268    lr: 0.0001     reward: 2.1\n",
      "epis: 250   score: 4.0   mem len: 47254   epsilon: 0.7    steps: 278    lr: 0.0001     reward: 2.12\n",
      "epis: 251   score: 2.0   mem len: 47436   epsilon: 0.7    steps: 182    lr: 0.0001     reward: 2.11\n",
      "epis: 252   score: 1.0   mem len: 47587   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.11\n",
      "epis: 253   score: 3.0   mem len: 47836   epsilon: 0.7    steps: 249    lr: 0.0001     reward: 2.13\n",
      "epis: 254   score: 4.0   mem len: 48134   epsilon: 0.7    steps: 298    lr: 0.0001     reward: 2.16\n",
      "epis: 255   score: 1.0   mem len: 48285   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.13\n",
      "epis: 256   score: 3.0   mem len: 48535   epsilon: 0.7    steps: 250    lr: 0.0001     reward: 2.14\n",
      "epis: 257   score: 1.0   mem len: 48686   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.12\n",
      "epis: 258   score: 1.0   mem len: 48836   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.09\n",
      "epis: 259   score: 5.0   mem len: 49182   epsilon: 0.7    steps: 346    lr: 0.0001     reward: 2.1\n",
      "epis: 260   score: 4.0   mem len: 49439   epsilon: 0.7    steps: 257    lr: 0.0001     reward: 2.12\n",
      "epis: 261   score: 0.0   mem len: 49562   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.11\n",
      "epis: 262   score: 3.0   mem len: 49812   epsilon: 0.7    steps: 250    lr: 0.0001     reward: 2.13\n",
      "epis: 263   score: 3.0   mem len: 50060   epsilon: 0.7    steps: 248    lr: 0.0001     reward: 2.15\n",
      "epis: 264   score: 1.0   mem len: 50211   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.13\n",
      "epis: 265   score: 4.0   mem len: 50468   epsilon: 0.7    steps: 257    lr: 0.0001     reward: 2.14\n",
      "epis: 266   score: 0.0   mem len: 50591   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.14\n",
      "epis: 267   score: 3.0   mem len: 50804   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 2.16\n",
      "epis: 268   score: 2.0   mem len: 50984   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 2.18\n",
      "epis: 269   score: 1.0   mem len: 51135   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.18\n",
      "epis: 270   score: 1.0   mem len: 51285   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.18\n",
      "epis: 271   score: 3.0   mem len: 51497   epsilon: 0.7    steps: 212    lr: 0.0001     reward: 2.2\n",
      "epis: 272   score: 1.0   mem len: 51648   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.17\n",
      "epis: 273   score: 3.0   mem len: 51894   epsilon: 0.7    steps: 246    lr: 0.0001     reward: 2.17\n",
      "epis: 274   score: 2.0   mem len: 52092   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 2.16\n",
      "epis: 275   score: 1.0   mem len: 52243   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.17\n",
      "epis: 276   score: 4.0   mem len: 52489   epsilon: 0.7    steps: 246    lr: 0.0001     reward: 2.18\n",
      "epis: 277   score: 1.0   mem len: 52640   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.19\n",
      "epis: 278   score: 5.0   mem len: 52955   epsilon: 0.7    steps: 315    lr: 0.0001     reward: 2.22\n",
      "epis: 279   score: 4.0   mem len: 53229   epsilon: 0.7    steps: 274    lr: 0.0001     reward: 2.17\n",
      "epis: 280   score: 5.0   mem len: 53534   epsilon: 0.7    steps: 305    lr: 0.0001     reward: 2.19\n",
      "epis: 281   score: 1.0   mem len: 53685   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.17\n",
      "epis: 282   score: 1.0   mem len: 53836   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.15\n",
      "epis: 283   score: 3.0   mem len: 54066   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 2.11\n",
      "epis: 284   score: 1.0   mem len: 54217   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.1\n",
      "epis: 285   score: 2.0   mem len: 54435   epsilon: 0.7    steps: 218    lr: 0.0001     reward: 2.11\n",
      "epis: 286   score: 5.0   mem len: 54767   epsilon: 0.7    steps: 332    lr: 0.0001     reward: 2.12\n",
      "epis: 287   score: 0.0   mem len: 54890   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.11\n",
      "epis: 288   score: 3.0   mem len: 55144   epsilon: 0.7    steps: 254    lr: 0.0001     reward: 2.1\n",
      "epis: 289   score: 1.0   mem len: 55295   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.09\n",
      "epis: 290   score: 2.0   mem len: 55476   epsilon: 0.7    steps: 181    lr: 0.0001     reward: 2.11\n",
      "epis: 291   score: 1.0   mem len: 55626   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.11\n",
      "epis: 292   score: 3.0   mem len: 55856   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 2.1\n",
      "epis: 293   score: 7.0   mem len: 56278   epsilon: 0.7    steps: 422    lr: 0.0001     reward: 2.17\n",
      "epis: 294   score: 0.0   mem len: 56400   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 2.17\n",
      "epis: 295   score: 1.0   mem len: 56550   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.17\n",
      "epis: 296   score: 0.0   mem len: 56673   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.17\n",
      "epis: 297   score: 3.0   mem len: 56886   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 2.2\n",
      "epis: 298   score: 2.0   mem len: 57066   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 2.19\n",
      "epis: 299   score: 1.0   mem len: 57217   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.19\n",
      "epis: 300   score: 1.0   mem len: 57367   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.19\n",
      "epis: 301   score: 3.0   mem len: 57618   epsilon: 0.7    steps: 251    lr: 0.0001     reward: 2.21\n",
      "epis: 302   score: 5.0   mem len: 57924   epsilon: 0.7    steps: 306    lr: 0.0001     reward: 2.24\n",
      "epis: 303   score: 1.0   mem len: 58074   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.23\n",
      "epis: 304   score: 2.0   mem len: 58271   epsilon: 0.7    steps: 197    lr: 0.0001     reward: 2.25\n",
      "epis: 305   score: 0.0   mem len: 58394   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.21\n",
      "epis: 306   score: 1.0   mem len: 58545   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.19\n",
      "epis: 307   score: 0.0   mem len: 58668   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.16\n",
      "epis: 308   score: 1.0   mem len: 58819   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.16\n",
      "epis: 309   score: 0.0   mem len: 58942   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.15\n",
      "epis: 310   score: 1.0   mem len: 59093   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.13\n",
      "epis: 311   score: 1.0   mem len: 59244   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.12\n",
      "epis: 312   score: 3.0   mem len: 59475   epsilon: 0.7    steps: 231    lr: 0.0001     reward: 2.14\n",
      "epis: 313   score: 2.0   mem len: 59657   epsilon: 0.7    steps: 182    lr: 0.0001     reward: 2.16\n",
      "epis: 314   score: 1.0   mem len: 59808   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.16\n",
      "epis: 315   score: 3.0   mem len: 60019   epsilon: 0.7    steps: 211    lr: 0.0001     reward: 2.18\n",
      "epis: 316   score: 4.0   mem len: 60316   epsilon: 0.7    steps: 297    lr: 0.0001     reward: 2.21\n",
      "epis: 317   score: 3.0   mem len: 60546   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 2.21\n",
      "epis: 318   score: 1.0   mem len: 60697   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.17\n",
      "epis: 319   score: 2.0   mem len: 60879   epsilon: 0.7    steps: 182    lr: 0.0001     reward: 2.18\n",
      "epis: 320   score: 1.0   mem len: 61030   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.14\n",
      "epis: 321   score: 1.0   mem len: 61181   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.13\n",
      "epis: 322   score: 0.0   mem len: 61304   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.11\n",
      "epis: 323   score: 4.0   mem len: 61563   epsilon: 0.7    steps: 259    lr: 0.0001     reward: 2.13\n",
      "epis: 324   score: 1.0   mem len: 61714   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.12\n",
      "epis: 325   score: 1.0   mem len: 61865   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.1\n",
      "epis: 326   score: 1.0   mem len: 62016   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.08\n",
      "epis: 327   score: 1.0   mem len: 62188   epsilon: 0.7    steps: 172    lr: 0.0001     reward: 2.07\n",
      "epis: 328   score: 3.0   mem len: 62401   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 2.09\n",
      "epis: 329   score: 4.0   mem len: 62699   epsilon: 0.7    steps: 298    lr: 0.0001     reward: 2.12\n",
      "epis: 330   score: 2.0   mem len: 62899   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 2.11\n",
      "epis: 331   score: 3.0   mem len: 63127   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 2.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 332   score: 0.0   mem len: 63249   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 2.06\n",
      "epis: 333   score: 1.0   mem len: 63419   epsilon: 0.7    steps: 170    lr: 0.0001     reward: 2.05\n",
      "epis: 334   score: 5.0   mem len: 63744   epsilon: 0.7    steps: 325    lr: 0.0001     reward: 2.07\n",
      "epis: 335   score: 1.0   mem len: 63914   epsilon: 0.7    steps: 170    lr: 0.0001     reward: 2.07\n",
      "epis: 336   score: 0.0   mem len: 64037   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.05\n",
      "epis: 337   score: 1.0   mem len: 64208   epsilon: 0.7    steps: 171    lr: 0.0001     reward: 2.05\n",
      "epis: 338   score: 3.0   mem len: 64436   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 2.04\n",
      "epis: 339   score: 2.0   mem len: 64633   epsilon: 0.7    steps: 197    lr: 0.0001     reward: 2.04\n",
      "epis: 340   score: 0.0   mem len: 64756   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.03\n",
      "epis: 341   score: 2.0   mem len: 64956   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 2.03\n",
      "epis: 342   score: 1.0   mem len: 65107   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.03\n",
      "epis: 343   score: 1.0   mem len: 65258   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.02\n",
      "epis: 344   score: 0.0   mem len: 65381   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.02\n",
      "epis: 345   score: 1.0   mem len: 65532   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.99\n",
      "epis: 346   score: 3.0   mem len: 65744   epsilon: 0.7    steps: 212    lr: 0.0001     reward: 2.01\n",
      "epis: 347   score: 1.0   mem len: 65895   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.99\n",
      "epis: 348   score: 1.0   mem len: 66046   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.96\n",
      "epis: 349   score: 1.0   mem len: 66197   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.94\n",
      "epis: 350   score: 1.0   mem len: 66348   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.91\n",
      "epis: 351   score: 3.0   mem len: 66576   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 1.92\n",
      "epis: 352   score: 2.0   mem len: 66776   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.93\n",
      "epis: 353   score: 1.0   mem len: 66927   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.91\n",
      "epis: 354   score: 1.0   mem len: 67077   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.88\n",
      "epis: 355   score: 5.0   mem len: 67420   epsilon: 0.7    steps: 343    lr: 0.0001     reward: 1.92\n",
      "epis: 356   score: 0.0   mem len: 67542   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.89\n",
      "epis: 357   score: 0.0   mem len: 67664   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.88\n",
      "epis: 358   score: 1.0   mem len: 67815   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.88\n",
      "epis: 359   score: 1.0   mem len: 67984   epsilon: 0.7    steps: 169    lr: 0.0001     reward: 1.84\n",
      "epis: 360   score: 2.0   mem len: 68186   epsilon: 0.7    steps: 202    lr: 0.0001     reward: 1.82\n",
      "epis: 361   score: 3.0   mem len: 68416   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.85\n",
      "epis: 362   score: 2.0   mem len: 68615   epsilon: 0.7    steps: 199    lr: 0.0001     reward: 1.84\n",
      "epis: 363   score: 3.0   mem len: 68826   epsilon: 0.7    steps: 211    lr: 0.0001     reward: 1.84\n",
      "epis: 364   score: 1.0   mem len: 68976   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.84\n",
      "epis: 365   score: 2.0   mem len: 69176   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.82\n",
      "epis: 366   score: 0.0   mem len: 69298   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.82\n",
      "epis: 367   score: 6.0   mem len: 69688   epsilon: 0.7    steps: 390    lr: 0.0001     reward: 1.85\n",
      "epis: 368   score: 1.0   mem len: 69839   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.84\n",
      "epis: 369   score: 3.0   mem len: 70065   epsilon: 0.7    steps: 226    lr: 0.0001     reward: 1.86\n",
      "epis: 370   score: 1.0   mem len: 70216   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.86\n",
      "epis: 371   score: 3.0   mem len: 70462   epsilon: 0.7    steps: 246    lr: 0.0001     reward: 1.86\n",
      "epis: 372   score: 3.0   mem len: 70671   epsilon: 0.7    steps: 209    lr: 0.0001     reward: 1.88\n",
      "epis: 373   score: 2.0   mem len: 70851   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.87\n",
      "epis: 374   score: 4.0   mem len: 71146   epsilon: 0.7    steps: 295    lr: 0.0001     reward: 1.89\n",
      "epis: 375   score: 1.0   mem len: 71297   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.89\n",
      "epis: 376   score: 1.0   mem len: 71448   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.86\n",
      "epis: 377   score: 4.0   mem len: 71724   epsilon: 0.7    steps: 276    lr: 0.0001     reward: 1.89\n",
      "epis: 378   score: 1.0   mem len: 71875   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.85\n",
      "epis: 379   score: 1.0   mem len: 72046   epsilon: 0.7    steps: 171    lr: 0.0001     reward: 1.82\n",
      "epis: 380   score: 2.0   mem len: 72246   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.79\n",
      "epis: 381   score: 2.0   mem len: 72444   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.8\n",
      "epis: 382   score: 5.0   mem len: 72753   epsilon: 0.7    steps: 309    lr: 0.0001     reward: 1.84\n",
      "epis: 383   score: 5.0   mem len: 73081   epsilon: 0.7    steps: 328    lr: 0.0001     reward: 1.86\n",
      "epis: 384   score: 1.0   mem len: 73253   epsilon: 0.7    steps: 172    lr: 0.0001     reward: 1.86\n",
      "epis: 385   score: 1.0   mem len: 73404   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.85\n",
      "epis: 386   score: 1.0   mem len: 73555   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.81\n",
      "epis: 387   score: 2.0   mem len: 73734   epsilon: 0.7    steps: 179    lr: 0.0001     reward: 1.83\n",
      "epis: 388   score: 2.0   mem len: 73934   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.82\n",
      "epis: 389   score: 2.0   mem len: 74114   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 1.83\n",
      "epis: 390   score: 4.0   mem len: 74399   epsilon: 0.7    steps: 285    lr: 0.0001     reward: 1.85\n",
      "epis: 391   score: 2.0   mem len: 74599   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.86\n",
      "epis: 392   score: 1.0   mem len: 74750   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.84\n",
      "epis: 393   score: 1.0   mem len: 74900   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.78\n",
      "epis: 394   score: 5.0   mem len: 75194   epsilon: 0.7    steps: 294    lr: 0.0001     reward: 1.83\n",
      "epis: 395   score: 1.0   mem len: 75345   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.83\n",
      "epis: 396   score: 0.0   mem len: 75468   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.83\n",
      "epis: 397   score: 3.0   mem len: 75679   epsilon: 0.7    steps: 211    lr: 0.0001     reward: 1.83\n",
      "epis: 398   score: 1.0   mem len: 75830   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.82\n",
      "epis: 399   score: 4.0   mem len: 76130   epsilon: 0.7    steps: 300    lr: 0.0001     reward: 1.85\n",
      "epis: 400   score: 1.0   mem len: 76281   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.85\n",
      "epis: 401   score: 3.0   mem len: 76528   epsilon: 0.7    steps: 247    lr: 0.0001     reward: 1.85\n",
      "epis: 402   score: 0.0   mem len: 76650   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.8\n",
      "epis: 403   score: 2.0   mem len: 76848   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.81\n",
      "epis: 404   score: 2.0   mem len: 77029   epsilon: 0.7    steps: 181    lr: 0.0001     reward: 1.81\n",
      "epis: 405   score: 3.0   mem len: 77242   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.84\n",
      "epis: 406   score: 5.0   mem len: 77548   epsilon: 0.7    steps: 306    lr: 0.0001     reward: 1.88\n",
      "epis: 407   score: 2.0   mem len: 77767   epsilon: 0.7    steps: 219    lr: 0.0001     reward: 1.9\n",
      "epis: 408   score: 1.0   mem len: 77918   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.9\n",
      "epis: 409   score: 1.0   mem len: 78069   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.91\n",
      "epis: 410   score: 1.0   mem len: 78238   epsilon: 0.7    steps: 169    lr: 0.0001     reward: 1.91\n",
      "epis: 411   score: 2.0   mem len: 78439   epsilon: 0.7    steps: 201    lr: 0.0001     reward: 1.92\n",
      "epis: 412   score: 1.0   mem len: 78589   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.9\n",
      "epis: 413   score: 3.0   mem len: 78802   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 1.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 414   score: 1.0   mem len: 78952   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.91\n",
      "epis: 415   score: 2.0   mem len: 79134   epsilon: 0.7    steps: 182    lr: 0.0001     reward: 1.9\n",
      "epis: 416   score: 2.0   mem len: 79334   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.88\n",
      "epis: 417   score: 5.0   mem len: 79662   epsilon: 0.7    steps: 328    lr: 0.0001     reward: 1.9\n",
      "epis: 418   score: 1.0   mem len: 79832   epsilon: 0.7    steps: 170    lr: 0.0001     reward: 1.9\n",
      "epis: 419   score: 4.0   mem len: 80110   epsilon: 0.7    steps: 278    lr: 0.0001     reward: 1.92\n",
      "epis: 420   score: 1.0   mem len: 80261   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.92\n",
      "epis: 421   score: 1.0   mem len: 80433   epsilon: 0.7    steps: 172    lr: 0.0001     reward: 1.92\n",
      "epis: 422   score: 3.0   mem len: 80678   epsilon: 0.7    steps: 245    lr: 0.0001     reward: 1.95\n",
      "epis: 423   score: 2.0   mem len: 80878   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.93\n",
      "epis: 424   score: 3.0   mem len: 81108   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.95\n",
      "epis: 425   score: 4.0   mem len: 81362   epsilon: 0.7    steps: 254    lr: 0.0001     reward: 1.98\n",
      "epis: 426   score: 4.0   mem len: 81621   epsilon: 0.7    steps: 259    lr: 0.0001     reward: 2.01\n",
      "epis: 427   score: 2.0   mem len: 81819   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 2.02\n",
      "epis: 428   score: 6.0   mem len: 82198   epsilon: 0.7    steps: 379    lr: 0.0001     reward: 2.05\n",
      "epis: 429   score: 3.0   mem len: 82442   epsilon: 0.7    steps: 244    lr: 0.0001     reward: 2.04\n",
      "epis: 430   score: 1.0   mem len: 82593   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.03\n",
      "epis: 431   score: 3.0   mem len: 82804   epsilon: 0.7    steps: 211    lr: 0.0001     reward: 2.03\n",
      "epis: 432   score: 3.0   mem len: 83032   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 2.06\n",
      "epis: 433   score: 4.0   mem len: 83289   epsilon: 0.7    steps: 257    lr: 0.0001     reward: 2.09\n",
      "epis: 434   score: 2.0   mem len: 83489   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 2.06\n",
      "epis: 435   score: 2.0   mem len: 83687   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 2.07\n",
      "epis: 436   score: 0.0   mem len: 83810   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.07\n",
      "epis: 437   score: 2.0   mem len: 84012   epsilon: 0.7    steps: 202    lr: 0.0001     reward: 2.08\n",
      "epis: 438   score: 1.0   mem len: 84163   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.06\n",
      "epis: 439   score: 1.0   mem len: 84314   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.05\n",
      "epis: 440   score: 1.0   mem len: 84464   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.06\n",
      "epis: 441   score: 2.0   mem len: 84663   epsilon: 0.7    steps: 199    lr: 0.0001     reward: 2.06\n",
      "epis: 442   score: 1.0   mem len: 84814   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.06\n",
      "epis: 443   score: 2.0   mem len: 85014   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 2.07\n",
      "epis: 444   score: 1.0   mem len: 85165   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.08\n",
      "epis: 445   score: 1.0   mem len: 85316   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.08\n",
      "epis: 446   score: 5.0   mem len: 85625   epsilon: 0.7    steps: 309    lr: 0.0001     reward: 2.1\n",
      "epis: 447   score: 1.0   mem len: 85776   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.1\n",
      "epis: 448   score: 3.0   mem len: 86004   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 2.12\n",
      "epis: 449   score: 2.0   mem len: 86184   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 2.13\n",
      "epis: 450   score: 2.0   mem len: 86405   epsilon: 0.7    steps: 221    lr: 0.0001     reward: 2.14\n",
      "epis: 451   score: 3.0   mem len: 86631   epsilon: 0.7    steps: 226    lr: 0.0001     reward: 2.14\n",
      "epis: 452   score: 1.0   mem len: 86782   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.13\n",
      "epis: 453   score: 0.0   mem len: 86905   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.12\n",
      "epis: 454   score: 2.0   mem len: 87105   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 2.13\n",
      "epis: 455   score: 3.0   mem len: 87369   epsilon: 0.7    steps: 264    lr: 0.0001     reward: 2.11\n",
      "epis: 456   score: 6.0   mem len: 87754   epsilon: 0.7    steps: 385    lr: 0.0001     reward: 2.17\n",
      "epis: 457   score: 3.0   mem len: 87980   epsilon: 0.7    steps: 226    lr: 0.0001     reward: 2.2\n",
      "epis: 458   score: 1.0   mem len: 88131   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.2\n",
      "epis: 459   score: 1.0   mem len: 88282   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.2\n",
      "epis: 460   score: 2.0   mem len: 88500   epsilon: 0.7    steps: 218    lr: 0.0001     reward: 2.2\n",
      "epis: 461   score: 1.0   mem len: 88651   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.18\n",
      "epis: 462   score: 1.0   mem len: 88801   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.17\n",
      "epis: 463   score: 3.0   mem len: 89051   epsilon: 0.7    steps: 250    lr: 0.0001     reward: 2.17\n",
      "epis: 464   score: 4.0   mem len: 89337   epsilon: 0.7    steps: 286    lr: 0.0001     reward: 2.2\n",
      "epis: 465   score: 1.0   mem len: 89488   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.19\n",
      "epis: 466   score: 1.0   mem len: 89639   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.2\n",
      "epis: 467   score: 2.0   mem len: 89821   epsilon: 0.7    steps: 182    lr: 0.0001     reward: 2.16\n",
      "epis: 468   score: 3.0   mem len: 90034   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 2.18\n",
      "epis: 469   score: 0.0   mem len: 90157   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.15\n",
      "epis: 470   score: 2.0   mem len: 90338   epsilon: 0.7    steps: 181    lr: 0.0001     reward: 2.16\n",
      "epis: 471   score: 3.0   mem len: 90550   epsilon: 0.7    steps: 212    lr: 0.0001     reward: 2.16\n",
      "epis: 472   score: 3.0   mem len: 90800   epsilon: 0.7    steps: 250    lr: 0.0001     reward: 2.16\n",
      "epis: 473   score: 2.0   mem len: 91023   epsilon: 0.7    steps: 223    lr: 0.0001     reward: 2.16\n",
      "epis: 474   score: 1.0   mem len: 91174   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.13\n",
      "epis: 475   score: 3.0   mem len: 91387   epsilon: 0.7    steps: 213    lr: 0.0001     reward: 2.15\n",
      "epis: 476   score: 0.0   mem len: 91509   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 2.14\n",
      "epis: 477   score: 3.0   mem len: 91721   epsilon: 0.7    steps: 212    lr: 0.0001     reward: 2.13\n",
      "epis: 478   score: 0.0   mem len: 91844   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 2.12\n",
      "epis: 479   score: 0.0   mem len: 91966   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 2.11\n",
      "epis: 480   score: 2.0   mem len: 92165   epsilon: 0.7    steps: 199    lr: 0.0001     reward: 2.11\n",
      "epis: 481   score: 2.0   mem len: 92345   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 2.11\n",
      "epis: 482   score: 1.0   mem len: 92496   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.07\n",
      "epis: 483   score: 1.0   mem len: 92647   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.03\n",
      "epis: 484   score: 1.0   mem len: 92798   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.03\n",
      "epis: 485   score: 1.0   mem len: 92949   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.03\n",
      "epis: 486   score: 1.0   mem len: 93100   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.03\n",
      "epis: 487   score: 1.0   mem len: 93270   epsilon: 0.7    steps: 170    lr: 0.0001     reward: 2.02\n",
      "epis: 488   score: 1.0   mem len: 93438   epsilon: 0.7    steps: 168    lr: 0.0001     reward: 2.01\n",
      "epis: 489   score: 0.0   mem len: 93560   epsilon: 0.7    steps: 122    lr: 0.0001     reward: 1.99\n",
      "epis: 490   score: 6.0   mem len: 93920   epsilon: 0.7    steps: 360    lr: 0.0001     reward: 2.01\n",
      "epis: 491   score: 3.0   mem len: 94169   epsilon: 0.7    steps: 249    lr: 0.0001     reward: 2.02\n",
      "epis: 492   score: 1.0   mem len: 94320   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.02\n",
      "epis: 493   score: 2.0   mem len: 94518   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 2.03\n",
      "epis: 494   score: 2.0   mem len: 94698   epsilon: 0.7    steps: 180    lr: 0.0001     reward: 2.0\n",
      "epis: 495   score: 4.0   mem len: 94994   epsilon: 0.7    steps: 296    lr: 0.0001     reward: 2.03\n",
      "epis: 496   score: 1.0   mem len: 95144   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 497   score: 2.0   mem len: 95344   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 2.03\n",
      "epis: 498   score: 1.0   mem len: 95494   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 2.03\n",
      "epis: 499   score: 1.0   mem len: 95645   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.0\n",
      "epis: 500   score: 3.0   mem len: 95855   epsilon: 0.7    steps: 210    lr: 0.0001     reward: 2.02\n",
      "epis: 501   score: 2.0   mem len: 96036   epsilon: 0.7    steps: 181    lr: 0.0001     reward: 2.01\n",
      "epis: 502   score: 1.0   mem len: 96207   epsilon: 0.7    steps: 171    lr: 0.0001     reward: 2.02\n",
      "epis: 503   score: 1.0   mem len: 96358   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.01\n",
      "epis: 504   score: 1.0   mem len: 96509   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 2.0\n",
      "epis: 505   score: 1.0   mem len: 96660   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.98\n",
      "epis: 506   score: 1.0   mem len: 96829   epsilon: 0.7    steps: 169    lr: 0.0001     reward: 1.94\n",
      "epis: 507   score: 1.0   mem len: 96980   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.93\n",
      "epis: 508   score: 3.0   mem len: 97206   epsilon: 0.7    steps: 226    lr: 0.0001     reward: 1.95\n",
      "epis: 509   score: 3.0   mem len: 97456   epsilon: 0.7    steps: 250    lr: 0.0001     reward: 1.97\n",
      "epis: 510   score: 3.0   mem len: 97686   epsilon: 0.7    steps: 230    lr: 0.0001     reward: 1.99\n",
      "epis: 511   score: 1.0   mem len: 97837   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.98\n",
      "epis: 512   score: 2.0   mem len: 98037   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.99\n",
      "epis: 513   score: 1.0   mem len: 98188   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.97\n",
      "epis: 514   score: 2.0   mem len: 98386   epsilon: 0.7    steps: 198    lr: 0.0001     reward: 1.98\n",
      "epis: 515   score: 1.0   mem len: 98537   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.97\n",
      "epis: 516   score: 3.0   mem len: 98765   epsilon: 0.7    steps: 228    lr: 0.0001     reward: 1.98\n",
      "epis: 517   score: 1.0   mem len: 98916   epsilon: 0.7    steps: 151    lr: 0.0001     reward: 1.94\n",
      "epis: 518   score: 1.0   mem len: 99066   epsilon: 0.7    steps: 150    lr: 0.0001     reward: 1.94\n",
      "epis: 519   score: 2.0   mem len: 99266   epsilon: 0.7    steps: 200    lr: 0.0001     reward: 1.92\n",
      "epis: 520   score: 3.0   mem len: 99492   epsilon: 0.7    steps: 226    lr: 0.0001     reward: 1.94\n",
      "epis: 521   score: 4.0   mem len: 99772   epsilon: 0.7    steps: 280    lr: 0.0001     reward: 1.97\n",
      "epis: 522   score: 0.0   mem len: 99895   epsilon: 0.7    steps: 123    lr: 0.0001     reward: 1.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 523   score: 1.0   mem len: 100046   epsilon: 0.6999    steps: 151    lr: 0.0001     reward: 1.93\n",
      "epis: 524   score: 2.0   mem len: 100267   epsilon: 0.6996    steps: 221    lr: 0.0001     reward: 1.92\n",
      "epis: 525   score: 0.0   mem len: 100390   epsilon: 0.6995    steps: 123    lr: 0.0001     reward: 1.88\n",
      "epis: 526   score: 3.0   mem len: 100616   epsilon: 0.6991    steps: 226    lr: 0.0001     reward: 1.87\n",
      "epis: 527   score: 0.0   mem len: 100738   epsilon: 0.699    steps: 122    lr: 0.0001     reward: 1.85\n",
      "epis: 528   score: 0.0   mem len: 100860   epsilon: 0.6988    steps: 122    lr: 0.0001     reward: 1.79\n",
      "epis: 529   score: 1.0   mem len: 101030   epsilon: 0.6986    steps: 170    lr: 0.0001     reward: 1.77\n",
      "epis: 530   score: 1.0   mem len: 101181   epsilon: 0.6984    steps: 151    lr: 0.0001     reward: 1.77\n",
      "epis: 531   score: 0.0   mem len: 101304   epsilon: 0.6982    steps: 123    lr: 0.0001     reward: 1.74\n",
      "epis: 532   score: 2.0   mem len: 101501   epsilon: 0.6979    steps: 197    lr: 0.0001     reward: 1.73\n",
      "epis: 533   score: 3.0   mem len: 101747   epsilon: 0.6976    steps: 246    lr: 0.0001     reward: 1.72\n",
      "epis: 534   score: 0.0   mem len: 101869   epsilon: 0.6974    steps: 122    lr: 0.0001     reward: 1.7\n",
      "epis: 535   score: 0.0   mem len: 101992   epsilon: 0.6972    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 536   score: 1.0   mem len: 102161   epsilon: 0.697    steps: 169    lr: 0.0001     reward: 1.69\n",
      "epis: 537   score: 1.0   mem len: 102330   epsilon: 0.6968    steps: 169    lr: 0.0001     reward: 1.68\n",
      "epis: 538   score: 2.0   mem len: 102529   epsilon: 0.6965    steps: 199    lr: 0.0001     reward: 1.69\n",
      "epis: 539   score: 2.0   mem len: 102728   epsilon: 0.6962    steps: 199    lr: 0.0001     reward: 1.7\n",
      "epis: 540   score: 0.0   mem len: 102851   epsilon: 0.6961    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 541   score: 2.0   mem len: 103050   epsilon: 0.6958    steps: 199    lr: 0.0001     reward: 1.69\n",
      "epis: 542   score: 2.0   mem len: 103248   epsilon: 0.6955    steps: 198    lr: 0.0001     reward: 1.7\n",
      "epis: 543   score: 0.0   mem len: 103371   epsilon: 0.6953    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 544   score: 1.0   mem len: 103540   epsilon: 0.6951    steps: 169    lr: 0.0001     reward: 1.68\n",
      "epis: 545   score: 0.0   mem len: 103663   epsilon: 0.6949    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 546   score: 3.0   mem len: 103889   epsilon: 0.6946    steps: 226    lr: 0.0001     reward: 1.65\n",
      "epis: 547   score: 2.0   mem len: 104089   epsilon: 0.6944    steps: 200    lr: 0.0001     reward: 1.66\n",
      "epis: 548   score: 3.0   mem len: 104332   epsilon: 0.694    steps: 243    lr: 0.0001     reward: 1.66\n",
      "epis: 549   score: 1.0   mem len: 104501   epsilon: 0.6938    steps: 169    lr: 0.0001     reward: 1.65\n",
      "epis: 550   score: 0.0   mem len: 104624   epsilon: 0.6936    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 551   score: 3.0   mem len: 104850   epsilon: 0.6933    steps: 226    lr: 0.0001     reward: 1.63\n",
      "epis: 552   score: 0.0   mem len: 104973   epsilon: 0.6931    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 553   score: 1.0   mem len: 105123   epsilon: 0.6929    steps: 150    lr: 0.0001     reward: 1.63\n",
      "epis: 554   score: 1.0   mem len: 105291   epsilon: 0.6927    steps: 168    lr: 0.0001     reward: 1.62\n",
      "epis: 555   score: 5.0   mem len: 105597   epsilon: 0.6923    steps: 306    lr: 0.0001     reward: 1.64\n",
      "epis: 556   score: 2.0   mem len: 105795   epsilon: 0.692    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 557   score: 4.0   mem len: 106108   epsilon: 0.6916    steps: 313    lr: 0.0001     reward: 1.61\n",
      "epis: 558   score: 0.0   mem len: 106230   epsilon: 0.6914    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 559   score: 2.0   mem len: 106413   epsilon: 0.6911    steps: 183    lr: 0.0001     reward: 1.61\n",
      "epis: 560   score: 1.0   mem len: 106584   epsilon: 0.6909    steps: 171    lr: 0.0001     reward: 1.6\n",
      "epis: 561   score: 1.0   mem len: 106735   epsilon: 0.6907    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 562   score: 2.0   mem len: 106933   epsilon: 0.6904    steps: 198    lr: 0.0001     reward: 1.61\n",
      "epis: 563   score: 1.0   mem len: 107103   epsilon: 0.6902    steps: 170    lr: 0.0001     reward: 1.59\n",
      "epis: 564   score: 3.0   mem len: 107330   epsilon: 0.6899    steps: 227    lr: 0.0001     reward: 1.58\n",
      "epis: 565   score: 0.0   mem len: 107453   epsilon: 0.6897    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 566   score: 3.0   mem len: 107699   epsilon: 0.6894    steps: 246    lr: 0.0001     reward: 1.59\n",
      "epis: 567   score: 3.0   mem len: 107966   epsilon: 0.689    steps: 267    lr: 0.0001     reward: 1.6\n",
      "epis: 568   score: 1.0   mem len: 108118   epsilon: 0.6888    steps: 152    lr: 0.0001     reward: 1.58\n",
      "epis: 569   score: 0.0   mem len: 108241   epsilon: 0.6886    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 570   score: 3.0   mem len: 108488   epsilon: 0.6883    steps: 247    lr: 0.0001     reward: 1.59\n",
      "epis: 571   score: 1.0   mem len: 108658   epsilon: 0.6881    steps: 170    lr: 0.0001     reward: 1.57\n",
      "epis: 572   score: 0.0   mem len: 108781   epsilon: 0.6879    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 573   score: 2.0   mem len: 108979   epsilon: 0.6876    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 574   score: 4.0   mem len: 109272   epsilon: 0.6872    steps: 293    lr: 0.0001     reward: 1.57\n",
      "epis: 575   score: 1.0   mem len: 109442   epsilon: 0.687    steps: 170    lr: 0.0001     reward: 1.55\n",
      "epis: 576   score: 0.0   mem len: 109565   epsilon: 0.6868    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 577   score: 0.0   mem len: 109687   epsilon: 0.6866    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 578   score: 4.0   mem len: 109956   epsilon: 0.6863    steps: 269    lr: 0.0001     reward: 1.56\n",
      "epis: 579   score: 3.0   mem len: 110203   epsilon: 0.6859    steps: 247    lr: 0.0001     reward: 1.59\n",
      "epis: 580   score: 2.0   mem len: 110400   epsilon: 0.6856    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 581   score: 2.0   mem len: 110616   epsilon: 0.6853    steps: 216    lr: 0.0001     reward: 1.59\n",
      "epis: 582   score: 1.0   mem len: 110785   epsilon: 0.6851    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 583   score: 1.0   mem len: 110954   epsilon: 0.6849    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 584   score: 4.0   mem len: 111249   epsilon: 0.6845    steps: 295    lr: 0.0001     reward: 1.62\n",
      "epis: 585   score: 4.0   mem len: 111543   epsilon: 0.6841    steps: 294    lr: 0.0001     reward: 1.65\n",
      "epis: 586   score: 4.0   mem len: 111858   epsilon: 0.6836    steps: 315    lr: 0.0001     reward: 1.68\n",
      "epis: 587   score: 3.0   mem len: 112104   epsilon: 0.6833    steps: 246    lr: 0.0001     reward: 1.7\n",
      "epis: 588   score: 4.0   mem len: 112420   epsilon: 0.6829    steps: 316    lr: 0.0001     reward: 1.73\n",
      "epis: 589   score: 2.0   mem len: 112618   epsilon: 0.6826    steps: 198    lr: 0.0001     reward: 1.75\n",
      "epis: 590   score: 2.0   mem len: 112816   epsilon: 0.6823    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 591   score: 2.0   mem len: 113013   epsilon: 0.682    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 592   score: 2.0   mem len: 113213   epsilon: 0.6818    steps: 200    lr: 0.0001     reward: 1.71\n",
      "epis: 593   score: 2.0   mem len: 113410   epsilon: 0.6815    steps: 197    lr: 0.0001     reward: 1.71\n",
      "epis: 594   score: 1.0   mem len: 113579   epsilon: 0.6813    steps: 169    lr: 0.0001     reward: 1.7\n",
      "epis: 595   score: 0.0   mem len: 113702   epsilon: 0.6811    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 596   score: 0.0   mem len: 113825   epsilon: 0.6809    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 597   score: 1.0   mem len: 113976   epsilon: 0.6807    steps: 151    lr: 0.0001     reward: 1.64\n",
      "epis: 598   score: 0.0   mem len: 114099   epsilon: 0.6805    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 599   score: 2.0   mem len: 114318   epsilon: 0.6802    steps: 219    lr: 0.0001     reward: 1.64\n",
      "epis: 600   score: 2.0   mem len: 114498   epsilon: 0.68    steps: 180    lr: 0.0001     reward: 1.63\n",
      "epis: 601   score: 2.0   mem len: 114696   epsilon: 0.6797    steps: 198    lr: 0.0001     reward: 1.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 602   score: 2.0   mem len: 114894   epsilon: 0.6794    steps: 198    lr: 0.0001     reward: 1.64\n",
      "epis: 603   score: 0.0   mem len: 115017   epsilon: 0.6793    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 604   score: 2.0   mem len: 115233   epsilon: 0.679    steps: 216    lr: 0.0001     reward: 1.64\n",
      "epis: 605   score: 0.0   mem len: 115356   epsilon: 0.6788    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 606   score: 0.0   mem len: 115479   epsilon: 0.6786    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 607   score: 2.0   mem len: 115701   epsilon: 0.6783    steps: 222    lr: 0.0001     reward: 1.63\n",
      "epis: 608   score: 1.0   mem len: 115872   epsilon: 0.6781    steps: 171    lr: 0.0001     reward: 1.61\n",
      "epis: 609   score: 1.0   mem len: 116040   epsilon: 0.6779    steps: 168    lr: 0.0001     reward: 1.59\n",
      "epis: 610   score: 1.0   mem len: 116209   epsilon: 0.6776    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 611   score: 3.0   mem len: 116435   epsilon: 0.6773    steps: 226    lr: 0.0001     reward: 1.59\n",
      "epis: 612   score: 0.0   mem len: 116557   epsilon: 0.6771    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 613   score: 1.0   mem len: 116726   epsilon: 0.6769    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 614   score: 0.0   mem len: 116848   epsilon: 0.6767    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 615   score: 2.0   mem len: 117065   epsilon: 0.6764    steps: 217    lr: 0.0001     reward: 1.56\n",
      "epis: 616   score: 2.0   mem len: 117263   epsilon: 0.6762    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 617   score: 1.0   mem len: 117413   epsilon: 0.676    steps: 150    lr: 0.0001     reward: 1.55\n",
      "epis: 618   score: 1.0   mem len: 117564   epsilon: 0.6758    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 619   score: 4.0   mem len: 117858   epsilon: 0.6754    steps: 294    lr: 0.0001     reward: 1.57\n",
      "epis: 620   score: 2.0   mem len: 118055   epsilon: 0.6751    steps: 197    lr: 0.0001     reward: 1.56\n",
      "epis: 621   score: 2.0   mem len: 118252   epsilon: 0.6748    steps: 197    lr: 0.0001     reward: 1.54\n",
      "epis: 622   score: 0.0   mem len: 118374   epsilon: 0.6746    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 623   score: 0.0   mem len: 118496   epsilon: 0.6745    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 624   score: 0.0   mem len: 118618   epsilon: 0.6743    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 625   score: 3.0   mem len: 118866   epsilon: 0.674    steps: 248    lr: 0.0001     reward: 1.54\n",
      "epis: 626   score: 2.0   mem len: 119082   epsilon: 0.6737    steps: 216    lr: 0.0001     reward: 1.53\n",
      "epis: 627   score: 1.0   mem len: 119251   epsilon: 0.6734    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 628   score: 1.0   mem len: 119422   epsilon: 0.6732    steps: 171    lr: 0.0001     reward: 1.55\n",
      "epis: 629   score: 1.0   mem len: 119573   epsilon: 0.673    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 630   score: 1.0   mem len: 119743   epsilon: 0.6728    steps: 170    lr: 0.0001     reward: 1.55\n",
      "epis: 631   score: 3.0   mem len: 119976   epsilon: 0.6724    steps: 233    lr: 0.0001     reward: 1.58\n",
      "epis: 632   score: 1.0   mem len: 120145   epsilon: 0.6722    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 633   score: 4.0   mem len: 120438   epsilon: 0.6718    steps: 293    lr: 0.0001     reward: 1.58\n",
      "epis: 634   score: 2.0   mem len: 120620   epsilon: 0.6715    steps: 182    lr: 0.0001     reward: 1.6\n",
      "epis: 635   score: 0.0   mem len: 120742   epsilon: 0.6714    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 636   score: 2.0   mem len: 120964   epsilon: 0.6711    steps: 222    lr: 0.0001     reward: 1.61\n",
      "epis: 637   score: 0.0   mem len: 121087   epsilon: 0.6709    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 638   score: 1.0   mem len: 121238   epsilon: 0.6707    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 639   score: 2.0   mem len: 121459   epsilon: 0.6704    steps: 221    lr: 0.0001     reward: 1.59\n",
      "epis: 640   score: 2.0   mem len: 121656   epsilon: 0.6701    steps: 197    lr: 0.0001     reward: 1.61\n",
      "epis: 641   score: 3.0   mem len: 121902   epsilon: 0.6698    steps: 246    lr: 0.0001     reward: 1.62\n",
      "epis: 642   score: 2.0   mem len: 122099   epsilon: 0.6695    steps: 197    lr: 0.0001     reward: 1.62\n",
      "epis: 643   score: 2.0   mem len: 122297   epsilon: 0.6692    steps: 198    lr: 0.0001     reward: 1.64\n",
      "epis: 644   score: 1.0   mem len: 122448   epsilon: 0.669    steps: 151    lr: 0.0001     reward: 1.64\n",
      "epis: 645   score: 3.0   mem len: 122697   epsilon: 0.6687    steps: 249    lr: 0.0001     reward: 1.67\n",
      "epis: 646   score: 0.0   mem len: 122820   epsilon: 0.6685    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 647   score: 1.0   mem len: 122992   epsilon: 0.6683    steps: 172    lr: 0.0001     reward: 1.63\n",
      "epis: 648   score: 6.0   mem len: 123349   epsilon: 0.6678    steps: 357    lr: 0.0001     reward: 1.66\n",
      "epis: 649   score: 2.0   mem len: 123566   epsilon: 0.6675    steps: 217    lr: 0.0001     reward: 1.67\n",
      "epis: 650   score: 1.0   mem len: 123717   epsilon: 0.6673    steps: 151    lr: 0.0001     reward: 1.68\n",
      "epis: 651   score: 0.0   mem len: 123840   epsilon: 0.6671    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 652   score: 1.0   mem len: 124010   epsilon: 0.6669    steps: 170    lr: 0.0001     reward: 1.66\n",
      "epis: 653   score: 1.0   mem len: 124178   epsilon: 0.6666    steps: 168    lr: 0.0001     reward: 1.66\n",
      "epis: 654   score: 2.0   mem len: 124376   epsilon: 0.6664    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 655   score: 0.0   mem len: 124498   epsilon: 0.6662    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 656   score: 2.0   mem len: 124715   epsilon: 0.6659    steps: 217    lr: 0.0001     reward: 1.62\n",
      "epis: 657   score: 0.0   mem len: 124838   epsilon: 0.6657    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 658   score: 1.0   mem len: 125007   epsilon: 0.6655    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 659   score: 0.0   mem len: 125130   epsilon: 0.6653    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 660   score: 1.0   mem len: 125281   epsilon: 0.6651    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 661   score: 3.0   mem len: 125524   epsilon: 0.6648    steps: 243    lr: 0.0001     reward: 1.59\n",
      "epis: 662   score: 4.0   mem len: 125803   epsilon: 0.6644    steps: 279    lr: 0.0001     reward: 1.61\n",
      "epis: 663   score: 1.0   mem len: 125972   epsilon: 0.6642    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 664   score: 2.0   mem len: 126170   epsilon: 0.6639    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 665   score: 3.0   mem len: 126435   epsilon: 0.6635    steps: 265    lr: 0.0001     reward: 1.63\n",
      "epis: 666   score: 1.0   mem len: 126606   epsilon: 0.6633    steps: 171    lr: 0.0001     reward: 1.61\n",
      "epis: 667   score: 0.0   mem len: 126729   epsilon: 0.6631    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 668   score: 4.0   mem len: 127021   epsilon: 0.6627    steps: 292    lr: 0.0001     reward: 1.61\n",
      "epis: 669   score: 2.0   mem len: 127202   epsilon: 0.6625    steps: 181    lr: 0.0001     reward: 1.63\n",
      "epis: 670   score: 0.0   mem len: 127324   epsilon: 0.6623    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 671   score: 2.0   mem len: 127524   epsilon: 0.662    steps: 200    lr: 0.0001     reward: 1.61\n",
      "epis: 672   score: 0.0   mem len: 127646   epsilon: 0.6618    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 673   score: 3.0   mem len: 127912   epsilon: 0.6615    steps: 266    lr: 0.0001     reward: 1.62\n",
      "epis: 674   score: 3.0   mem len: 128139   epsilon: 0.6612    steps: 227    lr: 0.0001     reward: 1.61\n",
      "epis: 675   score: 0.0   mem len: 128261   epsilon: 0.661    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 676   score: 2.0   mem len: 128459   epsilon: 0.6607    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 677   score: 0.0   mem len: 128582   epsilon: 0.6606    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 678   score: 1.0   mem len: 128732   epsilon: 0.6603    steps: 150    lr: 0.0001     reward: 1.59\n",
      "epis: 679   score: 1.0   mem len: 128903   epsilon: 0.6601    steps: 171    lr: 0.0001     reward: 1.57\n",
      "epis: 680   score: 0.0   mem len: 129025   epsilon: 0.6599    steps: 122    lr: 0.0001     reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 681   score: 0.0   mem len: 129148   epsilon: 0.6598    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 682   score: 1.0   mem len: 129299   epsilon: 0.6596    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 683   score: 1.0   mem len: 129450   epsilon: 0.6594    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 684   score: 2.0   mem len: 129649   epsilon: 0.6591    steps: 199    lr: 0.0001     reward: 1.51\n",
      "epis: 685   score: 2.0   mem len: 129847   epsilon: 0.6588    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 686   score: 1.0   mem len: 129998   epsilon: 0.6586    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 687   score: 0.0   mem len: 130120   epsilon: 0.6584    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 688   score: 0.0   mem len: 130242   epsilon: 0.6583    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 689   score: 0.0   mem len: 130365   epsilon: 0.6581    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 690   score: 0.0   mem len: 130487   epsilon: 0.6579    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 691   score: 4.0   mem len: 130783   epsilon: 0.6575    steps: 296    lr: 0.0001     reward: 1.37\n",
      "epis: 692   score: 1.0   mem len: 130955   epsilon: 0.6573    steps: 172    lr: 0.0001     reward: 1.36\n",
      "epis: 693   score: 1.0   mem len: 131123   epsilon: 0.657    steps: 168    lr: 0.0001     reward: 1.35\n",
      "epis: 694   score: 3.0   mem len: 131391   epsilon: 0.6567    steps: 268    lr: 0.0001     reward: 1.37\n",
      "epis: 695   score: 0.0   mem len: 131514   epsilon: 0.6565    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 696   score: 0.0   mem len: 131637   epsilon: 0.6563    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 697   score: 1.0   mem len: 131789   epsilon: 0.6561    steps: 152    lr: 0.0001     reward: 1.37\n",
      "epis: 698   score: 2.0   mem len: 131987   epsilon: 0.6559    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 699   score: 0.0   mem len: 132109   epsilon: 0.6557    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 700   score: 0.0   mem len: 132231   epsilon: 0.6555    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 701   score: 4.0   mem len: 132544   epsilon: 0.6551    steps: 313    lr: 0.0001     reward: 1.37\n",
      "epis: 702   score: 0.0   mem len: 132667   epsilon: 0.6549    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 703   score: 1.0   mem len: 132817   epsilon: 0.6547    steps: 150    lr: 0.0001     reward: 1.36\n",
      "epis: 704   score: 2.0   mem len: 133015   epsilon: 0.6544    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 705   score: 0.0   mem len: 133138   epsilon: 0.6543    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 706   score: 2.0   mem len: 133353   epsilon: 0.654    steps: 215    lr: 0.0001     reward: 1.38\n",
      "epis: 707   score: 0.0   mem len: 133475   epsilon: 0.6538    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 708   score: 2.0   mem len: 133673   epsilon: 0.6535    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 709   score: 0.0   mem len: 133795   epsilon: 0.6534    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 710   score: 0.0   mem len: 133918   epsilon: 0.6532    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 711   score: 2.0   mem len: 134136   epsilon: 0.6529    steps: 218    lr: 0.0001     reward: 1.34\n",
      "epis: 712   score: 1.0   mem len: 134305   epsilon: 0.6527    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 713   score: 0.0   mem len: 134428   epsilon: 0.6525    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 714   score: 0.0   mem len: 134551   epsilon: 0.6523    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 715   score: 1.0   mem len: 134720   epsilon: 0.6521    steps: 169    lr: 0.0001     reward: 1.33\n",
      "epis: 716   score: 0.0   mem len: 134842   epsilon: 0.6519    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 717   score: 1.0   mem len: 135011   epsilon: 0.6517    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 718   score: 0.0   mem len: 135133   epsilon: 0.6515    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 719   score: 6.0   mem len: 135497   epsilon: 0.651    steps: 364    lr: 0.0001     reward: 1.32\n",
      "epis: 720   score: 2.0   mem len: 135714   epsilon: 0.6507    steps: 217    lr: 0.0001     reward: 1.32\n",
      "epis: 721   score: 2.0   mem len: 135932   epsilon: 0.6504    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 722   score: 0.0   mem len: 136054   epsilon: 0.6502    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 723   score: 1.0   mem len: 136205   epsilon: 0.65    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 724   score: 0.0   mem len: 136327   epsilon: 0.6499    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 725   score: 0.0   mem len: 136449   epsilon: 0.6497    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 726   score: 4.0   mem len: 136729   epsilon: 0.6493    steps: 280    lr: 0.0001     reward: 1.32\n",
      "epis: 727   score: 2.0   mem len: 136946   epsilon: 0.649    steps: 217    lr: 0.0001     reward: 1.33\n",
      "epis: 728   score: 0.0   mem len: 137069   epsilon: 0.6488    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 729   score: 4.0   mem len: 137363   epsilon: 0.6484    steps: 294    lr: 0.0001     reward: 1.35\n",
      "epis: 730   score: 1.0   mem len: 137533   epsilon: 0.6482    steps: 170    lr: 0.0001     reward: 1.35\n",
      "epis: 731   score: 2.0   mem len: 137751   epsilon: 0.6479    steps: 218    lr: 0.0001     reward: 1.34\n",
      "epis: 732   score: 2.0   mem len: 137966   epsilon: 0.6476    steps: 215    lr: 0.0001     reward: 1.35\n",
      "epis: 733   score: 5.0   mem len: 138310   epsilon: 0.6471    steps: 344    lr: 0.0001     reward: 1.36\n",
      "epis: 734   score: 2.0   mem len: 138528   epsilon: 0.6468    steps: 218    lr: 0.0001     reward: 1.36\n",
      "epis: 735   score: 1.0   mem len: 138681   epsilon: 0.6466    steps: 153    lr: 0.0001     reward: 1.37\n",
      "epis: 736   score: 2.0   mem len: 138879   epsilon: 0.6463    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 737   score: 2.0   mem len: 139077   epsilon: 0.6461    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 738   score: 2.0   mem len: 139297   epsilon: 0.6458    steps: 220    lr: 0.0001     reward: 1.4\n",
      "epis: 739   score: 2.0   mem len: 139495   epsilon: 0.6455    steps: 198    lr: 0.0001     reward: 1.4\n",
      "epis: 740   score: 2.0   mem len: 139695   epsilon: 0.6452    steps: 200    lr: 0.0001     reward: 1.4\n",
      "epis: 741   score: 2.0   mem len: 139893   epsilon: 0.6449    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 742   score: 2.0   mem len: 140091   epsilon: 0.6447    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 743   score: 1.0   mem len: 140262   epsilon: 0.6444    steps: 171    lr: 0.0001     reward: 1.38\n",
      "epis: 744   score: 5.0   mem len: 140609   epsilon: 0.644    steps: 347    lr: 0.0001     reward: 1.42\n",
      "epis: 745   score: 2.0   mem len: 140830   epsilon: 0.6437    steps: 221    lr: 0.0001     reward: 1.41\n",
      "epis: 746   score: 2.0   mem len: 141029   epsilon: 0.6434    steps: 199    lr: 0.0001     reward: 1.43\n",
      "epis: 747   score: 3.0   mem len: 141254   epsilon: 0.6431    steps: 225    lr: 0.0001     reward: 1.45\n",
      "epis: 748   score: 3.0   mem len: 141501   epsilon: 0.6427    steps: 247    lr: 0.0001     reward: 1.42\n",
      "epis: 749   score: 0.0   mem len: 141624   epsilon: 0.6426    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 750   score: 0.0   mem len: 141747   epsilon: 0.6424    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 751   score: 2.0   mem len: 141927   epsilon: 0.6421    steps: 180    lr: 0.0001     reward: 1.41\n",
      "epis: 752   score: 3.0   mem len: 142171   epsilon: 0.6418    steps: 244    lr: 0.0001     reward: 1.43\n",
      "epis: 753   score: 1.0   mem len: 142322   epsilon: 0.6416    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 754   score: 1.0   mem len: 142473   epsilon: 0.6414    steps: 151    lr: 0.0001     reward: 1.42\n",
      "epis: 755   score: 1.0   mem len: 142642   epsilon: 0.6412    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 756   score: 2.0   mem len: 142840   epsilon: 0.6409    steps: 198    lr: 0.0001     reward: 1.43\n",
      "epis: 757   score: 1.0   mem len: 143009   epsilon: 0.6406    steps: 169    lr: 0.0001     reward: 1.44\n",
      "epis: 758   score: 4.0   mem len: 143302   epsilon: 0.6402    steps: 293    lr: 0.0001     reward: 1.47\n",
      "epis: 759   score: 1.0   mem len: 143453   epsilon: 0.64    steps: 151    lr: 0.0001     reward: 1.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 760   score: 0.0   mem len: 143576   epsilon: 0.6399    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 761   score: 0.0   mem len: 143699   epsilon: 0.6397    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 762   score: 0.0   mem len: 143822   epsilon: 0.6395    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 763   score: 0.0   mem len: 143945   epsilon: 0.6394    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 764   score: 0.0   mem len: 144068   epsilon: 0.6392    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 765   score: 1.0   mem len: 144238   epsilon: 0.639    steps: 170    lr: 0.0001     reward: 1.35\n",
      "epis: 766   score: 1.0   mem len: 144407   epsilon: 0.6387    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 767   score: 2.0   mem len: 144606   epsilon: 0.6384    steps: 199    lr: 0.0001     reward: 1.37\n",
      "epis: 768   score: 4.0   mem len: 144884   epsilon: 0.6381    steps: 278    lr: 0.0001     reward: 1.37\n",
      "epis: 769   score: 3.0   mem len: 145113   epsilon: 0.6377    steps: 229    lr: 0.0001     reward: 1.38\n",
      "epis: 770   score: 5.0   mem len: 145479   epsilon: 0.6372    steps: 366    lr: 0.0001     reward: 1.43\n",
      "epis: 771   score: 0.0   mem len: 145602   epsilon: 0.6371    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 772   score: 1.0   mem len: 145774   epsilon: 0.6368    steps: 172    lr: 0.0001     reward: 1.42\n",
      "epis: 773   score: 1.0   mem len: 145946   epsilon: 0.6366    steps: 172    lr: 0.0001     reward: 1.4\n",
      "epis: 774   score: 2.0   mem len: 146143   epsilon: 0.6363    steps: 197    lr: 0.0001     reward: 1.39\n",
      "epis: 775   score: 3.0   mem len: 146390   epsilon: 0.636    steps: 247    lr: 0.0001     reward: 1.42\n",
      "epis: 776   score: 0.0   mem len: 146513   epsilon: 0.6358    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 777   score: 2.0   mem len: 146732   epsilon: 0.6355    steps: 219    lr: 0.0001     reward: 1.42\n",
      "epis: 778   score: 0.0   mem len: 146855   epsilon: 0.6353    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 779   score: 1.0   mem len: 147023   epsilon: 0.6351    steps: 168    lr: 0.0001     reward: 1.41\n",
      "epis: 780   score: 0.0   mem len: 147146   epsilon: 0.6349    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 781   score: 1.0   mem len: 147315   epsilon: 0.6347    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 782   score: 2.0   mem len: 147512   epsilon: 0.6344    steps: 197    lr: 0.0001     reward: 1.43\n",
      "epis: 783   score: 2.0   mem len: 147728   epsilon: 0.6341    steps: 216    lr: 0.0001     reward: 1.44\n",
      "epis: 784   score: 1.0   mem len: 147897   epsilon: 0.6339    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 785   score: 1.0   mem len: 148049   epsilon: 0.6337    steps: 152    lr: 0.0001     reward: 1.42\n",
      "epis: 786   score: 3.0   mem len: 148277   epsilon: 0.6334    steps: 228    lr: 0.0001     reward: 1.44\n",
      "epis: 787   score: 1.0   mem len: 148446   epsilon: 0.6331    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 788   score: 1.0   mem len: 148597   epsilon: 0.6329    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 789   score: 0.0   mem len: 148720   epsilon: 0.6328    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 790   score: 3.0   mem len: 148966   epsilon: 0.6324    steps: 246    lr: 0.0001     reward: 1.49\n",
      "epis: 791   score: 2.0   mem len: 149183   epsilon: 0.6321    steps: 217    lr: 0.0001     reward: 1.47\n",
      "epis: 792   score: 3.0   mem len: 149409   epsilon: 0.6318    steps: 226    lr: 0.0001     reward: 1.49\n",
      "epis: 793   score: 1.0   mem len: 149581   epsilon: 0.6316    steps: 172    lr: 0.0001     reward: 1.49\n",
      "epis: 794   score: 2.0   mem len: 149763   epsilon: 0.6313    steps: 182    lr: 0.0001     reward: 1.48\n",
      "epis: 795   score: 3.0   mem len: 150030   epsilon: 0.631    steps: 267    lr: 0.0001     reward: 1.51\n",
      "epis: 796   score: 0.0   mem len: 150152   epsilon: 0.6308    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 797   score: 0.0   mem len: 150275   epsilon: 0.6306    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 798   score: 3.0   mem len: 150506   epsilon: 0.6303    steps: 231    lr: 0.0001     reward: 1.51\n",
      "epis: 799   score: 0.0   mem len: 150628   epsilon: 0.6301    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 800   score: 3.0   mem len: 150892   epsilon: 0.6298    steps: 264    lr: 0.0001     reward: 1.54\n",
      "epis: 801   score: 2.0   mem len: 151112   epsilon: 0.6295    steps: 220    lr: 0.0001     reward: 1.52\n",
      "epis: 802   score: 0.0   mem len: 151235   epsilon: 0.6293    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 803   score: 4.0   mem len: 151496   epsilon: 0.6289    steps: 261    lr: 0.0001     reward: 1.55\n",
      "epis: 804   score: 3.0   mem len: 151741   epsilon: 0.6286    steps: 245    lr: 0.0001     reward: 1.56\n",
      "epis: 805   score: 2.0   mem len: 151938   epsilon: 0.6283    steps: 197    lr: 0.0001     reward: 1.58\n",
      "epis: 806   score: 4.0   mem len: 152252   epsilon: 0.6279    steps: 314    lr: 0.0001     reward: 1.6\n",
      "epis: 807   score: 1.0   mem len: 152402   epsilon: 0.6277    steps: 150    lr: 0.0001     reward: 1.61\n",
      "epis: 808   score: 0.0   mem len: 152525   epsilon: 0.6275    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 809   score: 1.0   mem len: 152676   epsilon: 0.6273    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 810   score: 3.0   mem len: 152940   epsilon: 0.6269    steps: 264    lr: 0.0001     reward: 1.63\n",
      "epis: 811   score: 0.0   mem len: 153062   epsilon: 0.6268    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 812   score: 3.0   mem len: 153329   epsilon: 0.6264    steps: 267    lr: 0.0001     reward: 1.63\n",
      "epis: 813   score: 1.0   mem len: 153499   epsilon: 0.6262    steps: 170    lr: 0.0001     reward: 1.64\n",
      "epis: 814   score: 0.0   mem len: 153621   epsilon: 0.626    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 815   score: 4.0   mem len: 153916   epsilon: 0.6256    steps: 295    lr: 0.0001     reward: 1.67\n",
      "epis: 816   score: 0.0   mem len: 154039   epsilon: 0.6254    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 817   score: 2.0   mem len: 154237   epsilon: 0.6252    steps: 198    lr: 0.0001     reward: 1.68\n",
      "epis: 818   score: 1.0   mem len: 154388   epsilon: 0.6249    steps: 151    lr: 0.0001     reward: 1.69\n",
      "epis: 819   score: 1.0   mem len: 154558   epsilon: 0.6247    steps: 170    lr: 0.0001     reward: 1.64\n",
      "epis: 820   score: 2.0   mem len: 154756   epsilon: 0.6244    steps: 198    lr: 0.0001     reward: 1.64\n",
      "epis: 821   score: 0.0   mem len: 154878   epsilon: 0.6243    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 822   score: 0.0   mem len: 155001   epsilon: 0.6241    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 823   score: 2.0   mem len: 155217   epsilon: 0.6238    steps: 216    lr: 0.0001     reward: 1.63\n",
      "epis: 824   score: 4.0   mem len: 155514   epsilon: 0.6234    steps: 297    lr: 0.0001     reward: 1.67\n",
      "epis: 825   score: 4.0   mem len: 155791   epsilon: 0.623    steps: 277    lr: 0.0001     reward: 1.71\n",
      "epis: 826   score: 0.0   mem len: 155914   epsilon: 0.6228    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 827   score: 2.0   mem len: 156111   epsilon: 0.6226    steps: 197    lr: 0.0001     reward: 1.67\n",
      "epis: 828   score: 2.0   mem len: 156308   epsilon: 0.6223    steps: 197    lr: 0.0001     reward: 1.69\n",
      "epis: 829   score: 2.0   mem len: 156506   epsilon: 0.622    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 830   score: 3.0   mem len: 156735   epsilon: 0.6217    steps: 229    lr: 0.0001     reward: 1.69\n",
      "epis: 831   score: 0.0   mem len: 156858   epsilon: 0.6215    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 832   score: 2.0   mem len: 157075   epsilon: 0.6212    steps: 217    lr: 0.0001     reward: 1.67\n",
      "epis: 833   score: 3.0   mem len: 157342   epsilon: 0.6209    steps: 267    lr: 0.0001     reward: 1.65\n",
      "epis: 834   score: 1.0   mem len: 157493   epsilon: 0.6207    steps: 151    lr: 0.0001     reward: 1.64\n",
      "epis: 835   score: 0.0   mem len: 157615   epsilon: 0.6205    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 836   score: 0.0   mem len: 157737   epsilon: 0.6203    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 837   score: 0.0   mem len: 157860   epsilon: 0.6202    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 838   score: 1.0   mem len: 158011   epsilon: 0.6199    steps: 151    lr: 0.0001     reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 839   score: 0.0   mem len: 158134   epsilon: 0.6198    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 840   score: 1.0   mem len: 158303   epsilon: 0.6195    steps: 169    lr: 0.0001     reward: 1.55\n",
      "epis: 841   score: 1.0   mem len: 158454   epsilon: 0.6193    steps: 151    lr: 0.0001     reward: 1.54\n",
      "epis: 842   score: 0.0   mem len: 158577   epsilon: 0.6192    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 843   score: 1.0   mem len: 158728   epsilon: 0.619    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 844   score: 1.0   mem len: 158897   epsilon: 0.6187    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 845   score: 3.0   mem len: 159146   epsilon: 0.6184    steps: 249    lr: 0.0001     reward: 1.49\n",
      "epis: 846   score: 3.0   mem len: 159392   epsilon: 0.618    steps: 246    lr: 0.0001     reward: 1.5\n",
      "epis: 847   score: 6.0   mem len: 159761   epsilon: 0.6175    steps: 369    lr: 0.0001     reward: 1.53\n",
      "epis: 848   score: 0.0   mem len: 159884   epsilon: 0.6174    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 849   score: 4.0   mem len: 160162   epsilon: 0.617    steps: 278    lr: 0.0001     reward: 1.54\n",
      "epis: 850   score: 2.0   mem len: 160360   epsilon: 0.6167    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 851   score: 4.0   mem len: 160618   epsilon: 0.6163    steps: 258    lr: 0.0001     reward: 1.58\n",
      "epis: 852   score: 2.0   mem len: 160837   epsilon: 0.616    steps: 219    lr: 0.0001     reward: 1.57\n",
      "epis: 853   score: 2.0   mem len: 161034   epsilon: 0.6158    steps: 197    lr: 0.0001     reward: 1.58\n",
      "epis: 854   score: 3.0   mem len: 161281   epsilon: 0.6154    steps: 247    lr: 0.0001     reward: 1.6\n",
      "epis: 855   score: 1.0   mem len: 161450   epsilon: 0.6152    steps: 169    lr: 0.0001     reward: 1.6\n",
      "epis: 856   score: 1.0   mem len: 161619   epsilon: 0.615    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 857   score: 2.0   mem len: 161817   epsilon: 0.6147    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 858   score: 3.0   mem len: 162065   epsilon: 0.6143    steps: 248    lr: 0.0001     reward: 1.59\n",
      "epis: 859   score: 3.0   mem len: 162313   epsilon: 0.614    steps: 248    lr: 0.0001     reward: 1.61\n",
      "epis: 860   score: 2.0   mem len: 162511   epsilon: 0.6137    steps: 198    lr: 0.0001     reward: 1.63\n",
      "epis: 861   score: 0.0   mem len: 162633   epsilon: 0.6136    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 862   score: 2.0   mem len: 162831   epsilon: 0.6133    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 863   score: 0.0   mem len: 162954   epsilon: 0.6131    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 864   score: 1.0   mem len: 163125   epsilon: 0.6129    steps: 171    lr: 0.0001     reward: 1.66\n",
      "epis: 865   score: 3.0   mem len: 163368   epsilon: 0.6126    steps: 243    lr: 0.0001     reward: 1.68\n",
      "epis: 866   score: 1.0   mem len: 163519   epsilon: 0.6123    steps: 151    lr: 0.0001     reward: 1.68\n",
      "epis: 867   score: 2.0   mem len: 163717   epsilon: 0.6121    steps: 198    lr: 0.0001     reward: 1.68\n",
      "epis: 868   score: 0.0   mem len: 163839   epsilon: 0.6119    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 869   score: 1.0   mem len: 164008   epsilon: 0.6117    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 870   score: 1.0   mem len: 164159   epsilon: 0.6115    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 871   score: 0.0   mem len: 164282   epsilon: 0.6113    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 872   score: 3.0   mem len: 164509   epsilon: 0.611    steps: 227    lr: 0.0001     reward: 1.6\n",
      "epis: 873   score: 0.0   mem len: 164631   epsilon: 0.6108    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 874   score: 1.0   mem len: 164800   epsilon: 0.6106    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 875   score: 3.0   mem len: 165044   epsilon: 0.6102    steps: 244    lr: 0.0001     reward: 1.58\n",
      "epis: 876   score: 2.0   mem len: 165246   epsilon: 0.61    steps: 202    lr: 0.0001     reward: 1.6\n",
      "epis: 877   score: 1.0   mem len: 165415   epsilon: 0.6097    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 878   score: 0.0   mem len: 165538   epsilon: 0.6096    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 879   score: 1.0   mem len: 165707   epsilon: 0.6093    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 880   score: 2.0   mem len: 165905   epsilon: 0.609    steps: 198    lr: 0.0001     reward: 1.61\n",
      "epis: 881   score: 0.0   mem len: 166028   epsilon: 0.6089    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 882   score: 2.0   mem len: 166226   epsilon: 0.6086    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 883   score: 0.0   mem len: 166348   epsilon: 0.6084    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 884   score: 0.0   mem len: 166470   epsilon: 0.6083    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 885   score: 0.0   mem len: 166593   epsilon: 0.6081    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 886   score: 2.0   mem len: 166790   epsilon: 0.6078    steps: 197    lr: 0.0001     reward: 1.55\n",
      "epis: 887   score: 2.0   mem len: 167008   epsilon: 0.6075    steps: 218    lr: 0.0001     reward: 1.56\n",
      "epis: 888   score: 4.0   mem len: 167325   epsilon: 0.6071    steps: 317    lr: 0.0001     reward: 1.59\n",
      "epis: 889   score: 0.0   mem len: 167447   epsilon: 0.6069    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 890   score: 2.0   mem len: 167644   epsilon: 0.6066    steps: 197    lr: 0.0001     reward: 1.58\n",
      "epis: 891   score: 2.0   mem len: 167842   epsilon: 0.6064    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 892   score: 1.0   mem len: 167992   epsilon: 0.6062    steps: 150    lr: 0.0001     reward: 1.56\n",
      "epis: 893   score: 3.0   mem len: 168239   epsilon: 0.6058    steps: 247    lr: 0.0001     reward: 1.58\n",
      "epis: 894   score: 0.0   mem len: 168362   epsilon: 0.6057    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 895   score: 0.0   mem len: 168485   epsilon: 0.6055    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 896   score: 1.0   mem len: 168653   epsilon: 0.6053    steps: 168    lr: 0.0001     reward: 1.54\n",
      "epis: 897   score: 0.0   mem len: 168775   epsilon: 0.6051    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 898   score: 4.0   mem len: 169071   epsilon: 0.6047    steps: 296    lr: 0.0001     reward: 1.55\n",
      "epis: 899   score: 1.0   mem len: 169222   epsilon: 0.6045    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 900   score: 2.0   mem len: 169420   epsilon: 0.6042    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 901   score: 0.0   mem len: 169542   epsilon: 0.604    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 902   score: 2.0   mem len: 169759   epsilon: 0.6037    steps: 217    lr: 0.0001     reward: 1.55\n",
      "epis: 903   score: 2.0   mem len: 169956   epsilon: 0.6035    steps: 197    lr: 0.0001     reward: 1.53\n",
      "epis: 904   score: 0.0   mem len: 170079   epsilon: 0.6033    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 905   score: 3.0   mem len: 170345   epsilon: 0.6029    steps: 266    lr: 0.0001     reward: 1.51\n",
      "epis: 906   score: 3.0   mem len: 170553   epsilon: 0.6026    steps: 208    lr: 0.0001     reward: 1.5\n",
      "epis: 907   score: 2.0   mem len: 170751   epsilon: 0.6024    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 908   score: 0.0   mem len: 170874   epsilon: 0.6022    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 909   score: 1.0   mem len: 171043   epsilon: 0.602    steps: 169    lr: 0.0001     reward: 1.51\n",
      "epis: 910   score: 0.0   mem len: 171166   epsilon: 0.6018    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 911   score: 2.0   mem len: 171364   epsilon: 0.6015    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 912   score: 1.0   mem len: 171533   epsilon: 0.6013    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 913   score: 0.0   mem len: 171656   epsilon: 0.6011    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 914   score: 2.0   mem len: 171854   epsilon: 0.6008    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 915   score: 3.0   mem len: 172104   epsilon: 0.6005    steps: 250    lr: 0.0001     reward: 1.48\n",
      "epis: 916   score: 2.0   mem len: 172301   epsilon: 0.6002    steps: 197    lr: 0.0001     reward: 1.5\n",
      "epis: 917   score: 2.0   mem len: 172499   epsilon: 0.5999    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 918   score: 2.0   mem len: 172715   epsilon: 0.5997    steps: 216    lr: 0.0001     reward: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 919   score: 0.0   mem len: 172838   epsilon: 0.5995    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 920   score: 0.0   mem len: 172961   epsilon: 0.5993    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 921   score: 4.0   mem len: 173252   epsilon: 0.5989    steps: 291    lr: 0.0001     reward: 1.52\n",
      "epis: 922   score: 0.0   mem len: 173374   epsilon: 0.5987    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 923   score: 3.0   mem len: 173620   epsilon: 0.5984    steps: 246    lr: 0.0001     reward: 1.53\n",
      "epis: 924   score: 1.0   mem len: 173788   epsilon: 0.5982    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 925   score: 2.0   mem len: 174004   epsilon: 0.5979    steps: 216    lr: 0.0001     reward: 1.48\n",
      "epis: 926   score: 1.0   mem len: 174172   epsilon: 0.5976    steps: 168    lr: 0.0001     reward: 1.49\n",
      "epis: 927   score: 1.0   mem len: 174341   epsilon: 0.5974    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 928   score: 0.0   mem len: 174464   epsilon: 0.5972    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 929   score: 1.0   mem len: 174633   epsilon: 0.597    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 930   score: 1.0   mem len: 174804   epsilon: 0.5968    steps: 171    lr: 0.0001     reward: 1.43\n",
      "epis: 931   score: 3.0   mem len: 175052   epsilon: 0.5964    steps: 248    lr: 0.0001     reward: 1.46\n",
      "epis: 932   score: 4.0   mem len: 175311   epsilon: 0.5961    steps: 259    lr: 0.0001     reward: 1.48\n",
      "epis: 933   score: 1.0   mem len: 175462   epsilon: 0.5959    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 934   score: 1.0   mem len: 175612   epsilon: 0.5957    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 935   score: 0.0   mem len: 175735   epsilon: 0.5955    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 936   score: 2.0   mem len: 175914   epsilon: 0.5952    steps: 179    lr: 0.0001     reward: 1.48\n",
      "epis: 937   score: 0.0   mem len: 176037   epsilon: 0.5951    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 938   score: 1.0   mem len: 176206   epsilon: 0.5948    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 939   score: 0.0   mem len: 176329   epsilon: 0.5947    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 940   score: 2.0   mem len: 176546   epsilon: 0.5944    steps: 217    lr: 0.0001     reward: 1.49\n",
      "epis: 941   score: 0.0   mem len: 176669   epsilon: 0.5942    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 942   score: 0.0   mem len: 176792   epsilon: 0.594    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 943   score: 2.0   mem len: 177010   epsilon: 0.5937    steps: 218    lr: 0.0001     reward: 1.49\n",
      "epis: 944   score: 0.0   mem len: 177133   epsilon: 0.5936    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 945   score: 1.0   mem len: 177283   epsilon: 0.5933    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 946   score: 2.0   mem len: 177467   epsilon: 0.5931    steps: 184    lr: 0.0001     reward: 1.45\n",
      "epis: 947   score: 4.0   mem len: 177727   epsilon: 0.5927    steps: 260    lr: 0.0001     reward: 1.43\n",
      "epis: 948   score: 2.0   mem len: 177946   epsilon: 0.5924    steps: 219    lr: 0.0001     reward: 1.45\n",
      "epis: 949   score: 3.0   mem len: 178213   epsilon: 0.5921    steps: 267    lr: 0.0001     reward: 1.44\n",
      "epis: 950   score: 0.0   mem len: 178336   epsilon: 0.5919    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 951   score: 3.0   mem len: 178581   epsilon: 0.5916    steps: 245    lr: 0.0001     reward: 1.41\n",
      "epis: 952   score: 2.0   mem len: 178783   epsilon: 0.5913    steps: 202    lr: 0.0001     reward: 1.41\n",
      "epis: 953   score: 3.0   mem len: 179011   epsilon: 0.591    steps: 228    lr: 0.0001     reward: 1.42\n",
      "epis: 954   score: 3.0   mem len: 179239   epsilon: 0.5906    steps: 228    lr: 0.0001     reward: 1.42\n",
      "epis: 955   score: 1.0   mem len: 179390   epsilon: 0.5904    steps: 151    lr: 0.0001     reward: 1.42\n",
      "epis: 956   score: 1.0   mem len: 179560   epsilon: 0.5902    steps: 170    lr: 0.0001     reward: 1.42\n",
      "epis: 957   score: 0.0   mem len: 179683   epsilon: 0.59    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 958   score: 0.0   mem len: 179805   epsilon: 0.5899    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 959   score: 1.0   mem len: 179956   epsilon: 0.5897    steps: 151    lr: 0.0001     reward: 1.35\n",
      "epis: 960   score: 1.0   mem len: 180125   epsilon: 0.5894    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 961   score: 0.0   mem len: 180248   epsilon: 0.5893    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 962   score: 1.0   mem len: 180417   epsilon: 0.589    steps: 169    lr: 0.0001     reward: 1.33\n",
      "epis: 963   score: 2.0   mem len: 180615   epsilon: 0.5887    steps: 198    lr: 0.0001     reward: 1.35\n",
      "epis: 964   score: 0.0   mem len: 180737   epsilon: 0.5886    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 965   score: 0.0   mem len: 180860   epsilon: 0.5884    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 966   score: 2.0   mem len: 181059   epsilon: 0.5881    steps: 199    lr: 0.0001     reward: 1.32\n",
      "epis: 967   score: 1.0   mem len: 181231   epsilon: 0.5879    steps: 172    lr: 0.0001     reward: 1.31\n",
      "epis: 968   score: 0.0   mem len: 181353   epsilon: 0.5877    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 969   score: 0.0   mem len: 181476   epsilon: 0.5876    steps: 123    lr: 0.0001     reward: 1.3\n",
      "epis: 970   score: 0.0   mem len: 181599   epsilon: 0.5874    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 971   score: 0.0   mem len: 181721   epsilon: 0.5872    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 972   score: 3.0   mem len: 181969   epsilon: 0.5869    steps: 248    lr: 0.0001     reward: 1.29\n",
      "epis: 973   score: 0.0   mem len: 182092   epsilon: 0.5867    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 974   score: 0.0   mem len: 182215   epsilon: 0.5865    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 975   score: 1.0   mem len: 182366   epsilon: 0.5863    steps: 151    lr: 0.0001     reward: 1.26\n",
      "epis: 976   score: 0.0   mem len: 182489   epsilon: 0.5862    steps: 123    lr: 0.0001     reward: 1.24\n",
      "epis: 977   score: 4.0   mem len: 182765   epsilon: 0.5858    steps: 276    lr: 0.0001     reward: 1.27\n",
      "epis: 978   score: 0.0   mem len: 182888   epsilon: 0.5856    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 979   score: 4.0   mem len: 183162   epsilon: 0.5852    steps: 274    lr: 0.0001     reward: 1.3\n",
      "epis: 980   score: 4.0   mem len: 183479   epsilon: 0.5848    steps: 317    lr: 0.0001     reward: 1.32\n",
      "epis: 981   score: 3.0   mem len: 183728   epsilon: 0.5845    steps: 249    lr: 0.0001     reward: 1.35\n",
      "epis: 982   score: 4.0   mem len: 184005   epsilon: 0.5841    steps: 277    lr: 0.0001     reward: 1.37\n",
      "epis: 983   score: 0.0   mem len: 184127   epsilon: 0.5839    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 984   score: 2.0   mem len: 184324   epsilon: 0.5836    steps: 197    lr: 0.0001     reward: 1.39\n",
      "epis: 985   score: 0.0   mem len: 184447   epsilon: 0.5835    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 986   score: 3.0   mem len: 184675   epsilon: 0.5831    steps: 228    lr: 0.0001     reward: 1.4\n",
      "epis: 987   score: 2.0   mem len: 184895   epsilon: 0.5828    steps: 220    lr: 0.0001     reward: 1.4\n",
      "epis: 988   score: 0.0   mem len: 185017   epsilon: 0.5827    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 989   score: 2.0   mem len: 185199   epsilon: 0.5824    steps: 182    lr: 0.0001     reward: 1.38\n",
      "epis: 990   score: 0.0   mem len: 185321   epsilon: 0.5823    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 991   score: 0.0   mem len: 185443   epsilon: 0.5821    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 992   score: 0.0   mem len: 185565   epsilon: 0.5819    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 993   score: 1.0   mem len: 185734   epsilon: 0.5817    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 994   score: 0.0   mem len: 185856   epsilon: 0.5815    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 995   score: 1.0   mem len: 186025   epsilon: 0.5813    steps: 169    lr: 0.0001     reward: 1.32\n",
      "epis: 996   score: 3.0   mem len: 186275   epsilon: 0.5809    steps: 250    lr: 0.0001     reward: 1.34\n",
      "epis: 997   score: 2.0   mem len: 186473   epsilon: 0.5807    steps: 198    lr: 0.0001     reward: 1.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 998   score: 1.0   mem len: 186643   epsilon: 0.5804    steps: 170    lr: 0.0001     reward: 1.33\n",
      "epis: 999   score: 0.0   mem len: 186766   epsilon: 0.5803    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 1000   score: 2.0   mem len: 186946   epsilon: 0.58    steps: 180    lr: 0.0001     reward: 1.32\n",
      "epis: 1001   score: 1.0   mem len: 187096   epsilon: 0.5798    steps: 150    lr: 0.0001     reward: 1.33\n",
      "epis: 1002   score: 0.0   mem len: 187218   epsilon: 0.5796    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 1003   score: 1.0   mem len: 187369   epsilon: 0.5794    steps: 151    lr: 0.0001     reward: 1.3\n",
      "epis: 1004   score: 1.0   mem len: 187521   epsilon: 0.5792    steps: 152    lr: 0.0001     reward: 1.31\n",
      "epis: 1005   score: 0.0   mem len: 187644   epsilon: 0.579    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 1006   score: 1.0   mem len: 187812   epsilon: 0.5788    steps: 168    lr: 0.0001     reward: 1.26\n",
      "epis: 1007   score: 4.0   mem len: 188129   epsilon: 0.5784    steps: 317    lr: 0.0001     reward: 1.28\n",
      "epis: 1008   score: 0.0   mem len: 188251   epsilon: 0.5782    steps: 122    lr: 0.0001     reward: 1.28\n",
      "epis: 1009   score: 2.0   mem len: 188469   epsilon: 0.5779    steps: 218    lr: 0.0001     reward: 1.29\n",
      "epis: 1010   score: 2.0   mem len: 188667   epsilon: 0.5776    steps: 198    lr: 0.0001     reward: 1.31\n",
      "epis: 1011   score: 4.0   mem len: 188941   epsilon: 0.5773    steps: 274    lr: 0.0001     reward: 1.33\n",
      "epis: 1012   score: 0.0   mem len: 189063   epsilon: 0.5771    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 1013   score: 1.0   mem len: 189214   epsilon: 0.5769    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 1014   score: 2.0   mem len: 189415   epsilon: 0.5766    steps: 201    lr: 0.0001     reward: 1.33\n",
      "epis: 1015   score: 4.0   mem len: 189675   epsilon: 0.5762    steps: 260    lr: 0.0001     reward: 1.34\n",
      "epis: 1016   score: 0.0   mem len: 189798   epsilon: 0.5761    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 1017   score: 2.0   mem len: 190017   epsilon: 0.5758    steps: 219    lr: 0.0001     reward: 1.32\n",
      "epis: 1018   score: 0.0   mem len: 190139   epsilon: 0.5756    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 1019   score: 1.0   mem len: 190309   epsilon: 0.5754    steps: 170    lr: 0.0001     reward: 1.31\n",
      "epis: 1020   score: 1.0   mem len: 190459   epsilon: 0.5752    steps: 150    lr: 0.0001     reward: 1.32\n",
      "epis: 1021   score: 5.0   mem len: 190773   epsilon: 0.5747    steps: 314    lr: 0.0001     reward: 1.33\n",
      "epis: 1022   score: 1.0   mem len: 190923   epsilon: 0.5745    steps: 150    lr: 0.0001     reward: 1.34\n",
      "epis: 1023   score: 1.0   mem len: 191092   epsilon: 0.5743    steps: 169    lr: 0.0001     reward: 1.32\n",
      "epis: 1024   score: 0.0   mem len: 191214   epsilon: 0.5741    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 1025   score: 3.0   mem len: 191458   epsilon: 0.5738    steps: 244    lr: 0.0001     reward: 1.32\n",
      "epis: 1026   score: 0.0   mem len: 191580   epsilon: 0.5736    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 1027   score: 0.0   mem len: 191702   epsilon: 0.5734    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 1028   score: 0.0   mem len: 191824   epsilon: 0.5733    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 1029   score: 2.0   mem len: 192042   epsilon: 0.573    steps: 218    lr: 0.0001     reward: 1.31\n",
      "epis: 1030   score: 1.0   mem len: 192214   epsilon: 0.5727    steps: 172    lr: 0.0001     reward: 1.31\n",
      "epis: 1031   score: 1.0   mem len: 192385   epsilon: 0.5725    steps: 171    lr: 0.0001     reward: 1.29\n",
      "epis: 1032   score: 2.0   mem len: 192584   epsilon: 0.5722    steps: 199    lr: 0.0001     reward: 1.27\n",
      "epis: 1033   score: 1.0   mem len: 192753   epsilon: 0.572    steps: 169    lr: 0.0001     reward: 1.27\n",
      "epis: 1034   score: 3.0   mem len: 192997   epsilon: 0.5717    steps: 244    lr: 0.0001     reward: 1.29\n",
      "epis: 1035   score: 0.0   mem len: 193120   epsilon: 0.5715    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 1036   score: 1.0   mem len: 193271   epsilon: 0.5713    steps: 151    lr: 0.0001     reward: 1.28\n",
      "epis: 1037   score: 2.0   mem len: 193471   epsilon: 0.571    steps: 200    lr: 0.0001     reward: 1.3\n",
      "epis: 1038   score: 2.0   mem len: 193689   epsilon: 0.5707    steps: 218    lr: 0.0001     reward: 1.31\n",
      "epis: 1039   score: 2.0   mem len: 193886   epsilon: 0.5704    steps: 197    lr: 0.0001     reward: 1.33\n",
      "epis: 1040   score: 2.0   mem len: 194103   epsilon: 0.5701    steps: 217    lr: 0.0001     reward: 1.33\n",
      "epis: 1041   score: 1.0   mem len: 194254   epsilon: 0.5699    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 1042   score: 0.0   mem len: 194377   epsilon: 0.5698    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 1043   score: 0.0   mem len: 194499   epsilon: 0.5696    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 1044   score: 0.0   mem len: 194622   epsilon: 0.5694    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 1045   score: 1.0   mem len: 194791   epsilon: 0.5692    steps: 169    lr: 0.0001     reward: 1.32\n",
      "epis: 1046   score: 0.0   mem len: 194913   epsilon: 0.569    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 1047   score: 3.0   mem len: 195139   epsilon: 0.5687    steps: 226    lr: 0.0001     reward: 1.29\n",
      "epis: 1048   score: 0.0   mem len: 195262   epsilon: 0.5685    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 1049   score: 3.0   mem len: 195489   epsilon: 0.5682    steps: 227    lr: 0.0001     reward: 1.27\n",
      "epis: 1050   score: 3.0   mem len: 195738   epsilon: 0.5679    steps: 249    lr: 0.0001     reward: 1.3\n",
      "epis: 1051   score: 1.0   mem len: 195907   epsilon: 0.5676    steps: 169    lr: 0.0001     reward: 1.28\n",
      "epis: 1052   score: 1.0   mem len: 196058   epsilon: 0.5674    steps: 151    lr: 0.0001     reward: 1.27\n",
      "epis: 1053   score: 5.0   mem len: 196369   epsilon: 0.567    steps: 311    lr: 0.0001     reward: 1.29\n",
      "epis: 1054   score: 2.0   mem len: 196567   epsilon: 0.5667    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 1055   score: 0.0   mem len: 196690   epsilon: 0.5666    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 1056   score: 1.0   mem len: 196859   epsilon: 0.5663    steps: 169    lr: 0.0001     reward: 1.27\n",
      "epis: 1057   score: 0.0   mem len: 196982   epsilon: 0.5662    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 1058   score: 2.0   mem len: 197179   epsilon: 0.5659    steps: 197    lr: 0.0001     reward: 1.29\n",
      "epis: 1059   score: 1.0   mem len: 197331   epsilon: 0.5657    steps: 152    lr: 0.0001     reward: 1.29\n",
      "epis: 1060   score: 3.0   mem len: 197575   epsilon: 0.5653    steps: 244    lr: 0.0001     reward: 1.31\n",
      "epis: 1061   score: 0.0   mem len: 197698   epsilon: 0.5652    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 1062   score: 2.0   mem len: 197896   epsilon: 0.5649    steps: 198    lr: 0.0001     reward: 1.32\n",
      "epis: 1063   score: 3.0   mem len: 198126   epsilon: 0.5646    steps: 230    lr: 0.0001     reward: 1.33\n",
      "epis: 1064   score: 1.0   mem len: 198295   epsilon: 0.5644    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 1065   score: 5.0   mem len: 198638   epsilon: 0.5639    steps: 343    lr: 0.0001     reward: 1.39\n",
      "epis: 1066   score: 1.0   mem len: 198789   epsilon: 0.5637    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 1067   score: 1.0   mem len: 198958   epsilon: 0.5634    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 1068   score: 5.0   mem len: 199283   epsilon: 0.563    steps: 325    lr: 0.0001     reward: 1.43\n",
      "epis: 1069   score: 2.0   mem len: 199502   epsilon: 0.5627    steps: 219    lr: 0.0001     reward: 1.45\n",
      "epis: 1070   score: 3.0   mem len: 199728   epsilon: 0.5624    steps: 226    lr: 0.0001     reward: 1.48\n",
      "epis: 1071   score: 1.0   mem len: 199897   epsilon: 0.5621    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 1072   score: 1.0   mem len: 200048   epsilon: 0.5619    steps: 151    lr: 4e-05     reward: 1.47\n",
      "epis: 1073   score: 0.0   mem len: 200170   epsilon: 0.5618    steps: 122    lr: 4e-05     reward: 1.47\n",
      "epis: 1074   score: 3.0   mem len: 200437   epsilon: 0.5614    steps: 267    lr: 4e-05     reward: 1.5\n",
      "epis: 1075   score: 3.0   mem len: 200663   epsilon: 0.5611    steps: 226    lr: 4e-05     reward: 1.52\n",
      "epis: 1076   score: 4.0   mem len: 200922   epsilon: 0.5607    steps: 259    lr: 4e-05     reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1077   score: 5.0   mem len: 201211   epsilon: 0.5603    steps: 289    lr: 4e-05     reward: 1.57\n",
      "epis: 1078   score: 1.0   mem len: 201380   epsilon: 0.5601    steps: 169    lr: 4e-05     reward: 1.58\n",
      "epis: 1079   score: 1.0   mem len: 201531   epsilon: 0.5599    steps: 151    lr: 4e-05     reward: 1.55\n",
      "epis: 1080   score: 4.0   mem len: 201826   epsilon: 0.5595    steps: 295    lr: 4e-05     reward: 1.55\n",
      "epis: 1081   score: 1.0   mem len: 201977   epsilon: 0.5593    steps: 151    lr: 4e-05     reward: 1.53\n",
      "epis: 1082   score: 2.0   mem len: 202175   epsilon: 0.559    steps: 198    lr: 4e-05     reward: 1.51\n",
      "epis: 1083   score: 1.0   mem len: 202326   epsilon: 0.5588    steps: 151    lr: 4e-05     reward: 1.52\n",
      "epis: 1084   score: 2.0   mem len: 202548   epsilon: 0.5585    steps: 222    lr: 4e-05     reward: 1.52\n",
      "epis: 1085   score: 0.0   mem len: 202670   epsilon: 0.5583    steps: 122    lr: 4e-05     reward: 1.52\n",
      "epis: 1086   score: 2.0   mem len: 202867   epsilon: 0.558    steps: 197    lr: 4e-05     reward: 1.51\n",
      "epis: 1087   score: 2.0   mem len: 203088   epsilon: 0.5577    steps: 221    lr: 4e-05     reward: 1.51\n",
      "epis: 1088   score: 2.0   mem len: 203286   epsilon: 0.5575    steps: 198    lr: 4e-05     reward: 1.53\n",
      "epis: 1089   score: 1.0   mem len: 203437   epsilon: 0.5573    steps: 151    lr: 4e-05     reward: 1.52\n",
      "epis: 1090   score: 1.0   mem len: 203606   epsilon: 0.557    steps: 169    lr: 4e-05     reward: 1.53\n",
      "epis: 1091   score: 7.0   mem len: 203926   epsilon: 0.5566    steps: 320    lr: 4e-05     reward: 1.6\n",
      "epis: 1092   score: 2.0   mem len: 204108   epsilon: 0.5563    steps: 182    lr: 4e-05     reward: 1.62\n",
      "epis: 1093   score: 3.0   mem len: 204358   epsilon: 0.556    steps: 250    lr: 4e-05     reward: 1.64\n",
      "epis: 1094   score: 5.0   mem len: 204705   epsilon: 0.5555    steps: 347    lr: 4e-05     reward: 1.69\n",
      "epis: 1095   score: 1.0   mem len: 204873   epsilon: 0.5553    steps: 168    lr: 4e-05     reward: 1.69\n",
      "epis: 1096   score: 5.0   mem len: 205200   epsilon: 0.5548    steps: 327    lr: 4e-05     reward: 1.71\n",
      "epis: 1097   score: 2.0   mem len: 205398   epsilon: 0.5545    steps: 198    lr: 4e-05     reward: 1.71\n",
      "epis: 1098   score: 0.0   mem len: 205520   epsilon: 0.5544    steps: 122    lr: 4e-05     reward: 1.7\n",
      "epis: 1099   score: 0.0   mem len: 205642   epsilon: 0.5542    steps: 122    lr: 4e-05     reward: 1.7\n",
      "epis: 1100   score: 1.0   mem len: 205810   epsilon: 0.554    steps: 168    lr: 4e-05     reward: 1.69\n",
      "epis: 1101   score: 2.0   mem len: 206007   epsilon: 0.5537    steps: 197    lr: 4e-05     reward: 1.7\n",
      "epis: 1102   score: 3.0   mem len: 206233   epsilon: 0.5534    steps: 226    lr: 4e-05     reward: 1.73\n",
      "epis: 1103   score: 0.0   mem len: 206356   epsilon: 0.5532    steps: 123    lr: 4e-05     reward: 1.72\n",
      "epis: 1104   score: 3.0   mem len: 206584   epsilon: 0.5529    steps: 228    lr: 4e-05     reward: 1.74\n",
      "epis: 1105   score: 1.0   mem len: 206756   epsilon: 0.5527    steps: 172    lr: 4e-05     reward: 1.75\n",
      "epis: 1106   score: 1.0   mem len: 206927   epsilon: 0.5524    steps: 171    lr: 4e-05     reward: 1.75\n",
      "epis: 1107   score: 3.0   mem len: 207172   epsilon: 0.5521    steps: 245    lr: 4e-05     reward: 1.74\n",
      "epis: 1108   score: 1.0   mem len: 207323   epsilon: 0.5519    steps: 151    lr: 4e-05     reward: 1.75\n",
      "epis: 1109   score: 2.0   mem len: 207541   epsilon: 0.5516    steps: 218    lr: 4e-05     reward: 1.75\n",
      "epis: 1110   score: 2.0   mem len: 207759   epsilon: 0.5513    steps: 218    lr: 4e-05     reward: 1.75\n",
      "epis: 1111   score: 0.0   mem len: 207882   epsilon: 0.5511    steps: 123    lr: 4e-05     reward: 1.71\n",
      "epis: 1112   score: 5.0   mem len: 208207   epsilon: 0.5507    steps: 325    lr: 4e-05     reward: 1.76\n",
      "epis: 1113   score: 2.0   mem len: 208406   epsilon: 0.5504    steps: 199    lr: 4e-05     reward: 1.77\n",
      "epis: 1114   score: 0.0   mem len: 208528   epsilon: 0.5502    steps: 122    lr: 4e-05     reward: 1.75\n",
      "epis: 1115   score: 0.0   mem len: 208651   epsilon: 0.5501    steps: 123    lr: 4e-05     reward: 1.71\n",
      "epis: 1116   score: 2.0   mem len: 208848   epsilon: 0.5498    steps: 197    lr: 4e-05     reward: 1.73\n",
      "epis: 1117   score: 0.0   mem len: 208970   epsilon: 0.5496    steps: 122    lr: 4e-05     reward: 1.71\n",
      "epis: 1118   score: 4.0   mem len: 209264   epsilon: 0.5492    steps: 294    lr: 4e-05     reward: 1.75\n",
      "epis: 1119   score: 0.0   mem len: 209387   epsilon: 0.549    steps: 123    lr: 4e-05     reward: 1.74\n",
      "epis: 1120   score: 4.0   mem len: 209683   epsilon: 0.5486    steps: 296    lr: 4e-05     reward: 1.77\n",
      "epis: 1121   score: 2.0   mem len: 209881   epsilon: 0.5484    steps: 198    lr: 4e-05     reward: 1.74\n",
      "epis: 1122   score: 0.0   mem len: 210003   epsilon: 0.5482    steps: 122    lr: 4e-05     reward: 1.73\n",
      "epis: 1123   score: 0.0   mem len: 210125   epsilon: 0.548    steps: 122    lr: 4e-05     reward: 1.72\n",
      "epis: 1124   score: 2.0   mem len: 210306   epsilon: 0.5478    steps: 181    lr: 4e-05     reward: 1.74\n",
      "epis: 1125   score: 2.0   mem len: 210486   epsilon: 0.5475    steps: 180    lr: 4e-05     reward: 1.73\n",
      "epis: 1126   score: 1.0   mem len: 210637   epsilon: 0.5473    steps: 151    lr: 4e-05     reward: 1.74\n",
      "epis: 1127   score: 1.0   mem len: 210788   epsilon: 0.5471    steps: 151    lr: 4e-05     reward: 1.75\n",
      "epis: 1128   score: 0.0   mem len: 210911   epsilon: 0.5469    steps: 123    lr: 4e-05     reward: 1.75\n",
      "epis: 1129   score: 0.0   mem len: 211034   epsilon: 0.5468    steps: 123    lr: 4e-05     reward: 1.73\n",
      "epis: 1130   score: 3.0   mem len: 211281   epsilon: 0.5464    steps: 247    lr: 4e-05     reward: 1.75\n",
      "epis: 1131   score: 1.0   mem len: 211451   epsilon: 0.5462    steps: 170    lr: 4e-05     reward: 1.75\n",
      "epis: 1132   score: 0.0   mem len: 211574   epsilon: 0.546    steps: 123    lr: 4e-05     reward: 1.73\n",
      "epis: 1133   score: 5.0   mem len: 211880   epsilon: 0.5456    steps: 306    lr: 4e-05     reward: 1.77\n",
      "epis: 1134   score: 1.0   mem len: 212052   epsilon: 0.5454    steps: 172    lr: 4e-05     reward: 1.75\n",
      "epis: 1135   score: 1.0   mem len: 212220   epsilon: 0.5451    steps: 168    lr: 4e-05     reward: 1.76\n",
      "epis: 1136   score: 1.0   mem len: 212388   epsilon: 0.5449    steps: 168    lr: 4e-05     reward: 1.76\n",
      "epis: 1137   score: 1.0   mem len: 212559   epsilon: 0.5447    steps: 171    lr: 4e-05     reward: 1.75\n",
      "epis: 1138   score: 0.0   mem len: 212682   epsilon: 0.5445    steps: 123    lr: 4e-05     reward: 1.73\n",
      "epis: 1139   score: 1.0   mem len: 212833   epsilon: 0.5443    steps: 151    lr: 4e-05     reward: 1.72\n",
      "epis: 1140   score: 1.0   mem len: 212983   epsilon: 0.5441    steps: 150    lr: 4e-05     reward: 1.71\n",
      "epis: 1141   score: 0.0   mem len: 213105   epsilon: 0.5439    steps: 122    lr: 4e-05     reward: 1.7\n",
      "epis: 1142   score: 7.0   mem len: 213389   epsilon: 0.5435    steps: 284    lr: 4e-05     reward: 1.77\n",
      "epis: 1143   score: 2.0   mem len: 213570   epsilon: 0.5433    steps: 181    lr: 4e-05     reward: 1.79\n",
      "epis: 1144   score: 1.0   mem len: 213739   epsilon: 0.543    steps: 169    lr: 4e-05     reward: 1.8\n",
      "epis: 1145   score: 0.0   mem len: 213861   epsilon: 0.5429    steps: 122    lr: 4e-05     reward: 1.79\n",
      "epis: 1146   score: 1.0   mem len: 214012   epsilon: 0.5427    steps: 151    lr: 4e-05     reward: 1.8\n",
      "epis: 1147   score: 2.0   mem len: 214212   epsilon: 0.5424    steps: 200    lr: 4e-05     reward: 1.79\n",
      "epis: 1148   score: 2.0   mem len: 214432   epsilon: 0.5421    steps: 220    lr: 4e-05     reward: 1.81\n",
      "epis: 1149   score: 0.0   mem len: 214555   epsilon: 0.5419    steps: 123    lr: 4e-05     reward: 1.78\n",
      "epis: 1150   score: 0.0   mem len: 214678   epsilon: 0.5417    steps: 123    lr: 4e-05     reward: 1.75\n",
      "epis: 1151   score: 0.0   mem len: 214800   epsilon: 0.5416    steps: 122    lr: 4e-05     reward: 1.74\n",
      "epis: 1152   score: 1.0   mem len: 214971   epsilon: 0.5413    steps: 171    lr: 4e-05     reward: 1.74\n",
      "epis: 1153   score: 4.0   mem len: 215267   epsilon: 0.5409    steps: 296    lr: 4e-05     reward: 1.73\n",
      "epis: 1154   score: 0.0   mem len: 215390   epsilon: 0.5408    steps: 123    lr: 4e-05     reward: 1.71\n",
      "epis: 1155   score: 1.0   mem len: 215541   epsilon: 0.5406    steps: 151    lr: 4e-05     reward: 1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1156   score: 0.0   mem len: 215664   epsilon: 0.5404    steps: 123    lr: 4e-05     reward: 1.71\n",
      "epis: 1157   score: 3.0   mem len: 215890   epsilon: 0.5401    steps: 226    lr: 4e-05     reward: 1.74\n",
      "epis: 1158   score: 0.0   mem len: 216013   epsilon: 0.5399    steps: 123    lr: 4e-05     reward: 1.72\n",
      "epis: 1159   score: 1.0   mem len: 216163   epsilon: 0.5397    steps: 150    lr: 4e-05     reward: 1.72\n",
      "epis: 1160   score: 1.0   mem len: 216314   epsilon: 0.5395    steps: 151    lr: 4e-05     reward: 1.7\n",
      "epis: 1161   score: 3.0   mem len: 216582   epsilon: 0.5391    steps: 268    lr: 4e-05     reward: 1.73\n",
      "epis: 1162   score: 0.0   mem len: 216705   epsilon: 0.5389    steps: 123    lr: 4e-05     reward: 1.71\n",
      "epis: 1163   score: 1.0   mem len: 216874   epsilon: 0.5387    steps: 169    lr: 4e-05     reward: 1.69\n",
      "epis: 1164   score: 2.0   mem len: 217071   epsilon: 0.5384    steps: 197    lr: 4e-05     reward: 1.7\n",
      "epis: 1165   score: 0.0   mem len: 217194   epsilon: 0.5383    steps: 123    lr: 4e-05     reward: 1.65\n",
      "epis: 1166   score: 1.0   mem len: 217344   epsilon: 0.5381    steps: 150    lr: 4e-05     reward: 1.65\n",
      "epis: 1167   score: 3.0   mem len: 217569   epsilon: 0.5378    steps: 225    lr: 4e-05     reward: 1.67\n",
      "epis: 1168   score: 0.0   mem len: 217692   epsilon: 0.5376    steps: 123    lr: 4e-05     reward: 1.62\n",
      "epis: 1169   score: 0.0   mem len: 217815   epsilon: 0.5374    steps: 123    lr: 4e-05     reward: 1.6\n",
      "epis: 1170   score: 1.0   mem len: 217986   epsilon: 0.5372    steps: 171    lr: 4e-05     reward: 1.58\n",
      "epis: 1171   score: 0.0   mem len: 218109   epsilon: 0.537    steps: 123    lr: 4e-05     reward: 1.57\n",
      "epis: 1172   score: 2.0   mem len: 218307   epsilon: 0.5367    steps: 198    lr: 4e-05     reward: 1.58\n",
      "epis: 1173   score: 0.0   mem len: 218430   epsilon: 0.5366    steps: 123    lr: 4e-05     reward: 1.58\n",
      "epis: 1174   score: 2.0   mem len: 218632   epsilon: 0.5363    steps: 202    lr: 4e-05     reward: 1.57\n",
      "epis: 1175   score: 2.0   mem len: 218830   epsilon: 0.536    steps: 198    lr: 4e-05     reward: 1.56\n",
      "epis: 1176   score: 2.0   mem len: 219028   epsilon: 0.5357    steps: 198    lr: 4e-05     reward: 1.54\n",
      "epis: 1177   score: 2.0   mem len: 219246   epsilon: 0.5354    steps: 218    lr: 4e-05     reward: 1.51\n",
      "epis: 1178   score: 4.0   mem len: 219526   epsilon: 0.5351    steps: 280    lr: 4e-05     reward: 1.54\n",
      "epis: 1179   score: 3.0   mem len: 219757   epsilon: 0.5347    steps: 231    lr: 4e-05     reward: 1.56\n",
      "epis: 1180   score: 1.0   mem len: 219926   epsilon: 0.5345    steps: 169    lr: 4e-05     reward: 1.53\n",
      "epis: 1181   score: 1.0   mem len: 220097   epsilon: 0.5343    steps: 171    lr: 4e-05     reward: 1.53\n",
      "epis: 1182   score: 1.0   mem len: 220269   epsilon: 0.534    steps: 172    lr: 4e-05     reward: 1.52\n",
      "epis: 1183   score: 4.0   mem len: 220547   epsilon: 0.5336    steps: 278    lr: 4e-05     reward: 1.55\n",
      "epis: 1184   score: 4.0   mem len: 220848   epsilon: 0.5332    steps: 301    lr: 4e-05     reward: 1.57\n",
      "epis: 1185   score: 3.0   mem len: 221074   epsilon: 0.5329    steps: 226    lr: 4e-05     reward: 1.6\n",
      "epis: 1186   score: 3.0   mem len: 221301   epsilon: 0.5326    steps: 227    lr: 4e-05     reward: 1.61\n",
      "epis: 1187   score: 1.0   mem len: 221473   epsilon: 0.5324    steps: 172    lr: 4e-05     reward: 1.6\n",
      "epis: 1188   score: 2.0   mem len: 221671   epsilon: 0.5321    steps: 198    lr: 4e-05     reward: 1.6\n",
      "epis: 1189   score: 1.0   mem len: 221841   epsilon: 0.5319    steps: 170    lr: 4e-05     reward: 1.6\n",
      "epis: 1190   score: 3.0   mem len: 222069   epsilon: 0.5315    steps: 228    lr: 4e-05     reward: 1.62\n",
      "epis: 1191   score: 3.0   mem len: 222317   epsilon: 0.5312    steps: 248    lr: 4e-05     reward: 1.58\n",
      "epis: 1192   score: 0.0   mem len: 222440   epsilon: 0.531    steps: 123    lr: 4e-05     reward: 1.56\n",
      "epis: 1193   score: 0.0   mem len: 222563   epsilon: 0.5309    steps: 123    lr: 4e-05     reward: 1.53\n",
      "epis: 1194   score: 3.0   mem len: 222791   epsilon: 0.5305    steps: 228    lr: 4e-05     reward: 1.51\n",
      "epis: 1195   score: 3.0   mem len: 223035   epsilon: 0.5302    steps: 244    lr: 4e-05     reward: 1.53\n",
      "epis: 1196   score: 2.0   mem len: 223233   epsilon: 0.5299    steps: 198    lr: 4e-05     reward: 1.5\n",
      "epis: 1197   score: 4.0   mem len: 223520   epsilon: 0.5295    steps: 287    lr: 4e-05     reward: 1.52\n",
      "epis: 1198   score: 4.0   mem len: 223795   epsilon: 0.5292    steps: 275    lr: 4e-05     reward: 1.56\n",
      "epis: 1199   score: 2.0   mem len: 224012   epsilon: 0.5289    steps: 217    lr: 4e-05     reward: 1.58\n",
      "epis: 1200   score: 0.0   mem len: 224135   epsilon: 0.5287    steps: 123    lr: 4e-05     reward: 1.57\n",
      "epis: 1201   score: 4.0   mem len: 224412   epsilon: 0.5283    steps: 277    lr: 4e-05     reward: 1.59\n",
      "epis: 1202   score: 2.0   mem len: 224632   epsilon: 0.528    steps: 220    lr: 4e-05     reward: 1.58\n",
      "epis: 1203   score: 1.0   mem len: 224800   epsilon: 0.5278    steps: 168    lr: 4e-05     reward: 1.59\n",
      "epis: 1204   score: 0.0   mem len: 224923   epsilon: 0.5276    steps: 123    lr: 4e-05     reward: 1.56\n",
      "epis: 1205   score: 1.0   mem len: 225091   epsilon: 0.5274    steps: 168    lr: 4e-05     reward: 1.56\n",
      "epis: 1206   score: 2.0   mem len: 225307   epsilon: 0.5271    steps: 216    lr: 4e-05     reward: 1.57\n",
      "epis: 1207   score: 2.0   mem len: 225505   epsilon: 0.5268    steps: 198    lr: 4e-05     reward: 1.56\n",
      "epis: 1208   score: 2.0   mem len: 225705   epsilon: 0.5265    steps: 200    lr: 4e-05     reward: 1.57\n",
      "epis: 1209   score: 0.0   mem len: 225828   epsilon: 0.5264    steps: 123    lr: 4e-05     reward: 1.55\n",
      "epis: 1210   score: 1.0   mem len: 225998   epsilon: 0.5261    steps: 170    lr: 4e-05     reward: 1.54\n",
      "epis: 1211   score: 1.0   mem len: 226149   epsilon: 0.5259    steps: 151    lr: 4e-05     reward: 1.55\n",
      "epis: 1212   score: 2.0   mem len: 226348   epsilon: 0.5256    steps: 199    lr: 4e-05     reward: 1.52\n",
      "epis: 1213   score: 1.0   mem len: 226500   epsilon: 0.5254    steps: 152    lr: 4e-05     reward: 1.51\n",
      "epis: 1214   score: 0.0   mem len: 226623   epsilon: 0.5253    steps: 123    lr: 4e-05     reward: 1.51\n",
      "epis: 1215   score: 2.0   mem len: 226840   epsilon: 0.525    steps: 217    lr: 4e-05     reward: 1.53\n",
      "epis: 1216   score: 3.0   mem len: 227087   epsilon: 0.5246    steps: 247    lr: 4e-05     reward: 1.54\n",
      "epis: 1217   score: 1.0   mem len: 227258   epsilon: 0.5244    steps: 171    lr: 4e-05     reward: 1.55\n",
      "epis: 1218   score: 2.0   mem len: 227479   epsilon: 0.5241    steps: 221    lr: 4e-05     reward: 1.53\n",
      "epis: 1219   score: 0.0   mem len: 227602   epsilon: 0.5239    steps: 123    lr: 4e-05     reward: 1.53\n",
      "epis: 1220   score: 0.0   mem len: 227725   epsilon: 0.5237    steps: 123    lr: 4e-05     reward: 1.49\n",
      "epis: 1221   score: 3.0   mem len: 227953   epsilon: 0.5234    steps: 228    lr: 4e-05     reward: 1.5\n",
      "epis: 1222   score: 0.0   mem len: 228075   epsilon: 0.5233    steps: 122    lr: 4e-05     reward: 1.5\n",
      "epis: 1223   score: 0.0   mem len: 228198   epsilon: 0.5231    steps: 123    lr: 4e-05     reward: 1.5\n",
      "epis: 1224   score: 1.0   mem len: 228367   epsilon: 0.5229    steps: 169    lr: 4e-05     reward: 1.49\n",
      "epis: 1225   score: 0.0   mem len: 228490   epsilon: 0.5227    steps: 123    lr: 4e-05     reward: 1.47\n",
      "epis: 1226   score: 0.0   mem len: 228612   epsilon: 0.5225    steps: 122    lr: 4e-05     reward: 1.46\n",
      "epis: 1227   score: 0.0   mem len: 228735   epsilon: 0.5223    steps: 123    lr: 4e-05     reward: 1.45\n",
      "epis: 1228   score: 0.0   mem len: 228857   epsilon: 0.5222    steps: 122    lr: 4e-05     reward: 1.45\n",
      "epis: 1229   score: 2.0   mem len: 229075   epsilon: 0.5219    steps: 218    lr: 4e-05     reward: 1.47\n",
      "epis: 1230   score: 1.0   mem len: 229244   epsilon: 0.5216    steps: 169    lr: 4e-05     reward: 1.45\n",
      "epis: 1231   score: 3.0   mem len: 229511   epsilon: 0.5213    steps: 267    lr: 4e-05     reward: 1.47\n",
      "epis: 1232   score: 0.0   mem len: 229634   epsilon: 0.5211    steps: 123    lr: 4e-05     reward: 1.47\n",
      "epis: 1233   score: 2.0   mem len: 229852   epsilon: 0.5208    steps: 218    lr: 4e-05     reward: 1.44\n",
      "epis: 1234   score: 1.0   mem len: 230002   epsilon: 0.5206    steps: 150    lr: 4e-05     reward: 1.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1235   score: 0.0   mem len: 230124   epsilon: 0.5204    steps: 122    lr: 4e-05     reward: 1.43\n",
      "epis: 1236   score: 4.0   mem len: 230420   epsilon: 0.52    steps: 296    lr: 4e-05     reward: 1.46\n",
      "epis: 1237   score: 4.0   mem len: 230698   epsilon: 0.5196    steps: 278    lr: 4e-05     reward: 1.49\n",
      "epis: 1238   score: 5.0   mem len: 231001   epsilon: 0.5192    steps: 303    lr: 4e-05     reward: 1.54\n",
      "epis: 1239   score: 5.0   mem len: 231326   epsilon: 0.5188    steps: 325    lr: 4e-05     reward: 1.58\n",
      "epis: 1240   score: 2.0   mem len: 231544   epsilon: 0.5185    steps: 218    lr: 4e-05     reward: 1.59\n",
      "epis: 1241   score: 0.0   mem len: 231667   epsilon: 0.5183    steps: 123    lr: 4e-05     reward: 1.59\n",
      "epis: 1242   score: 3.0   mem len: 231910   epsilon: 0.518    steps: 243    lr: 4e-05     reward: 1.55\n",
      "epis: 1243   score: 2.0   mem len: 232090   epsilon: 0.5177    steps: 180    lr: 4e-05     reward: 1.55\n",
      "epis: 1244   score: 0.0   mem len: 232213   epsilon: 0.5175    steps: 123    lr: 4e-05     reward: 1.54\n",
      "epis: 1245   score: 0.0   mem len: 232336   epsilon: 0.5174    steps: 123    lr: 4e-05     reward: 1.54\n",
      "epis: 1246   score: 3.0   mem len: 232601   epsilon: 0.517    steps: 265    lr: 4e-05     reward: 1.56\n",
      "epis: 1247   score: 2.0   mem len: 232798   epsilon: 0.5167    steps: 197    lr: 4e-05     reward: 1.56\n",
      "epis: 1248   score: 0.0   mem len: 232920   epsilon: 0.5166    steps: 122    lr: 4e-05     reward: 1.54\n",
      "epis: 1249   score: 5.0   mem len: 233255   epsilon: 0.5161    steps: 335    lr: 4e-05     reward: 1.59\n",
      "epis: 1250   score: 2.0   mem len: 233453   epsilon: 0.5158    steps: 198    lr: 4e-05     reward: 1.61\n",
      "epis: 1251   score: 1.0   mem len: 233622   epsilon: 0.5156    steps: 169    lr: 4e-05     reward: 1.62\n",
      "epis: 1252   score: 1.0   mem len: 233773   epsilon: 0.5154    steps: 151    lr: 4e-05     reward: 1.62\n",
      "epis: 1253   score: 3.0   mem len: 234019   epsilon: 0.5151    steps: 246    lr: 4e-05     reward: 1.61\n",
      "epis: 1254   score: 2.0   mem len: 234237   epsilon: 0.5148    steps: 218    lr: 4e-05     reward: 1.63\n",
      "epis: 1255   score: 2.0   mem len: 234435   epsilon: 0.5145    steps: 198    lr: 4e-05     reward: 1.64\n",
      "epis: 1256   score: 1.0   mem len: 234607   epsilon: 0.5142    steps: 172    lr: 4e-05     reward: 1.65\n",
      "epis: 1257   score: 3.0   mem len: 234874   epsilon: 0.5139    steps: 267    lr: 4e-05     reward: 1.65\n",
      "epis: 1258   score: 2.0   mem len: 235074   epsilon: 0.5136    steps: 200    lr: 4e-05     reward: 1.67\n",
      "epis: 1259   score: 4.0   mem len: 235369   epsilon: 0.5132    steps: 295    lr: 4e-05     reward: 1.7\n",
      "epis: 1260   score: 2.0   mem len: 235587   epsilon: 0.5129    steps: 218    lr: 4e-05     reward: 1.71\n",
      "epis: 1261   score: 2.0   mem len: 235786   epsilon: 0.5126    steps: 199    lr: 4e-05     reward: 1.7\n",
      "epis: 1262   score: 2.0   mem len: 235983   epsilon: 0.5123    steps: 197    lr: 4e-05     reward: 1.72\n",
      "epis: 1263   score: 0.0   mem len: 236106   epsilon: 0.5122    steps: 123    lr: 4e-05     reward: 1.71\n",
      "epis: 1264   score: 0.0   mem len: 236229   epsilon: 0.512    steps: 123    lr: 4e-05     reward: 1.69\n",
      "epis: 1265   score: 2.0   mem len: 236428   epsilon: 0.5117    steps: 199    lr: 4e-05     reward: 1.71\n",
      "epis: 1266   score: 2.0   mem len: 236646   epsilon: 0.5114    steps: 218    lr: 4e-05     reward: 1.72\n",
      "epis: 1267   score: 0.0   mem len: 236769   epsilon: 0.5113    steps: 123    lr: 4e-05     reward: 1.69\n",
      "epis: 1268   score: 1.0   mem len: 236922   epsilon: 0.511    steps: 153    lr: 4e-05     reward: 1.7\n",
      "epis: 1269   score: 2.0   mem len: 237141   epsilon: 0.5107    steps: 219    lr: 4e-05     reward: 1.72\n",
      "epis: 1270   score: 3.0   mem len: 237388   epsilon: 0.5104    steps: 247    lr: 4e-05     reward: 1.74\n",
      "epis: 1271   score: 0.0   mem len: 237511   epsilon: 0.5102    steps: 123    lr: 4e-05     reward: 1.74\n",
      "epis: 1272   score: 1.0   mem len: 237680   epsilon: 0.51    steps: 169    lr: 4e-05     reward: 1.73\n",
      "epis: 1273   score: 4.0   mem len: 237974   epsilon: 0.5096    steps: 294    lr: 4e-05     reward: 1.77\n",
      "epis: 1274   score: 2.0   mem len: 238190   epsilon: 0.5093    steps: 216    lr: 4e-05     reward: 1.77\n",
      "epis: 1275   score: 0.0   mem len: 238313   epsilon: 0.5091    steps: 123    lr: 4e-05     reward: 1.75\n",
      "epis: 1276   score: 2.0   mem len: 238531   epsilon: 0.5088    steps: 218    lr: 4e-05     reward: 1.75\n",
      "epis: 1277   score: 3.0   mem len: 238757   epsilon: 0.5085    steps: 226    lr: 4e-05     reward: 1.76\n",
      "epis: 1278   score: 2.0   mem len: 238955   epsilon: 0.5082    steps: 198    lr: 4e-05     reward: 1.74\n",
      "epis: 1279   score: 1.0   mem len: 239124   epsilon: 0.508    steps: 169    lr: 4e-05     reward: 1.72\n",
      "epis: 1280   score: 1.0   mem len: 239293   epsilon: 0.5078    steps: 169    lr: 4e-05     reward: 1.72\n",
      "epis: 1281   score: 1.0   mem len: 239463   epsilon: 0.5075    steps: 170    lr: 4e-05     reward: 1.72\n",
      "epis: 1282   score: 1.0   mem len: 239614   epsilon: 0.5073    steps: 151    lr: 4e-05     reward: 1.72\n",
      "epis: 1283   score: 2.0   mem len: 239812   epsilon: 0.5071    steps: 198    lr: 4e-05     reward: 1.7\n",
      "epis: 1284   score: 0.0   mem len: 239934   epsilon: 0.5069    steps: 122    lr: 4e-05     reward: 1.66\n",
      "epis: 1285   score: 3.0   mem len: 240161   epsilon: 0.5066    steps: 227    lr: 4e-05     reward: 1.66\n",
      "epis: 1286   score: 3.0   mem len: 240407   epsilon: 0.5062    steps: 246    lr: 4e-05     reward: 1.66\n",
      "epis: 1287   score: 1.0   mem len: 240578   epsilon: 0.506    steps: 171    lr: 4e-05     reward: 1.66\n",
      "epis: 1288   score: 0.0   mem len: 240701   epsilon: 0.5058    steps: 123    lr: 4e-05     reward: 1.64\n",
      "epis: 1289   score: 0.0   mem len: 240824   epsilon: 0.5057    steps: 123    lr: 4e-05     reward: 1.63\n",
      "epis: 1290   score: 3.0   mem len: 241075   epsilon: 0.5053    steps: 251    lr: 4e-05     reward: 1.63\n",
      "epis: 1291   score: 2.0   mem len: 241293   epsilon: 0.505    steps: 218    lr: 4e-05     reward: 1.62\n",
      "epis: 1292   score: 0.0   mem len: 241415   epsilon: 0.5048    steps: 122    lr: 4e-05     reward: 1.62\n",
      "epis: 1293   score: 2.0   mem len: 241633   epsilon: 0.5045    steps: 218    lr: 4e-05     reward: 1.64\n",
      "epis: 1294   score: 2.0   mem len: 241831   epsilon: 0.5043    steps: 198    lr: 4e-05     reward: 1.63\n",
      "epis: 1295   score: 3.0   mem len: 242075   epsilon: 0.5039    steps: 244    lr: 4e-05     reward: 1.63\n",
      "epis: 1296   score: 1.0   mem len: 242245   epsilon: 0.5037    steps: 170    lr: 4e-05     reward: 1.62\n",
      "epis: 1297   score: 0.0   mem len: 242368   epsilon: 0.5035    steps: 123    lr: 4e-05     reward: 1.58\n",
      "epis: 1298   score: 0.0   mem len: 242490   epsilon: 0.5034    steps: 122    lr: 4e-05     reward: 1.54\n",
      "epis: 1299   score: 0.0   mem len: 242613   epsilon: 0.5032    steps: 123    lr: 4e-05     reward: 1.52\n",
      "epis: 1300   score: 1.0   mem len: 242782   epsilon: 0.503    steps: 169    lr: 4e-05     reward: 1.53\n",
      "epis: 1301   score: 2.0   mem len: 243003   epsilon: 0.5027    steps: 221    lr: 4e-05     reward: 1.51\n",
      "epis: 1302   score: 1.0   mem len: 243154   epsilon: 0.5024    steps: 151    lr: 4e-05     reward: 1.5\n",
      "epis: 1303   score: 1.0   mem len: 243323   epsilon: 0.5022    steps: 169    lr: 4e-05     reward: 1.5\n",
      "epis: 1304   score: 0.0   mem len: 243446   epsilon: 0.502    steps: 123    lr: 4e-05     reward: 1.5\n",
      "epis: 1305   score: 2.0   mem len: 243644   epsilon: 0.5018    steps: 198    lr: 4e-05     reward: 1.51\n",
      "epis: 1306   score: 2.0   mem len: 243842   epsilon: 0.5015    steps: 198    lr: 4e-05     reward: 1.51\n",
      "epis: 1307   score: 1.0   mem len: 244012   epsilon: 0.5013    steps: 170    lr: 4e-05     reward: 1.5\n",
      "epis: 1308   score: 1.0   mem len: 244180   epsilon: 0.501    steps: 168    lr: 4e-05     reward: 1.49\n",
      "epis: 1309   score: 0.0   mem len: 244303   epsilon: 0.5009    steps: 123    lr: 4e-05     reward: 1.49\n",
      "epis: 1310   score: 0.0   mem len: 244426   epsilon: 0.5007    steps: 123    lr: 4e-05     reward: 1.48\n",
      "epis: 1311   score: 0.0   mem len: 244548   epsilon: 0.5005    steps: 122    lr: 4e-05     reward: 1.47\n",
      "epis: 1312   score: 2.0   mem len: 244766   epsilon: 0.5002    steps: 218    lr: 4e-05     reward: 1.47\n",
      "epis: 1313   score: 5.0   mem len: 245108   epsilon: 0.4997    steps: 342    lr: 4e-05     reward: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1314   score: 3.0   mem len: 245333   epsilon: 0.4994    steps: 225    lr: 4e-05     reward: 1.54\n",
      "epis: 1315   score: 1.0   mem len: 245503   epsilon: 0.4992    steps: 170    lr: 4e-05     reward: 1.53\n",
      "epis: 1316   score: 3.0   mem len: 245730   epsilon: 0.4989    steps: 227    lr: 4e-05     reward: 1.53\n",
      "epis: 1317   score: 1.0   mem len: 245881   epsilon: 0.4987    steps: 151    lr: 4e-05     reward: 1.53\n",
      "epis: 1318   score: 1.0   mem len: 246050   epsilon: 0.4984    steps: 169    lr: 4e-05     reward: 1.52\n",
      "epis: 1319   score: 2.0   mem len: 246269   epsilon: 0.4981    steps: 219    lr: 4e-05     reward: 1.54\n",
      "epis: 1320   score: 0.0   mem len: 246391   epsilon: 0.498    steps: 122    lr: 4e-05     reward: 1.54\n",
      "epis: 1321   score: 0.0   mem len: 246514   epsilon: 0.4978    steps: 123    lr: 4e-05     reward: 1.51\n",
      "epis: 1322   score: 0.0   mem len: 246637   epsilon: 0.4976    steps: 123    lr: 4e-05     reward: 1.51\n",
      "epis: 1323   score: 0.0   mem len: 246760   epsilon: 0.4975    steps: 123    lr: 4e-05     reward: 1.51\n",
      "epis: 1324   score: 0.0   mem len: 246883   epsilon: 0.4973    steps: 123    lr: 4e-05     reward: 1.5\n",
      "epis: 1325   score: 0.0   mem len: 247006   epsilon: 0.4971    steps: 123    lr: 4e-05     reward: 1.5\n",
      "epis: 1326   score: 1.0   mem len: 247178   epsilon: 0.4969    steps: 172    lr: 4e-05     reward: 1.51\n",
      "epis: 1327   score: 2.0   mem len: 247396   epsilon: 0.4966    steps: 218    lr: 4e-05     reward: 1.53\n",
      "epis: 1328   score: 0.0   mem len: 247519   epsilon: 0.4964    steps: 123    lr: 4e-05     reward: 1.53\n",
      "epis: 1329   score: 1.0   mem len: 247670   epsilon: 0.4962    steps: 151    lr: 4e-05     reward: 1.52\n",
      "epis: 1330   score: 2.0   mem len: 247868   epsilon: 0.4959    steps: 198    lr: 4e-05     reward: 1.53\n",
      "epis: 1331   score: 2.0   mem len: 248083   epsilon: 0.4956    steps: 215    lr: 4e-05     reward: 1.52\n",
      "epis: 1332   score: 3.0   mem len: 248328   epsilon: 0.4953    steps: 245    lr: 4e-05     reward: 1.55\n",
      "epis: 1333   score: 2.0   mem len: 248526   epsilon: 0.495    steps: 198    lr: 4e-05     reward: 1.55\n",
      "epis: 1334   score: 2.0   mem len: 248744   epsilon: 0.4947    steps: 218    lr: 4e-05     reward: 1.56\n",
      "epis: 1335   score: 1.0   mem len: 248915   epsilon: 0.4945    steps: 171    lr: 4e-05     reward: 1.57\n",
      "epis: 1336   score: 2.0   mem len: 249135   epsilon: 0.4942    steps: 220    lr: 4e-05     reward: 1.55\n",
      "epis: 1337   score: 1.0   mem len: 249286   epsilon: 0.494    steps: 151    lr: 4e-05     reward: 1.52\n",
      "epis: 1338   score: 1.0   mem len: 249458   epsilon: 0.4937    steps: 172    lr: 4e-05     reward: 1.48\n",
      "epis: 1339   score: 1.0   mem len: 249630   epsilon: 0.4935    steps: 172    lr: 4e-05     reward: 1.44\n",
      "epis: 1340   score: 3.0   mem len: 249875   epsilon: 0.4932    steps: 245    lr: 4e-05     reward: 1.45\n",
      "epis: 1341   score: 4.0   mem len: 250151   epsilon: 0.4928    steps: 276    lr: 4e-05     reward: 1.49\n",
      "epis: 1342   score: 0.0   mem len: 250274   epsilon: 0.4926    steps: 123    lr: 4e-05     reward: 1.46\n",
      "epis: 1343   score: 2.0   mem len: 250474   epsilon: 0.4923    steps: 200    lr: 4e-05     reward: 1.46\n",
      "epis: 1344   score: 0.0   mem len: 250597   epsilon: 0.4922    steps: 123    lr: 4e-05     reward: 1.46\n",
      "epis: 1345   score: 2.0   mem len: 250796   epsilon: 0.4919    steps: 199    lr: 4e-05     reward: 1.48\n",
      "epis: 1346   score: 0.0   mem len: 250919   epsilon: 0.4917    steps: 123    lr: 4e-05     reward: 1.45\n",
      "epis: 1347   score: 2.0   mem len: 251100   epsilon: 0.4915    steps: 181    lr: 4e-05     reward: 1.45\n",
      "epis: 1348   score: 0.0   mem len: 251223   epsilon: 0.4913    steps: 123    lr: 4e-05     reward: 1.45\n",
      "epis: 1349   score: 1.0   mem len: 251393   epsilon: 0.4911    steps: 170    lr: 4e-05     reward: 1.41\n",
      "epis: 1350   score: 5.0   mem len: 251738   epsilon: 0.4906    steps: 345    lr: 4e-05     reward: 1.44\n",
      "epis: 1351   score: 0.0   mem len: 251861   epsilon: 0.4904    steps: 123    lr: 4e-05     reward: 1.43\n",
      "epis: 1352   score: 4.0   mem len: 252156   epsilon: 0.49    steps: 295    lr: 4e-05     reward: 1.46\n",
      "epis: 1353   score: 4.0   mem len: 252450   epsilon: 0.4896    steps: 294    lr: 4e-05     reward: 1.47\n",
      "epis: 1354   score: 1.0   mem len: 252622   epsilon: 0.4894    steps: 172    lr: 4e-05     reward: 1.46\n",
      "epis: 1355   score: 0.0   mem len: 252745   epsilon: 0.4892    steps: 123    lr: 4e-05     reward: 1.44\n",
      "epis: 1356   score: 0.0   mem len: 252868   epsilon: 0.489    steps: 123    lr: 4e-05     reward: 1.43\n",
      "epis: 1357   score: 1.0   mem len: 253038   epsilon: 0.4888    steps: 170    lr: 4e-05     reward: 1.41\n",
      "epis: 1358   score: 3.0   mem len: 253288   epsilon: 0.4885    steps: 250    lr: 4e-05     reward: 1.42\n",
      "epis: 1359   score: 7.0   mem len: 253666   epsilon: 0.4879    steps: 378    lr: 4e-05     reward: 1.45\n",
      "epis: 1360   score: 1.0   mem len: 253817   epsilon: 0.4877    steps: 151    lr: 4e-05     reward: 1.44\n",
      "epis: 1361   score: 1.0   mem len: 253968   epsilon: 0.4875    steps: 151    lr: 4e-05     reward: 1.43\n",
      "epis: 1362   score: 0.0   mem len: 254090   epsilon: 0.4874    steps: 122    lr: 4e-05     reward: 1.41\n",
      "epis: 1363   score: 4.0   mem len: 254387   epsilon: 0.4869    steps: 297    lr: 4e-05     reward: 1.45\n",
      "epis: 1364   score: 2.0   mem len: 254585   epsilon: 0.4867    steps: 198    lr: 4e-05     reward: 1.47\n",
      "epis: 1365   score: 4.0   mem len: 254844   epsilon: 0.4863    steps: 259    lr: 4e-05     reward: 1.49\n",
      "epis: 1366   score: 4.0   mem len: 255104   epsilon: 0.486    steps: 260    lr: 4e-05     reward: 1.51\n",
      "epis: 1367   score: 0.0   mem len: 255226   epsilon: 0.4858    steps: 122    lr: 4e-05     reward: 1.51\n",
      "epis: 1368   score: 1.0   mem len: 255398   epsilon: 0.4855    steps: 172    lr: 4e-05     reward: 1.51\n",
      "epis: 1369   score: 4.0   mem len: 255713   epsilon: 0.4851    steps: 315    lr: 4e-05     reward: 1.53\n",
      "epis: 1370   score: 3.0   mem len: 255942   epsilon: 0.4848    steps: 229    lr: 4e-05     reward: 1.53\n",
      "epis: 1371   score: 2.0   mem len: 256159   epsilon: 0.4845    steps: 217    lr: 4e-05     reward: 1.55\n",
      "epis: 1372   score: 0.0   mem len: 256282   epsilon: 0.4843    steps: 123    lr: 4e-05     reward: 1.54\n",
      "epis: 1373   score: 0.0   mem len: 256404   epsilon: 0.4842    steps: 122    lr: 4e-05     reward: 1.5\n",
      "epis: 1374   score: 3.0   mem len: 256653   epsilon: 0.4838    steps: 249    lr: 4e-05     reward: 1.51\n",
      "epis: 1375   score: 0.0   mem len: 256775   epsilon: 0.4836    steps: 122    lr: 4e-05     reward: 1.51\n",
      "epis: 1376   score: 2.0   mem len: 256976   epsilon: 0.4834    steps: 201    lr: 4e-05     reward: 1.51\n",
      "epis: 1377   score: 1.0   mem len: 257145   epsilon: 0.4831    steps: 169    lr: 4e-05     reward: 1.49\n",
      "epis: 1378   score: 1.0   mem len: 257315   epsilon: 0.4829    steps: 170    lr: 4e-05     reward: 1.48\n",
      "epis: 1379   score: 0.0   mem len: 257438   epsilon: 0.4827    steps: 123    lr: 4e-05     reward: 1.47\n",
      "epis: 1380   score: 2.0   mem len: 257635   epsilon: 0.4825    steps: 197    lr: 4e-05     reward: 1.48\n",
      "epis: 1381   score: 4.0   mem len: 257916   epsilon: 0.4821    steps: 281    lr: 4e-05     reward: 1.51\n",
      "epis: 1382   score: 2.0   mem len: 258113   epsilon: 0.4818    steps: 197    lr: 4e-05     reward: 1.52\n",
      "epis: 1383   score: 3.0   mem len: 258378   epsilon: 0.4814    steps: 265    lr: 4e-05     reward: 1.53\n",
      "epis: 1384   score: 2.0   mem len: 258596   epsilon: 0.4811    steps: 218    lr: 4e-05     reward: 1.55\n",
      "epis: 1385   score: 4.0   mem len: 258892   epsilon: 0.4807    steps: 296    lr: 4e-05     reward: 1.56\n",
      "epis: 1386   score: 0.0   mem len: 259015   epsilon: 0.4806    steps: 123    lr: 4e-05     reward: 1.53\n",
      "epis: 1387   score: 2.0   mem len: 259230   epsilon: 0.4803    steps: 215    lr: 4e-05     reward: 1.54\n",
      "epis: 1388   score: 2.0   mem len: 259427   epsilon: 0.48    steps: 197    lr: 4e-05     reward: 1.56\n",
      "epis: 1389   score: 1.0   mem len: 259579   epsilon: 0.4798    steps: 152    lr: 4e-05     reward: 1.57\n",
      "epis: 1390   score: 0.0   mem len: 259702   epsilon: 0.4796    steps: 123    lr: 4e-05     reward: 1.54\n",
      "epis: 1391   score: 1.0   mem len: 259871   epsilon: 0.4794    steps: 169    lr: 4e-05     reward: 1.53\n",
      "epis: 1392   score: 2.0   mem len: 260069   epsilon: 0.4791    steps: 198    lr: 4e-05     reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1393   score: 3.0   mem len: 260313   epsilon: 0.4788    steps: 244    lr: 4e-05     reward: 1.56\n",
      "epis: 1394   score: 0.0   mem len: 260435   epsilon: 0.4786    steps: 122    lr: 4e-05     reward: 1.54\n",
      "epis: 1395   score: 1.0   mem len: 260585   epsilon: 0.4784    steps: 150    lr: 4e-05     reward: 1.52\n",
      "epis: 1396   score: 3.0   mem len: 260829   epsilon: 0.4781    steps: 244    lr: 4e-05     reward: 1.54\n",
      "epis: 1397   score: 3.0   mem len: 261073   epsilon: 0.4777    steps: 244    lr: 4e-05     reward: 1.57\n",
      "epis: 1398   score: 1.0   mem len: 261242   epsilon: 0.4775    steps: 169    lr: 4e-05     reward: 1.58\n",
      "epis: 1399   score: 3.0   mem len: 261467   epsilon: 0.4772    steps: 225    lr: 4e-05     reward: 1.61\n",
      "epis: 1400   score: 3.0   mem len: 261714   epsilon: 0.4768    steps: 247    lr: 4e-05     reward: 1.63\n",
      "epis: 1401   score: 2.0   mem len: 261930   epsilon: 0.4765    steps: 216    lr: 4e-05     reward: 1.63\n",
      "epis: 1402   score: 0.0   mem len: 262052   epsilon: 0.4764    steps: 122    lr: 4e-05     reward: 1.62\n",
      "epis: 1403   score: 1.0   mem len: 262221   epsilon: 0.4761    steps: 169    lr: 4e-05     reward: 1.62\n",
      "epis: 1404   score: 2.0   mem len: 262401   epsilon: 0.4759    steps: 180    lr: 4e-05     reward: 1.64\n",
      "epis: 1405   score: 0.0   mem len: 262523   epsilon: 0.4757    steps: 122    lr: 4e-05     reward: 1.62\n",
      "epis: 1406   score: 2.0   mem len: 262721   epsilon: 0.4754    steps: 198    lr: 4e-05     reward: 1.62\n",
      "epis: 1407   score: 1.0   mem len: 262871   epsilon: 0.4752    steps: 150    lr: 4e-05     reward: 1.62\n",
      "epis: 1408   score: 0.0   mem len: 262994   epsilon: 0.4751    steps: 123    lr: 4e-05     reward: 1.61\n",
      "epis: 1409   score: 0.0   mem len: 263116   epsilon: 0.4749    steps: 122    lr: 4e-05     reward: 1.61\n",
      "epis: 1410   score: 2.0   mem len: 263314   epsilon: 0.4746    steps: 198    lr: 4e-05     reward: 1.63\n",
      "epis: 1411   score: 3.0   mem len: 263563   epsilon: 0.4743    steps: 249    lr: 4e-05     reward: 1.66\n",
      "epis: 1412   score: 4.0   mem len: 263820   epsilon: 0.4739    steps: 257    lr: 4e-05     reward: 1.68\n",
      "epis: 1413   score: 1.0   mem len: 263971   epsilon: 0.4737    steps: 151    lr: 4e-05     reward: 1.64\n",
      "epis: 1414   score: 1.0   mem len: 264121   epsilon: 0.4735    steps: 150    lr: 4e-05     reward: 1.62\n",
      "epis: 1415   score: 3.0   mem len: 264390   epsilon: 0.4731    steps: 269    lr: 4e-05     reward: 1.64\n",
      "epis: 1416   score: 4.0   mem len: 264686   epsilon: 0.4727    steps: 296    lr: 4e-05     reward: 1.65\n",
      "epis: 1417   score: 3.0   mem len: 264951   epsilon: 0.4724    steps: 265    lr: 4e-05     reward: 1.67\n",
      "epis: 1418   score: 2.0   mem len: 265132   epsilon: 0.4721    steps: 181    lr: 4e-05     reward: 1.68\n",
      "epis: 1419   score: 2.0   mem len: 265311   epsilon: 0.4719    steps: 179    lr: 4e-05     reward: 1.68\n",
      "epis: 1420   score: 0.0   mem len: 265434   epsilon: 0.4717    steps: 123    lr: 4e-05     reward: 1.68\n",
      "epis: 1421   score: 1.0   mem len: 265603   epsilon: 0.4715    steps: 169    lr: 4e-05     reward: 1.69\n",
      "epis: 1422   score: 2.0   mem len: 265819   epsilon: 0.4712    steps: 216    lr: 4e-05     reward: 1.71\n",
      "epis: 1423   score: 3.0   mem len: 266048   epsilon: 0.4709    steps: 229    lr: 4e-05     reward: 1.74\n",
      "epis: 1424   score: 2.0   mem len: 266266   epsilon: 0.4706    steps: 218    lr: 4e-05     reward: 1.76\n",
      "epis: 1425   score: 3.0   mem len: 266513   epsilon: 0.4702    steps: 247    lr: 4e-05     reward: 1.79\n",
      "epis: 1426   score: 1.0   mem len: 266682   epsilon: 0.47    steps: 169    lr: 4e-05     reward: 1.79\n",
      "epis: 1427   score: 1.0   mem len: 266832   epsilon: 0.4698    steps: 150    lr: 4e-05     reward: 1.78\n",
      "epis: 1428   score: 1.0   mem len: 266982   epsilon: 0.4696    steps: 150    lr: 4e-05     reward: 1.79\n",
      "epis: 1429   score: 3.0   mem len: 267228   epsilon: 0.4692    steps: 246    lr: 4e-05     reward: 1.81\n",
      "epis: 1430   score: 0.0   mem len: 267351   epsilon: 0.4691    steps: 123    lr: 4e-05     reward: 1.79\n",
      "epis: 1431   score: 1.0   mem len: 267519   epsilon: 0.4688    steps: 168    lr: 4e-05     reward: 1.78\n",
      "epis: 1432   score: 3.0   mem len: 267765   epsilon: 0.4685    steps: 246    lr: 4e-05     reward: 1.78\n",
      "epis: 1433   score: 1.0   mem len: 267936   epsilon: 0.4682    steps: 171    lr: 4e-05     reward: 1.77\n",
      "epis: 1434   score: 2.0   mem len: 268153   epsilon: 0.4679    steps: 217    lr: 4e-05     reward: 1.77\n",
      "epis: 1435   score: 0.0   mem len: 268275   epsilon: 0.4678    steps: 122    lr: 4e-05     reward: 1.76\n",
      "epis: 1436   score: 1.0   mem len: 268426   epsilon: 0.4676    steps: 151    lr: 4e-05     reward: 1.75\n",
      "epis: 1437   score: 0.0   mem len: 268549   epsilon: 0.4674    steps: 123    lr: 4e-05     reward: 1.74\n",
      "epis: 1438   score: 2.0   mem len: 268767   epsilon: 0.4671    steps: 218    lr: 4e-05     reward: 1.75\n",
      "epis: 1439   score: 2.0   mem len: 268965   epsilon: 0.4668    steps: 198    lr: 4e-05     reward: 1.76\n",
      "epis: 1440   score: 3.0   mem len: 269191   epsilon: 0.4665    steps: 226    lr: 4e-05     reward: 1.76\n",
      "epis: 1441   score: 2.0   mem len: 269388   epsilon: 0.4662    steps: 197    lr: 4e-05     reward: 1.74\n",
      "epis: 1442   score: 3.0   mem len: 269634   epsilon: 0.4659    steps: 246    lr: 4e-05     reward: 1.77\n",
      "epis: 1443   score: 3.0   mem len: 269880   epsilon: 0.4656    steps: 246    lr: 4e-05     reward: 1.78\n",
      "epis: 1444   score: 2.0   mem len: 270078   epsilon: 0.4653    steps: 198    lr: 4e-05     reward: 1.8\n",
      "epis: 1445   score: 1.0   mem len: 270228   epsilon: 0.4651    steps: 150    lr: 4e-05     reward: 1.79\n",
      "epis: 1446   score: 0.0   mem len: 270351   epsilon: 0.4649    steps: 123    lr: 4e-05     reward: 1.79\n",
      "epis: 1447   score: 1.0   mem len: 270521   epsilon: 0.4647    steps: 170    lr: 4e-05     reward: 1.78\n",
      "epis: 1448   score: 1.0   mem len: 270671   epsilon: 0.4645    steps: 150    lr: 4e-05     reward: 1.79\n",
      "epis: 1449   score: 4.0   mem len: 270940   epsilon: 0.4641    steps: 269    lr: 4e-05     reward: 1.82\n",
      "epis: 1450   score: 2.0   mem len: 271158   epsilon: 0.4638    steps: 218    lr: 4e-05     reward: 1.79\n",
      "epis: 1451   score: 2.0   mem len: 271376   epsilon: 0.4635    steps: 218    lr: 4e-05     reward: 1.81\n",
      "epis: 1452   score: 0.0   mem len: 271498   epsilon: 0.4633    steps: 122    lr: 4e-05     reward: 1.77\n",
      "epis: 1453   score: 3.0   mem len: 271723   epsilon: 0.463    steps: 225    lr: 4e-05     reward: 1.76\n",
      "epis: 1454   score: 0.0   mem len: 271846   epsilon: 0.4629    steps: 123    lr: 4e-05     reward: 1.75\n",
      "epis: 1455   score: 1.0   mem len: 272014   epsilon: 0.4626    steps: 168    lr: 4e-05     reward: 1.76\n",
      "epis: 1456   score: 1.0   mem len: 272165   epsilon: 0.4624    steps: 151    lr: 4e-05     reward: 1.77\n",
      "epis: 1457   score: 2.0   mem len: 272364   epsilon: 0.4621    steps: 199    lr: 4e-05     reward: 1.78\n",
      "epis: 1458   score: 2.0   mem len: 272581   epsilon: 0.4618    steps: 217    lr: 4e-05     reward: 1.77\n",
      "epis: 1459   score: 1.0   mem len: 272750   epsilon: 0.4616    steps: 169    lr: 4e-05     reward: 1.71\n",
      "epis: 1460   score: 1.0   mem len: 272922   epsilon: 0.4614    steps: 172    lr: 4e-05     reward: 1.71\n",
      "epis: 1461   score: 1.0   mem len: 273073   epsilon: 0.4612    steps: 151    lr: 4e-05     reward: 1.71\n",
      "epis: 1462   score: 2.0   mem len: 273291   epsilon: 0.4609    steps: 218    lr: 4e-05     reward: 1.73\n",
      "epis: 1463   score: 2.0   mem len: 273488   epsilon: 0.4606    steps: 197    lr: 4e-05     reward: 1.71\n",
      "epis: 1464   score: 3.0   mem len: 273733   epsilon: 0.4602    steps: 245    lr: 4e-05     reward: 1.72\n",
      "epis: 1465   score: 2.0   mem len: 273933   epsilon: 0.46    steps: 200    lr: 4e-05     reward: 1.7\n",
      "epis: 1466   score: 3.0   mem len: 274178   epsilon: 0.4596    steps: 245    lr: 4e-05     reward: 1.69\n",
      "epis: 1467   score: 2.0   mem len: 274375   epsilon: 0.4594    steps: 197    lr: 4e-05     reward: 1.71\n",
      "epis: 1468   score: 5.0   mem len: 274701   epsilon: 0.4589    steps: 326    lr: 4e-05     reward: 1.75\n",
      "epis: 1469   score: 3.0   mem len: 274950   epsilon: 0.4586    steps: 249    lr: 4e-05     reward: 1.74\n",
      "epis: 1470   score: 0.0   mem len: 275072   epsilon: 0.4584    steps: 122    lr: 4e-05     reward: 1.71\n",
      "epis: 1471   score: 4.0   mem len: 275349   epsilon: 0.458    steps: 277    lr: 4e-05     reward: 1.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1472   score: 5.0   mem len: 275695   epsilon: 0.4575    steps: 346    lr: 4e-05     reward: 1.78\n",
      "epis: 1473   score: 1.0   mem len: 275864   epsilon: 0.4573    steps: 169    lr: 4e-05     reward: 1.79\n",
      "epis: 1474   score: 2.0   mem len: 276063   epsilon: 0.457    steps: 199    lr: 4e-05     reward: 1.78\n",
      "epis: 1475   score: 4.0   mem len: 276358   epsilon: 0.4566    steps: 295    lr: 4e-05     reward: 1.82\n",
      "epis: 1476   score: 1.0   mem len: 276528   epsilon: 0.4564    steps: 170    lr: 4e-05     reward: 1.81\n",
      "epis: 1477   score: 1.0   mem len: 276699   epsilon: 0.4562    steps: 171    lr: 4e-05     reward: 1.81\n",
      "epis: 1478   score: 5.0   mem len: 277045   epsilon: 0.4557    steps: 346    lr: 4e-05     reward: 1.85\n",
      "epis: 1479   score: 1.0   mem len: 277214   epsilon: 0.4554    steps: 169    lr: 4e-05     reward: 1.86\n",
      "epis: 1480   score: 4.0   mem len: 277489   epsilon: 0.4551    steps: 275    lr: 4e-05     reward: 1.88\n",
      "epis: 1481   score: 3.0   mem len: 277735   epsilon: 0.4547    steps: 246    lr: 4e-05     reward: 1.87\n",
      "epis: 1482   score: 4.0   mem len: 278031   epsilon: 0.4543    steps: 296    lr: 4e-05     reward: 1.89\n",
      "epis: 1483   score: 3.0   mem len: 278258   epsilon: 0.454    steps: 227    lr: 4e-05     reward: 1.89\n",
      "epis: 1484   score: 2.0   mem len: 278476   epsilon: 0.4537    steps: 218    lr: 4e-05     reward: 1.89\n",
      "epis: 1485   score: 2.0   mem len: 278673   epsilon: 0.4534    steps: 197    lr: 4e-05     reward: 1.87\n",
      "epis: 1486   score: 2.0   mem len: 278874   epsilon: 0.4532    steps: 201    lr: 4e-05     reward: 1.89\n",
      "epis: 1487   score: 4.0   mem len: 279172   epsilon: 0.4527    steps: 298    lr: 4e-05     reward: 1.91\n",
      "epis: 1488   score: 3.0   mem len: 279417   epsilon: 0.4524    steps: 245    lr: 4e-05     reward: 1.92\n",
      "epis: 1489   score: 2.0   mem len: 279634   epsilon: 0.4521    steps: 217    lr: 4e-05     reward: 1.93\n",
      "epis: 1490   score: 2.0   mem len: 279851   epsilon: 0.4518    steps: 217    lr: 4e-05     reward: 1.95\n",
      "epis: 1491   score: 1.0   mem len: 280019   epsilon: 0.4516    steps: 168    lr: 4e-05     reward: 1.95\n",
      "epis: 1492   score: 2.0   mem len: 280218   epsilon: 0.4513    steps: 199    lr: 4e-05     reward: 1.95\n",
      "epis: 1493   score: 1.0   mem len: 280387   epsilon: 0.4511    steps: 169    lr: 4e-05     reward: 1.93\n",
      "epis: 1494   score: 3.0   mem len: 280636   epsilon: 0.4507    steps: 249    lr: 4e-05     reward: 1.96\n",
      "epis: 1495   score: 0.0   mem len: 280759   epsilon: 0.4506    steps: 123    lr: 4e-05     reward: 1.95\n",
      "epis: 1496   score: 0.0   mem len: 280881   epsilon: 0.4504    steps: 122    lr: 4e-05     reward: 1.92\n",
      "epis: 1497   score: 2.0   mem len: 281099   epsilon: 0.4501    steps: 218    lr: 4e-05     reward: 1.91\n",
      "epis: 1498   score: 2.0   mem len: 281296   epsilon: 0.4498    steps: 197    lr: 4e-05     reward: 1.92\n",
      "epis: 1499   score: 0.0   mem len: 281419   epsilon: 0.4496    steps: 123    lr: 4e-05     reward: 1.89\n",
      "epis: 1500   score: 0.0   mem len: 281542   epsilon: 0.4495    steps: 123    lr: 4e-05     reward: 1.86\n",
      "epis: 1501   score: 1.0   mem len: 281711   epsilon: 0.4492    steps: 169    lr: 4e-05     reward: 1.85\n",
      "epis: 1502   score: 2.0   mem len: 281891   epsilon: 0.449    steps: 180    lr: 4e-05     reward: 1.87\n",
      "epis: 1503   score: 2.0   mem len: 282107   epsilon: 0.4487    steps: 216    lr: 4e-05     reward: 1.88\n",
      "epis: 1504   score: 1.0   mem len: 282258   epsilon: 0.4485    steps: 151    lr: 4e-05     reward: 1.87\n",
      "epis: 1505   score: 0.0   mem len: 282380   epsilon: 0.4483    steps: 122    lr: 4e-05     reward: 1.87\n",
      "epis: 1506   score: 2.0   mem len: 282562   epsilon: 0.4481    steps: 182    lr: 4e-05     reward: 1.87\n",
      "epis: 1507   score: 1.0   mem len: 282713   epsilon: 0.4479    steps: 151    lr: 4e-05     reward: 1.87\n",
      "epis: 1508   score: 0.0   mem len: 282835   epsilon: 0.4477    steps: 122    lr: 4e-05     reward: 1.87\n",
      "epis: 1509   score: 0.0   mem len: 282957   epsilon: 0.4475    steps: 122    lr: 4e-05     reward: 1.87\n",
      "epis: 1510   score: 1.0   mem len: 283107   epsilon: 0.4473    steps: 150    lr: 4e-05     reward: 1.86\n",
      "epis: 1511   score: 1.0   mem len: 283275   epsilon: 0.4471    steps: 168    lr: 4e-05     reward: 1.84\n",
      "epis: 1512   score: 3.0   mem len: 283523   epsilon: 0.4467    steps: 248    lr: 4e-05     reward: 1.83\n",
      "epis: 1513   score: 1.0   mem len: 283691   epsilon: 0.4465    steps: 168    lr: 4e-05     reward: 1.83\n",
      "epis: 1514   score: 0.0   mem len: 283814   epsilon: 0.4463    steps: 123    lr: 4e-05     reward: 1.82\n",
      "epis: 1515   score: 0.0   mem len: 283936   epsilon: 0.4462    steps: 122    lr: 4e-05     reward: 1.79\n",
      "epis: 1516   score: 0.0   mem len: 284058   epsilon: 0.446    steps: 122    lr: 4e-05     reward: 1.75\n",
      "epis: 1517   score: 1.0   mem len: 284226   epsilon: 0.4458    steps: 168    lr: 4e-05     reward: 1.73\n",
      "epis: 1518   score: 0.0   mem len: 284348   epsilon: 0.4456    steps: 122    lr: 4e-05     reward: 1.71\n",
      "epis: 1519   score: 0.0   mem len: 284471   epsilon: 0.4454    steps: 123    lr: 4e-05     reward: 1.69\n",
      "epis: 1520   score: 1.0   mem len: 284641   epsilon: 0.4452    steps: 170    lr: 4e-05     reward: 1.7\n",
      "epis: 1521   score: 2.0   mem len: 284856   epsilon: 0.4449    steps: 215    lr: 4e-05     reward: 1.71\n",
      "epis: 1522   score: 0.0   mem len: 284979   epsilon: 0.4447    steps: 123    lr: 4e-05     reward: 1.69\n",
      "epis: 1523   score: 1.0   mem len: 285147   epsilon: 0.4445    steps: 168    lr: 4e-05     reward: 1.67\n",
      "epis: 1524   score: 6.0   mem len: 285514   epsilon: 0.444    steps: 367    lr: 4e-05     reward: 1.71\n",
      "epis: 1525   score: 0.0   mem len: 285637   epsilon: 0.4438    steps: 123    lr: 4e-05     reward: 1.68\n",
      "epis: 1526   score: 1.0   mem len: 285805   epsilon: 0.4436    steps: 168    lr: 4e-05     reward: 1.68\n",
      "epis: 1527   score: 1.0   mem len: 285973   epsilon: 0.4434    steps: 168    lr: 4e-05     reward: 1.68\n",
      "epis: 1528   score: 3.0   mem len: 286222   epsilon: 0.443    steps: 249    lr: 4e-05     reward: 1.7\n",
      "epis: 1529   score: 1.0   mem len: 286394   epsilon: 0.4428    steps: 172    lr: 4e-05     reward: 1.68\n",
      "epis: 1530   score: 2.0   mem len: 286574   epsilon: 0.4425    steps: 180    lr: 4e-05     reward: 1.7\n",
      "epis: 1531   score: 3.0   mem len: 286842   epsilon: 0.4422    steps: 268    lr: 4e-05     reward: 1.72\n",
      "epis: 1532   score: 2.0   mem len: 287040   epsilon: 0.4419    steps: 198    lr: 4e-05     reward: 1.71\n",
      "epis: 1533   score: 2.0   mem len: 287238   epsilon: 0.4416    steps: 198    lr: 4e-05     reward: 1.72\n",
      "epis: 1534   score: 0.0   mem len: 287360   epsilon: 0.4414    steps: 122    lr: 4e-05     reward: 1.7\n",
      "epis: 1535   score: 2.0   mem len: 287557   epsilon: 0.4412    steps: 197    lr: 4e-05     reward: 1.72\n",
      "epis: 1536   score: 1.0   mem len: 287725   epsilon: 0.4409    steps: 168    lr: 4e-05     reward: 1.72\n",
      "epis: 1537   score: 2.0   mem len: 287922   epsilon: 0.4407    steps: 197    lr: 4e-05     reward: 1.74\n",
      "epis: 1538   score: 4.0   mem len: 288238   epsilon: 0.4402    steps: 316    lr: 4e-05     reward: 1.76\n",
      "epis: 1539   score: 2.0   mem len: 288436   epsilon: 0.44    steps: 198    lr: 4e-05     reward: 1.76\n",
      "epis: 1540   score: 0.0   mem len: 288559   epsilon: 0.4398    steps: 123    lr: 4e-05     reward: 1.73\n",
      "epis: 1541   score: 2.0   mem len: 288757   epsilon: 0.4395    steps: 198    lr: 4e-05     reward: 1.73\n",
      "epis: 1542   score: 2.0   mem len: 288954   epsilon: 0.4392    steps: 197    lr: 4e-05     reward: 1.72\n",
      "epis: 1543   score: 2.0   mem len: 289152   epsilon: 0.439    steps: 198    lr: 4e-05     reward: 1.71\n",
      "epis: 1544   score: 2.0   mem len: 289349   epsilon: 0.4387    steps: 197    lr: 4e-05     reward: 1.71\n",
      "epis: 1545   score: 1.0   mem len: 289517   epsilon: 0.4385    steps: 168    lr: 4e-05     reward: 1.71\n",
      "epis: 1546   score: 0.0   mem len: 289640   epsilon: 0.4383    steps: 123    lr: 4e-05     reward: 1.71\n",
      "epis: 1547   score: 1.0   mem len: 289791   epsilon: 0.4381    steps: 151    lr: 4e-05     reward: 1.71\n",
      "epis: 1548   score: 2.0   mem len: 289989   epsilon: 0.4378    steps: 198    lr: 4e-05     reward: 1.72\n",
      "epis: 1549   score: 2.0   mem len: 290187   epsilon: 0.4375    steps: 198    lr: 4e-05     reward: 1.7\n",
      "epis: 1550   score: 0.0   mem len: 290310   epsilon: 0.4374    steps: 123    lr: 4e-05     reward: 1.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1551   score: 1.0   mem len: 290478   epsilon: 0.4371    steps: 168    lr: 4e-05     reward: 1.67\n",
      "epis: 1552   score: 2.0   mem len: 290676   epsilon: 0.4369    steps: 198    lr: 4e-05     reward: 1.69\n",
      "epis: 1553   score: 0.0   mem len: 290799   epsilon: 0.4367    steps: 123    lr: 4e-05     reward: 1.66\n",
      "epis: 1554   score: 1.0   mem len: 290950   epsilon: 0.4365    steps: 151    lr: 4e-05     reward: 1.67\n",
      "epis: 1555   score: 2.0   mem len: 291167   epsilon: 0.4362    steps: 217    lr: 4e-05     reward: 1.68\n",
      "epis: 1556   score: 0.0   mem len: 291290   epsilon: 0.436    steps: 123    lr: 4e-05     reward: 1.67\n",
      "epis: 1557   score: 0.0   mem len: 291413   epsilon: 0.4358    steps: 123    lr: 4e-05     reward: 1.65\n",
      "epis: 1558   score: 2.0   mem len: 291593   epsilon: 0.4356    steps: 180    lr: 4e-05     reward: 1.65\n",
      "epis: 1559   score: 1.0   mem len: 291762   epsilon: 0.4354    steps: 169    lr: 4e-05     reward: 1.65\n",
      "epis: 1560   score: 1.0   mem len: 291931   epsilon: 0.4351    steps: 169    lr: 4e-05     reward: 1.65\n",
      "epis: 1561   score: 3.0   mem len: 292197   epsilon: 0.4348    steps: 266    lr: 4e-05     reward: 1.67\n",
      "epis: 1562   score: 2.0   mem len: 292395   epsilon: 0.4345    steps: 198    lr: 4e-05     reward: 1.67\n",
      "epis: 1563   score: 1.0   mem len: 292545   epsilon: 0.4343    steps: 150    lr: 4e-05     reward: 1.66\n",
      "epis: 1564   score: 1.0   mem len: 292695   epsilon: 0.4341    steps: 150    lr: 4e-05     reward: 1.64\n",
      "epis: 1565   score: 1.0   mem len: 292846   epsilon: 0.4339    steps: 151    lr: 4e-05     reward: 1.63\n",
      "epis: 1566   score: 2.0   mem len: 293046   epsilon: 0.4336    steps: 200    lr: 4e-05     reward: 1.62\n",
      "epis: 1567   score: 0.0   mem len: 293169   epsilon: 0.4334    steps: 123    lr: 4e-05     reward: 1.6\n",
      "epis: 1568   score: 3.0   mem len: 293415   epsilon: 0.4331    steps: 246    lr: 4e-05     reward: 1.58\n",
      "epis: 1569   score: 0.0   mem len: 293538   epsilon: 0.4329    steps: 123    lr: 4e-05     reward: 1.55\n",
      "epis: 1570   score: 1.0   mem len: 293707   epsilon: 0.4327    steps: 169    lr: 4e-05     reward: 1.56\n",
      "epis: 1571   score: 2.0   mem len: 293904   epsilon: 0.4324    steps: 197    lr: 4e-05     reward: 1.54\n",
      "epis: 1572   score: 1.0   mem len: 294073   epsilon: 0.4322    steps: 169    lr: 4e-05     reward: 1.5\n",
      "epis: 1573   score: 1.0   mem len: 294224   epsilon: 0.432    steps: 151    lr: 4e-05     reward: 1.5\n",
      "epis: 1574   score: 3.0   mem len: 294451   epsilon: 0.4317    steps: 227    lr: 4e-05     reward: 1.51\n",
      "epis: 1575   score: 3.0   mem len: 294696   epsilon: 0.4313    steps: 245    lr: 4e-05     reward: 1.5\n",
      "epis: 1576   score: 0.0   mem len: 294818   epsilon: 0.4311    steps: 122    lr: 4e-05     reward: 1.49\n",
      "epis: 1577   score: 3.0   mem len: 295082   epsilon: 0.4308    steps: 264    lr: 4e-05     reward: 1.51\n",
      "epis: 1578   score: 1.0   mem len: 295233   epsilon: 0.4306    steps: 151    lr: 4e-05     reward: 1.47\n",
      "epis: 1579   score: 0.0   mem len: 295355   epsilon: 0.4304    steps: 122    lr: 4e-05     reward: 1.46\n",
      "epis: 1580   score: 0.0   mem len: 295477   epsilon: 0.4302    steps: 122    lr: 4e-05     reward: 1.42\n",
      "epis: 1581   score: 0.0   mem len: 295600   epsilon: 0.4301    steps: 123    lr: 4e-05     reward: 1.39\n",
      "epis: 1582   score: 2.0   mem len: 295782   epsilon: 0.4298    steps: 182    lr: 4e-05     reward: 1.37\n",
      "epis: 1583   score: 1.0   mem len: 295933   epsilon: 0.4296    steps: 151    lr: 4e-05     reward: 1.35\n",
      "epis: 1584   score: 2.0   mem len: 296115   epsilon: 0.4294    steps: 182    lr: 4e-05     reward: 1.35\n",
      "epis: 1585   score: 5.0   mem len: 296405   epsilon: 0.429    steps: 290    lr: 4e-05     reward: 1.38\n",
      "epis: 1586   score: 1.0   mem len: 296574   epsilon: 0.4287    steps: 169    lr: 4e-05     reward: 1.37\n",
      "epis: 1587   score: 5.0   mem len: 296902   epsilon: 0.4283    steps: 328    lr: 4e-05     reward: 1.38\n",
      "epis: 1588   score: 2.0   mem len: 297100   epsilon: 0.428    steps: 198    lr: 4e-05     reward: 1.37\n",
      "epis: 1589   score: 2.0   mem len: 297297   epsilon: 0.4277    steps: 197    lr: 4e-05     reward: 1.37\n",
      "epis: 1590   score: 0.0   mem len: 297419   epsilon: 0.4276    steps: 122    lr: 4e-05     reward: 1.35\n",
      "epis: 1591   score: 1.0   mem len: 297590   epsilon: 0.4273    steps: 171    lr: 4e-05     reward: 1.35\n",
      "epis: 1592   score: 3.0   mem len: 297834   epsilon: 0.427    steps: 244    lr: 4e-05     reward: 1.36\n",
      "epis: 1593   score: 2.0   mem len: 298053   epsilon: 0.4267    steps: 219    lr: 4e-05     reward: 1.37\n",
      "epis: 1594   score: 2.0   mem len: 298251   epsilon: 0.4264    steps: 198    lr: 4e-05     reward: 1.36\n",
      "epis: 1595   score: 1.0   mem len: 298420   epsilon: 0.4262    steps: 169    lr: 4e-05     reward: 1.37\n",
      "epis: 1596   score: 0.0   mem len: 298543   epsilon: 0.426    steps: 123    lr: 4e-05     reward: 1.37\n",
      "epis: 1597   score: 1.0   mem len: 298712   epsilon: 0.4258    steps: 169    lr: 4e-05     reward: 1.36\n",
      "epis: 1598   score: 2.0   mem len: 298893   epsilon: 0.4255    steps: 181    lr: 4e-05     reward: 1.36\n",
      "epis: 1599   score: 2.0   mem len: 299091   epsilon: 0.4253    steps: 198    lr: 4e-05     reward: 1.38\n",
      "epis: 1600   score: 1.0   mem len: 299260   epsilon: 0.425    steps: 169    lr: 4e-05     reward: 1.39\n",
      "epis: 1601   score: 2.0   mem len: 299478   epsilon: 0.4247    steps: 218    lr: 4e-05     reward: 1.4\n",
      "epis: 1602   score: 0.0   mem len: 299601   epsilon: 0.4245    steps: 123    lr: 4e-05     reward: 1.38\n",
      "epis: 1603   score: 3.0   mem len: 299866   epsilon: 0.4242    steps: 265    lr: 4e-05     reward: 1.39\n",
      "epis: 1604   score: 2.0   mem len: 300063   epsilon: 0.4239    steps: 197    lr: 1.6000000000000003e-05     reward: 1.4\n",
      "epis: 1605   score: 3.0   mem len: 300312   epsilon: 0.4236    steps: 249    lr: 1.6000000000000003e-05     reward: 1.43\n",
      "epis: 1606   score: 2.0   mem len: 300533   epsilon: 0.4233    steps: 221    lr: 1.6000000000000003e-05     reward: 1.43\n",
      "epis: 1607   score: 0.0   mem len: 300655   epsilon: 0.4231    steps: 122    lr: 1.6000000000000003e-05     reward: 1.42\n",
      "epis: 1608   score: 0.0   mem len: 300778   epsilon: 0.4229    steps: 123    lr: 1.6000000000000003e-05     reward: 1.42\n",
      "epis: 1609   score: 6.0   mem len: 301194   epsilon: 0.4224    steps: 416    lr: 1.6000000000000003e-05     reward: 1.48\n",
      "epis: 1610   score: 0.0   mem len: 301317   epsilon: 0.4222    steps: 123    lr: 1.6000000000000003e-05     reward: 1.47\n",
      "epis: 1611   score: 3.0   mem len: 301549   epsilon: 0.4219    steps: 232    lr: 1.6000000000000003e-05     reward: 1.49\n",
      "epis: 1612   score: 2.0   mem len: 301747   epsilon: 0.4216    steps: 198    lr: 1.6000000000000003e-05     reward: 1.48\n",
      "epis: 1613   score: 3.0   mem len: 301976   epsilon: 0.4213    steps: 229    lr: 1.6000000000000003e-05     reward: 1.5\n",
      "epis: 1614   score: 1.0   mem len: 302144   epsilon: 0.421    steps: 168    lr: 1.6000000000000003e-05     reward: 1.51\n",
      "epis: 1615   score: 2.0   mem len: 302342   epsilon: 0.4208    steps: 198    lr: 1.6000000000000003e-05     reward: 1.53\n",
      "epis: 1616   score: 3.0   mem len: 302588   epsilon: 0.4204    steps: 246    lr: 1.6000000000000003e-05     reward: 1.56\n",
      "epis: 1617   score: 1.0   mem len: 302758   epsilon: 0.4202    steps: 170    lr: 1.6000000000000003e-05     reward: 1.56\n",
      "epis: 1618   score: 3.0   mem len: 302987   epsilon: 0.4199    steps: 229    lr: 1.6000000000000003e-05     reward: 1.59\n",
      "epis: 1619   score: 3.0   mem len: 303214   epsilon: 0.4196    steps: 227    lr: 1.6000000000000003e-05     reward: 1.62\n",
      "epis: 1620   score: 1.0   mem len: 303383   epsilon: 0.4193    steps: 169    lr: 1.6000000000000003e-05     reward: 1.62\n",
      "epis: 1621   score: 0.0   mem len: 303506   epsilon: 0.4192    steps: 123    lr: 1.6000000000000003e-05     reward: 1.6\n",
      "epis: 1622   score: 2.0   mem len: 303704   epsilon: 0.4189    steps: 198    lr: 1.6000000000000003e-05     reward: 1.62\n",
      "epis: 1623   score: 2.0   mem len: 303902   epsilon: 0.4186    steps: 198    lr: 1.6000000000000003e-05     reward: 1.63\n",
      "epis: 1624   score: 0.0   mem len: 304025   epsilon: 0.4184    steps: 123    lr: 1.6000000000000003e-05     reward: 1.57\n",
      "epis: 1625   score: 3.0   mem len: 304269   epsilon: 0.4181    steps: 244    lr: 1.6000000000000003e-05     reward: 1.6\n",
      "epis: 1626   score: 3.0   mem len: 304515   epsilon: 0.4178    steps: 246    lr: 1.6000000000000003e-05     reward: 1.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1627   score: 3.0   mem len: 304760   epsilon: 0.4174    steps: 245    lr: 1.6000000000000003e-05     reward: 1.64\n",
      "epis: 1628   score: 2.0   mem len: 304942   epsilon: 0.4172    steps: 182    lr: 1.6000000000000003e-05     reward: 1.63\n",
      "epis: 1629   score: 2.0   mem len: 305140   epsilon: 0.4169    steps: 198    lr: 1.6000000000000003e-05     reward: 1.64\n",
      "epis: 1630   score: 0.0   mem len: 305263   epsilon: 0.4167    steps: 123    lr: 1.6000000000000003e-05     reward: 1.62\n",
      "epis: 1631   score: 1.0   mem len: 305413   epsilon: 0.4165    steps: 150    lr: 1.6000000000000003e-05     reward: 1.6\n",
      "epis: 1632   score: 2.0   mem len: 305631   epsilon: 0.4162    steps: 218    lr: 1.6000000000000003e-05     reward: 1.6\n",
      "epis: 1633   score: 6.0   mem len: 306024   epsilon: 0.4157    steps: 393    lr: 1.6000000000000003e-05     reward: 1.64\n",
      "epis: 1634   score: 1.0   mem len: 306175   epsilon: 0.4155    steps: 151    lr: 1.6000000000000003e-05     reward: 1.65\n",
      "epis: 1635   score: 0.0   mem len: 306298   epsilon: 0.4153    steps: 123    lr: 1.6000000000000003e-05     reward: 1.63\n",
      "epis: 1636   score: 2.0   mem len: 306495   epsilon: 0.415    steps: 197    lr: 1.6000000000000003e-05     reward: 1.64\n",
      "epis: 1637   score: 4.0   mem len: 306772   epsilon: 0.4147    steps: 277    lr: 1.6000000000000003e-05     reward: 1.66\n",
      "epis: 1638   score: 1.0   mem len: 306940   epsilon: 0.4144    steps: 168    lr: 1.6000000000000003e-05     reward: 1.63\n",
      "epis: 1639   score: 1.0   mem len: 307109   epsilon: 0.4142    steps: 169    lr: 1.6000000000000003e-05     reward: 1.62\n",
      "epis: 1640   score: 6.0   mem len: 307500   epsilon: 0.4136    steps: 391    lr: 1.6000000000000003e-05     reward: 1.68\n",
      "epis: 1641   score: 4.0   mem len: 307756   epsilon: 0.4133    steps: 256    lr: 1.6000000000000003e-05     reward: 1.7\n",
      "epis: 1642   score: 3.0   mem len: 308009   epsilon: 0.4129    steps: 253    lr: 1.6000000000000003e-05     reward: 1.71\n",
      "epis: 1643   score: 2.0   mem len: 308227   epsilon: 0.4126    steps: 218    lr: 1.6000000000000003e-05     reward: 1.71\n",
      "epis: 1644   score: 0.0   mem len: 308350   epsilon: 0.4125    steps: 123    lr: 1.6000000000000003e-05     reward: 1.69\n",
      "epis: 1645   score: 5.0   mem len: 308695   epsilon: 0.412    steps: 345    lr: 1.6000000000000003e-05     reward: 1.73\n",
      "epis: 1646   score: 2.0   mem len: 308914   epsilon: 0.4117    steps: 219    lr: 1.6000000000000003e-05     reward: 1.75\n",
      "epis: 1647   score: 2.0   mem len: 309094   epsilon: 0.4114    steps: 180    lr: 1.6000000000000003e-05     reward: 1.76\n",
      "epis: 1648   score: 1.0   mem len: 309245   epsilon: 0.4112    steps: 151    lr: 1.6000000000000003e-05     reward: 1.75\n",
      "epis: 1649   score: 1.0   mem len: 309396   epsilon: 0.411    steps: 151    lr: 1.6000000000000003e-05     reward: 1.74\n",
      "epis: 1650   score: 5.0   mem len: 309694   epsilon: 0.4106    steps: 298    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 1651   score: 0.0   mem len: 309816   epsilon: 0.4105    steps: 122    lr: 1.6000000000000003e-05     reward: 1.78\n",
      "epis: 1652   score: 2.0   mem len: 310016   epsilon: 0.4102    steps: 200    lr: 1.6000000000000003e-05     reward: 1.78\n",
      "epis: 1653   score: 1.0   mem len: 310167   epsilon: 0.41    steps: 151    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 1654   score: 2.0   mem len: 310365   epsilon: 0.4097    steps: 198    lr: 1.6000000000000003e-05     reward: 1.8\n",
      "epis: 1655   score: 7.0   mem len: 310752   epsilon: 0.4092    steps: 387    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1656   score: 2.0   mem len: 310949   epsilon: 0.4089    steps: 197    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1657   score: 2.0   mem len: 311131   epsilon: 0.4086    steps: 182    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1658   score: 2.0   mem len: 311329   epsilon: 0.4084    steps: 198    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1659   score: 3.0   mem len: 311555   epsilon: 0.4081    steps: 226    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1660   score: 0.0   mem len: 311678   epsilon: 0.4079    steps: 123    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1661   score: 0.0   mem len: 311800   epsilon: 0.4077    steps: 122    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1662   score: 2.0   mem len: 312018   epsilon: 0.4074    steps: 218    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1663   score: 0.0   mem len: 312141   epsilon: 0.4072    steps: 123    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1664   score: 2.0   mem len: 312360   epsilon: 0.4069    steps: 219    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1665   score: 0.0   mem len: 312483   epsilon: 0.4068    steps: 123    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1666   score: 3.0   mem len: 312709   epsilon: 0.4065    steps: 226    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1667   score: 1.0   mem len: 312878   epsilon: 0.4062    steps: 169    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1668   score: 0.0   mem len: 313001   epsilon: 0.4061    steps: 123    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1669   score: 0.0   mem len: 313123   epsilon: 0.4059    steps: 122    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1670   score: 0.0   mem len: 313246   epsilon: 0.4057    steps: 123    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1671   score: 2.0   mem len: 313426   epsilon: 0.4055    steps: 180    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1672   score: 2.0   mem len: 313623   epsilon: 0.4052    steps: 197    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1673   score: 0.0   mem len: 313746   epsilon: 0.405    steps: 123    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1674   score: 1.0   mem len: 313915   epsilon: 0.4048    steps: 169    lr: 1.6000000000000003e-05     reward: 1.82\n",
      "epis: 1675   score: 2.0   mem len: 314113   epsilon: 0.4045    steps: 198    lr: 1.6000000000000003e-05     reward: 1.81\n",
      "epis: 1676   score: 2.0   mem len: 314311   epsilon: 0.4042    steps: 198    lr: 1.6000000000000003e-05     reward: 1.83\n",
      "epis: 1677   score: 1.0   mem len: 314480   epsilon: 0.404    steps: 169    lr: 1.6000000000000003e-05     reward: 1.81\n",
      "epis: 1678   score: 2.0   mem len: 314677   epsilon: 0.4037    steps: 197    lr: 1.6000000000000003e-05     reward: 1.82\n",
      "epis: 1679   score: 2.0   mem len: 314875   epsilon: 0.4035    steps: 198    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1680   score: 2.0   mem len: 315072   epsilon: 0.4032    steps: 197    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1681   score: 3.0   mem len: 315317   epsilon: 0.4029    steps: 245    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1682   score: 3.0   mem len: 315545   epsilon: 0.4025    steps: 228    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1683   score: 0.0   mem len: 315667   epsilon: 0.4024    steps: 122    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1684   score: 3.0   mem len: 315913   epsilon: 0.402    steps: 246    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1685   score: 2.0   mem len: 316110   epsilon: 0.4018    steps: 197    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1686   score: 2.0   mem len: 316307   epsilon: 0.4015    steps: 197    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1687   score: 1.0   mem len: 316476   epsilon: 0.4013    steps: 169    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1688   score: 2.0   mem len: 316673   epsilon: 0.401    steps: 197    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1689   score: 2.0   mem len: 316870   epsilon: 0.4007    steps: 197    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1690   score: 0.0   mem len: 316993   epsilon: 0.4005    steps: 123    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1691   score: 2.0   mem len: 317190   epsilon: 0.4003    steps: 197    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1692   score: 1.0   mem len: 317358   epsilon: 0.4    steps: 168    lr: 1.6000000000000003e-05     reward: 1.83\n",
      "epis: 1693   score: 2.0   mem len: 317556   epsilon: 0.3998    steps: 198    lr: 1.6000000000000003e-05     reward: 1.83\n",
      "epis: 1694   score: 4.0   mem len: 317831   epsilon: 0.3994    steps: 275    lr: 1.6000000000000003e-05     reward: 1.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1695   score: 0.0   mem len: 317953   epsilon: 0.3992    steps: 122    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1696   score: 4.0   mem len: 318210   epsilon: 0.3989    steps: 257    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1697   score: 0.0   mem len: 318333   epsilon: 0.3987    steps: 123    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1698   score: 0.0   mem len: 318456   epsilon: 0.3985    steps: 123    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1699   score: 1.0   mem len: 318625   epsilon: 0.3983    steps: 169    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1700   score: 2.0   mem len: 318823   epsilon: 0.398    steps: 198    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1701   score: 3.0   mem len: 319069   epsilon: 0.3977    steps: 246    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1702   score: 3.0   mem len: 319297   epsilon: 0.3974    steps: 228    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1703   score: 3.0   mem len: 319525   epsilon: 0.3971    steps: 228    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1704   score: 2.0   mem len: 319722   epsilon: 0.3968    steps: 197    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1705   score: 1.0   mem len: 319891   epsilon: 0.3965    steps: 169    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1706   score: 1.0   mem len: 320041   epsilon: 0.3963    steps: 150    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1707   score: 4.0   mem len: 320303   epsilon: 0.396    steps: 262    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1708   score: 2.0   mem len: 320501   epsilon: 0.3957    steps: 198    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1709   score: 3.0   mem len: 320728   epsilon: 0.3954    steps: 227    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1710   score: 3.0   mem len: 320956   epsilon: 0.3951    steps: 228    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1711   score: 0.0   mem len: 321078   epsilon: 0.3949    steps: 122    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1712   score: 0.0   mem len: 321201   epsilon: 0.3947    steps: 123    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1713   score: 3.0   mem len: 321431   epsilon: 0.3944    steps: 230    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1714   score: 1.0   mem len: 321581   epsilon: 0.3942    steps: 150    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1715   score: 2.0   mem len: 321779   epsilon: 0.3939    steps: 198    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1716   score: 0.0   mem len: 321902   epsilon: 0.3938    steps: 123    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1717   score: 3.0   mem len: 322131   epsilon: 0.3935    steps: 229    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1718   score: 3.0   mem len: 322358   epsilon: 0.3931    steps: 227    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1719   score: 2.0   mem len: 322556   epsilon: 0.3929    steps: 198    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1720   score: 3.0   mem len: 322784   epsilon: 0.3926    steps: 228    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1721   score: 3.0   mem len: 323011   epsilon: 0.3922    steps: 227    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1722   score: 1.0   mem len: 323180   epsilon: 0.392    steps: 169    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1723   score: 0.0   mem len: 323303   epsilon: 0.3918    steps: 123    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1724   score: 4.0   mem len: 323578   epsilon: 0.3915    steps: 275    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1725   score: 1.0   mem len: 323747   epsilon: 0.3912    steps: 169    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1726   score: 2.0   mem len: 323929   epsilon: 0.391    steps: 182    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1727   score: 3.0   mem len: 324173   epsilon: 0.3906    steps: 244    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1728   score: 2.0   mem len: 324371   epsilon: 0.3904    steps: 198    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1729   score: 1.0   mem len: 324540   epsilon: 0.3901    steps: 169    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1730   score: 1.0   mem len: 324709   epsilon: 0.3899    steps: 169    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1731   score: 3.0   mem len: 324959   epsilon: 0.3896    steps: 250    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1732   score: 3.0   mem len: 325223   epsilon: 0.3892    steps: 264    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1733   score: 2.0   mem len: 325421   epsilon: 0.3889    steps: 198    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1734   score: 4.0   mem len: 325690   epsilon: 0.3885    steps: 269    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1735   score: 3.0   mem len: 325936   epsilon: 0.3882    steps: 246    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1736   score: 2.0   mem len: 326135   epsilon: 0.3879    steps: 199    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1737   score: 0.0   mem len: 326258   epsilon: 0.3878    steps: 123    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1738   score: 3.0   mem len: 326504   epsilon: 0.3874    steps: 246    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1739   score: 5.0   mem len: 326830   epsilon: 0.387    steps: 326    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1740   score: 0.0   mem len: 326953   epsilon: 0.3868    steps: 123    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1741   score: 2.0   mem len: 327152   epsilon: 0.3865    steps: 199    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1742   score: 3.0   mem len: 327381   epsilon: 0.3862    steps: 229    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1743   score: 0.0   mem len: 327503   epsilon: 0.386    steps: 122    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1744   score: 1.0   mem len: 327671   epsilon: 0.3858    steps: 168    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1745   score: 2.0   mem len: 327869   epsilon: 0.3855    steps: 198    lr: 1.6000000000000003e-05     reward: 1.83\n",
      "epis: 1746   score: 4.0   mem len: 328167   epsilon: 0.3851    steps: 298    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1747   score: 3.0   mem len: 328392   epsilon: 0.3848    steps: 225    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1748   score: 4.0   mem len: 328687   epsilon: 0.3844    steps: 295    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1749   score: 3.0   mem len: 328935   epsilon: 0.3841    steps: 248    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1750   score: 4.0   mem len: 329211   epsilon: 0.3837    steps: 276    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1751   score: 1.0   mem len: 329379   epsilon: 0.3835    steps: 168    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1752   score: 1.0   mem len: 329529   epsilon: 0.3832    steps: 150    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1753   score: 0.0   mem len: 329652   epsilon: 0.3831    steps: 123    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1754   score: 2.0   mem len: 329850   epsilon: 0.3828    steps: 198    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1755   score: 3.0   mem len: 330099   epsilon: 0.3825    steps: 249    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1756   score: 2.0   mem len: 330296   epsilon: 0.3822    steps: 197    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 1757   score: 1.0   mem len: 330447   epsilon: 0.382    steps: 151    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 1758   score: 0.0   mem len: 330569   epsilon: 0.3818    steps: 122    lr: 1.6000000000000003e-05     reward: 1.82\n",
      "epis: 1759   score: 4.0   mem len: 330867   epsilon: 0.3814    steps: 298    lr: 1.6000000000000003e-05     reward: 1.83\n",
      "epis: 1760   score: 3.0   mem len: 331093   epsilon: 0.3811    steps: 226    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1761   score: 3.0   mem len: 331320   epsilon: 0.3808    steps: 227    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1762   score: 3.0   mem len: 331546   epsilon: 0.3805    steps: 226    lr: 1.6000000000000003e-05     reward: 1.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1763   score: 3.0   mem len: 331792   epsilon: 0.3801    steps: 246    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1764   score: 1.0   mem len: 331961   epsilon: 0.3799    steps: 169    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1765   score: 1.0   mem len: 332130   epsilon: 0.3797    steps: 169    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1766   score: 5.0   mem len: 332490   epsilon: 0.3792    steps: 360    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1767   score: 2.0   mem len: 332707   epsilon: 0.3789    steps: 217    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1768   score: 0.0   mem len: 332830   epsilon: 0.3787    steps: 123    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1769   score: 0.0   mem len: 332952   epsilon: 0.3785    steps: 122    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1770   score: 0.0   mem len: 333074   epsilon: 0.3784    steps: 122    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1771   score: 1.0   mem len: 333243   epsilon: 0.3781    steps: 169    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1772   score: 1.0   mem len: 333413   epsilon: 0.3779    steps: 170    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1773   score: 0.0   mem len: 333536   epsilon: 0.3777    steps: 123    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1774   score: 5.0   mem len: 333835   epsilon: 0.3773    steps: 299    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1775   score: 1.0   mem len: 334004   epsilon: 0.3771    steps: 169    lr: 1.6000000000000003e-05     reward: 1.97\n",
      "epis: 1776   score: 0.0   mem len: 334126   epsilon: 0.3769    steps: 122    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1777   score: 1.0   mem len: 334278   epsilon: 0.3767    steps: 152    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1778   score: 0.0   mem len: 334401   epsilon: 0.3765    steps: 123    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1779   score: 0.0   mem len: 334524   epsilon: 0.3764    steps: 123    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1780   score: 2.0   mem len: 334706   epsilon: 0.3761    steps: 182    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1781   score: 1.0   mem len: 334875   epsilon: 0.3759    steps: 169    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1782   score: 1.0   mem len: 335043   epsilon: 0.3756    steps: 168    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1783   score: 4.0   mem len: 335321   epsilon: 0.3753    steps: 278    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1784   score: 5.0   mem len: 335632   epsilon: 0.3748    steps: 311    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1785   score: 2.0   mem len: 335830   epsilon: 0.3746    steps: 198    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1786   score: 0.0   mem len: 335952   epsilon: 0.3744    steps: 122    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1787   score: 2.0   mem len: 336133   epsilon: 0.3741    steps: 181    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1788   score: 0.0   mem len: 336255   epsilon: 0.374    steps: 122    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1789   score: 0.0   mem len: 336378   epsilon: 0.3738    steps: 123    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 1790   score: 1.0   mem len: 336547   epsilon: 0.3736    steps: 169    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1791   score: 3.0   mem len: 336792   epsilon: 0.3732    steps: 245    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1792   score: 1.0   mem len: 336961   epsilon: 0.373    steps: 169    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1793   score: 2.0   mem len: 337159   epsilon: 0.3727    steps: 198    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1794   score: 3.0   mem len: 337388   epsilon: 0.3724    steps: 229    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1795   score: 2.0   mem len: 337585   epsilon: 0.3721    steps: 197    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1796   score: 2.0   mem len: 337783   epsilon: 0.3719    steps: 198    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1797   score: 4.0   mem len: 338040   epsilon: 0.3715    steps: 257    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1798   score: 3.0   mem len: 338289   epsilon: 0.3712    steps: 249    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1799   score: 2.0   mem len: 338486   epsilon: 0.3709    steps: 197    lr: 1.6000000000000003e-05     reward: 1.97\n",
      "epis: 1800   score: 2.0   mem len: 338704   epsilon: 0.3706    steps: 218    lr: 1.6000000000000003e-05     reward: 1.97\n",
      "epis: 1801   score: 2.0   mem len: 338902   epsilon: 0.3703    steps: 198    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1802   score: 2.0   mem len: 339103   epsilon: 0.37    steps: 201    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1803   score: 0.0   mem len: 339226   epsilon: 0.3699    steps: 123    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1804   score: 2.0   mem len: 339423   epsilon: 0.3696    steps: 197    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1805   score: 7.0   mem len: 339809   epsilon: 0.3691    steps: 386    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1806   score: 2.0   mem len: 340027   epsilon: 0.3688    steps: 218    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 1807   score: 1.0   mem len: 340196   epsilon: 0.3685    steps: 169    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1808   score: 0.0   mem len: 340319   epsilon: 0.3684    steps: 123    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1809   score: 1.0   mem len: 340488   epsilon: 0.3681    steps: 169    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1810   score: 3.0   mem len: 340715   epsilon: 0.3678    steps: 227    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1811   score: 1.0   mem len: 340884   epsilon: 0.3676    steps: 169    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1812   score: 3.0   mem len: 341112   epsilon: 0.3673    steps: 228    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1813   score: 3.0   mem len: 341358   epsilon: 0.3669    steps: 246    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1814   score: 3.0   mem len: 341604   epsilon: 0.3666    steps: 246    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1815   score: 4.0   mem len: 341879   epsilon: 0.3662    steps: 275    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1816   score: 3.0   mem len: 342107   epsilon: 0.3659    steps: 228    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 1817   score: 1.0   mem len: 342276   epsilon: 0.3657    steps: 169    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1818   score: 0.0   mem len: 342398   epsilon: 0.3655    steps: 122    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1819   score: 2.0   mem len: 342596   epsilon: 0.3652    steps: 198    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1820   score: 3.0   mem len: 342863   epsilon: 0.3648    steps: 267    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1821   score: 1.0   mem len: 343032   epsilon: 0.3646    steps: 169    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1822   score: 2.0   mem len: 343230   epsilon: 0.3643    steps: 198    lr: 1.6000000000000003e-05     reward: 1.97\n",
      "epis: 1823   score: 4.0   mem len: 343505   epsilon: 0.364    steps: 275    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1824   score: 4.0   mem len: 343816   epsilon: 0.3635    steps: 311    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1825   score: 0.0   mem len: 343938   epsilon: 0.3634    steps: 122    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1826   score: 2.0   mem len: 344118   epsilon: 0.3631    steps: 180    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1827   score: 2.0   mem len: 344315   epsilon: 0.3628    steps: 197    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 1828   score: 2.0   mem len: 344513   epsilon: 0.3626    steps: 198    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 1829   score: 2.0   mem len: 344711   epsilon: 0.3623    steps: 198    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1830   score: 1.0   mem len: 344879   epsilon: 0.3621    steps: 168    lr: 1.6000000000000003e-05     reward: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1831   score: 1.0   mem len: 345048   epsilon: 0.3618    steps: 169    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1832   score: 3.0   mem len: 345280   epsilon: 0.3615    steps: 232    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1833   score: 3.0   mem len: 345549   epsilon: 0.3611    steps: 269    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 1834   score: 5.0   mem len: 345873   epsilon: 0.3607    steps: 324    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1835   score: 3.0   mem len: 346120   epsilon: 0.3604    steps: 247    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1836   score: 3.0   mem len: 346370   epsilon: 0.36    steps: 250    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1837   score: 2.0   mem len: 346568   epsilon: 0.3597    steps: 198    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 1838   score: 3.0   mem len: 346813   epsilon: 0.3594    steps: 245    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 1839   score: 2.0   mem len: 347011   epsilon: 0.3591    steps: 198    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1840   score: 3.0   mem len: 347279   epsilon: 0.3588    steps: 268    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 1841   score: 0.0   mem len: 347401   epsilon: 0.3586    steps: 122    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1842   score: 2.0   mem len: 347599   epsilon: 0.3583    steps: 198    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1843   score: 2.0   mem len: 347798   epsilon: 0.358    steps: 199    lr: 1.6000000000000003e-05     reward: 2.02\n",
      "epis: 1844   score: 0.0   mem len: 347920   epsilon: 0.3579    steps: 122    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1845   score: 1.0   mem len: 348071   epsilon: 0.3577    steps: 151    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1846   score: 2.0   mem len: 348270   epsilon: 0.3574    steps: 199    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1847   score: 4.0   mem len: 348568   epsilon: 0.357    steps: 298    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 1848   score: 2.0   mem len: 348765   epsilon: 0.3567    steps: 197    lr: 1.6000000000000003e-05     reward: 1.97\n",
      "epis: 1849   score: 0.0   mem len: 348888   epsilon: 0.3565    steps: 123    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1850   score: 2.0   mem len: 349068   epsilon: 0.3563    steps: 180    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1851   score: 2.0   mem len: 349266   epsilon: 0.356    steps: 198    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1852   score: 2.0   mem len: 349464   epsilon: 0.3557    steps: 198    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1853   score: 2.0   mem len: 349662   epsilon: 0.3555    steps: 198    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1854   score: 1.0   mem len: 349831   epsilon: 0.3552    steps: 169    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1855   score: 2.0   mem len: 350029   epsilon: 0.355    steps: 198    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1856   score: 0.0   mem len: 350152   epsilon: 0.3548    steps: 123    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1857   score: 4.0   mem len: 350427   epsilon: 0.3544    steps: 275    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1858   score: 3.0   mem len: 350677   epsilon: 0.3541    steps: 250    lr: 1.6000000000000003e-05     reward: 1.98\n",
      "epis: 1859   score: 2.0   mem len: 350896   epsilon: 0.3538    steps: 219    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1860   score: 2.0   mem len: 351094   epsilon: 0.3535    steps: 198    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1861   score: 0.0   mem len: 351217   epsilon: 0.3533    steps: 123    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1862   score: 0.0   mem len: 351340   epsilon: 0.3531    steps: 123    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1863   score: 4.0   mem len: 351618   epsilon: 0.3528    steps: 278    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 1864   score: 3.0   mem len: 351845   epsilon: 0.3525    steps: 227    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1865   score: 0.0   mem len: 351967   epsilon: 0.3523    steps: 122    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 1866   score: 1.0   mem len: 352136   epsilon: 0.3521    steps: 169    lr: 1.6000000000000003e-05     reward: 1.87\n",
      "epis: 1867   score: 1.0   mem len: 352286   epsilon: 0.3518    steps: 150    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 1868   score: 3.0   mem len: 352513   epsilon: 0.3515    steps: 227    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1869   score: 0.0   mem len: 352635   epsilon: 0.3514    steps: 122    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 1870   score: 3.0   mem len: 352882   epsilon: 0.351    steps: 247    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 1871   score: 2.0   mem len: 353080   epsilon: 0.3507    steps: 198    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1872   score: 3.0   mem len: 353326   epsilon: 0.3504    steps: 246    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1873   score: 2.0   mem len: 353524   epsilon: 0.3501    steps: 198    lr: 1.6000000000000003e-05     reward: 1.97\n",
      "epis: 1874   score: 2.0   mem len: 353722   epsilon: 0.3499    steps: 198    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1875   score: 0.0   mem len: 353845   epsilon: 0.3497    steps: 123    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 1876   score: 2.0   mem len: 354043   epsilon: 0.3494    steps: 198    lr: 1.6000000000000003e-05     reward: 1.95\n",
      "epis: 1877   score: 2.0   mem len: 354241   epsilon: 0.3491    steps: 198    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1878   score: 3.0   mem len: 354467   epsilon: 0.3488    steps: 226    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 1879   score: 1.0   mem len: 354635   epsilon: 0.3486    steps: 168    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1880   score: 1.0   mem len: 354804   epsilon: 0.3484    steps: 169    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 1881   score: 4.0   mem len: 355101   epsilon: 0.348    steps: 297    lr: 1.6000000000000003e-05     reward: 2.02\n",
      "epis: 1882   score: 2.0   mem len: 355298   epsilon: 0.3477    steps: 197    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 1883   score: 1.0   mem len: 355467   epsilon: 0.3475    steps: 169    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1884   score: 1.0   mem len: 355636   epsilon: 0.3472    steps: 169    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1885   score: 0.0   mem len: 355759   epsilon: 0.3471    steps: 123    lr: 1.6000000000000003e-05     reward: 1.94\n",
      "epis: 1886   score: 3.0   mem len: 356026   epsilon: 0.3467    steps: 267    lr: 1.6000000000000003e-05     reward: 1.97\n",
      "epis: 1887   score: 1.0   mem len: 356177   epsilon: 0.3465    steps: 151    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 1888   score: 4.0   mem len: 356473   epsilon: 0.3461    steps: 296    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1889   score: 0.0   mem len: 356596   epsilon: 0.3459    steps: 123    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1890   score: 2.0   mem len: 356794   epsilon: 0.3456    steps: 198    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1891   score: 3.0   mem len: 357021   epsilon: 0.3453    steps: 227    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1892   score: 2.0   mem len: 357219   epsilon: 0.345    steps: 198    lr: 1.6000000000000003e-05     reward: 2.02\n",
      "epis: 1893   score: 2.0   mem len: 357417   epsilon: 0.3448    steps: 198    lr: 1.6000000000000003e-05     reward: 2.02\n",
      "epis: 1894   score: 1.0   mem len: 357585   epsilon: 0.3445    steps: 168    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1895   score: 2.0   mem len: 357805   epsilon: 0.3442    steps: 220    lr: 1.6000000000000003e-05     reward: 2.0\n",
      "epis: 1896   score: 5.0   mem len: 358116   epsilon: 0.3438    steps: 311    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 1897   score: 2.0   mem len: 358334   epsilon: 0.3435    steps: 218    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1898   score: 3.0   mem len: 358582   epsilon: 0.3432    steps: 248    lr: 1.6000000000000003e-05     reward: 2.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1899   score: 2.0   mem len: 358780   epsilon: 0.3429    steps: 198    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1900   score: 2.0   mem len: 358962   epsilon: 0.3426    steps: 182    lr: 1.6000000000000003e-05     reward: 2.01\n",
      "epis: 1901   score: 5.0   mem len: 359284   epsilon: 0.3422    steps: 322    lr: 1.6000000000000003e-05     reward: 2.04\n",
      "epis: 1902   score: 2.0   mem len: 359481   epsilon: 0.3419    steps: 197    lr: 1.6000000000000003e-05     reward: 2.04\n",
      "epis: 1903   score: 9.0   mem len: 359880   epsilon: 0.3414    steps: 399    lr: 1.6000000000000003e-05     reward: 2.13\n",
      "epis: 1904   score: 2.0   mem len: 360078   epsilon: 0.3411    steps: 198    lr: 1.6000000000000003e-05     reward: 2.13\n",
      "epis: 1905   score: 1.0   mem len: 360246   epsilon: 0.3409    steps: 168    lr: 1.6000000000000003e-05     reward: 2.07\n",
      "epis: 1906   score: 5.0   mem len: 360590   epsilon: 0.3404    steps: 344    lr: 1.6000000000000003e-05     reward: 2.1\n",
      "epis: 1907   score: 5.0   mem len: 360919   epsilon: 0.3399    steps: 329    lr: 1.6000000000000003e-05     reward: 2.14\n",
      "epis: 1908   score: 3.0   mem len: 361164   epsilon: 0.3396    steps: 245    lr: 1.6000000000000003e-05     reward: 2.17\n",
      "epis: 1909   score: 2.0   mem len: 361362   epsilon: 0.3393    steps: 198    lr: 1.6000000000000003e-05     reward: 2.18\n",
      "epis: 1910   score: 4.0   mem len: 361657   epsilon: 0.3389    steps: 295    lr: 1.6000000000000003e-05     reward: 2.19\n",
      "epis: 1911   score: 3.0   mem len: 361905   epsilon: 0.3386    steps: 248    lr: 1.6000000000000003e-05     reward: 2.21\n",
      "epis: 1912   score: 4.0   mem len: 362203   epsilon: 0.3382    steps: 298    lr: 1.6000000000000003e-05     reward: 2.22\n",
      "epis: 1913   score: 1.0   mem len: 362372   epsilon: 0.3379    steps: 169    lr: 1.6000000000000003e-05     reward: 2.2\n",
      "epis: 1914   score: 0.0   mem len: 362495   epsilon: 0.3378    steps: 123    lr: 1.6000000000000003e-05     reward: 2.17\n",
      "epis: 1915   score: 1.0   mem len: 362664   epsilon: 0.3375    steps: 169    lr: 1.6000000000000003e-05     reward: 2.14\n",
      "epis: 1916   score: 2.0   mem len: 362879   epsilon: 0.3372    steps: 215    lr: 1.6000000000000003e-05     reward: 2.13\n",
      "epis: 1917   score: 2.0   mem len: 363097   epsilon: 0.3369    steps: 218    lr: 1.6000000000000003e-05     reward: 2.14\n",
      "epis: 1918   score: 0.0   mem len: 363220   epsilon: 0.3368    steps: 123    lr: 1.6000000000000003e-05     reward: 2.14\n",
      "epis: 1919   score: 3.0   mem len: 363449   epsilon: 0.3364    steps: 229    lr: 1.6000000000000003e-05     reward: 2.15\n",
      "epis: 1920   score: 0.0   mem len: 363572   epsilon: 0.3363    steps: 123    lr: 1.6000000000000003e-05     reward: 2.12\n",
      "epis: 1921   score: 2.0   mem len: 363770   epsilon: 0.336    steps: 198    lr: 1.6000000000000003e-05     reward: 2.13\n",
      "epis: 1922   score: 1.0   mem len: 363921   epsilon: 0.3358    steps: 151    lr: 1.6000000000000003e-05     reward: 2.12\n",
      "epis: 1923   score: 3.0   mem len: 364150   epsilon: 0.3355    steps: 229    lr: 1.6000000000000003e-05     reward: 2.11\n",
      "epis: 1924   score: 1.0   mem len: 364319   epsilon: 0.3352    steps: 169    lr: 1.6000000000000003e-05     reward: 2.08\n",
      "epis: 1925   score: 0.0   mem len: 364441   epsilon: 0.3351    steps: 122    lr: 1.6000000000000003e-05     reward: 2.08\n",
      "epis: 1926   score: 4.0   mem len: 364734   epsilon: 0.3347    steps: 293    lr: 1.6000000000000003e-05     reward: 2.1\n",
      "epis: 1927   score: 4.0   mem len: 365031   epsilon: 0.3343    steps: 297    lr: 1.6000000000000003e-05     reward: 2.12\n",
      "epis: 1928   score: 0.0   mem len: 365154   epsilon: 0.3341    steps: 123    lr: 1.6000000000000003e-05     reward: 2.1\n",
      "epis: 1929   score: 1.0   mem len: 365304   epsilon: 0.3339    steps: 150    lr: 1.6000000000000003e-05     reward: 2.09\n",
      "epis: 1930   score: 1.0   mem len: 365472   epsilon: 0.3336    steps: 168    lr: 1.6000000000000003e-05     reward: 2.09\n",
      "epis: 1931   score: 1.0   mem len: 365640   epsilon: 0.3334    steps: 168    lr: 1.6000000000000003e-05     reward: 2.09\n",
      "epis: 1932   score: 1.0   mem len: 365808   epsilon: 0.3332    steps: 168    lr: 1.6000000000000003e-05     reward: 2.07\n",
      "epis: 1933   score: 2.0   mem len: 366025   epsilon: 0.3329    steps: 217    lr: 1.6000000000000003e-05     reward: 2.06\n",
      "epis: 1934   score: 3.0   mem len: 366253   epsilon: 0.3326    steps: 228    lr: 1.6000000000000003e-05     reward: 2.04\n",
      "epis: 1935   score: 4.0   mem len: 366522   epsilon: 0.3322    steps: 269    lr: 1.6000000000000003e-05     reward: 2.05\n",
      "epis: 1936   score: 2.0   mem len: 366719   epsilon: 0.3319    steps: 197    lr: 1.6000000000000003e-05     reward: 2.04\n",
      "epis: 1937   score: 5.0   mem len: 367031   epsilon: 0.3315    steps: 312    lr: 1.6000000000000003e-05     reward: 2.07\n",
      "epis: 1938   score: 1.0   mem len: 367201   epsilon: 0.3313    steps: 170    lr: 1.6000000000000003e-05     reward: 2.05\n",
      "epis: 1939   score: 3.0   mem len: 367447   epsilon: 0.3309    steps: 246    lr: 1.6000000000000003e-05     reward: 2.06\n",
      "epis: 1940   score: 0.0   mem len: 367570   epsilon: 0.3308    steps: 123    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 1941   score: 4.0   mem len: 367846   epsilon: 0.3304    steps: 276    lr: 1.6000000000000003e-05     reward: 2.07\n",
      "epis: 1942   score: 2.0   mem len: 368044   epsilon: 0.3301    steps: 198    lr: 1.6000000000000003e-05     reward: 2.07\n",
      "epis: 1943   score: 4.0   mem len: 368302   epsilon: 0.3297    steps: 258    lr: 1.6000000000000003e-05     reward: 2.09\n",
      "epis: 1944   score: 1.0   mem len: 368470   epsilon: 0.3295    steps: 168    lr: 1.6000000000000003e-05     reward: 2.1\n",
      "epis: 1945   score: 4.0   mem len: 368750   epsilon: 0.3291    steps: 280    lr: 1.6000000000000003e-05     reward: 2.13\n",
      "epis: 1946   score: 10.0   mem len: 369188   epsilon: 0.3285    steps: 438    lr: 1.6000000000000003e-05     reward: 2.21\n",
      "epis: 1947   score: 8.0   mem len: 369639   epsilon: 0.3279    steps: 451    lr: 1.6000000000000003e-05     reward: 2.25\n",
      "epis: 1948   score: 1.0   mem len: 369808   epsilon: 0.3277    steps: 169    lr: 1.6000000000000003e-05     reward: 2.24\n",
      "epis: 1949   score: 7.0   mem len: 370249   epsilon: 0.3271    steps: 441    lr: 1.6000000000000003e-05     reward: 2.31\n",
      "epis: 1950   score: 3.0   mem len: 370495   epsilon: 0.3267    steps: 246    lr: 1.6000000000000003e-05     reward: 2.32\n",
      "epis: 1951   score: 2.0   mem len: 370677   epsilon: 0.3265    steps: 182    lr: 1.6000000000000003e-05     reward: 2.32\n",
      "epis: 1952   score: 6.0   mem len: 371070   epsilon: 0.3259    steps: 393    lr: 1.6000000000000003e-05     reward: 2.36\n",
      "epis: 1953   score: 0.0   mem len: 371192   epsilon: 0.3258    steps: 122    lr: 1.6000000000000003e-05     reward: 2.34\n",
      "epis: 1954   score: 1.0   mem len: 371343   epsilon: 0.3255    steps: 151    lr: 1.6000000000000003e-05     reward: 2.34\n",
      "epis: 1955   score: 4.0   mem len: 371602   epsilon: 0.3252    steps: 259    lr: 1.6000000000000003e-05     reward: 2.36\n",
      "epis: 1956   score: 3.0   mem len: 371835   epsilon: 0.3249    steps: 233    lr: 1.6000000000000003e-05     reward: 2.39\n",
      "epis: 1957   score: 2.0   mem len: 372017   epsilon: 0.3246    steps: 182    lr: 1.6000000000000003e-05     reward: 2.37\n",
      "epis: 1958   score: 2.0   mem len: 372197   epsilon: 0.3244    steps: 180    lr: 1.6000000000000003e-05     reward: 2.36\n",
      "epis: 1959   score: 2.0   mem len: 372395   epsilon: 0.3241    steps: 198    lr: 1.6000000000000003e-05     reward: 2.36\n",
      "epis: 1960   score: 6.0   mem len: 372732   epsilon: 0.3236    steps: 337    lr: 1.6000000000000003e-05     reward: 2.4\n",
      "epis: 1961   score: 3.0   mem len: 373000   epsilon: 0.3233    steps: 268    lr: 1.6000000000000003e-05     reward: 2.43\n",
      "epis: 1962   score: 1.0   mem len: 373169   epsilon: 0.323    steps: 169    lr: 1.6000000000000003e-05     reward: 2.44\n",
      "epis: 1963   score: 1.0   mem len: 373320   epsilon: 0.3228    steps: 151    lr: 1.6000000000000003e-05     reward: 2.41\n",
      "epis: 1964   score: 4.0   mem len: 373614   epsilon: 0.3224    steps: 294    lr: 1.6000000000000003e-05     reward: 2.42\n",
      "epis: 1965   score: 1.0   mem len: 373783   epsilon: 0.3222    steps: 169    lr: 1.6000000000000003e-05     reward: 2.43\n",
      "epis: 1966   score: 0.0   mem len: 373906   epsilon: 0.322    steps: 123    lr: 1.6000000000000003e-05     reward: 2.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1967   score: 1.0   mem len: 374075   epsilon: 0.3218    steps: 169    lr: 1.6000000000000003e-05     reward: 2.42\n",
      "epis: 1968   score: 2.0   mem len: 374275   epsilon: 0.3215    steps: 200    lr: 1.6000000000000003e-05     reward: 2.41\n",
      "epis: 1969   score: 2.0   mem len: 374473   epsilon: 0.3212    steps: 198    lr: 1.6000000000000003e-05     reward: 2.43\n",
      "epis: 1970   score: 3.0   mem len: 374700   epsilon: 0.3209    steps: 227    lr: 1.6000000000000003e-05     reward: 2.43\n",
      "epis: 1971   score: 2.0   mem len: 374898   epsilon: 0.3206    steps: 198    lr: 1.6000000000000003e-05     reward: 2.43\n",
      "epis: 1972   score: 0.0   mem len: 375021   epsilon: 0.3205    steps: 123    lr: 1.6000000000000003e-05     reward: 2.4\n",
      "epis: 1973   score: 1.0   mem len: 375190   epsilon: 0.3202    steps: 169    lr: 1.6000000000000003e-05     reward: 2.39\n",
      "epis: 1974   score: 2.0   mem len: 375388   epsilon: 0.32    steps: 198    lr: 1.6000000000000003e-05     reward: 2.39\n",
      "epis: 1975   score: 0.0   mem len: 375510   epsilon: 0.3198    steps: 122    lr: 1.6000000000000003e-05     reward: 2.39\n",
      "epis: 1976   score: 2.0   mem len: 375708   epsilon: 0.3195    steps: 198    lr: 1.6000000000000003e-05     reward: 2.39\n",
      "epis: 1977   score: 0.0   mem len: 375831   epsilon: 0.3194    steps: 123    lr: 1.6000000000000003e-05     reward: 2.37\n",
      "epis: 1978   score: 0.0   mem len: 375953   epsilon: 0.3192    steps: 122    lr: 1.6000000000000003e-05     reward: 2.34\n",
      "epis: 1979   score: 0.0   mem len: 376076   epsilon: 0.319    steps: 123    lr: 1.6000000000000003e-05     reward: 2.33\n",
      "epis: 1980   score: 0.0   mem len: 376199   epsilon: 0.3188    steps: 123    lr: 1.6000000000000003e-05     reward: 2.32\n",
      "epis: 1981   score: 1.0   mem len: 376368   epsilon: 0.3186    steps: 169    lr: 1.6000000000000003e-05     reward: 2.29\n",
      "epis: 1982   score: 1.0   mem len: 376519   epsilon: 0.3184    steps: 151    lr: 1.6000000000000003e-05     reward: 2.28\n",
      "epis: 1983   score: 2.0   mem len: 376717   epsilon: 0.3181    steps: 198    lr: 1.6000000000000003e-05     reward: 2.29\n",
      "epis: 1984   score: 2.0   mem len: 376938   epsilon: 0.3178    steps: 221    lr: 1.6000000000000003e-05     reward: 2.3\n",
      "epis: 1985   score: 2.0   mem len: 377156   epsilon: 0.3175    steps: 218    lr: 1.6000000000000003e-05     reward: 2.32\n",
      "epis: 1986   score: 1.0   mem len: 377307   epsilon: 0.3173    steps: 151    lr: 1.6000000000000003e-05     reward: 2.3\n",
      "epis: 1987   score: 1.0   mem len: 377476   epsilon: 0.3171    steps: 169    lr: 1.6000000000000003e-05     reward: 2.3\n",
      "epis: 1988   score: 1.0   mem len: 377646   epsilon: 0.3168    steps: 170    lr: 1.6000000000000003e-05     reward: 2.27\n",
      "epis: 1989   score: 3.0   mem len: 377909   epsilon: 0.3165    steps: 263    lr: 1.6000000000000003e-05     reward: 2.3\n",
      "epis: 1990   score: 2.0   mem len: 378128   epsilon: 0.3162    steps: 219    lr: 1.6000000000000003e-05     reward: 2.3\n",
      "epis: 1991   score: 0.0   mem len: 378250   epsilon: 0.316    steps: 122    lr: 1.6000000000000003e-05     reward: 2.27\n",
      "epis: 1992   score: 1.0   mem len: 378420   epsilon: 0.3158    steps: 170    lr: 1.6000000000000003e-05     reward: 2.26\n",
      "epis: 1993   score: 2.0   mem len: 378617   epsilon: 0.3155    steps: 197    lr: 1.6000000000000003e-05     reward: 2.26\n",
      "epis: 1994   score: 1.0   mem len: 378768   epsilon: 0.3153    steps: 151    lr: 1.6000000000000003e-05     reward: 2.26\n",
      "epis: 1995   score: 2.0   mem len: 378969   epsilon: 0.315    steps: 201    lr: 1.6000000000000003e-05     reward: 2.26\n",
      "epis: 1996   score: 3.0   mem len: 379232   epsilon: 0.3147    steps: 263    lr: 1.6000000000000003e-05     reward: 2.24\n",
      "epis: 1997   score: 0.0   mem len: 379355   epsilon: 0.3145    steps: 123    lr: 1.6000000000000003e-05     reward: 2.22\n",
      "epis: 1998   score: 2.0   mem len: 379553   epsilon: 0.3142    steps: 198    lr: 1.6000000000000003e-05     reward: 2.21\n",
      "epis: 1999   score: 2.0   mem len: 379751   epsilon: 0.3139    steps: 198    lr: 1.6000000000000003e-05     reward: 2.21\n",
      "epis: 2000   score: 3.0   mem len: 379980   epsilon: 0.3136    steps: 229    lr: 1.6000000000000003e-05     reward: 2.22\n",
      "epis: 2001   score: 3.0   mem len: 380193   epsilon: 0.3133    steps: 213    lr: 1.6000000000000003e-05     reward: 2.2\n",
      "epis: 2002   score: 1.0   mem len: 380345   epsilon: 0.3131    steps: 152    lr: 1.6000000000000003e-05     reward: 2.19\n",
      "epis: 2003   score: 1.0   mem len: 380515   epsilon: 0.3129    steps: 170    lr: 1.6000000000000003e-05     reward: 2.11\n",
      "epis: 2004   score: 0.0   mem len: 380638   epsilon: 0.3127    steps: 123    lr: 1.6000000000000003e-05     reward: 2.09\n",
      "epis: 2005   score: 0.0   mem len: 380760   epsilon: 0.3125    steps: 122    lr: 1.6000000000000003e-05     reward: 2.08\n",
      "epis: 2006   score: 2.0   mem len: 380940   epsilon: 0.3123    steps: 180    lr: 1.6000000000000003e-05     reward: 2.05\n",
      "epis: 2007   score: 5.0   mem len: 381266   epsilon: 0.3119    steps: 326    lr: 1.6000000000000003e-05     reward: 2.05\n",
      "epis: 2008   score: 0.0   mem len: 381389   epsilon: 0.3117    steps: 123    lr: 1.6000000000000003e-05     reward: 2.02\n",
      "epis: 2009   score: 3.0   mem len: 381600   epsilon: 0.3114    steps: 211    lr: 1.6000000000000003e-05     reward: 2.03\n",
      "epis: 2010   score: 0.0   mem len: 381722   epsilon: 0.3112    steps: 122    lr: 1.6000000000000003e-05     reward: 1.99\n",
      "epis: 2011   score: 0.0   mem len: 381844   epsilon: 0.3111    steps: 122    lr: 1.6000000000000003e-05     reward: 1.96\n",
      "epis: 2012   score: 1.0   mem len: 382013   epsilon: 0.3108    steps: 169    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 2013   score: 0.0   mem len: 382136   epsilon: 0.3107    steps: 123    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 2014   score: 1.0   mem len: 382305   epsilon: 0.3104    steps: 169    lr: 1.6000000000000003e-05     reward: 1.93\n",
      "epis: 2015   score: 0.0   mem len: 382428   epsilon: 0.3102    steps: 123    lr: 1.6000000000000003e-05     reward: 1.92\n",
      "epis: 2016   score: 0.0   mem len: 382551   epsilon: 0.3101    steps: 123    lr: 1.6000000000000003e-05     reward: 1.9\n",
      "epis: 2017   score: 1.0   mem len: 382719   epsilon: 0.3098    steps: 168    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 2018   score: 2.0   mem len: 382916   epsilon: 0.3096    steps: 197    lr: 1.6000000000000003e-05     reward: 1.91\n",
      "epis: 2019   score: 0.0   mem len: 383039   epsilon: 0.3094    steps: 123    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 2020   score: 0.0   mem len: 383162   epsilon: 0.3092    steps: 123    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 2021   score: 0.0   mem len: 383285   epsilon: 0.3091    steps: 123    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 2022   score: 1.0   mem len: 383455   epsilon: 0.3088    steps: 170    lr: 1.6000000000000003e-05     reward: 1.86\n",
      "epis: 2023   score: 0.0   mem len: 383578   epsilon: 0.3087    steps: 123    lr: 1.6000000000000003e-05     reward: 1.83\n",
      "epis: 2024   score: 6.0   mem len: 384002   epsilon: 0.3081    steps: 424    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 2025   score: 1.0   mem len: 384173   epsilon: 0.3078    steps: 171    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 2026   score: 0.0   mem len: 384296   epsilon: 0.3077    steps: 123    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 2027   score: 1.0   mem len: 384467   epsilon: 0.3074    steps: 171    lr: 1.6000000000000003e-05     reward: 1.82\n",
      "epis: 2028   score: 0.0   mem len: 384590   epsilon: 0.3073    steps: 123    lr: 1.6000000000000003e-05     reward: 1.82\n",
      "epis: 2029   score: 2.0   mem len: 384788   epsilon: 0.307    steps: 198    lr: 1.6000000000000003e-05     reward: 1.83\n",
      "epis: 2030   score: 2.0   mem len: 384987   epsilon: 0.3067    steps: 199    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 2031   score: 5.0   mem len: 385321   epsilon: 0.3063    steps: 334    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 2032   score: 1.0   mem len: 385489   epsilon: 0.306    steps: 168    lr: 1.6000000000000003e-05     reward: 1.88\n",
      "epis: 2033   score: 3.0   mem len: 385735   epsilon: 0.3057    steps: 246    lr: 1.6000000000000003e-05     reward: 1.89\n",
      "epis: 2034   score: 1.0   mem len: 385904   epsilon: 0.3055    steps: 169    lr: 1.6000000000000003e-05     reward: 1.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2035   score: 2.0   mem len: 386120   epsilon: 0.3052    steps: 216    lr: 1.6000000000000003e-05     reward: 1.85\n",
      "epis: 2036   score: 1.0   mem len: 386288   epsilon: 0.3049    steps: 168    lr: 1.6000000000000003e-05     reward: 1.84\n",
      "epis: 2037   score: 0.0   mem len: 386410   epsilon: 0.3048    steps: 122    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 2038   score: 2.0   mem len: 386589   epsilon: 0.3045    steps: 179    lr: 1.6000000000000003e-05     reward: 1.8\n",
      "epis: 2039   score: 2.0   mem len: 386789   epsilon: 0.3042    steps: 200    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 2040   score: 2.0   mem len: 387008   epsilon: 0.3039    steps: 219    lr: 1.6000000000000003e-05     reward: 1.81\n",
      "epis: 2041   score: 2.0   mem len: 387205   epsilon: 0.3037    steps: 197    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 2042   score: 5.0   mem len: 387513   epsilon: 0.3032    steps: 308    lr: 1.6000000000000003e-05     reward: 1.82\n",
      "epis: 2043   score: 1.0   mem len: 387664   epsilon: 0.303    steps: 151    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 2044   score: 2.0   mem len: 387861   epsilon: 0.3028    steps: 197    lr: 1.6000000000000003e-05     reward: 1.8\n",
      "epis: 2045   score: 2.0   mem len: 388041   epsilon: 0.3025    steps: 180    lr: 1.6000000000000003e-05     reward: 1.78\n",
      "epis: 2046   score: 2.0   mem len: 388256   epsilon: 0.3022    steps: 215    lr: 1.6000000000000003e-05     reward: 1.7\n",
      "epis: 2047   score: 2.0   mem len: 388454   epsilon: 0.3019    steps: 198    lr: 1.6000000000000003e-05     reward: 1.64\n",
      "epis: 2048   score: 0.0   mem len: 388577   epsilon: 0.3018    steps: 123    lr: 1.6000000000000003e-05     reward: 1.63\n",
      "epis: 2049   score: 0.0   mem len: 388700   epsilon: 0.3016    steps: 123    lr: 1.6000000000000003e-05     reward: 1.56\n",
      "epis: 2050   score: 1.0   mem len: 388869   epsilon: 0.3014    steps: 169    lr: 1.6000000000000003e-05     reward: 1.54\n",
      "epis: 2051   score: 2.0   mem len: 389066   epsilon: 0.3011    steps: 197    lr: 1.6000000000000003e-05     reward: 1.54\n",
      "epis: 2052   score: 3.0   mem len: 389313   epsilon: 0.3007    steps: 247    lr: 1.6000000000000003e-05     reward: 1.51\n",
      "epis: 2053   score: 1.0   mem len: 389463   epsilon: 0.3005    steps: 150    lr: 1.6000000000000003e-05     reward: 1.52\n",
      "epis: 2054   score: 2.0   mem len: 389645   epsilon: 0.3003    steps: 182    lr: 1.6000000000000003e-05     reward: 1.53\n",
      "epis: 2055   score: 6.0   mem len: 390041   epsilon: 0.2997    steps: 396    lr: 1.6000000000000003e-05     reward: 1.55\n",
      "epis: 2056   score: 2.0   mem len: 390239   epsilon: 0.2995    steps: 198    lr: 1.6000000000000003e-05     reward: 1.54\n",
      "epis: 2057   score: 1.0   mem len: 390409   epsilon: 0.2992    steps: 170    lr: 1.6000000000000003e-05     reward: 1.53\n",
      "epis: 2058   score: 2.0   mem len: 390607   epsilon: 0.299    steps: 198    lr: 1.6000000000000003e-05     reward: 1.53\n",
      "epis: 2059   score: 4.0   mem len: 390884   epsilon: 0.2986    steps: 277    lr: 1.6000000000000003e-05     reward: 1.55\n",
      "epis: 2060   score: 2.0   mem len: 391102   epsilon: 0.2983    steps: 218    lr: 1.6000000000000003e-05     reward: 1.51\n",
      "epis: 2061   score: 3.0   mem len: 391331   epsilon: 0.298    steps: 229    lr: 1.6000000000000003e-05     reward: 1.51\n",
      "epis: 2062   score: 2.0   mem len: 391529   epsilon: 0.2977    steps: 198    lr: 1.6000000000000003e-05     reward: 1.52\n",
      "epis: 2063   score: 4.0   mem len: 391824   epsilon: 0.2973    steps: 295    lr: 1.6000000000000003e-05     reward: 1.55\n",
      "epis: 2064   score: 0.0   mem len: 391946   epsilon: 0.2971    steps: 122    lr: 1.6000000000000003e-05     reward: 1.51\n",
      "epis: 2065   score: 2.0   mem len: 392145   epsilon: 0.2968    steps: 199    lr: 1.6000000000000003e-05     reward: 1.52\n",
      "epis: 2066   score: 2.0   mem len: 392342   epsilon: 0.2966    steps: 197    lr: 1.6000000000000003e-05     reward: 1.54\n",
      "epis: 2067   score: 0.0   mem len: 392465   epsilon: 0.2964    steps: 123    lr: 1.6000000000000003e-05     reward: 1.53\n",
      "epis: 2068   score: 3.0   mem len: 392731   epsilon: 0.296    steps: 266    lr: 1.6000000000000003e-05     reward: 1.54\n",
      "epis: 2069   score: 1.0   mem len: 392882   epsilon: 0.2958    steps: 151    lr: 1.6000000000000003e-05     reward: 1.53\n",
      "epis: 2070   score: 3.0   mem len: 393110   epsilon: 0.2955    steps: 228    lr: 1.6000000000000003e-05     reward: 1.53\n",
      "epis: 2071   score: 0.0   mem len: 393233   epsilon: 0.2953    steps: 123    lr: 1.6000000000000003e-05     reward: 1.51\n",
      "epis: 2072   score: 3.0   mem len: 393459   epsilon: 0.295    steps: 226    lr: 1.6000000000000003e-05     reward: 1.54\n",
      "epis: 2073   score: 2.0   mem len: 393658   epsilon: 0.2948    steps: 199    lr: 1.6000000000000003e-05     reward: 1.55\n",
      "epis: 2074   score: 3.0   mem len: 393907   epsilon: 0.2944    steps: 249    lr: 1.6000000000000003e-05     reward: 1.56\n",
      "epis: 2075   score: 3.0   mem len: 394153   epsilon: 0.2941    steps: 246    lr: 1.6000000000000003e-05     reward: 1.59\n",
      "epis: 2076   score: 3.0   mem len: 394423   epsilon: 0.2937    steps: 270    lr: 1.6000000000000003e-05     reward: 1.6\n",
      "epis: 2077   score: 0.0   mem len: 394546   epsilon: 0.2935    steps: 123    lr: 1.6000000000000003e-05     reward: 1.6\n",
      "epis: 2078   score: 1.0   mem len: 394696   epsilon: 0.2933    steps: 150    lr: 1.6000000000000003e-05     reward: 1.61\n",
      "epis: 2079   score: 2.0   mem len: 394894   epsilon: 0.293    steps: 198    lr: 1.6000000000000003e-05     reward: 1.63\n",
      "epis: 2080   score: 2.0   mem len: 395112   epsilon: 0.2927    steps: 218    lr: 1.6000000000000003e-05     reward: 1.65\n",
      "epis: 2081   score: 2.0   mem len: 395309   epsilon: 0.2925    steps: 197    lr: 1.6000000000000003e-05     reward: 1.66\n",
      "epis: 2082   score: 1.0   mem len: 395477   epsilon: 0.2922    steps: 168    lr: 1.6000000000000003e-05     reward: 1.66\n",
      "epis: 2083   score: 1.0   mem len: 395648   epsilon: 0.292    steps: 171    lr: 1.6000000000000003e-05     reward: 1.65\n",
      "epis: 2084   score: 1.0   mem len: 395799   epsilon: 0.2918    steps: 151    lr: 1.6000000000000003e-05     reward: 1.64\n",
      "epis: 2085   score: 1.0   mem len: 395971   epsilon: 0.2916    steps: 172    lr: 1.6000000000000003e-05     reward: 1.63\n",
      "epis: 2086   score: 3.0   mem len: 396200   epsilon: 0.2912    steps: 229    lr: 1.6000000000000003e-05     reward: 1.65\n",
      "epis: 2087   score: 2.0   mem len: 396379   epsilon: 0.291    steps: 179    lr: 1.6000000000000003e-05     reward: 1.66\n",
      "epis: 2088   score: 2.0   mem len: 396577   epsilon: 0.2907    steps: 198    lr: 1.6000000000000003e-05     reward: 1.67\n",
      "epis: 2089   score: 2.0   mem len: 396775   epsilon: 0.2904    steps: 198    lr: 1.6000000000000003e-05     reward: 1.66\n",
      "epis: 2090   score: 2.0   mem len: 396974   epsilon: 0.2902    steps: 199    lr: 1.6000000000000003e-05     reward: 1.66\n",
      "epis: 2091   score: 5.0   mem len: 397302   epsilon: 0.2897    steps: 328    lr: 1.6000000000000003e-05     reward: 1.71\n",
      "epis: 2092   score: 3.0   mem len: 397531   epsilon: 0.2894    steps: 229    lr: 1.6000000000000003e-05     reward: 1.73\n",
      "epis: 2093   score: 0.0   mem len: 397653   epsilon: 0.2892    steps: 122    lr: 1.6000000000000003e-05     reward: 1.71\n",
      "epis: 2094   score: 3.0   mem len: 397881   epsilon: 0.2889    steps: 228    lr: 1.6000000000000003e-05     reward: 1.73\n",
      "epis: 2095   score: 3.0   mem len: 398127   epsilon: 0.2886    steps: 246    lr: 1.6000000000000003e-05     reward: 1.74\n",
      "epis: 2096   score: 4.0   mem len: 398401   epsilon: 0.2882    steps: 274    lr: 1.6000000000000003e-05     reward: 1.75\n",
      "epis: 2097   score: 4.0   mem len: 398661   epsilon: 0.2878    steps: 260    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 2098   score: 0.0   mem len: 398784   epsilon: 0.2877    steps: 123    lr: 1.6000000000000003e-05     reward: 1.77\n",
      "epis: 2099   score: 2.0   mem len: 398982   epsilon: 0.2874    steps: 198    lr: 1.6000000000000003e-05     reward: 1.77\n",
      "epis: 2100   score: 5.0   mem len: 399305   epsilon: 0.287    steps: 323    lr: 1.6000000000000003e-05     reward: 1.79\n",
      "epis: 2101   score: 2.0   mem len: 399503   epsilon: 0.2867    steps: 198    lr: 1.6000000000000003e-05     reward: 1.78\n",
      "epis: 2102   score: 2.0   mem len: 399701   epsilon: 0.2864    steps: 198    lr: 1.6000000000000003e-05     reward: 1.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2103   score: 3.0   mem len: 399932   epsilon: 0.2861    steps: 231    lr: 1.6000000000000003e-05     reward: 1.81\n",
      "epis: 2104   score: 3.0   mem len: 400177   epsilon: 0.2858    steps: 245    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2105   score: 4.0   mem len: 400455   epsilon: 0.2854    steps: 278    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2106   score: 2.0   mem len: 400653   epsilon: 0.2851    steps: 198    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2107   score: 1.0   mem len: 400822   epsilon: 0.2849    steps: 169    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2108   score: 2.0   mem len: 401004   epsilon: 0.2846    steps: 182    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2109   score: 2.0   mem len: 401201   epsilon: 0.2843    steps: 197    lr: 6.400000000000001e-06     reward: 1.85\n",
      "epis: 2110   score: 2.0   mem len: 401399   epsilon: 0.2841    steps: 198    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2111   score: 2.0   mem len: 401597   epsilon: 0.2838    steps: 198    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2112   score: 3.0   mem len: 401825   epsilon: 0.2835    steps: 228    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2113   score: 2.0   mem len: 402023   epsilon: 0.2832    steps: 198    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2114   score: 3.0   mem len: 402251   epsilon: 0.2829    steps: 228    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2115   score: 2.0   mem len: 402449   epsilon: 0.2826    steps: 198    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2116   score: 3.0   mem len: 402676   epsilon: 0.2823    steps: 227    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2117   score: 1.0   mem len: 402844   epsilon: 0.2821    steps: 168    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2118   score: 3.0   mem len: 403071   epsilon: 0.2818    steps: 227    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2119   score: 3.0   mem len: 403317   epsilon: 0.2814    steps: 246    lr: 6.400000000000001e-06     reward: 2.04\n",
      "epis: 2120   score: 2.0   mem len: 403515   epsilon: 0.2811    steps: 198    lr: 6.400000000000001e-06     reward: 2.06\n",
      "epis: 2121   score: 2.0   mem len: 403713   epsilon: 0.2809    steps: 198    lr: 6.400000000000001e-06     reward: 2.08\n",
      "epis: 2122   score: 3.0   mem len: 403942   epsilon: 0.2806    steps: 229    lr: 6.400000000000001e-06     reward: 2.1\n",
      "epis: 2123   score: 2.0   mem len: 404139   epsilon: 0.2803    steps: 197    lr: 6.400000000000001e-06     reward: 2.12\n",
      "epis: 2124   score: 2.0   mem len: 404356   epsilon: 0.28    steps: 217    lr: 6.400000000000001e-06     reward: 2.08\n",
      "epis: 2125   score: 2.0   mem len: 404553   epsilon: 0.2797    steps: 197    lr: 6.400000000000001e-06     reward: 2.09\n",
      "epis: 2126   score: 1.0   mem len: 404722   epsilon: 0.2795    steps: 169    lr: 6.400000000000001e-06     reward: 2.1\n",
      "epis: 2127   score: 2.0   mem len: 404919   epsilon: 0.2792    steps: 197    lr: 6.400000000000001e-06     reward: 2.11\n",
      "epis: 2128   score: 5.0   mem len: 405241   epsilon: 0.2788    steps: 322    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2129   score: 4.0   mem len: 405517   epsilon: 0.2784    steps: 276    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2130   score: 4.0   mem len: 405815   epsilon: 0.278    steps: 298    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2131   score: 3.0   mem len: 406060   epsilon: 0.2776    steps: 245    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2132   score: 1.0   mem len: 406229   epsilon: 0.2774    steps: 169    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2133   score: 3.0   mem len: 406474   epsilon: 0.2771    steps: 245    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2134   score: 2.0   mem len: 406672   epsilon: 0.2768    steps: 198    lr: 6.400000000000001e-06     reward: 2.19\n",
      "epis: 2135   score: 3.0   mem len: 406902   epsilon: 0.2765    steps: 230    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2136   score: 3.0   mem len: 407149   epsilon: 0.2761    steps: 247    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2137   score: 2.0   mem len: 407347   epsilon: 0.2759    steps: 198    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2138   score: 1.0   mem len: 407515   epsilon: 0.2756    steps: 168    lr: 6.400000000000001e-06     reward: 2.23\n",
      "epis: 2139   score: 2.0   mem len: 407712   epsilon: 0.2754    steps: 197    lr: 6.400000000000001e-06     reward: 2.23\n",
      "epis: 2140   score: 2.0   mem len: 407909   epsilon: 0.2751    steps: 197    lr: 6.400000000000001e-06     reward: 2.23\n",
      "epis: 2141   score: 3.0   mem len: 408154   epsilon: 0.2747    steps: 245    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2142   score: 2.0   mem len: 408352   epsilon: 0.2745    steps: 198    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2143   score: 1.0   mem len: 408505   epsilon: 0.2743    steps: 153    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2144   score: 1.0   mem len: 408674   epsilon: 0.274    steps: 169    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2145   score: 2.0   mem len: 408872   epsilon: 0.2738    steps: 198    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2146   score: 4.0   mem len: 409164   epsilon: 0.2734    steps: 292    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2147   score: 2.0   mem len: 409362   epsilon: 0.2731    steps: 198    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2148   score: 3.0   mem len: 409590   epsilon: 0.2728    steps: 228    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2149   score: 2.0   mem len: 409788   epsilon: 0.2725    steps: 198    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2150   score: 1.0   mem len: 409957   epsilon: 0.2723    steps: 169    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2151   score: 3.0   mem len: 410203   epsilon: 0.2719    steps: 246    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2152   score: 1.0   mem len: 410372   epsilon: 0.2717    steps: 169    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2153   score: 2.0   mem len: 410554   epsilon: 0.2714    steps: 182    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2154   score: 2.0   mem len: 410752   epsilon: 0.2712    steps: 198    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2155   score: 2.0   mem len: 410949   epsilon: 0.2709    steps: 197    lr: 6.400000000000001e-06     reward: 2.23\n",
      "epis: 2156   score: 3.0   mem len: 411194   epsilon: 0.2706    steps: 245    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2157   score: 2.0   mem len: 411391   epsilon: 0.2703    steps: 197    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2158   score: 5.0   mem len: 411714   epsilon: 0.2698    steps: 323    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2159   score: 3.0   mem len: 411959   epsilon: 0.2695    steps: 245    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2160   score: 4.0   mem len: 412270   epsilon: 0.2691    steps: 311    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2161   score: 0.0   mem len: 412392   epsilon: 0.2689    steps: 122    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2162   score: 2.0   mem len: 412590   epsilon: 0.2686    steps: 198    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2163   score: 2.0   mem len: 412787   epsilon: 0.2684    steps: 197    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2164   score: 1.0   mem len: 412956   epsilon: 0.2681    steps: 169    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2165   score: 2.0   mem len: 413154   epsilon: 0.2678    steps: 198    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2166   score: 0.0   mem len: 413276   epsilon: 0.2677    steps: 122    lr: 6.400000000000001e-06     reward: 2.23\n",
      "epis: 2167   score: 1.0   mem len: 413445   epsilon: 0.2674    steps: 169    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2168   score: 3.0   mem len: 413691   epsilon: 0.2671    steps: 246    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2169   score: 2.0   mem len: 413889   epsilon: 0.2668    steps: 198    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2170   score: 3.0   mem len: 414135   epsilon: 0.2665    steps: 246    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2171   score: 2.0   mem len: 414332   epsilon: 0.2662    steps: 197    lr: 6.400000000000001e-06     reward: 2.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2172   score: 2.0   mem len: 414530   epsilon: 0.2659    steps: 198    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2173   score: 2.0   mem len: 414728   epsilon: 0.2657    steps: 198    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2174   score: 3.0   mem len: 414955   epsilon: 0.2654    steps: 227    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2175   score: 1.0   mem len: 415125   epsilon: 0.2651    steps: 170    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2176   score: 3.0   mem len: 415354   epsilon: 0.2648    steps: 229    lr: 6.400000000000001e-06     reward: 2.24\n",
      "epis: 2177   score: 2.0   mem len: 415552   epsilon: 0.2645    steps: 198    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2178   score: 2.0   mem len: 415750   epsilon: 0.2643    steps: 198    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2179   score: 2.0   mem len: 415948   epsilon: 0.264    steps: 198    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2180   score: 3.0   mem len: 416194   epsilon: 0.2637    steps: 246    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2181   score: 0.0   mem len: 416317   epsilon: 0.2635    steps: 123    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2182   score: 4.0   mem len: 416592   epsilon: 0.2631    steps: 275    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2183   score: 1.0   mem len: 416761   epsilon: 0.2629    steps: 169    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2184   score: 2.0   mem len: 416962   epsilon: 0.2626    steps: 201    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2185   score: 2.0   mem len: 417159   epsilon: 0.2623    steps: 197    lr: 6.400000000000001e-06     reward: 2.31\n",
      "epis: 2186   score: 2.0   mem len: 417357   epsilon: 0.262    steps: 198    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2187   score: 2.0   mem len: 417555   epsilon: 0.2618    steps: 198    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2188   score: 3.0   mem len: 417801   epsilon: 0.2614    steps: 246    lr: 6.400000000000001e-06     reward: 2.31\n",
      "epis: 2189   score: 1.0   mem len: 417970   epsilon: 0.2612    steps: 169    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2190   score: 2.0   mem len: 418168   epsilon: 0.2609    steps: 198    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2191   score: 2.0   mem len: 418384   epsilon: 0.2606    steps: 216    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2192   score: 2.0   mem len: 418581   epsilon: 0.2604    steps: 197    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2193   score: 0.0   mem len: 418704   epsilon: 0.2602    steps: 123    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2194   score: 3.0   mem len: 418931   epsilon: 0.2599    steps: 227    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2195   score: 5.0   mem len: 419257   epsilon: 0.2594    steps: 326    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2196   score: 2.0   mem len: 419454   epsilon: 0.2592    steps: 197    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2197   score: 1.0   mem len: 419623   epsilon: 0.2589    steps: 169    lr: 6.400000000000001e-06     reward: 2.23\n",
      "epis: 2198   score: 2.0   mem len: 419821   epsilon: 0.2586    steps: 198    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2199   score: 2.0   mem len: 420019   epsilon: 0.2584    steps: 198    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2200   score: 2.0   mem len: 420217   epsilon: 0.2581    steps: 198    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2201   score: 2.0   mem len: 420415   epsilon: 0.2578    steps: 198    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2202   score: 2.0   mem len: 420613   epsilon: 0.2576    steps: 198    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2203   score: 0.0   mem len: 420736   epsilon: 0.2574    steps: 123    lr: 6.400000000000001e-06     reward: 2.19\n",
      "epis: 2204   score: 2.0   mem len: 420934   epsilon: 0.2571    steps: 198    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2205   score: 5.0   mem len: 421246   epsilon: 0.2567    steps: 312    lr: 6.400000000000001e-06     reward: 2.19\n",
      "epis: 2206   score: 0.0   mem len: 421369   epsilon: 0.2565    steps: 123    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2207   score: 3.0   mem len: 421596   epsilon: 0.2562    steps: 227    lr: 6.400000000000001e-06     reward: 2.19\n",
      "epis: 2208   score: 2.0   mem len: 421794   epsilon: 0.2559    steps: 198    lr: 6.400000000000001e-06     reward: 2.19\n",
      "epis: 2209   score: 3.0   mem len: 422039   epsilon: 0.2556    steps: 245    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2210   score: 2.0   mem len: 422239   epsilon: 0.2553    steps: 200    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2211   score: 2.0   mem len: 422454   epsilon: 0.255    steps: 215    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2212   score: 6.0   mem len: 422795   epsilon: 0.2545    steps: 341    lr: 6.400000000000001e-06     reward: 2.23\n",
      "epis: 2213   score: 6.0   mem len: 423046   epsilon: 0.2542    steps: 251    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2214   score: 3.0   mem len: 423291   epsilon: 0.2539    steps: 245    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2215   score: 3.0   mem len: 423520   epsilon: 0.2535    steps: 229    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2216   score: 1.0   mem len: 423691   epsilon: 0.2533    steps: 171    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2217   score: 2.0   mem len: 423889   epsilon: 0.253    steps: 198    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2218   score: 2.0   mem len: 424086   epsilon: 0.2528    steps: 197    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2219   score: 5.0   mem len: 424450   epsilon: 0.2523    steps: 364    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2220   score: 1.0   mem len: 424619   epsilon: 0.252    steps: 169    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2221   score: 2.0   mem len: 424816   epsilon: 0.2518    steps: 197    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2222   score: 1.0   mem len: 424967   epsilon: 0.2515    steps: 151    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2223   score: 6.0   mem len: 425348   epsilon: 0.251    steps: 381    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2224   score: 3.0   mem len: 425593   epsilon: 0.2507    steps: 245    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2225   score: 2.0   mem len: 425790   epsilon: 0.2504    steps: 197    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2226   score: 3.0   mem len: 426000   epsilon: 0.2501    steps: 210    lr: 6.400000000000001e-06     reward: 2.32\n",
      "epis: 2227   score: 2.0   mem len: 426198   epsilon: 0.2498    steps: 198    lr: 6.400000000000001e-06     reward: 2.32\n",
      "epis: 2228   score: 3.0   mem len: 426444   epsilon: 0.2495    steps: 246    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2229   score: 4.0   mem len: 426721   epsilon: 0.2491    steps: 277    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2230   score: 0.0   mem len: 426844   epsilon: 0.249    steps: 123    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2231   score: 2.0   mem len: 427042   epsilon: 0.2487    steps: 198    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2232   score: 3.0   mem len: 427287   epsilon: 0.2483    steps: 245    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2233   score: 1.0   mem len: 427437   epsilon: 0.2481    steps: 150    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2234   score: 4.0   mem len: 427713   epsilon: 0.2478    steps: 276    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2235   score: 2.0   mem len: 427911   epsilon: 0.2475    steps: 198    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2236   score: 2.0   mem len: 428109   epsilon: 0.2472    steps: 198    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2237   score: 3.0   mem len: 428336   epsilon: 0.2469    steps: 227    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2238   score: 4.0   mem len: 428650   epsilon: 0.2465    steps: 314    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2239   score: 3.0   mem len: 428879   epsilon: 0.2461    steps: 229    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2240   score: 1.0   mem len: 429048   epsilon: 0.2459    steps: 169    lr: 6.400000000000001e-06     reward: 2.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2241   score: 3.0   mem len: 429294   epsilon: 0.2456    steps: 246    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2242   score: 2.0   mem len: 429492   epsilon: 0.2453    steps: 198    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2243   score: 2.0   mem len: 429689   epsilon: 0.245    steps: 197    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2244   score: 1.0   mem len: 429858   epsilon: 0.2448    steps: 169    lr: 6.400000000000001e-06     reward: 2.3\n",
      "epis: 2245   score: 3.0   mem len: 430105   epsilon: 0.2445    steps: 247    lr: 6.400000000000001e-06     reward: 2.31\n",
      "epis: 2246   score: 2.0   mem len: 430302   epsilon: 0.2442    steps: 197    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2247   score: 1.0   mem len: 430471   epsilon: 0.2439    steps: 169    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2248   score: 2.0   mem len: 430668   epsilon: 0.2437    steps: 197    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2249   score: 2.0   mem len: 430866   epsilon: 0.2434    steps: 198    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2250   score: 2.0   mem len: 431064   epsilon: 0.2431    steps: 198    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2251   score: 2.0   mem len: 431262   epsilon: 0.2429    steps: 198    lr: 6.400000000000001e-06     reward: 2.27\n",
      "epis: 2252   score: 2.0   mem len: 431460   epsilon: 0.2426    steps: 198    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2253   score: 3.0   mem len: 431706   epsilon: 0.2422    steps: 246    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2254   score: 2.0   mem len: 431904   epsilon: 0.242    steps: 198    lr: 6.400000000000001e-06     reward: 2.29\n",
      "epis: 2255   score: 1.0   mem len: 432073   epsilon: 0.2417    steps: 169    lr: 6.400000000000001e-06     reward: 2.28\n",
      "epis: 2256   score: 1.0   mem len: 432241   epsilon: 0.2415    steps: 168    lr: 6.400000000000001e-06     reward: 2.26\n",
      "epis: 2257   score: 1.0   mem len: 432410   epsilon: 0.2413    steps: 169    lr: 6.400000000000001e-06     reward: 2.25\n",
      "epis: 2258   score: 1.0   mem len: 432579   epsilon: 0.241    steps: 169    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2259   score: 0.0   mem len: 432701   epsilon: 0.2409    steps: 122    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2260   score: 3.0   mem len: 432929   epsilon: 0.2406    steps: 228    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2261   score: 1.0   mem len: 433098   epsilon: 0.2403    steps: 169    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2262   score: 1.0   mem len: 433266   epsilon: 0.2401    steps: 168    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2263   score: 0.0   mem len: 433388   epsilon: 0.2399    steps: 122    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2264   score: 3.0   mem len: 433651   epsilon: 0.2396    steps: 263    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2265   score: 0.0   mem len: 433773   epsilon: 0.2394    steps: 122    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2266   score: 2.0   mem len: 433991   epsilon: 0.2391    steps: 218    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2267   score: 2.0   mem len: 434188   epsilon: 0.2388    steps: 197    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2268   score: 2.0   mem len: 434386   epsilon: 0.2385    steps: 198    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2269   score: 2.0   mem len: 434586   epsilon: 0.2383    steps: 200    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2270   score: 2.0   mem len: 434783   epsilon: 0.238    steps: 197    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2271   score: 2.0   mem len: 434981   epsilon: 0.2377    steps: 198    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2272   score: 2.0   mem len: 435198   epsilon: 0.2374    steps: 217    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2273   score: 0.0   mem len: 435320   epsilon: 0.2373    steps: 122    lr: 6.400000000000001e-06     reward: 2.14\n",
      "epis: 2274   score: 2.0   mem len: 435502   epsilon: 0.237    steps: 182    lr: 6.400000000000001e-06     reward: 2.13\n",
      "epis: 2275   score: 1.0   mem len: 435670   epsilon: 0.2368    steps: 168    lr: 6.400000000000001e-06     reward: 2.13\n",
      "epis: 2276   score: 1.0   mem len: 435838   epsilon: 0.2365    steps: 168    lr: 6.400000000000001e-06     reward: 2.11\n",
      "epis: 2277   score: 2.0   mem len: 436036   epsilon: 0.2363    steps: 198    lr: 6.400000000000001e-06     reward: 2.11\n",
      "epis: 2278   score: 5.0   mem len: 436352   epsilon: 0.2358    steps: 316    lr: 6.400000000000001e-06     reward: 2.14\n",
      "epis: 2279   score: 3.0   mem len: 436615   epsilon: 0.2355    steps: 263    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2280   score: 2.0   mem len: 436812   epsilon: 0.2352    steps: 197    lr: 6.400000000000001e-06     reward: 2.14\n",
      "epis: 2281   score: 2.0   mem len: 437011   epsilon: 0.2349    steps: 199    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2282   score: 3.0   mem len: 437239   epsilon: 0.2346    steps: 228    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2283   score: 3.0   mem len: 437466   epsilon: 0.2343    steps: 227    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2284   score: 2.0   mem len: 437664   epsilon: 0.234    steps: 198    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2285   score: 3.0   mem len: 437877   epsilon: 0.2337    steps: 213    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2286   score: 2.0   mem len: 438099   epsilon: 0.2334    steps: 222    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2287   score: 5.0   mem len: 438446   epsilon: 0.2329    steps: 347    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2288   score: 4.0   mem len: 438739   epsilon: 0.2325    steps: 293    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2289   score: 1.0   mem len: 438908   epsilon: 0.2323    steps: 169    lr: 6.400000000000001e-06     reward: 2.22\n",
      "epis: 2290   score: 0.0   mem len: 439031   epsilon: 0.2321    steps: 123    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2291   score: 3.0   mem len: 439277   epsilon: 0.2318    steps: 246    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2292   score: 1.0   mem len: 439446   epsilon: 0.2316    steps: 169    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2293   score: 1.0   mem len: 439598   epsilon: 0.2314    steps: 152    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2294   score: 3.0   mem len: 439844   epsilon: 0.231    steps: 246    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2295   score: 2.0   mem len: 440041   epsilon: 0.2307    steps: 197    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2296   score: 1.0   mem len: 440210   epsilon: 0.2305    steps: 169    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2297   score: 1.0   mem len: 440379   epsilon: 0.2303    steps: 169    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2298   score: 1.0   mem len: 440530   epsilon: 0.2301    steps: 151    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2299   score: 1.0   mem len: 440698   epsilon: 0.2298    steps: 168    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2300   score: 2.0   mem len: 440896   epsilon: 0.2296    steps: 198    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2301   score: 4.0   mem len: 441171   epsilon: 0.2292    steps: 275    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2302   score: 3.0   mem len: 441399   epsilon: 0.2289    steps: 228    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2303   score: 2.0   mem len: 441597   epsilon: 0.2286    steps: 198    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2304   score: 2.0   mem len: 441795   epsilon: 0.2283    steps: 198    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2305   score: 2.0   mem len: 441993   epsilon: 0.228    steps: 198    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2306   score: 3.0   mem len: 442238   epsilon: 0.2277    steps: 245    lr: 6.400000000000001e-06     reward: 2.2\n",
      "epis: 2307   score: 4.0   mem len: 442553   epsilon: 0.2273    steps: 315    lr: 6.400000000000001e-06     reward: 2.21\n",
      "epis: 2308   score: 0.0   mem len: 442676   epsilon: 0.2271    steps: 123    lr: 6.400000000000001e-06     reward: 2.19\n",
      "epis: 2309   score: 2.0   mem len: 442874   epsilon: 0.2268    steps: 198    lr: 6.400000000000001e-06     reward: 2.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2310   score: 2.0   mem len: 443053   epsilon: 0.2266    steps: 179    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2311   score: 0.0   mem len: 443176   epsilon: 0.2264    steps: 123    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2312   score: 4.0   mem len: 443454   epsilon: 0.226    steps: 278    lr: 6.400000000000001e-06     reward: 2.14\n",
      "epis: 2313   score: 7.0   mem len: 443885   epsilon: 0.2254    steps: 431    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2314   score: 6.0   mem len: 444285   epsilon: 0.2249    steps: 400    lr: 6.400000000000001e-06     reward: 2.18\n",
      "epis: 2315   score: 2.0   mem len: 444485   epsilon: 0.2246    steps: 200    lr: 6.400000000000001e-06     reward: 2.17\n",
      "epis: 2316   score: 0.0   mem len: 444608   epsilon: 0.2244    steps: 123    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2317   score: 1.0   mem len: 444759   epsilon: 0.2242    steps: 151    lr: 6.400000000000001e-06     reward: 2.15\n",
      "epis: 2318   score: 3.0   mem len: 444985   epsilon: 0.2239    steps: 226    lr: 6.400000000000001e-06     reward: 2.16\n",
      "epis: 2319   score: 2.0   mem len: 445185   epsilon: 0.2236    steps: 200    lr: 6.400000000000001e-06     reward: 2.13\n",
      "epis: 2320   score: 0.0   mem len: 445308   epsilon: 0.2235    steps: 123    lr: 6.400000000000001e-06     reward: 2.12\n",
      "epis: 2321   score: 2.0   mem len: 445505   epsilon: 0.2232    steps: 197    lr: 6.400000000000001e-06     reward: 2.12\n",
      "epis: 2322   score: 0.0   mem len: 445627   epsilon: 0.223    steps: 122    lr: 6.400000000000001e-06     reward: 2.11\n",
      "epis: 2323   score: 3.0   mem len: 445891   epsilon: 0.2227    steps: 264    lr: 6.400000000000001e-06     reward: 2.08\n",
      "epis: 2324   score: 0.0   mem len: 446013   epsilon: 0.2225    steps: 122    lr: 6.400000000000001e-06     reward: 2.05\n",
      "epis: 2325   score: 2.0   mem len: 446211   epsilon: 0.2222    steps: 198    lr: 6.400000000000001e-06     reward: 2.05\n",
      "epis: 2326   score: 2.0   mem len: 446408   epsilon: 0.222    steps: 197    lr: 6.400000000000001e-06     reward: 2.04\n",
      "epis: 2327   score: 5.0   mem len: 446721   epsilon: 0.2215    steps: 313    lr: 6.400000000000001e-06     reward: 2.07\n",
      "epis: 2328   score: 0.0   mem len: 446843   epsilon: 0.2214    steps: 122    lr: 6.400000000000001e-06     reward: 2.04\n",
      "epis: 2329   score: 1.0   mem len: 447011   epsilon: 0.2211    steps: 168    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2330   score: 0.0   mem len: 447133   epsilon: 0.221    steps: 122    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2331   score: 5.0   mem len: 447468   epsilon: 0.2205    steps: 335    lr: 6.400000000000001e-06     reward: 2.04\n",
      "epis: 2332   score: 0.0   mem len: 447590   epsilon: 0.2203    steps: 122    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2333   score: 1.0   mem len: 447741   epsilon: 0.2201    steps: 151    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2334   score: 3.0   mem len: 447970   epsilon: 0.2198    steps: 229    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2335   score: 1.0   mem len: 448122   epsilon: 0.2196    steps: 152    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2336   score: 3.0   mem len: 448369   epsilon: 0.2192    steps: 247    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2337   score: 2.0   mem len: 448566   epsilon: 0.219    steps: 197    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2338   score: 3.0   mem len: 448811   epsilon: 0.2186    steps: 245    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2339   score: 4.0   mem len: 449106   epsilon: 0.2182    steps: 295    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2340   score: 1.0   mem len: 449275   epsilon: 0.218    steps: 169    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2341   score: 2.0   mem len: 449472   epsilon: 0.2177    steps: 197    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2342   score: 2.0   mem len: 449669   epsilon: 0.2175    steps: 197    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2343   score: 2.0   mem len: 449866   epsilon: 0.2172    steps: 197    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2344   score: 2.0   mem len: 450063   epsilon: 0.2169    steps: 197    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2345   score: 2.0   mem len: 450261   epsilon: 0.2166    steps: 198    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2346   score: 3.0   mem len: 450508   epsilon: 0.2163    steps: 247    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2347   score: 0.0   mem len: 450630   epsilon: 0.2161    steps: 122    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2348   score: 2.0   mem len: 450827   epsilon: 0.2159    steps: 197    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2349   score: 1.0   mem len: 450996   epsilon: 0.2156    steps: 169    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2350   score: 0.0   mem len: 451119   epsilon: 0.2155    steps: 123    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2351   score: 2.0   mem len: 451317   epsilon: 0.2152    steps: 198    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2352   score: 0.0   mem len: 451440   epsilon: 0.215    steps: 123    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2353   score: 1.0   mem len: 451609   epsilon: 0.2148    steps: 169    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2354   score: 1.0   mem len: 451778   epsilon: 0.2145    steps: 169    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2355   score: 0.0   mem len: 451901   epsilon: 0.2144    steps: 123    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2356   score: 2.0   mem len: 452099   epsilon: 0.2141    steps: 198    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2357   score: 2.0   mem len: 452316   epsilon: 0.2138    steps: 217    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2358   score: 0.0   mem len: 452439   epsilon: 0.2136    steps: 123    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2359   score: 0.0   mem len: 452562   epsilon: 0.2135    steps: 123    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2360   score: 2.0   mem len: 452759   epsilon: 0.2132    steps: 197    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2361   score: 1.0   mem len: 452909   epsilon: 0.213    steps: 150    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2362   score: 3.0   mem len: 453136   epsilon: 0.2127    steps: 227    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2363   score: 2.0   mem len: 453334   epsilon: 0.2124    steps: 198    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2364   score: 2.0   mem len: 453532   epsilon: 0.2121    steps: 198    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2365   score: 2.0   mem len: 453729   epsilon: 0.2119    steps: 197    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2366   score: 1.0   mem len: 453880   epsilon: 0.2116    steps: 151    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2367   score: 1.0   mem len: 454049   epsilon: 0.2114    steps: 169    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2368   score: 1.0   mem len: 454218   epsilon: 0.2112    steps: 169    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2369   score: 3.0   mem len: 454463   epsilon: 0.2108    steps: 245    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2370   score: 2.0   mem len: 454660   epsilon: 0.2106    steps: 197    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2371   score: 2.0   mem len: 454857   epsilon: 0.2103    steps: 197    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2372   score: 2.0   mem len: 455054   epsilon: 0.21    steps: 197    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2373   score: 3.0   mem len: 455282   epsilon: 0.2097    steps: 228    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2374   score: 2.0   mem len: 455480   epsilon: 0.2094    steps: 198    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2375   score: 3.0   mem len: 455726   epsilon: 0.2091    steps: 246    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2376   score: 3.0   mem len: 455991   epsilon: 0.2087    steps: 265    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2377   score: 2.0   mem len: 456189   epsilon: 0.2085    steps: 198    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2378   score: 2.0   mem len: 456386   epsilon: 0.2082    steps: 197    lr: 6.400000000000001e-06     reward: 1.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2379   score: 4.0   mem len: 456679   epsilon: 0.2078    steps: 293    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2380   score: 2.0   mem len: 456876   epsilon: 0.2075    steps: 197    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2381   score: 2.0   mem len: 457074   epsilon: 0.2072    steps: 198    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2382   score: 2.0   mem len: 457297   epsilon: 0.2069    steps: 223    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2383   score: 1.0   mem len: 457466   epsilon: 0.2067    steps: 169    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2384   score: 1.0   mem len: 457635   epsilon: 0.2065    steps: 169    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2385   score: 2.0   mem len: 457853   epsilon: 0.2062    steps: 218    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2386   score: 3.0   mem len: 458100   epsilon: 0.2058    steps: 247    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2387   score: 1.0   mem len: 458251   epsilon: 0.2056    steps: 151    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2388   score: 0.0   mem len: 458374   epsilon: 0.2054    steps: 123    lr: 6.400000000000001e-06     reward: 1.85\n",
      "epis: 2389   score: 1.0   mem len: 458543   epsilon: 0.2052    steps: 169    lr: 6.400000000000001e-06     reward: 1.85\n",
      "epis: 2390   score: 2.0   mem len: 458758   epsilon: 0.2049    steps: 215    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2391   score: 1.0   mem len: 458927   epsilon: 0.2047    steps: 169    lr: 6.400000000000001e-06     reward: 1.85\n",
      "epis: 2392   score: 3.0   mem len: 459155   epsilon: 0.2044    steps: 228    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2393   score: 2.0   mem len: 459353   epsilon: 0.2041    steps: 198    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2394   score: 3.0   mem len: 459617   epsilon: 0.2037    steps: 264    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2395   score: 1.0   mem len: 459786   epsilon: 0.2035    steps: 169    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2396   score: 2.0   mem len: 459966   epsilon: 0.2032    steps: 180    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2397   score: 2.0   mem len: 460164   epsilon: 0.203    steps: 198    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2398   score: 3.0   mem len: 460410   epsilon: 0.2026    steps: 246    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2399   score: 1.0   mem len: 460579   epsilon: 0.2024    steps: 169    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2400   score: 4.0   mem len: 460874   epsilon: 0.202    steps: 295    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2401   score: 3.0   mem len: 461141   epsilon: 0.2016    steps: 267    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2402   score: 2.0   mem len: 461339   epsilon: 0.2014    steps: 198    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2403   score: 1.0   mem len: 461509   epsilon: 0.2011    steps: 170    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2404   score: 2.0   mem len: 461727   epsilon: 0.2008    steps: 218    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2405   score: 1.0   mem len: 461896   epsilon: 0.2006    steps: 169    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2406   score: 0.0   mem len: 462019   epsilon: 0.2004    steps: 123    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2407   score: 3.0   mem len: 462287   epsilon: 0.2    steps: 268    lr: 6.400000000000001e-06     reward: 1.85\n",
      "epis: 2408   score: 4.0   mem len: 462601   epsilon: 0.1996    steps: 314    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2409   score: 2.0   mem len: 462817   epsilon: 0.1993    steps: 216    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2410   score: 1.0   mem len: 462986   epsilon: 0.1991    steps: 169    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2411   score: 2.0   mem len: 463183   epsilon: 0.1988    steps: 197    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2412   score: 3.0   mem len: 463412   epsilon: 0.1985    steps: 229    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2413   score: 2.0   mem len: 463610   epsilon: 0.1982    steps: 198    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2414   score: 2.0   mem len: 463808   epsilon: 0.1979    steps: 198    lr: 6.400000000000001e-06     reward: 1.8\n",
      "epis: 2415   score: 2.0   mem len: 464005   epsilon: 0.1977    steps: 197    lr: 6.400000000000001e-06     reward: 1.8\n",
      "epis: 2416   score: 2.0   mem len: 464202   epsilon: 0.1974    steps: 197    lr: 6.400000000000001e-06     reward: 1.82\n",
      "epis: 2417   score: 2.0   mem len: 464400   epsilon: 0.1971    steps: 198    lr: 6.400000000000001e-06     reward: 1.83\n",
      "epis: 2418   score: 2.0   mem len: 464597   epsilon: 0.1969    steps: 197    lr: 6.400000000000001e-06     reward: 1.82\n",
      "epis: 2419   score: 0.0   mem len: 464720   epsilon: 0.1967    steps: 123    lr: 6.400000000000001e-06     reward: 1.8\n",
      "epis: 2420   score: 3.0   mem len: 464967   epsilon: 0.1963    steps: 247    lr: 6.400000000000001e-06     reward: 1.83\n",
      "epis: 2421   score: 0.0   mem len: 465090   epsilon: 0.1962    steps: 123    lr: 6.400000000000001e-06     reward: 1.81\n",
      "epis: 2422   score: 3.0   mem len: 465337   epsilon: 0.1958    steps: 247    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2423   score: 3.0   mem len: 465605   epsilon: 0.1955    steps: 268    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2424   score: 4.0   mem len: 465899   epsilon: 0.1951    steps: 294    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2425   score: 2.0   mem len: 466100   epsilon: 0.1948    steps: 201    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2426   score: 2.0   mem len: 466297   epsilon: 0.1945    steps: 197    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2427   score: 3.0   mem len: 466543   epsilon: 0.1942    steps: 246    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2428   score: 1.0   mem len: 466712   epsilon: 0.1939    steps: 169    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2429   score: 1.0   mem len: 466882   epsilon: 0.1937    steps: 170    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2430   score: 0.0   mem len: 467004   epsilon: 0.1935    steps: 122    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2431   score: 1.0   mem len: 467156   epsilon: 0.1933    steps: 152    lr: 6.400000000000001e-06     reward: 1.83\n",
      "epis: 2432   score: 1.0   mem len: 467325   epsilon: 0.1931    steps: 169    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2433   score: 3.0   mem len: 467570   epsilon: 0.1928    steps: 245    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2434   score: 2.0   mem len: 467768   epsilon: 0.1925    steps: 198    lr: 6.400000000000001e-06     reward: 1.85\n",
      "epis: 2435   score: 2.0   mem len: 467984   epsilon: 0.1922    steps: 216    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2436   score: 3.0   mem len: 468229   epsilon: 0.1918    steps: 245    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2437   score: 2.0   mem len: 468427   epsilon: 0.1916    steps: 198    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2438   score: 0.0   mem len: 468550   epsilon: 0.1914    steps: 123    lr: 6.400000000000001e-06     reward: 1.83\n",
      "epis: 2439   score: 2.0   mem len: 468748   epsilon: 0.1911    steps: 198    lr: 6.400000000000001e-06     reward: 1.81\n",
      "epis: 2440   score: 1.0   mem len: 468918   epsilon: 0.1909    steps: 170    lr: 6.400000000000001e-06     reward: 1.81\n",
      "epis: 2441   score: 3.0   mem len: 469145   epsilon: 0.1906    steps: 227    lr: 6.400000000000001e-06     reward: 1.82\n",
      "epis: 2442   score: 4.0   mem len: 469457   epsilon: 0.1901    steps: 312    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2443   score: 2.0   mem len: 469676   epsilon: 0.1898    steps: 219    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2444   score: 0.0   mem len: 469798   epsilon: 0.1897    steps: 122    lr: 6.400000000000001e-06     reward: 1.82\n",
      "epis: 2445   score: 3.0   mem len: 470046   epsilon: 0.1893    steps: 248    lr: 6.400000000000001e-06     reward: 1.83\n",
      "epis: 2446   score: 2.0   mem len: 470243   epsilon: 0.1891    steps: 197    lr: 6.400000000000001e-06     reward: 1.82\n",
      "epis: 2447   score: 1.0   mem len: 470412   epsilon: 0.1888    steps: 169    lr: 6.400000000000001e-06     reward: 1.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2448   score: 2.0   mem len: 470610   epsilon: 0.1886    steps: 198    lr: 6.400000000000001e-06     reward: 1.83\n",
      "epis: 2449   score: 2.0   mem len: 470808   epsilon: 0.1883    steps: 198    lr: 6.400000000000001e-06     reward: 1.84\n",
      "epis: 2450   score: 2.0   mem len: 471006   epsilon: 0.188    steps: 198    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2451   score: 1.0   mem len: 471156   epsilon: 0.1878    steps: 150    lr: 6.400000000000001e-06     reward: 1.85\n",
      "epis: 2452   score: 1.0   mem len: 471325   epsilon: 0.1876    steps: 169    lr: 6.400000000000001e-06     reward: 1.86\n",
      "epis: 2453   score: 3.0   mem len: 471553   epsilon: 0.1873    steps: 228    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2454   score: 3.0   mem len: 471800   epsilon: 0.1869    steps: 247    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2455   score: 3.0   mem len: 472072   epsilon: 0.1865    steps: 272    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2456   score: 2.0   mem len: 472269   epsilon: 0.1863    steps: 197    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2457   score: 0.0   mem len: 472391   epsilon: 0.1861    steps: 122    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2458   score: 1.0   mem len: 472560   epsilon: 0.1859    steps: 169    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2459   score: 2.0   mem len: 472757   epsilon: 0.1856    steps: 197    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2460   score: 0.0   mem len: 472879   epsilon: 0.1854    steps: 122    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2461   score: 2.0   mem len: 473098   epsilon: 0.1851    steps: 219    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2462   score: 1.0   mem len: 473269   epsilon: 0.1849    steps: 171    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2463   score: 1.0   mem len: 473438   epsilon: 0.1847    steps: 169    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2464   score: 0.0   mem len: 473561   epsilon: 0.1845    steps: 123    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2465   score: 2.0   mem len: 473780   epsilon: 0.1842    steps: 219    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2466   score: 1.0   mem len: 473930   epsilon: 0.184    steps: 150    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2467   score: 2.0   mem len: 474128   epsilon: 0.1837    steps: 198    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2468   score: 0.0   mem len: 474251   epsilon: 0.1835    steps: 123    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2469   score: 3.0   mem len: 474459   epsilon: 0.1832    steps: 208    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2470   score: 2.0   mem len: 474657   epsilon: 0.183    steps: 198    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2471   score: 5.0   mem len: 474958   epsilon: 0.1826    steps: 301    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2472   score: 5.0   mem len: 475301   epsilon: 0.1821    steps: 343    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2473   score: 1.0   mem len: 475469   epsilon: 0.1819    steps: 168    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2474   score: 2.0   mem len: 475666   epsilon: 0.1816    steps: 197    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2475   score: 2.0   mem len: 475864   epsilon: 0.1813    steps: 198    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2476   score: 2.0   mem len: 476082   epsilon: 0.181    steps: 218    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2477   score: 0.0   mem len: 476205   epsilon: 0.1808    steps: 123    lr: 6.400000000000001e-06     reward: 1.88\n",
      "epis: 2478   score: 3.0   mem len: 476452   epsilon: 0.1805    steps: 247    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2479   score: 2.0   mem len: 476634   epsilon: 0.1802    steps: 182    lr: 6.400000000000001e-06     reward: 1.87\n",
      "epis: 2480   score: 6.0   mem len: 476996   epsilon: 0.1797    steps: 362    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2481   score: 3.0   mem len: 477223   epsilon: 0.1794    steps: 227    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2482   score: 2.0   mem len: 477420   epsilon: 0.1792    steps: 197    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2483   score: 1.0   mem len: 477571   epsilon: 0.179    steps: 151    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2484   score: 2.0   mem len: 477769   epsilon: 0.1787    steps: 198    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2485   score: 2.0   mem len: 477986   epsilon: 0.1784    steps: 217    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2486   score: 2.0   mem len: 478184   epsilon: 0.1781    steps: 198    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2487   score: 2.0   mem len: 478382   epsilon: 0.1778    steps: 198    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2488   score: 1.0   mem len: 478534   epsilon: 0.1776    steps: 152    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2489   score: 1.0   mem len: 478702   epsilon: 0.1774    steps: 168    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2490   score: 3.0   mem len: 478949   epsilon: 0.177    steps: 247    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2491   score: 2.0   mem len: 479169   epsilon: 0.1767    steps: 220    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2492   score: 2.0   mem len: 479385   epsilon: 0.1764    steps: 216    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2493   score: 1.0   mem len: 479554   epsilon: 0.1762    steps: 169    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2494   score: 0.0   mem len: 479677   epsilon: 0.176    steps: 123    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2495   score: 1.0   mem len: 479848   epsilon: 0.1758    steps: 171    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2496   score: 2.0   mem len: 480065   epsilon: 0.1755    steps: 217    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2497   score: 4.0   mem len: 480360   epsilon: 0.1751    steps: 295    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2498   score: 3.0   mem len: 480608   epsilon: 0.1748    steps: 248    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2499   score: 2.0   mem len: 480806   epsilon: 0.1745    steps: 198    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2500   score: 1.0   mem len: 480975   epsilon: 0.1743    steps: 169    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2501   score: 2.0   mem len: 481176   epsilon: 0.174    steps: 201    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2502   score: 1.0   mem len: 481327   epsilon: 0.1738    steps: 151    lr: 6.400000000000001e-06     reward: 1.89\n",
      "epis: 2503   score: 7.0   mem len: 481709   epsilon: 0.1732    steps: 382    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2504   score: 1.0   mem len: 481877   epsilon: 0.173    steps: 168    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2505   score: 1.0   mem len: 482045   epsilon: 0.1728    steps: 168    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2506   score: 1.0   mem len: 482214   epsilon: 0.1725    steps: 169    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2507   score: 2.0   mem len: 482414   epsilon: 0.1723    steps: 200    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2508   score: 1.0   mem len: 482583   epsilon: 0.172    steps: 169    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2509   score: 1.0   mem len: 482733   epsilon: 0.1718    steps: 150    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2510   score: 4.0   mem len: 483049   epsilon: 0.1714    steps: 316    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2511   score: 3.0   mem len: 483293   epsilon: 0.1711    steps: 244    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2512   score: 4.0   mem len: 483549   epsilon: 0.1707    steps: 256    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2513   score: 2.0   mem len: 483765   epsilon: 0.1704    steps: 216    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2514   score: 0.0   mem len: 483888   epsilon: 0.1702    steps: 123    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2515   score: 0.0   mem len: 484011   epsilon: 0.1701    steps: 123    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2516   score: 2.0   mem len: 484211   epsilon: 0.1698    steps: 200    lr: 6.400000000000001e-06     reward: 1.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2517   score: 1.0   mem len: 484379   epsilon: 0.1696    steps: 168    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2518   score: 2.0   mem len: 484577   epsilon: 0.1693    steps: 198    lr: 6.400000000000001e-06     reward: 1.9\n",
      "epis: 2519   score: 1.0   mem len: 484727   epsilon: 0.1691    steps: 150    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2520   score: 6.0   mem len: 485112   epsilon: 0.1685    steps: 385    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2521   score: 2.0   mem len: 485309   epsilon: 0.1683    steps: 197    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2522   score: 1.0   mem len: 485481   epsilon: 0.168    steps: 172    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2523   score: 2.0   mem len: 485660   epsilon: 0.1678    steps: 179    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2524   score: 4.0   mem len: 485975   epsilon: 0.1674    steps: 315    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2525   score: 2.0   mem len: 486173   epsilon: 0.1671    steps: 198    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2526   score: 1.0   mem len: 486342   epsilon: 0.1668    steps: 169    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2527   score: 2.0   mem len: 486540   epsilon: 0.1666    steps: 198    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2528   score: 2.0   mem len: 486738   epsilon: 0.1663    steps: 198    lr: 6.400000000000001e-06     reward: 1.92\n",
      "epis: 2529   score: 0.0   mem len: 486861   epsilon: 0.1661    steps: 123    lr: 6.400000000000001e-06     reward: 1.91\n",
      "epis: 2530   score: 4.0   mem len: 487136   epsilon: 0.1658    steps: 275    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2531   score: 2.0   mem len: 487334   epsilon: 0.1655    steps: 198    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2532   score: 4.0   mem len: 487609   epsilon: 0.1651    steps: 275    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2533   score: 2.0   mem len: 487791   epsilon: 0.1648    steps: 182    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2534   score: 2.0   mem len: 487989   epsilon: 0.1646    steps: 198    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2535   score: 3.0   mem len: 488253   epsilon: 0.1642    steps: 264    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2536   score: 2.0   mem len: 488471   epsilon: 0.1639    steps: 218    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2537   score: 2.0   mem len: 488669   epsilon: 0.1636    steps: 198    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2538   score: 1.0   mem len: 488838   epsilon: 0.1634    steps: 169    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2539   score: 5.0   mem len: 489165   epsilon: 0.163    steps: 327    lr: 6.400000000000001e-06     reward: 2.02\n",
      "epis: 2540   score: 3.0   mem len: 489390   epsilon: 0.1626    steps: 225    lr: 6.400000000000001e-06     reward: 2.04\n",
      "epis: 2541   score: 1.0   mem len: 489540   epsilon: 0.1624    steps: 150    lr: 6.400000000000001e-06     reward: 2.02\n",
      "epis: 2542   score: 4.0   mem len: 489817   epsilon: 0.1621    steps: 277    lr: 6.400000000000001e-06     reward: 2.02\n",
      "epis: 2543   score: 0.0   mem len: 489939   epsilon: 0.1619    steps: 122    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2544   score: 2.0   mem len: 490158   epsilon: 0.1616    steps: 219    lr: 6.400000000000001e-06     reward: 2.02\n",
      "epis: 2545   score: 0.0   mem len: 490280   epsilon: 0.1614    steps: 122    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2546   score: 1.0   mem len: 490448   epsilon: 0.1612    steps: 168    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2547   score: 3.0   mem len: 490678   epsilon: 0.1609    steps: 230    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2548   score: 1.0   mem len: 490846   epsilon: 0.1606    steps: 168    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2549   score: 3.0   mem len: 491090   epsilon: 0.1603    steps: 244    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2550   score: 3.0   mem len: 491358   epsilon: 0.1599    steps: 268    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2551   score: 2.0   mem len: 491557   epsilon: 0.1596    steps: 199    lr: 6.400000000000001e-06     reward: 2.02\n",
      "epis: 2552   score: 1.0   mem len: 491726   epsilon: 0.1594    steps: 169    lr: 6.400000000000001e-06     reward: 2.02\n",
      "epis: 2553   score: 0.0   mem len: 491848   epsilon: 0.1592    steps: 122    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2554   score: 0.0   mem len: 491970   epsilon: 0.1591    steps: 122    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2555   score: 1.0   mem len: 492138   epsilon: 0.1588    steps: 168    lr: 6.400000000000001e-06     reward: 1.94\n",
      "epis: 2556   score: 1.0   mem len: 492306   epsilon: 0.1586    steps: 168    lr: 6.400000000000001e-06     reward: 1.93\n",
      "epis: 2557   score: 2.0   mem len: 492503   epsilon: 0.1583    steps: 197    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2558   score: 2.0   mem len: 492701   epsilon: 0.1581    steps: 198    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2559   score: 3.0   mem len: 492927   epsilon: 0.1578    steps: 226    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2560   score: 3.0   mem len: 493156   epsilon: 0.1574    steps: 229    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2561   score: 2.0   mem len: 493374   epsilon: 0.1571    steps: 218    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2562   score: 0.0   mem len: 493497   epsilon: 0.157    steps: 123    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2563   score: 3.0   mem len: 493764   epsilon: 0.1566    steps: 267    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2564   score: 8.0   mem len: 494095   epsilon: 0.1561    steps: 331    lr: 6.400000000000001e-06     reward: 2.09\n",
      "epis: 2565   score: 1.0   mem len: 494263   epsilon: 0.1559    steps: 168    lr: 6.400000000000001e-06     reward: 2.08\n",
      "epis: 2566   score: 3.0   mem len: 494507   epsilon: 0.1556    steps: 244    lr: 6.400000000000001e-06     reward: 2.1\n",
      "epis: 2567   score: 2.0   mem len: 494724   epsilon: 0.1553    steps: 217    lr: 6.400000000000001e-06     reward: 2.1\n",
      "epis: 2568   score: 4.0   mem len: 495022   epsilon: 0.1549    steps: 298    lr: 6.400000000000001e-06     reward: 2.14\n",
      "epis: 2569   score: 3.0   mem len: 495233   epsilon: 0.1546    steps: 211    lr: 6.400000000000001e-06     reward: 2.14\n",
      "epis: 2570   score: 2.0   mem len: 495430   epsilon: 0.1543    steps: 197    lr: 6.400000000000001e-06     reward: 2.14\n",
      "epis: 2571   score: 1.0   mem len: 495599   epsilon: 0.1541    steps: 169    lr: 6.400000000000001e-06     reward: 2.1\n",
      "epis: 2572   score: 2.0   mem len: 495817   epsilon: 0.1538    steps: 218    lr: 6.400000000000001e-06     reward: 2.07\n",
      "epis: 2573   score: 2.0   mem len: 496033   epsilon: 0.1535    steps: 216    lr: 6.400000000000001e-06     reward: 2.08\n",
      "epis: 2574   score: 1.0   mem len: 496202   epsilon: 0.1532    steps: 169    lr: 6.400000000000001e-06     reward: 2.07\n",
      "epis: 2575   score: 2.0   mem len: 496421   epsilon: 0.1529    steps: 219    lr: 6.400000000000001e-06     reward: 2.07\n",
      "epis: 2576   score: 2.0   mem len: 496637   epsilon: 0.1526    steps: 216    lr: 6.400000000000001e-06     reward: 2.07\n",
      "epis: 2577   score: 1.0   mem len: 496805   epsilon: 0.1524    steps: 168    lr: 6.400000000000001e-06     reward: 2.08\n",
      "epis: 2578   score: 2.0   mem len: 497002   epsilon: 0.1521    steps: 197    lr: 6.400000000000001e-06     reward: 2.07\n",
      "epis: 2579   score: 0.0   mem len: 497124   epsilon: 0.152    steps: 122    lr: 6.400000000000001e-06     reward: 2.05\n",
      "epis: 2580   score: 1.0   mem len: 497294   epsilon: 0.1517    steps: 170    lr: 6.400000000000001e-06     reward: 2.0\n",
      "epis: 2581   score: 2.0   mem len: 497491   epsilon: 0.1515    steps: 197    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2582   score: 2.0   mem len: 497706   epsilon: 0.1512    steps: 215    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2583   score: 3.0   mem len: 497933   epsilon: 0.1509    steps: 227    lr: 6.400000000000001e-06     reward: 2.01\n",
      "epis: 2584   score: 0.0   mem len: 498055   epsilon: 0.1507    steps: 122    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2585   score: 2.0   mem len: 498275   epsilon: 0.1504    steps: 220    lr: 6.400000000000001e-06     reward: 1.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2586   score: 2.0   mem len: 498454   epsilon: 0.1501    steps: 179    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2587   score: 0.0   mem len: 498576   epsilon: 0.15    steps: 122    lr: 6.400000000000001e-06     reward: 1.97\n",
      "epis: 2588   score: 0.0   mem len: 498699   epsilon: 0.1498    steps: 123    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2589   score: 1.0   mem len: 498850   epsilon: 0.1496    steps: 151    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2590   score: 2.0   mem len: 499048   epsilon: 0.1493    steps: 198    lr: 6.400000000000001e-06     reward: 1.95\n",
      "epis: 2591   score: 3.0   mem len: 499277   epsilon: 0.149    steps: 229    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2592   score: 2.0   mem len: 499497   epsilon: 0.1487    steps: 220    lr: 6.400000000000001e-06     reward: 1.96\n",
      "epis: 2593   score: 3.0   mem len: 499746   epsilon: 0.1483    steps: 249    lr: 6.400000000000001e-06     reward: 1.98\n",
      "epis: 2594   score: 1.0   mem len: 499914   epsilon: 0.1481    steps: 168    lr: 6.400000000000001e-06     reward: 1.99\n",
      "epis: 2595   score: 2.0   mem len: 500133   epsilon: 0.1478    steps: 219    lr: 2.560000000000001e-06     reward: 2.0\n",
      "epis: 2596   score: 1.0   mem len: 500284   epsilon: 0.1476    steps: 151    lr: 2.560000000000001e-06     reward: 1.99\n",
      "epis: 2597   score: 2.0   mem len: 500482   epsilon: 0.1473    steps: 198    lr: 2.560000000000001e-06     reward: 1.97\n",
      "epis: 2598   score: 2.0   mem len: 500680   epsilon: 0.1471    steps: 198    lr: 2.560000000000001e-06     reward: 1.96\n",
      "epis: 2599   score: 1.0   mem len: 500849   epsilon: 0.1468    steps: 169    lr: 2.560000000000001e-06     reward: 1.95\n",
      "epis: 2600   score: 5.0   mem len: 501191   epsilon: 0.1464    steps: 342    lr: 2.560000000000001e-06     reward: 1.99\n",
      "epis: 2601   score: 4.0   mem len: 501469   epsilon: 0.146    steps: 278    lr: 2.560000000000001e-06     reward: 2.01\n",
      "epis: 2602   score: 3.0   mem len: 501714   epsilon: 0.1456    steps: 245    lr: 2.560000000000001e-06     reward: 2.03\n",
      "epis: 2603   score: 0.0   mem len: 501836   epsilon: 0.1455    steps: 122    lr: 2.560000000000001e-06     reward: 1.96\n",
      "epis: 2604   score: 4.0   mem len: 502130   epsilon: 0.1451    steps: 294    lr: 2.560000000000001e-06     reward: 1.99\n",
      "epis: 2605   score: 1.0   mem len: 502299   epsilon: 0.1448    steps: 169    lr: 2.560000000000001e-06     reward: 1.99\n",
      "epis: 2606   score: 0.0   mem len: 502421   epsilon: 0.1447    steps: 122    lr: 2.560000000000001e-06     reward: 1.98\n",
      "epis: 2607   score: 2.0   mem len: 502618   epsilon: 0.1444    steps: 197    lr: 2.560000000000001e-06     reward: 1.98\n",
      "epis: 2608   score: 1.0   mem len: 502769   epsilon: 0.1442    steps: 151    lr: 2.560000000000001e-06     reward: 1.98\n",
      "epis: 2609   score: 4.0   mem len: 503043   epsilon: 0.1438    steps: 274    lr: 2.560000000000001e-06     reward: 2.01\n",
      "epis: 2610   score: 0.0   mem len: 503165   epsilon: 0.1436    steps: 122    lr: 2.560000000000001e-06     reward: 1.97\n",
      "epis: 2611   score: 0.0   mem len: 503287   epsilon: 0.1435    steps: 122    lr: 2.560000000000001e-06     reward: 1.94\n",
      "epis: 2612   score: 2.0   mem len: 503486   epsilon: 0.1432    steps: 199    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2613   score: 0.0   mem len: 503608   epsilon: 0.143    steps: 122    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2614   score: 3.0   mem len: 503821   epsilon: 0.1427    steps: 213    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2615   score: 3.0   mem len: 504051   epsilon: 0.1424    steps: 230    lr: 2.560000000000001e-06     reward: 1.96\n",
      "epis: 2616   score: 0.0   mem len: 504173   epsilon: 0.1422    steps: 122    lr: 2.560000000000001e-06     reward: 1.94\n",
      "epis: 2617   score: 2.0   mem len: 504370   epsilon: 0.142    steps: 197    lr: 2.560000000000001e-06     reward: 1.95\n",
      "epis: 2618   score: 0.0   mem len: 504492   epsilon: 0.1418    steps: 122    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2619   score: 2.0   mem len: 504689   epsilon: 0.1415    steps: 197    lr: 2.560000000000001e-06     reward: 1.94\n",
      "epis: 2620   score: 5.0   mem len: 505033   epsilon: 0.1411    steps: 344    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2621   score: 0.0   mem len: 505155   epsilon: 0.1409    steps: 122    lr: 2.560000000000001e-06     reward: 1.91\n",
      "epis: 2622   score: 2.0   mem len: 505352   epsilon: 0.1406    steps: 197    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2623   score: 2.0   mem len: 505549   epsilon: 0.1403    steps: 197    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2624   score: 5.0   mem len: 505877   epsilon: 0.1399    steps: 328    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2625   score: 1.0   mem len: 506028   epsilon: 0.1397    steps: 151    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2626   score: 2.0   mem len: 506250   epsilon: 0.1394    steps: 222    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2627   score: 0.0   mem len: 506372   epsilon: 0.1392    steps: 122    lr: 2.560000000000001e-06     reward: 1.91\n",
      "epis: 2628   score: 5.0   mem len: 506687   epsilon: 0.1388    steps: 315    lr: 2.560000000000001e-06     reward: 1.94\n",
      "epis: 2629   score: 1.0   mem len: 506857   epsilon: 0.1385    steps: 170    lr: 2.560000000000001e-06     reward: 1.95\n",
      "epis: 2630   score: 3.0   mem len: 507103   epsilon: 0.1382    steps: 246    lr: 2.560000000000001e-06     reward: 1.94\n",
      "epis: 2631   score: 1.0   mem len: 507254   epsilon: 0.138    steps: 151    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2632   score: 3.0   mem len: 507464   epsilon: 0.1377    steps: 210    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2633   score: 3.0   mem len: 507677   epsilon: 0.1374    steps: 213    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2634   score: 1.0   mem len: 507845   epsilon: 0.1372    steps: 168    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2635   score: 0.0   mem len: 507968   epsilon: 0.137    steps: 123    lr: 2.560000000000001e-06     reward: 1.89\n",
      "epis: 2636   score: 0.0   mem len: 508090   epsilon: 0.1368    steps: 122    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2637   score: 0.0   mem len: 508213   epsilon: 0.1367    steps: 123    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2638   score: 3.0   mem len: 508457   epsilon: 0.1363    steps: 244    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2639   score: 4.0   mem len: 508752   epsilon: 0.1359    steps: 295    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2640   score: 2.0   mem len: 508952   epsilon: 0.1356    steps: 200    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2641   score: 3.0   mem len: 509195   epsilon: 0.1353    steps: 243    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2642   score: 1.0   mem len: 509346   epsilon: 0.1351    steps: 151    lr: 2.560000000000001e-06     reward: 1.84\n",
      "epis: 2643   score: 3.0   mem len: 509571   epsilon: 0.1348    steps: 225    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2644   score: 3.0   mem len: 509796   epsilon: 0.1345    steps: 225    lr: 2.560000000000001e-06     reward: 1.88\n",
      "epis: 2645   score: 2.0   mem len: 509995   epsilon: 0.1342    steps: 199    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2646   score: 1.0   mem len: 510145   epsilon: 0.134    steps: 150    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2647   score: 3.0   mem len: 510357   epsilon: 0.1337    steps: 212    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2648   score: 1.0   mem len: 510507   epsilon: 0.1335    steps: 150    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2649   score: 0.0   mem len: 510630   epsilon: 0.1333    steps: 123    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2650   score: 3.0   mem len: 510874   epsilon: 0.133    steps: 244    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2651   score: 2.0   mem len: 511073   epsilon: 0.1327    steps: 199    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2652   score: 2.0   mem len: 511271   epsilon: 0.1324    steps: 198    lr: 2.560000000000001e-06     reward: 1.88\n",
      "epis: 2653   score: 2.0   mem len: 511468   epsilon: 0.1322    steps: 197    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2654   score: 3.0   mem len: 511711   epsilon: 0.1318    steps: 243    lr: 2.560000000000001e-06     reward: 1.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2655   score: 0.0   mem len: 511834   epsilon: 0.1317    steps: 123    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2656   score: 1.0   mem len: 512002   epsilon: 0.1314    steps: 168    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2657   score: 3.0   mem len: 512212   epsilon: 0.1311    steps: 210    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2658   score: 1.0   mem len: 512364   epsilon: 0.1309    steps: 152    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2659   score: 1.0   mem len: 512514   epsilon: 0.1307    steps: 150    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2660   score: 0.0   mem len: 512637   epsilon: 0.1306    steps: 123    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2661   score: 1.0   mem len: 512787   epsilon: 0.1304    steps: 150    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2662   score: 0.0   mem len: 512909   epsilon: 0.1302    steps: 122    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2663   score: 2.0   mem len: 513106   epsilon: 0.1299    steps: 197    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2664   score: 2.0   mem len: 513303   epsilon: 0.1296    steps: 197    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2665   score: 1.0   mem len: 513454   epsilon: 0.1294    steps: 151    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2666   score: 1.0   mem len: 513622   epsilon: 0.1292    steps: 168    lr: 2.560000000000001e-06     reward: 1.77\n",
      "epis: 2667   score: 1.0   mem len: 513790   epsilon: 0.129    steps: 168    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2668   score: 1.0   mem len: 513958   epsilon: 0.1287    steps: 168    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2669   score: 5.0   mem len: 514246   epsilon: 0.1283    steps: 288    lr: 2.560000000000001e-06     reward: 1.75\n",
      "epis: 2670   score: 3.0   mem len: 514493   epsilon: 0.128    steps: 247    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2671   score: 5.0   mem len: 514785   epsilon: 0.1276    steps: 292    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2672   score: 0.0   mem len: 514907   epsilon: 0.1274    steps: 122    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2673   score: 3.0   mem len: 515132   epsilon: 0.1271    steps: 225    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2674   score: 0.0   mem len: 515255   epsilon: 0.1269    steps: 123    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2675   score: 3.0   mem len: 515501   epsilon: 0.1266    steps: 246    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2676   score: 2.0   mem len: 515720   epsilon: 0.1263    steps: 219    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2677   score: 2.0   mem len: 515918   epsilon: 0.126    steps: 198    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2678   score: 2.0   mem len: 516098   epsilon: 0.1258    steps: 180    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2679   score: 0.0   mem len: 516221   epsilon: 0.1256    steps: 123    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2680   score: 2.0   mem len: 516441   epsilon: 0.1253    steps: 220    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2681   score: 0.0   mem len: 516563   epsilon: 0.1251    steps: 122    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2682   score: 2.0   mem len: 516760   epsilon: 0.1249    steps: 197    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2683   score: 3.0   mem len: 516988   epsilon: 0.1246    steps: 228    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2684   score: 1.0   mem len: 517138   epsilon: 0.1243    steps: 150    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2685   score: 0.0   mem len: 517260   epsilon: 0.1242    steps: 122    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2686   score: 2.0   mem len: 517476   epsilon: 0.1239    steps: 216    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2687   score: 7.0   mem len: 517900   epsilon: 0.1233    steps: 424    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2688   score: 1.0   mem len: 518050   epsilon: 0.1231    steps: 150    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2689   score: 2.0   mem len: 518266   epsilon: 0.1228    steps: 216    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2690   score: 0.0   mem len: 518388   epsilon: 0.1226    steps: 122    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2691   score: 1.0   mem len: 518558   epsilon: 0.1224    steps: 170    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2692   score: 2.0   mem len: 518755   epsilon: 0.1221    steps: 197    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2693   score: 0.0   mem len: 518878   epsilon: 0.1219    steps: 123    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2694   score: 2.0   mem len: 519079   epsilon: 0.1217    steps: 201    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2695   score: 1.0   mem len: 519230   epsilon: 0.1215    steps: 151    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2696   score: 0.0   mem len: 519352   epsilon: 0.1213    steps: 122    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2697   score: 0.0   mem len: 519474   epsilon: 0.1211    steps: 122    lr: 2.560000000000001e-06     reward: 1.77\n",
      "epis: 2698   score: 1.0   mem len: 519624   epsilon: 0.1209    steps: 150    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2699   score: 4.0   mem len: 519918   epsilon: 0.1205    steps: 294    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2700   score: 4.0   mem len: 520195   epsilon: 0.1201    steps: 277    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2701   score: 2.0   mem len: 520396   epsilon: 0.1199    steps: 201    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2702   score: 2.0   mem len: 520614   epsilon: 0.1196    steps: 218    lr: 2.560000000000001e-06     reward: 1.75\n",
      "epis: 2703   score: 2.0   mem len: 520832   epsilon: 0.1193    steps: 218    lr: 2.560000000000001e-06     reward: 1.77\n",
      "epis: 2704   score: 3.0   mem len: 521057   epsilon: 0.1189    steps: 225    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2705   score: 1.0   mem len: 521225   epsilon: 0.1187    steps: 168    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2706   score: 1.0   mem len: 521397   epsilon: 0.1185    steps: 172    lr: 2.560000000000001e-06     reward: 1.77\n",
      "epis: 2707   score: 4.0   mem len: 521673   epsilon: 0.1181    steps: 276    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2708   score: 2.0   mem len: 521895   epsilon: 0.1178    steps: 222    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2709   score: 0.0   mem len: 522017   epsilon: 0.1176    steps: 122    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2710   score: 0.0   mem len: 522139   epsilon: 0.1174    steps: 122    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2711   score: 2.0   mem len: 522355   epsilon: 0.1171    steps: 216    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2712   score: 1.0   mem len: 522505   epsilon: 0.1169    steps: 150    lr: 2.560000000000001e-06     reward: 1.77\n",
      "epis: 2713   score: 3.0   mem len: 522715   epsilon: 0.1167    steps: 210    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2714   score: 2.0   mem len: 522930   epsilon: 0.1164    steps: 215    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2715   score: 0.0   mem len: 523052   epsilon: 0.1162    steps: 122    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2716   score: 4.0   mem len: 523369   epsilon: 0.1157    steps: 317    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2717   score: 1.0   mem len: 523537   epsilon: 0.1155    steps: 168    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2718   score: 3.0   mem len: 523763   epsilon: 0.1152    steps: 226    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2719   score: 2.0   mem len: 523961   epsilon: 0.1149    steps: 198    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2720   score: 3.0   mem len: 524189   epsilon: 0.1146    steps: 228    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2721   score: 2.0   mem len: 524387   epsilon: 0.1143    steps: 198    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2722   score: 1.0   mem len: 524555   epsilon: 0.1141    steps: 168    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2723   score: 2.0   mem len: 524752   epsilon: 0.1138    steps: 197    lr: 2.560000000000001e-06     reward: 1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2724   score: 3.0   mem len: 524978   epsilon: 0.1135    steps: 226    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2725   score: 3.0   mem len: 525204   epsilon: 0.1132    steps: 226    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2726   score: 2.0   mem len: 525401   epsilon: 0.1129    steps: 197    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2727   score: 2.0   mem len: 525618   epsilon: 0.1126    steps: 217    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2728   score: 1.0   mem len: 525786   epsilon: 0.1124    steps: 168    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2729   score: 2.0   mem len: 525983   epsilon: 0.1121    steps: 197    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2730   score: 2.0   mem len: 526203   epsilon: 0.1118    steps: 220    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2731   score: 1.0   mem len: 526371   epsilon: 0.1116    steps: 168    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2732   score: 0.0   mem len: 526493   epsilon: 0.1114    steps: 122    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2733   score: 3.0   mem len: 526754   epsilon: 0.1111    steps: 261    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2734   score: 4.0   mem len: 527050   epsilon: 0.1107    steps: 296    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2735   score: 1.0   mem len: 527219   epsilon: 0.1104    steps: 169    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2736   score: 2.0   mem len: 527398   epsilon: 0.1102    steps: 179    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2737   score: 0.0   mem len: 527520   epsilon: 0.11    steps: 122    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2738   score: 2.0   mem len: 527737   epsilon: 0.1097    steps: 217    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2739   score: 3.0   mem len: 527963   epsilon: 0.1094    steps: 226    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2740   score: 1.0   mem len: 528131   epsilon: 0.1092    steps: 168    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2741   score: 1.0   mem len: 528282   epsilon: 0.109    steps: 151    lr: 2.560000000000001e-06     reward: 1.77\n",
      "epis: 2742   score: 2.0   mem len: 528480   epsilon: 0.1087    steps: 198    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2743   score: 0.0   mem len: 528602   epsilon: 0.1085    steps: 122    lr: 2.560000000000001e-06     reward: 1.75\n",
      "epis: 2744   score: 3.0   mem len: 528827   epsilon: 0.1082    steps: 225    lr: 2.560000000000001e-06     reward: 1.75\n",
      "epis: 2745   score: 3.0   mem len: 529052   epsilon: 0.1079    steps: 225    lr: 2.560000000000001e-06     reward: 1.76\n",
      "epis: 2746   score: 2.0   mem len: 529267   epsilon: 0.1076    steps: 215    lr: 2.560000000000001e-06     reward: 1.77\n",
      "epis: 2747   score: 0.0   mem len: 529389   epsilon: 0.1074    steps: 122    lr: 2.560000000000001e-06     reward: 1.74\n",
      "epis: 2748   score: 2.0   mem len: 529587   epsilon: 0.1072    steps: 198    lr: 2.560000000000001e-06     reward: 1.75\n",
      "epis: 2749   score: 5.0   mem len: 529897   epsilon: 0.1067    steps: 310    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2750   score: 2.0   mem len: 530113   epsilon: 0.1064    steps: 216    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2751   score: 1.0   mem len: 530281   epsilon: 0.1062    steps: 168    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2752   score: 4.0   mem len: 530541   epsilon: 0.1059    steps: 260    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2753   score: 2.0   mem len: 530739   epsilon: 0.1056    steps: 198    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2754   score: 2.0   mem len: 530937   epsilon: 0.1053    steps: 198    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2755   score: 1.0   mem len: 531087   epsilon: 0.1051    steps: 150    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2756   score: 0.0   mem len: 531209   epsilon: 0.1049    steps: 122    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2757   score: 3.0   mem len: 531475   epsilon: 0.1046    steps: 266    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2758   score: 3.0   mem len: 531743   epsilon: 0.1042    steps: 268    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2759   score: 2.0   mem len: 531962   epsilon: 0.1039    steps: 219    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2760   score: 1.0   mem len: 532115   epsilon: 0.1037    steps: 153    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2761   score: 3.0   mem len: 532341   epsilon: 0.1034    steps: 226    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2762   score: 1.0   mem len: 532512   epsilon: 0.1031    steps: 171    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2763   score: 3.0   mem len: 532779   epsilon: 0.1028    steps: 267    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2764   score: 3.0   mem len: 533023   epsilon: 0.1024    steps: 244    lr: 2.560000000000001e-06     reward: 1.88\n",
      "epis: 2765   score: 0.0   mem len: 533145   epsilon: 0.1023    steps: 122    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2766   score: 1.0   mem len: 533314   epsilon: 0.102    steps: 169    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2767   score: 0.0   mem len: 533437   epsilon: 0.1019    steps: 123    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2768   score: 2.0   mem len: 533652   epsilon: 0.1016    steps: 215    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2769   score: 1.0   mem len: 533802   epsilon: 0.1014    steps: 150    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2770   score: 2.0   mem len: 534000   epsilon: 0.1011    steps: 198    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2771   score: 2.0   mem len: 534198   epsilon: 0.1008    steps: 198    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2772   score: 4.0   mem len: 534471   epsilon: 0.1004    steps: 273    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2773   score: 1.0   mem len: 534621   epsilon: 0.1002    steps: 150    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2774   score: 1.0   mem len: 534772   epsilon: 0.1    steps: 151    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2775   score: 1.0   mem len: 534941   epsilon: 0.0998    steps: 169    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2776   score: 2.0   mem len: 535139   epsilon: 0.0995    steps: 198    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2777   score: 2.0   mem len: 535337   epsilon: 0.0992    steps: 198    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2778   score: 0.0   mem len: 535459   epsilon: 0.0991    steps: 122    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2779   score: 3.0   mem len: 535709   epsilon: 0.0987    steps: 250    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2780   score: 0.0   mem len: 535832   epsilon: 0.0986    steps: 123    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2781   score: 2.0   mem len: 536029   epsilon: 0.0983    steps: 197    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2782   score: 2.0   mem len: 536226   epsilon: 0.098    steps: 197    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2783   score: 0.0   mem len: 536348   epsilon: 0.0978    steps: 122    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2784   score: 2.0   mem len: 536565   epsilon: 0.0975    steps: 217    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2785   score: 4.0   mem len: 536861   epsilon: 0.0971    steps: 296    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2786   score: 2.0   mem len: 537058   epsilon: 0.0969    steps: 197    lr: 2.560000000000001e-06     reward: 1.83\n",
      "epis: 2787   score: 2.0   mem len: 537255   epsilon: 0.0966    steps: 197    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2788   score: 1.0   mem len: 537405   epsilon: 0.0964    steps: 150    lr: 2.560000000000001e-06     reward: 1.78\n",
      "epis: 2789   score: 5.0   mem len: 537768   epsilon: 0.0959    steps: 363    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2790   score: 5.0   mem len: 538134   epsilon: 0.0954    steps: 366    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2791   score: 4.0   mem len: 538410   epsilon: 0.095    steps: 276    lr: 2.560000000000001e-06     reward: 1.89\n",
      "epis: 2792   score: 0.0   mem len: 538533   epsilon: 0.0948    steps: 123    lr: 2.560000000000001e-06     reward: 1.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2793   score: 1.0   mem len: 538701   epsilon: 0.0946    steps: 168    lr: 2.560000000000001e-06     reward: 1.88\n",
      "epis: 2794   score: 3.0   mem len: 538970   epsilon: 0.0942    steps: 269    lr: 2.560000000000001e-06     reward: 1.89\n",
      "epis: 2795   score: 1.0   mem len: 539139   epsilon: 0.094    steps: 169    lr: 2.560000000000001e-06     reward: 1.89\n",
      "epis: 2796   score: 1.0   mem len: 539307   epsilon: 0.0938    steps: 168    lr: 2.560000000000001e-06     reward: 1.9\n",
      "epis: 2797   score: 2.0   mem len: 539504   epsilon: 0.0935    steps: 197    lr: 2.560000000000001e-06     reward: 1.92\n",
      "epis: 2798   score: 2.0   mem len: 539722   epsilon: 0.0932    steps: 218    lr: 2.560000000000001e-06     reward: 1.93\n",
      "epis: 2799   score: 2.0   mem len: 539920   epsilon: 0.0929    steps: 198    lr: 2.560000000000001e-06     reward: 1.91\n",
      "epis: 2800   score: 0.0   mem len: 540043   epsilon: 0.0927    steps: 123    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2801   score: 2.0   mem len: 540241   epsilon: 0.0925    steps: 198    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2802   score: 1.0   mem len: 540409   epsilon: 0.0922    steps: 168    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2803   score: 3.0   mem len: 540674   epsilon: 0.0919    steps: 265    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2804   score: 2.0   mem len: 540890   epsilon: 0.0916    steps: 216    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2805   score: 1.0   mem len: 541059   epsilon: 0.0913    steps: 169    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2806   score: 2.0   mem len: 541257   epsilon: 0.0911    steps: 198    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2807   score: 3.0   mem len: 541484   epsilon: 0.0908    steps: 227    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2808   score: 0.0   mem len: 541607   epsilon: 0.0906    steps: 123    lr: 2.560000000000001e-06     reward: 1.84\n",
      "epis: 2809   score: 1.0   mem len: 541775   epsilon: 0.0903    steps: 168    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2810   score: 1.0   mem len: 541943   epsilon: 0.0901    steps: 168    lr: 2.560000000000001e-06     reward: 1.86\n",
      "epis: 2811   score: 3.0   mem len: 542188   epsilon: 0.0898    steps: 245    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2812   score: 1.0   mem len: 542356   epsilon: 0.0895    steps: 168    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2813   score: 1.0   mem len: 542525   epsilon: 0.0893    steps: 169    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2814   score: 1.0   mem len: 542693   epsilon: 0.0891    steps: 168    lr: 2.560000000000001e-06     reward: 1.84\n",
      "epis: 2815   score: 3.0   mem len: 542919   epsilon: 0.0888    steps: 226    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2816   score: 1.0   mem len: 543088   epsilon: 0.0885    steps: 169    lr: 2.560000000000001e-06     reward: 1.84\n",
      "epis: 2817   score: 4.0   mem len: 543385   epsilon: 0.0881    steps: 297    lr: 2.560000000000001e-06     reward: 1.87\n",
      "epis: 2818   score: 1.0   mem len: 543553   epsilon: 0.0879    steps: 168    lr: 2.560000000000001e-06     reward: 1.85\n",
      "epis: 2819   score: 1.0   mem len: 543704   epsilon: 0.0877    steps: 151    lr: 2.560000000000001e-06     reward: 1.84\n",
      "epis: 2820   score: 0.0   mem len: 543826   epsilon: 0.0875    steps: 122    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2821   score: 1.0   mem len: 543994   epsilon: 0.0873    steps: 168    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2822   score: 2.0   mem len: 544192   epsilon: 0.087    steps: 198    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2823   score: 5.0   mem len: 544558   epsilon: 0.0865    steps: 366    lr: 2.560000000000001e-06     reward: 1.84\n",
      "epis: 2824   score: 1.0   mem len: 544726   epsilon: 0.0863    steps: 168    lr: 2.560000000000001e-06     reward: 1.82\n",
      "epis: 2825   score: 2.0   mem len: 544924   epsilon: 0.086    steps: 198    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2826   score: 1.0   mem len: 545092   epsilon: 0.0858    steps: 168    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2827   score: 2.0   mem len: 545310   epsilon: 0.0855    steps: 218    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2828   score: 0.0   mem len: 545432   epsilon: 0.0853    steps: 122    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2829   score: 4.0   mem len: 545727   epsilon: 0.0849    steps: 295    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2830   score: 2.0   mem len: 545943   epsilon: 0.0846    steps: 216    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2831   score: 0.0   mem len: 546065   epsilon: 0.0844    steps: 122    lr: 2.560000000000001e-06     reward: 1.8\n",
      "epis: 2832   score: 1.0   mem len: 546216   epsilon: 0.0842    steps: 151    lr: 2.560000000000001e-06     reward: 1.81\n",
      "epis: 2833   score: 1.0   mem len: 546367   epsilon: 0.084    steps: 151    lr: 2.560000000000001e-06     reward: 1.79\n",
      "epis: 2834   score: 0.0   mem len: 546489   epsilon: 0.0838    steps: 122    lr: 2.560000000000001e-06     reward: 1.75\n",
      "epis: 2835   score: 0.0   mem len: 546611   epsilon: 0.0837    steps: 122    lr: 2.560000000000001e-06     reward: 1.74\n",
      "epis: 2836   score: 1.0   mem len: 546779   epsilon: 0.0834    steps: 168    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2837   score: 1.0   mem len: 546929   epsilon: 0.0832    steps: 150    lr: 2.560000000000001e-06     reward: 1.74\n",
      "epis: 2838   score: 1.0   mem len: 547098   epsilon: 0.083    steps: 169    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2839   score: 4.0   mem len: 547411   epsilon: 0.0826    steps: 313    lr: 2.560000000000001e-06     reward: 1.74\n",
      "epis: 2840   score: 0.0   mem len: 547534   epsilon: 0.0824    steps: 123    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2841   score: 1.0   mem len: 547684   epsilon: 0.0822    steps: 150    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2842   score: 1.0   mem len: 547835   epsilon: 0.082    steps: 151    lr: 2.560000000000001e-06     reward: 1.72\n",
      "epis: 2843   score: 1.0   mem len: 548003   epsilon: 0.0818    steps: 168    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2844   score: 2.0   mem len: 548201   epsilon: 0.0815    steps: 198    lr: 2.560000000000001e-06     reward: 1.72\n",
      "epis: 2845   score: 2.0   mem len: 548383   epsilon: 0.0812    steps: 182    lr: 2.560000000000001e-06     reward: 1.71\n",
      "epis: 2846   score: 2.0   mem len: 548581   epsilon: 0.081    steps: 198    lr: 2.560000000000001e-06     reward: 1.71\n",
      "epis: 2847   score: 3.0   mem len: 548826   epsilon: 0.0806    steps: 245    lr: 2.560000000000001e-06     reward: 1.74\n",
      "epis: 2848   score: 3.0   mem len: 549072   epsilon: 0.0803    steps: 246    lr: 2.560000000000001e-06     reward: 1.75\n",
      "epis: 2849   score: 3.0   mem len: 549318   epsilon: 0.0799    steps: 246    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2850   score: 3.0   mem len: 549583   epsilon: 0.0796    steps: 265    lr: 2.560000000000001e-06     reward: 1.74\n",
      "epis: 2851   score: 0.0   mem len: 549705   epsilon: 0.0794    steps: 122    lr: 2.560000000000001e-06     reward: 1.73\n",
      "epis: 2852   score: 2.0   mem len: 549922   epsilon: 0.0791    steps: 217    lr: 2.560000000000001e-06     reward: 1.71\n",
      "epis: 2853   score: 1.0   mem len: 550092   epsilon: 0.0789    steps: 170    lr: 2.560000000000001e-06     reward: 1.7\n",
      "epis: 2854   score: 1.0   mem len: 550261   epsilon: 0.0786    steps: 169    lr: 2.560000000000001e-06     reward: 1.69\n",
      "epis: 2855   score: 0.0   mem len: 550384   epsilon: 0.0785    steps: 123    lr: 2.560000000000001e-06     reward: 1.68\n",
      "epis: 2856   score: 4.0   mem len: 550642   epsilon: 0.0781    steps: 258    lr: 2.560000000000001e-06     reward: 1.72\n",
      "epis: 2857   score: 3.0   mem len: 550867   epsilon: 0.0778    steps: 225    lr: 2.560000000000001e-06     reward: 1.72\n",
      "epis: 2858   score: 2.0   mem len: 551085   epsilon: 0.0775    steps: 218    lr: 2.560000000000001e-06     reward: 1.71\n",
      "epis: 2859   score: 2.0   mem len: 551284   epsilon: 0.0772    steps: 199    lr: 2.560000000000001e-06     reward: 1.71\n",
      "epis: 2860   score: 1.0   mem len: 551434   epsilon: 0.077    steps: 150    lr: 2.560000000000001e-06     reward: 1.71\n",
      "epis: 2861   score: 1.0   mem len: 551584   epsilon: 0.0768    steps: 150    lr: 2.560000000000001e-06     reward: 1.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 2862   score: 2.0   mem len: 551763   epsilon: 0.0766    steps: 179    lr: 2.560000000000001e-06     reward: 1.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37847/2096004702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rewards'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episodes vs Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./save_graph/breakout_dqn.png\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save graph for training visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# every episode, plot the play time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    407\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3074\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3075\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   3076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3105\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3107\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3108\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dash_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgbFace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mcant_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAHHCAYAAAC1G/yyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHoklEQVR4nO3de3wU9b3/8fcmkE0gNyDkAgQMgiCCQUEw3C1RRGqlx1ZEWoEqPkRoQdQW7KmorU2PVorHKtjjT2mrFQQEfShSEAVEAUUuiiIKIveEaxIIECD5/v6gWWaTzW2zu7OX1/Px2Adkdnb2s7O7M+/9zne+4zDGGAEAAECSFGV3AQAAAMGEcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBIS5Rx99VA6HI6DP+f3338vhcGjOnDkBfV40nMPh0KOPPmp3GYCtCEdAEJkzZ44cDke1t3Xr1tldYsSq/N40atRIrVu31pgxY7R//367ywPgQ43sLgBAVY8//riysrKqTO/QoUO9l/Xf//3fmjp1qi/Kgi6+N2fOnNG6des0Z84crVmzRlu3blVsbKzd5QHwAcIREISGDh2qnj17+mRZjRo1UqNGfNV9xfre3H333UpJSdH//M//6K233tJtt91mc3W1KykpUdOmTe0uAwhqHFYDQlBFn54///nP+stf/qJ27dopLi5OAwcO1NatW93m9dTnaPny5erXr5+Sk5MVHx+vTp066eGHH3ab59ChQ7rrrruUlpam2NhYZWdn6+9//3uVWgoLCzVmzBglJSUpOTlZo0ePVmFhoce6v/76a/3kJz9R8+bNFRsbq549e+qtt95ym+fcuXN67LHH1LFjR8XGxqpFixbq16+fli9fXu362LBhgxwOh8f6/v3vf8vhcOjtt9+WJJ04cUKTJ0/WJZdcIqfTqdTUVF1//fXauHFjtcuvSf/+/SVJO3furNdrLSwsVHR0tP73f//XNe3IkSOKiopSixYtZIxxTR8/frzS09Ndf3/44Yf66U9/qrZt28rpdCozM1P333+/Tp8+7VbDmDFjFB8fr507d+qmm25SQkKCRo0aJUkqLS3V/fffr5YtWyohIUE/+tGPtG/fPq/WARBu+DkJBKGioiIdOXLEbZrD4VCLFi3cpv3jH//QiRMnNGHCBJ05c0bPPPOMfvCDH+iLL75QWlqax2V/+eWX+uEPf6grr7xSjz/+uJxOp3bs2KGPPvrINc/p06c1aNAg7dixQxMnTlRWVpbmz5+vMWPGqLCwUJMmTZIkGWN0yy23aM2aNbr33nt1+eWXa9GiRRo9erTH5+3bt69at26tqVOnqmnTpnr99dc1fPhwLVy4UD/+8Y8lXQhzeXl5uvvuu9WrVy8VFxdrw4YN2rhxo66//nqPr6lnz55q3769Xn/99SrPPW/ePDVr1kxDhgyRJN17771asGCBJk6cqC5duujo0aNas2aNtm3bpquvvrqmt8Wj77//XpLUrFmzer3W5ORkde3aVatXr9avfvUrSdKaNWvkcDh07NgxffXVV7riiiskXQhDFSFMkubPn69Tp05p/PjxatGihT755BM9++yz2rdvn+bPn+9W3/nz5zVkyBD169dPf/7zn9WkSRNJF1q9XnnlFd1xxx3q06eP3n//fQ0bNqzerx8ISwZA0Hj55ZeNJI83p9Ppmm/Xrl1GkomLizP79u1zTV+/fr2RZO6//37XtOnTpxvrV/0vf/mLkWQOHz5cbR0zZ840kswrr7zimnb27FmTk5Nj4uPjTXFxsTHGmMWLFxtJ5sknn3TNd/78edO/f38jybz88suu6YMHDzbdunUzZ86ccU0rLy83ffr0MR07dnRNy87ONsOGDavrKnOZNm2aady4sTl27JhrWmlpqUlOTja/+MUvXNOSkpLMhAkT6r38ivfmvffeM4cPHzZ79+41CxYsMC1btjROp9Ps3bvXNW9dX+uECRNMWlqa6+8pU6aYAQMGmNTUVDNr1ixjjDFHjx41DofDPPPMM675Tp06VaW+vLw843A4zO7du13TRo8ebSSZqVOnus27efNmI8ncd999btPvuOMOI8lMnz69nmsHCC8cVgOC0HPPPafly5e73d59990q8w0fPlytW7d2/d2rVy/17t1bS5YsqXbZycnJkqQ333xT5eXlHudZsmSJ0tPTNXLkSNe0xo0b61e/+pVOnjypVatWueZr1KiRxo8f75ovOjpav/zlL92Wd+zYMb3//vu67bbbdOLECR05ckRHjhzR0aNHNWTIEH377beuM76Sk5P15Zdf6ttvv61lLbkbMWKEzp07pzfeeMM1bdmyZSosLNSIESPcXv/69et14MCBei2/Qm5urlq2bKnMzEz95Cc/UdOmTfXWW2+pTZs29X6t/fv3V0FBgbZv3y7pQgvRgAED1L9/f3344YeSLrQmGWPcWo7i4uJc/y8pKdGRI0fUp08fGWO0adOmKjVb3x9Jrs9HRYtVhcmTJ3u1ToBwQzgCglCvXr2Um5vrdrvuuuuqzNexY8cq0y677DLXoR5PRowYob59++ruu+9WWlqabr/9dr3++utuQWn37t3q2LGjoqLcNxGXX3656/6KfzMyMhQfH+82X6dOndz+3rFjh4wx+t3vfqeWLVu63aZPny7pQh8n6cLZYIWFhbrsssvUrVs3PfTQQ/r888+rfT0VsrOz1blzZ82bN881bd68eUpJSdEPfvAD17Qnn3xSW7duVWZmpnr16qVHH31U3333Xa3Lr1ARXBcsWKCbbrpJR44ckdPp9Oq1VgSeDz/8UCUlJdq0aZP69++vAQMGuMLRhx9+qMTERGVnZ7ueY8+ePRozZoyaN2+u+Ph4tWzZUgMHDpR04ZCsVaNGjVzBrcLu3bsVFRWlSy+91G165fcNiFT0OQIiTFxcnFavXq0PPvhA77zzjpYuXap58+bpBz/4gZYtW6bo6GifP2dF8HrwwQddfX8qqximYMCAAdq5c6fefPNNLVu2TC+++KL+8pe/aPbs2br77rtrfJ4RI0boiSee0JEjR5SQkKC33npLI0eOdDtb77bbblP//v21aNEiLVu2TE899ZT+53/+R2+88YaGDh1a62vp1auX62y14cOHq1+/frrjjju0fft2xcfH1+u1tmrVSllZWVq9erUuueQSGWOUk5Ojli1batKkSdq9e7c+/PBD9enTxxVUy8rKdP311+vYsWP6zW9+o86dO6tp06bav3+/xowZU6U10Ol0Vgm5AGpGOAJCmKdDT998840uueSSGh8XFRWlwYMHa/DgwZoxY4b++Mc/6re//a0++OAD5ebmql27dvr8889VXl7utmP9+uuvJUnt2rVz/btixQqdPHnSrfWo4jBRhfbt20u6cGguNze31tfVvHlzjR07VmPHjtXJkyc1YMAAPfroo3UKR4899pgWLlyotLQ0FRcX6/bbb68yX0ZGhu677z7dd999OnTokK6++mo98cQTdQpHVtHR0crLy9N1112nv/71r5o6dWq9X2v//v21evVqZWVlqXv37kpISFB2draSkpK0dOlSbdy4UY899phr/i+++ELffPON/v73v+vOO+90Ta/pbL7K2rVrp/Lycu3cudOttajy+wZEKn5OACFs8eLFbqMzf/LJJ1q/fn2NO/ljx45Vmda9e3dJF07vlqSbbrpJ+fn5boeozp8/r2effVbx8fGuQzg33XSTzp8/r1mzZrnmKysr07PPPuu2/NTUVA0aNEgvvPCCDh48WOX5Dx8+7Pr/0aNH3e6Lj49Xhw4dXLXV5PLLL1e3bt00b948zZs3TxkZGRowYIBbbZUPO6WmpqpVq1Z1Wr4ngwYNUq9evTRz5kydOXOmXq9VuhCOvv/+e82bN891mC0qKkp9+vTRjBkzdO7cObf+RhUte8Zyqr8xRs8880yda674fFiHEZCkmTNn1nkZQDij5QgIQu+++66rlcaqT58+rpYJ6cLhmX79+mn8+PEqLS3VzJkz1aJFC/3617+udtmPP/64Vq9erWHDhqldu3Y6dOiQnn/+ebVp00b9+vWTJN1zzz164YUXNGbMGH322We65JJLtGDBAn300UeaOXOmEhISJEk333yz+vbtq6lTp+r7779Xly5d9MYbb1QJINKFvjr9+vVTt27dNG7cOLVv314FBQVau3at9u3bpy1btkiSunTpokGDBqlHjx5q3ry5NmzY4Dr1vi5GjBihRx55RLGxsbrrrrvcWr5OnDihNm3a6Cc/+Ymys7MVHx+v9957T59++qmefvrpOi3fk4ceekg//elPNWfOHN177711fq3SxX5H27dv1x//+EfX9AEDBujdd9+V0+nUNddc45reuXNnXXrppXrwwQe1f/9+JSYmauHChTp+/Hid6+3evbtGjhyp559/XkVFRerTp49WrFihHTt2eL0OgLBi45lyACqp6VR+WU6NrziV/6mnnjJPP/20yczMNE6n0/Tv399s2bLFbZmVT+VfsWKFueWWW0yrVq1MTEyMadWqlRk5cqT55ptv3B5XUFBgxo4da1JSUkxMTIzp1q2b26n5FY4ePWp+/vOfm8TERJOUlGR+/vOfm02bNlU5ld8YY3bu3GnuvPNOk56ebho3bmxat25tfvjDH5oFCxa45vnDH/5gevXqZZKTk01cXJzp3LmzeeKJJ8zZs2frtA6//fZb1/pas2aN232lpaXmoYceMtnZ2SYhIcE0bdrUZGdnm+eff77W5Va8N59++mmV+8rKysyll15qLr30UnP+/Pk6v9YKqampRpIpKChwTVuzZo2RZPr3719l/q+++srk5uaa+Ph4k5KSYsaNG2e2bNlSZZ2PHj3aNG3a1OPrOX36tPnVr35lWrRoYZo2bWpuvvlms3fvXk7lB4wxDmMsbbMAQsL333+vrKwsPfXUU3rwwQftLgcAwgp9jgAAACwIRwAAABaEIwAAAAv6HAEAAFjQcgQAAGBBOAIAALCIuEEgy8vLdeDAASUkJMjhcNhdDgAAqANjjE6cOKFWrVr5/XqBEReODhw4oMzMTLvLAAAAXti7d6/atGnj1+eIuHBUcdmDvXv3KjEx0eZqAABAXRQXFyszM9O1H/eniAtHFYfSEhMTCUcAAISYQHSJoUM2AACAha3hKC8vT9dcc40SEhKUmpqq4cOHa/v27TU+Zs6cOXI4HG632NjYAFUMAADCna3haNWqVZowYYLWrVun5cuX69y5c7rhhhtUUlJS4+MSExN18OBB12337t0BqhgAAIQ7W/scLV261O3vOXPmKDU1VZ999pkGDBhQ7eMcDofS09P9XR4AAIhAQdXnqKioSJLUvHnzGuc7efKk2rVrp8zMTN1yyy368ssvA1EeAACIAEETjsrLyzV58mT17dtXXbt2rXa+Tp066aWXXtKbb76pV155ReXl5erTp4/27dvncf7S0lIVFxe73QAAAKoTNBeeHT9+vN59912tWbOmXoM7nTt3TpdffrlGjhyp3//+91Xuf/TRR/XYY49VmV5UVMSp/AAAhIji4mIlJSUFZP8dFC1HEydO1Ntvv60PPvig3qNeNm7cWFdddZV27Njh8f5p06apqKjIddu7d68vSgYAAGHK1g7Zxhj98pe/1KJFi7Ry5UplZWXVexllZWX64osvdNNNN3m83+l0yul0NrRUAAAQIWwNRxMmTNC//vUvvfnmm0pISFB+fr4kKSkpSXFxcZKkO++8U61bt1ZeXp4k6fHHH9e1116rDh06qLCwUE899ZR2796tu+++27bXAQAAwoet4WjWrFmSpEGDBrlNf/nllzVmzBhJ0p49e9yuvnv8+HGNGzdO+fn5atasmXr06KGPP/5YXbp0CVTZAAAgjAVNh+xACWSHLgAA4BsR1yEbABBczp6VHI4LNyDSEI4AAFVwHgsiGeEIAFAjWpAQaQhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAHBz+rTn6efOBbYOwC6EIwCAmyZNPE+PiQlsHYBdCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAVMsYuysAAo9wBAAAYEE4AgAAsCAcAQBczp+v+X6HIzB11JXDcfEG+ArhCADg0rix3RVUZczFAHToUPXzVcxz7FjgakN4IhwBADw6etTuCi6Isuyp0tJqn79FC//VgshAOAIAeNS8uefpHMJCuCMcAQCCTsUhspKSmufx5j6gNoQjAEDQKCx0Dzbx8VXnOXNGOnEiYCUhAjWyuwAAACo0a1b7PHFxdVuWw8EglvAOLUcAAAAWhCMAQI08tb6cPRv4OoBAIRxBEk3PAGpWuWO002lPHUAg0OcIktzHESEoAaisSRO7KwACh5YjAECd+POHE5cAQTAhHAEAwkZ5ud0VIBxwWA0AENLoCgBfo+UoQnElayCyWbcBZ87YXU3NzpxpeABim4f6IByhCjYgQGSp66CKgWbMhVvFmXHeBqTKj6vYxh040LD6EL4IRxGoqMj975quXeQrZ8/yyw2AZ0eOVJ3my75D5855nt66te+eA+GFcBSBkpPd//Z07SJfqzwmCgEJCH2++h63bOm/ZUuMyYT6IxwBAABYEI5QrUAcbgMQWgJxqvz583Wft7Z6amuBOniw7s+FyMGp/BGmPk3V8fGcIgvAnb8Pidd3m9PQelq1YjuHqmg5AgCEjMonlAD+QDiKEMF4lliw1QMg8Oo7xlJiou9rKCvz/TIR2ghHAIB69fPxpYaMsXTqlOfpJ07UbznR0d7XgPBEnyMAgBo3Dp2+N7XVWd/hSRyO0HntCAzCEfyOw2cAUD/W7SbBLfA4rAYAESZYf7D4MgR4Wta5c4EZigChj3CEGvm7H0KwbqSBSBTuLRQVJ6ZUdzkRoALhCDVq3NjuCgAESlSY7RHOnnX/u6LjdaNG4R8E0TBh9lVAKGCjBCAQ+HEHbxGOIpSnw2WVf2UBAALPrmEVcBHhKAJ46tfjqVNiKJ3KC8C/Cgv9/xz0OfSMFi/7EY4iVE1fPl/8aikv9zwqN+ELCA3+GInaDsZcvAF1RTiKYNVtLCqPFuvNrztGnAVCG606wYP3IvAIRxHo6NGq006f9v/z8ssNQLAqLra7AgQTwlGEMUZq3tz9b2Ok2Fj7agJgnwMH7K4gOCQl2V0BggnhCAAiWEZGcLTqBkMNQAXCETxiQwUAwYN+R4FFOEKdlJX5b9mezmoDgEhRVnZhG8j4RsGDcIQ6adSoYY8/dar2edgwAKEnnH7Y+PNHYE0qtq+MbxQ8GrjLQzgzxjcbvroeomMQSqB+rN/Pyt/Xyt+lcAoxvlJ5nQX7Nddqen/hW7a2HOXl5emaa65RQkKCUlNTNXz4cG3fvr3Wx82fP1+dO3dWbGysunXrpiVLlgSgWgSiZcfTyN0A6q+kpH7zFxT4pw7UDyE2ONgajlatWqUJEyZo3bp1Wr58uc6dO6cbbrhBJTV8qz/++GONHDlSd911lzZt2qThw4dr+PDh2rp1awArj0x1bfKty5e7uksTsGEAfCM+vn7zp6bWfV5aLRDuHMYEz8f88OHDSk1N1apVqzRgwACP84wYMUIlJSV6++23XdOuvfZade/eXbNnz671OYqLi5WUlKSioiIlhsv4+DVo6OU7vHl8XR9TXRAKnk8kENxq+zFR8V0qL69+1Hpvv2++OMQTDIeJguESR97+KIy0bWUg999B1SG7qKhIktTcOkphJWvXrlVubq7btCFDhmjt2rV+rS1SRdqXDwhHXM4HqJ+gCUfl5eWaPHmy+vbtq65du1Y7X35+vtLS0tympaWlKT8/3+P8paWlKi4udrtFqkOHAv+c3oSrY8d8XwcQbnxxCPrs2YYvI9QdPuz+t8PhfjmliqFGTpzwz/MfOeKf5aJhgiYcTZgwQVu3btXcuXN9uty8vDwlJSW5bpmZmT5dfihp2dLuCuqmRQu7KwAig52njp87Z99zW6WkVJ3WpMmFf60B1F9HcUJluxxpgiIcTZw4UW+//bY++OADtWnTpsZ509PTVVDptIqCggKlp6d7nH/atGkqKipy3fbu3euzuoNdKB8So2M2cFFF60V9vxc1Pcbu7YO/WmIAX7A1HBljNHHiRC1atEjvv/++srKyan1MTk6OVqxY4TZt+fLlysnJ8Ti/0+lUYmKi2y1SRAVF9K1exUVv7d5IA6EmHH480EJcu/JyBse1i627zwkTJuiVV17Rv/71LyUkJCg/P1/5+fk6bTnge+edd2ratGmuvydNmqSlS5fq6aef1tdff61HH31UGzZs0MSJE+14CSHj+HG7KwCCx6lToRMw/nOeigs/JnwvEOv06FH3z1zFJUNq4nDU3Jmez4L/2BqOZs2apaKiIg0aNEgZGRmu27x581zz7NmzRwcPHnT93adPH/3rX//S3/72N2VnZ2vBggVavHhxjZ24ISUn210BEDyaNr3wbygEpMrf3ZpahNlZes+bYUrqo6JvU8UyPF2Sqb6D4Ab70YFQFlTjHAVCJIxz5OkL3JB3ub5jkXgzdkmw9otA+AmGcW2qU9PlQOqiPo9p6Otu6BhFwTDGUWXWmvLzJU9dWRv6WmtS3SVg6vL4YFmH/hTI/TfXVgOAMFCfs78iYUfaUNWc41MnodAiiZrRKAcAYcDTYRrUj3V8o1BDIPMtvk6oF4eDX51AMCsvD3xflHDZLsTG1j5PRQip72WRGqLyc1X3HP48ZByMh0H9iZYj+Iw347AAkWr//ovfmf37fbfcul5vDUD1CEdhpC6nhgarykP4A+GgIvx46g9kHe+2TZuq3936fJfrGnhC+bBRsPH3trbyEA4ILMJRGAm2Pgf1GYw8JYVftPC/sjJ7njcmxp7nrawuh43qimsg+kfFwLhhejJ1yAiy3SnqK1haijzVUcuVYDzy5vRloK7s/AFh/Vzv2+ebZVb3gyIQ36Nmzfy7/FAQLn2tUBUtRyEsUCHCU2fAiptlfE4AFjV9P7354RDsKrYJJSV2V+JbgQg/wX4ZpUj8wUo4QoO0amV3BUDdVDf6cLhu+O3a2cbH1z5PfcZkCnYOR/1Htg52hYXSiy/aXYW9CEcAIkJN16hCYAVLHyxf8eazVVbmfYuRvy/a3ayZNG5c+P5wqAv6HAGIeKHYdyTU6g0V3q7XunyG7HrP6jNG0f/9n2+XF6poOUKdFBdXf5+nXxfh1GwOBJszZ+r/mHDdiQWTYGtp8eZzcs89vq8jFBGOUCcJCXWf15jgG1YACCdOp90VXEDgqlkg1k9N14CLi/P/84crwhGAiBOsO/Wa6mpoP5Ngfc3hzt/9g2o7Y3jYMP88b7gjHIUp6xeSjSIQmazbgGA75BPugmW7u2SJf5efkuLf5duFcAQACu5+csGyo/WH8+ftrqB2paUX/79zp3111EV9Piv79l0IzTX1Tdq1q+ZlHD1a9+cLJfQMARCRKremxMTYG0IKCqq/L5xbfkJhiAW7Pxu1KSyUkpMv/L8+dWZmXvg3Lq76x7Vv35DKQhctRwDCUsWIzcH8y9Z62Cs1tfr5Qi0YhVq9oS4pqX6HT2+7zfsLHQdzSPQlwlGICrcRWRF8rJeJCQUnT3quuaY+EXZ9j2rqC1heHlo7oPquw1D5PIWr48el+fM93/f4494tMxTHCasN4SiIlZZW/4ELhaZoX2BDirqqz3ATFYLx8xWMNdUk1OoNhBMn7K6ges2bV3/f9Ol1W0b37lWnhdvngHDkY778tR0bK0VF1X9Z1s6DvsTZb5Ej3DZ0waSuHb9D6fsWCjUGUl2uLxfKPv7Y7gr8j3AUBHx9+CLcrlsENFSgw17l57MGHQZIvSgUzlSri0gZOqVJkwv/ehpccvXqwNbib4SjINPQjbi/Wo3qyhenQx8/3vBlAAh+odo9IJwDUE1ee+3i/yuPn3TffYGtxd8IR37kcFwYvdThuHAFZn+xhonGjf33PLXx1a/iil8nABCsIqGlqLIf/eji/4cOdb9v377A1uJvhCM/a9Xqwr/1DQ31aUFKTg78KLj+3ChwWDDwrId2/fUZCpaz3wLdMnnyZGCfzy512R7Y/d6jbvr2rdv7aZ3n1Cn/1WMHwlEQCqXDSpH46ync2LHD8uZq4b5SMVhedXwd4po29c1yAG/Vd/u8Zs3Fx9V1Gx/MI8x7g3AUYBUb3YqOiJ42wDWdarlzJ7++KhQVXVgXwXzabLA7e7Zu8/n6MxesVwvnuwV475ln7O/36iuEowCybni97RvUoYNvagkHFS0AiYm2lhHSnE67K/ANQk3wqe09CbfDMJAmTw7eHz71RTgCUCdHjnj/2EgKL5H0WhsiXHaioaLyYTF/dYkIly4WhKMQEQwdWRHZWra0uwIADXH2rOfL03A5qqoYjsxGhB1EAn8OYwGg7qrrzsG+qCpajkJcuDRhInwFekRoXx4uaOhOg+8n7GBtCTp6tG6PCabvTTAgHCEsBMuXMVjG8qmLUKgRQP05HBeDTk1nP/tCdWGq8gjaoYZwFCTqekp1JHI4OLMlkE6f9u4+gEOokcnT2GEPPxzwMnyKcBQk7LzsRyhgIL3AiY2t/j4u7YKaRLFHiUgHDlSdtmVL4OvwJT7KIYpRqYNP5cNUoXbYKhw/U55OX66vUHsfgUCLiwu/bQfhKASE24cu0Cr6AO3eHRx1hMLO1ledMytGgvcnOk0D8DXCUYiItA24r17vsWMX/3/JJb5ZpjeCPRDV1JeoIWOg1HS4OFDr5NChhi8j2N+/UFBcbHcFCKRVq+yuoGEIR0GgvkEgP98/dYSjFi3sriA01NTPKNSDQcuW4XnIMNQkJdldAfwtJ+fCv0OHSgMG2FtLQzEIZJCzbtAjbeNujPuOueL0VIkRXRvKjsBT+f0MVhWfMW9qDYXXZwfWS2T4+GO7K/AdWo5ssG+f5+nl5YHpoxEOoqPtrqBu2Cn4V+X16+0PiHPnLnz3ano87yUQOQhHfuapOb91a8/zOhwXdvpFRf6vC74Vaa163vLlevLlsho1anjgLi31TS2RgB+BCHaEIxvUtlFPTKSPRHVq+vXucEgnTgSuFivGd/FOQ1pj/L3O6/v9q6nfFtyFSssvIhebdBsQeuquvusqMfHi/z3teEPpdHrYr66fPz5PQHghHNkgKsq3F/mDd7Zts7uC4ODtwIgOR82nZ0fKYSa7WisB+A/hCBGrSxe7Kwh9NZ2eXd1hpoIC979D/QeCtbUSQHggHAUILUW+U5czlAJ5qr+3p3yH6qGYhtadmur+d7j21zpzxu4KghPbQIQCxjlC0PNmfBw7O3x6Gp8J/hdMO91gqsVufP4RisL0NxsiWSjtmMJhx2EdnNM6zYp+OQBCCeEIYSdcD9MEs9rWeXy8+9/nzjXs+crK3P8OpUAcyXifECrYjSAkhPNGNZCtR4Ho61SX5Tdq4AH9YL6+YDh/VoFIQThC2KvcyuBP4bpjrMvrOnnSN89VEeCqW57DIbVp45vnAgBPCEcIaXXZaXOYLTASEhq+DGurk6flhWv4BBBc2G0AAVJQUP1wDuHQMTsQPAXdQAYm3icgMhCOEDJq2gnWtoMMhnGmKo/vE478FR5CZVwoWrbchcJ7BnhCOAIaoKTE8/RADkIZDkIl/ACIDIQjhLXaxtdp6A658inqDVkurQ7ByVcdzQGEDsIRQsr+/dXf5ylcVBde7Oap1qioiy0otKIEj6ZN7a4AQKDZGo5Wr16tm2++Wa1atZLD4dDixYtrnH/lypVyOBxVbvnBPOgJfKpVq4YvgxYa73iz3uzu5wUA3rA1HJWUlCg7O1vPPfdcvR63fft2HTx40HVLjYSergg7R4/aXUFoCLZwZa2HFj4gPNl64dmhQ4dq6NCh9X5camqqkpOTfV8QQp43F6n1VkOfp3nzwNYbqp3Eg22cqqio4AtsAHwryDY7ddO9e3dlZGTo+uuv10cffVTjvKWlpSouLna7IbQFw2n5ngRbPZVFR9tdgW8F22fg1Cm7KwDgKyEVjjIyMjR79mwtXLhQCxcuVGZmpgYNGqSNGzdW+5i8vDwlJSW5bpmZmQGsGHYoKrrwbzDtOL0RiM7Z9W1Nqjj77/x539fiSWlpYJ7HF+Li7K4gOFT3vQv17yMii8OY4PjIOhwOLVq0SMOHD6/X4wYOHKi2bdvqn//8p8f7S0tLVWrZwhYXFyszM1NFRUVKTExsSMkeWXdkBw5IGRk+fwr4gPV98vYbUDm0VF5ObfdXN58nvviW1rWe+i7vwIHqO8rX9BwnTkgN+QraOTJ2xXP74nMEoG6Ki4uVlJTkt/23la19jnyhV69eWrNmTbX3O51OOZ3OAFZ0EcEofNS2E6zLCN013Rfojr2+2JFbl3HiRP2vrVbdtdOCsZNzsNYFwD9C6rCaJ5s3b1YGKQQ2i/RWA2/Hk4r09QYgONnacnTy5Ent2LHD9feuXbu0efNmNW/eXG3bttW0adO0f/9+/eMf/5AkzZw5U1lZWbriiit05swZvfjii3r//fe1bNkyu14CIlRddurFxQ07bAQAsIet4WjDhg267rrrXH9PmTJFkjR69GjNmTNHBw8e1J49e1z3nz17Vg888ID279+vJk2a6Morr9R7773ntgwgECqfXu7pkEtCAi0j/nL8uN0VAAhnQdMhO1D83aGLDpqhoa7vk6fQ46n/ia/e65r6tfiqA7UvllXb8hvyPP5cBw3haf3xfQcCJ5AdskO+z1EwYeMYmXz5vvMZAgD7EY586Mkn7a4A3qjvWUicteRZpAW7M2fsrgCAvxCOfCjSdg4IHYG6dEioXqLEGwz6CIQvwpEPrVxpdwXwpXPn7K7AXUNarAL1WnzRqhZslwUBEHkIRz60c6fdFaCu6tLCERPj/zo8qTgTy5etMLGxvltWbY4cufAvAQdAqCIc+VBBgd0VoK6Cud9QcnLtIzIHc/Bo0SK46/OHo0ftrgCAL/kkHBUXF2vx4sXatm2bLxYXsiouyonIYVcIiIry/0VpUb3K6715c3vqAOAfXoWj2267TX/9618lSadPn1bPnj1122236corr9TChQt9WiAQCJV3dsESOjyFr2Cpzd8q+h5ZbwAQCF6Fo9WrV6t///6SpEWLFskYo8LCQv3v//6v/vCHP/i0QCBQHA6prKzu8589679a/ImQAQA18yocFRUVqfl/2pGXLl2qW2+9VU2aNNGwYcP07bff+rRAIJAa1eOCOo0b+68OAIB9vApHmZmZWrt2rUpKSrR06VLdcMMNkqTjx48rNpCnxQABUrm1hdYXAAhfXl14dvLkyRo1apTi4+PVrl07DRo0SNKFw23dunXzZX2A7YI9CDkcwV9jOGPdA+HHq3B03333qVevXtq7d6+uv/56Rf3nEuXt27enzxFCXrAN/ggACCyHMZH1u8efV/X115Xa4R91PevL7iuw16XO2urh6vG+4em9YH0CgeHP/XdldW45mjJlSp0XOmPGDK+KAeCdUB0wMtSxboHwVOdwtGnTJre/N27cqPPnz6tTp06SpG+++UbR0dHq0aOHbysE/KS4WKrPj4+jRy+M/nzypP9q8odIGRcp0E6dsrsCAP5S53D0wQcfuP4/Y8YMJSQk6O9//7uaNWsm6cKZamPHjnWNfwQEu4SEi7/8Dx+WUlOrznP69MX/N29uT0tBbZcSQeDQUgREBq/6HLVu3VrLli3TFVdc4TZ969atuuGGG3TgwAGfFehr9DlCdYK5P4kvw1GwvCYAqI9A9jnyapyj4uJiHT58uMr0w4cP6wQXGAMAACHMq3D04x//WGPHjtUbb7yhffv2ad++fVq4cKHuuusu/dd//ZevawQAAAgYr8Y5mj17th588EHdcccdOvefQWEaNWqku+66S0899ZRPCwTgOxxSA4Da1bvPUVlZmT766CN169ZNMTEx2rlzpyTp0ksvVdOmTf1SpC/R5wjVCeY+R5Jv+h0F0+sBgPoIynGOKkRHR+uGG27Qtm3blJWVpSuvvNIfdQEBV14ulZXV7+Kzdisrk6KjL/597hwXxAWAhvKqz1HXrl313Xff+boWwFYOR2gFI2OkqErf4FCqHwCClVfh6A9/+IMefPBBvf322zp48KCKi4vdbgB8j0NiABAYXo1zFGX5ueqwdIQwxsjhcKisrMw31fkBfY4QyjxdI63yNC4lAiAcBXWfI8l9tGwAweXIESkl5cKo3y1bXpweapc9AQC7eBWOBg4c6Os6APhIixYXW4hoKQKA+mtQ981Tp05pz549Onv2rNt0zmADAocABAC+5VU4Onz4sMaOHat3333X4/3B3OcIAACgJl6drTZ58mQVFhZq/fr1iouL09KlS/X3v/9dHTt21FtvveXrGgH8x+7ddlcAAOHPq5aj999/X2+++aZ69uypqKgotWvXTtdff70SExOVl5enYcOG+bpOAJLatuUwGgD4m1ctRyUlJUpNTZUkNWvWTIcPH5YkdevWTRs3bvRddQAAAAHmVTjq1KmTtm/fLknKzs7WCy+8oP3792v27NnKyMjwaYEAAACB5NVhtUmTJungwYOSpOnTp+vGG2/Uq6++qpiYGM2ZM8eX9QEAAASUVyNkV3bq1Cl9/fXXatu2rVJSUnxRl98EaoRs+oUAAOA7gRwh26vDapUvOtukSRNdffXVQR+MAAAAauPVYbUOHTqoTZs2GjhwoAYNGqSBAweqQ4cOvq4NAAAg4LxqOdq7d6/y8vIUFxenJ598UpdddpnatGmjUaNG6cUXX/R1jQAAAAHjkz5H3377rZ544gm9+uqrKi8vD+oRsulzBABA6AlknyOvDqudOnVKa9as0cqVK7Vy5Upt2rRJnTt31sSJEzVo0CAflwgAABA4XoWj5ORkNWvWTKNGjdLUqVPVv39/NWvWzNe1AQAABJxX4eimm27SmjVrNHfuXOXn5ys/P1+DBg3SZZdd5uv6AAAAAsqrDtmLFy/WkSNHtHTpUuXk5GjZsmXq37+/WrdurVGjRvm6RgAAgIDxquWoQrdu3XT+/HmdPXtWZ86c0b///W/NmzdPr776qq/qCxm//73dFQAAAF/wquVoxowZ+tGPfqQWLVqod+/eeu2113TZZZdp4cKFrovQRpqVK+2uAAAA+IJXLUevvfaaBg4cqHvuuUf9+/dXUlKSr+sKOTt22F0BAADwBa/C0aeffurrOkJeQYHdFQAAAF/w6rCaJH344Yf62c9+ppycHO3fv1+S9M9//lNr1qzxWXGhpLTU7goAAIAveBWOFi5cqCFDhiguLk6bNm1S6X+SQVFRkf74xz/6tEAAAIBA8ioc/eEPf9Ds2bP1f//3f2rcuLFret++fbVx40afFQcAABBoXoWj7du3a8CAAVWmJyUlqbCwsKE1AQAA2MarcJSenq4dHk7PWrNmjdq3b9/gogAAAOziVTgaN26cJk2apPXr18vhcOjAgQN69dVX9cADD2j8+PG+rhEAACBgvDqVf+rUqSovL9fgwYN16tQpDRgwQE6nUw899JDuvvtuX9cIAAAQMF61HDkcDv32t7/VsWPHtHXrVq1bt06HDx9WUlKSsrKyfF0jAABAwNQrHJWWlmratGnq2bOn+vbtqyVLlqhLly768ssv1alTJz3zzDO6//77/VUrAACA39XrsNojjzyiF154Qbm5ufr444/105/+VGPHjtW6dev09NNP66c//amio6P9VSsAAIDf1avlaP78+frHP/6hBQsWaNmyZSorK9P58+e1ZcsW3X777fUORqtXr9bNN9+sVq1ayeFwaPHixbU+ZuXKlbr66qvldDrVoUMHzZkzp17PCQAAUJN6haN9+/apR48ekqSuXbvK6XTq/vvvl8Ph8OrJS0pKlJ2dreeee65O8+/atUvDhg3Tddddp82bN2vy5Mm6++679e9//9ur5wcAAKisXofVysrKFBMTc/HBjRopPj7e6ycfOnSohg4dWuf5Z8+eraysLD399NOSpMsvv1xr1qzRX/7yFw0ZMsTrOgAAACrUKxwZYzRmzBg5nU5J0pkzZ3TvvfeqadOmbvO98cYbvqvQYu3atcrNzXWbNmTIEE2ePLnax5SWlrqu/SZJxcXFfqkNAACEh3qFo9GjR7v9/bOf/cynxdQmPz9faWlpbtPS0tJUXFys06dPKy4urspj8vLy9NhjjwWqRAAAEOLqFY5efvllf9XhN9OmTdOUKVNcfxcXFyszM9PGigAAQDDzaoRsu6Snp6ugoMBtWkFBgRITEz22GkmS0+l0HQYEAACojVcjZNslJydHK1ascJu2fPly5eTk2FQRAAAIN7aGo5MnT2rz5s3avHmzpAun6m/evFl79uyRdOGQ2J133uma/95779V3332nX//61/r666/1/PPP6/XXX2dUbgAA4DO2hqMNGzboqquu0lVXXSVJmjJliq666io98sgjkqSDBw+6gpIkZWVl6Z133tHy5cuVnZ2tp59+Wi+++CKn8QMAAJ9xGGOM3UUEUnFxsZKSklRUVKTExESfLbfyOJiRtVYBAPAvf+2/PQmpPkcAAAD+RjgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBz5gTF2VwAAALxFOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwCIowtFzzz2nSy65RLGxserdu7c++eSTauedM2eOHA6H2y02NjaA1QIAgHBmeziaN2+epkyZounTp2vjxo3Kzs7WkCFDdOjQoWofk5iYqIMHD7puu3fvDmDFAAAgnNkejmbMmKFx48Zp7Nix6tKli2bPnq0mTZropZdeqvYxDodD6enprltaWloAKwYAAOHM1nB09uxZffbZZ8rNzXVNi4qKUm5urtauXVvt406ePKl27dopMzNTt9xyi7788stq5y0tLVVxcbHbDQAAoDq2hqMjR46orKysSstPWlqa8vPzPT6mU6dOeumll/Tmm2/qlVdeUXl5ufr06aN9+/Z5nD8vL09JSUmuW2Zmps9fBwAACB+2H1arr5ycHN15553q3r27Bg4cqDfeeEMtW7bUCy+84HH+adOmqaioyHXbu3dvgCsGAAChpJGdT56SkqLo6GgVFBS4TS8oKFB6enqdltG4cWNdddVV2rFjh8f7nU6nnE5ng2sFAACRwdaWo5iYGPXo0UMrVqxwTSsvL9eKFSuUk5NTp2WUlZXpiy++UEZGhr/KBAAAEcTWliNJmjJlikaPHq2ePXuqV69emjlzpkpKSjR27FhJ0p133qnWrVsrLy9PkvT444/r2muvVYcOHVRYWKinnnpKu3fv1t13323nywAAAGHC9nA0YsQIHT58WI888ojy8/PVvXt3LV261NVJe8+ePYqKutjAdfz4cY0bN075+flq1qyZevTooY8//lhdunSx6yUAAIAw4jDGGLuLCKTi4mIlJSWpqKhIiYmJPluuw3Hx/5G1RgEA8D9/7b89Cbmz1QAAAPyJcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYBEU4ei5557TJZdcotjYWPXu3VuffPJJjfPPnz9fnTt3VmxsrLp166YlS5YEqFIAABDubA9H8+bN05QpUzR9+nRt3LhR2dnZGjJkiA4dOuRx/o8//lgjR47UXXfdpU2bNmn48OEaPny4tm7dGuDKAQBAOHIYY4ydBfTu3VvXXHON/vrXv0qSysvLlZmZqV/+8peaOnVqlflHjBihkpISvf32265p1157rbp3767Zs2fX+nzFxcVKSkpSUVGREhMTffY6HI6L/7d3jQIAEH78tf/2xNaWo7Nnz+qzzz5Tbm6ua1pUVJRyc3O1du1aj49Zu3at2/ySNGTIkGrnLy0tVXFxsdsNAACgOraGoyNHjqisrExpaWlu09PS0pSfn+/xMfn5+fWaPy8vT0lJSa5bZmamb4oHAABhyfY+R/42bdo0FRUVuW579+71y/MYc/EGAABCVyM7nzwlJUXR0dEqKChwm15QUKD09HSPj0lPT6/X/E6nU06n0zcFAwCAsGdry1FMTIx69OihFStWuKaVl5drxYoVysnJ8fiYnJwct/klafny5dXODwAAUB+2thxJ0pQpUzR69Gj17NlTvXr10syZM1VSUqKxY8dKku688061bt1aeXl5kqRJkyZp4MCBevrppzVs2DDNnTtXGzZs0N/+9jc7XwYAAAgTtoejESNG6PDhw3rkkUeUn5+v7t27a+nSpa5O13v27FFU1MUGrj59+uhf//qX/vu//1sPP/ywOnbsqMWLF6tr1652vQQAABBGbB/nKNACOU4CAADwjYgZ5wgAACDYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFjYfvmQQKsYELy4uNjmSgAAQF1V7LcDcWGPiAtHJ06ckCRlZmbaXAkAAKivEydOKCkpya/PEXHXVisvL9eBAweUkJAgh8Ph02UXFxcrMzNTe/fu5bpt9cS68w7rzXusO++x7rzHuvNOxXr76quv1KlTJ7cL0vtDxLUcRUVFqU2bNn59jsTERD70XmLdeYf15j3WnfdYd95j3XmndevWfg9GEh2yAQAA3BCOAAAALAhHPuR0OjV9+nQ5nU67Swk5rDvvsN68x7rzHuvOe6w77wR6vUVch2wAAICa0HIEAABgQTgCAACwIBwBAABYEI4AAAAsCEc+8txzz+mSSy5RbGysevfurU8++cTukmz36KOPyuFwuN06d+7suv/MmTOaMGGCWrRoofj4eN16660qKChwW8aePXs0bNgwNWnSRKmpqXrooYd0/vz5QL8Uv1q9erVuvvlmtWrVSg6HQ4sXL3a73xijRx55RBkZGYqLi1Nubq6+/fZbt3mOHTumUaNGKTExUcnJybrrrrt08uRJt3k+//xz9e/fX7GxscrMzNSTTz7p75fmd7WtuzFjxlT5DN54441u80TiusvLy9M111yjhIQEpaamavjw4dq+fbvbPL76fq5cuVJXX321nE6nOnTooDlz5vj75flVXdbdoEGDqnzu7r33Xrd5InHdzZo1S1deeaVrAMycnBy9++67rvuD6jNn0GBz5841MTEx5qWXXjJffvmlGTdunElOTjYFBQV2l2ar6dOnmyuuuMIcPHjQdTt8+LDr/nvvvddkZmaaFStWmA0bNphrr73W9OnTx3X/+fPnTdeuXU1ubq7ZtGmTWbJkiUlJSTHTpk2z4+X4zZIlS8xvf/tb88YbbxhJZtGiRW73/+lPfzJJSUlm8eLFZsuWLeZHP/qRycrKMqdPn3bNc+ONN5rs7Gyzbt068+GHH5oOHTqYkSNHuu4vKioyaWlpZtSoUWbr1q3mtddeM3FxceaFF14I1Mv0i9rW3ejRo82NN97o9hk8duyY2zyRuO6GDBliXn75ZbN161azefNmc9NNN5m2bduakydPuubxxffzu+++M02aNDFTpkwxX331lXn22WdNdHS0Wbp0aUBfry/VZd0NHDjQjBs3zu1zV1RU5Lo/UtfdW2+9Zd555x3zzTffmO3bt5uHH37YNG7c2GzdutUYE1yfOcKRD/Tq1ctMmDDB9XdZWZlp1aqVycvLs7Eq+02fPt1kZ2d7vK+wsNA0btzYzJ8/3zVt27ZtRpJZu3atMebCji8qKsrk5+e75pk1a5ZJTEw0paWlfq3dLpV38OXl5SY9Pd089dRTrmmFhYXG6XSa1157zRhjzFdffWUkmU8//dQ1z7vvvmscDofZv3+/McaY559/3jRr1sxtvf3mN78xnTp18vMrCpzqwtEtt9xS7WNYdxccOnTISDKrVq0yxvju+/nrX//aXHHFFW7PNWLECDNkyBB/v6SAqbzujLkQjiZNmlTtY1h3FzVr1sy8+OKLQfeZ47BaA509e1afffaZcnNzXdOioqKUm5urtWvX2lhZcPj222/VqlUrtW/fXqNGjdKePXskSZ999pnOnTvntt46d+6stm3butbb2rVr1a1bN6WlpbnmGTJkiIqLi/Xll18G9oXYZNeuXcrPz3dbT0lJSerdu7fbekpOTlbPnj1d8+Tm5ioqKkrr1693zTNgwADFxMS45hkyZIi2b9+u48ePB+jV2GPlypVKTU1Vp06dNH78eB09etR1H+vugqKiIklS8+bNJfnu+7l27Vq3ZVTME07bxsrrrsKrr76qlJQUde3aVdOmTdOpU6dc97HupLKyMs2dO1clJSXKyckJus9cxF141teOHDmisrIytzdLktLS0vT111/bVFVw6N27t+bMmaNOnTrp4MGDeuyxx9S/f39t3bpV+fn5iomJUXJysttj0tLSlJ+fL0nKz8/3uF4r7osEFa/T03qwrqfU1FS3+xs1aqTmzZu7zZOVlVVlGRX3NWvWzC/12+3GG2/Uf/3XfykrK0s7d+7Uww8/rKFDh2rt2rWKjo5m3UkqLy/X5MmT1bdvX3Xt2lWSfPb9rG6e4uJinT59WnFxcf54SQHjad1J0h133KF27dqpVatW+vzzz/Wb3/xG27dv1xtvvCEpstfdF198oZycHJ05c0bx8fFatGiRunTpos2bNwfVZ45wBL8ZOnSo6/9XXnmlevfurXbt2un1118P2S82Qsvtt9/u+n+3bt105ZVX6tJLL9XKlSs1ePBgGysLHhMmTNDWrVu1Zs0au0sJOdWtu3vuucf1/27duikjI0ODBw/Wzp07demllwa6zKDSqVMnbd68WUVFRVqwYIFGjx6tVatW2V1WFRxWa6CUlBRFR0dX6VFfUFCg9PR0m6oKTsnJybrsssu0Y8cOpaen6+zZsyosLHSbx7re0tPTPa7XivsiQcXrrOnzlZ6erkOHDrndf/78eR07dox1WUn79u2VkpKiHTt2SGLdTZw4UW+//bY++OADtWnTxjXdV9/P6uZJTEwM+R9I1a07T3r37i1Jbp+7SF13MTEx6tChg3r06KG8vDxlZ2frmWeeCbrPHOGogWJiYtSjRw+tWLHCNa28vFwrVqxQTk6OjZUFn5MnT2rnzp3KyMhQjx491LhxY7f1tn37du3Zs8e13nJycvTFF1+47byWL1+uxMREdenSJeD12yErK0vp6elu66m4uFjr1693W0+FhYX67LPPXPO8//77Ki8vd22Uc3JytHr1ap07d841z/Lly9WpU6eQPyxUH/v27dPRo0eVkZEhKXLXnTFGEydO1KJFi/T+++9XOWzoq+9nTk6O2zIq5gnlbWNt686TzZs3S5Lb5y4S150n5eXlKi0tDb7PnHf9y2E1d+5c43Q6zZw5c8xXX31l7rnnHpOcnOzWoz4SPfDAA2blypVm165d5qOPPjK5ubkmJSXFHDp0yBhz4bTNtm3bmvfff99s2LDB5OTkmJycHNfjK07bvOGGG8zmzZvN0qVLTcuWLcPuVP4TJ06YTZs2mU2bNhlJZsaMGWbTpk1m9+7dxpgLp/InJyebN99803z++efmlltu8Xgq/1VXXWXWr19v1qxZYzp27Oh2OnphYaFJS0szP//5z83WrVvN3LlzTZMmTUL6dHRjal53J06cMA8++KBZu3at2bVrl3nvvffM1VdfbTp27GjOnDnjWkYkrrvx48ebpKQks3LlSrfTzU+dOuWaxxffz4rTqh966CGzbds289xzz4X86ei1rbsdO3aYxx9/3GzYsMHs2rXLvPnmm6Z9+/ZmwIABrmVE6rqbOnWqWbVqldm1a5f5/PPPzdSpU43D4TDLli0zxgTXZ45w5CPPPvusadu2rYmJiTG9evUy69ats7sk240YMcJkZGSYmJgY07p1azNixAizY8cO1/2nT5829913n2nWrJlp0qSJ+fGPf2wOHjzotozvv//eDB061MTFxZmUlBTzwAMPmHPnzgX6pfjVBx98YCRVuY0ePdoYc+F0/t/97ncmLS3NOJ1OM3jwYLN9+3a3ZRw9etSMHDnSxMfHm8TERDN27Fhz4sQJt3m2bNli+vXrZ5xOp2ndurX505/+FKiX6Dc1rbtTp06ZG264wbRs2dI0btzYtGvXzowbN67Kj5ZIXHee1pkk8/LLL7vm8dX384MPPjDdu3c3MTExpn379m7PEYpqW3d79uwxAwYMMM2bNzdOp9N06NDBPPTQQ27jHBkTmevuF7/4hWnXrp2JiYkxLVu2NIMHD3YFI2OC6zPnMMaY+rU1AQAAhC/6HAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AhC0vv/+ezkcDtflF/xhzJgxGj58uN+WDyD0EI4A+M2YMWPkcDiq3G688cY6PT4zM1MHDx5U165d/VwpAFzUyO4CAIS3G2+8US+//LLbNKfTWafHRkdHu662DQCBQssRAL9yOp1KT093u1Vczd7hcGjWrFkaOnSo4uLi1L59ey1YsMD12MqH1Y4fP65Ro0apZcuWiouLU8eOHd2C1xdffKEf/OAHiouLU4sWLXTPPffo5MmTrvvLyso0ZcoUJScnq0WLFvr1r3+tyldQKi8vV15enrKyshQXF6fs7Gy3mmqrAUDoIxwBsNXvfvc73XrrrdqyZYtGjRql22+/Xdu2bat23q+++krvvvuutm3bplmzZiklJUWSVFJSoiFDhqhZs2b69NNPNX/+fL333nuaOHGi6/FPP/205syZo5deeklr1qzRsWPHtGjRIrfnyMvL0z/+8Q/Nnj1bX375pe6//3797Gc/06pVq2qtAUCYqP91dQGgbkaPHm2io6NN06ZN3W5PPPGEMebCFc7vvfdet8f07t3bjB8/3hhjzK5du4wks2nTJmOMMTfffLMZO3asx+f629/+Zpo1a2ZOnjzpmvbOO++YqKgok5+fb4wxJiMjwzz55JOu+8+dO2fatGljbrnlFmOMMWfOnDFNmjQxH3/8sduy77rrLjNy5MhaawAQHuhzBMCvrrvuOs2aNcttWvPmzV3/z8nJcbsvJyen2rPTxo8fr1tvvVUbN27UDTfcoOHDh6tPnz6SpG3btik7O1tNmzZ1zd+3b1+Vl5dr+/btio2N1cGDB9W7d2/X/Y0aNVLPnj1dh9Z27NihU6dO6frrr3d73rNnz+qqq66qtQYA4YFwBMCvmjZtqg4dOvhkWUOHDtXu3bu1ZMkSLV++XIMHD9aECRP05z//2SfLr+if9M4776h169Zu91V0Ivd3DQDsR58jALZat25dlb8vv/zyaudv2bKlRo8erVdeeUUzZ87U3/72N0nS5Zdfri1btqikpMQ170cffaSoqCh16tRJSUlJysjI0Pr16133nz9/Xp999pnr7y5dusjpdGrPnj3q0KGD2y0zM7PWGgCEB1qOAPhVaWmp8vPz3aY1atTI1Yl5/vz56tmzp/r166dXX31Vn3zyif7f//t/Hpf1yCOPqEePHrriiitUWlqqt99+2xWkRo0apenTp2v06NF69NFHdfjwYf3yl7/Uz3/+c6WlpUmSJk2apD/96U/q2LGjOnfurBkzZqiwsNC1/ISEBD344IO6//77VV5ern79+qmoqEgfffSREhMTNXr06BprABAeCEcA/Grp0qXKyMhwm9apUyd9/fXXkqTHHntMc+fO1X333aeMjAy99tpr6tKli8dlxcTEaNq0afr+++8VFxen/v37a+7cuZKkJk2a6N///rcmTZqka665Rk2aNNGtt96qGTNmuB7/wAMP6ODBgxo9erSioqL0i1/8Qj/+8Y9VVFTkmuf3v/+9WrZsqby8PH333XdKTk7W1VdfrYcffrjWGgCEB4cxlQb5AIAAcTgcWrRoEZfvABBU6HMEAABgQTgCAACwoM8RANtwVB9AMKLlCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADA4v8DfqGOz3C/kzcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"epis:\", e, \"  score:\", score, \"  mem len:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 4), \"   steps:\", step,\n",
    "                  \"   lr:\", round(agent.optimizer.param_groups[0]['lr'], 7), \"    reward:\", round(np.mean(evaluation_reward), 2))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
