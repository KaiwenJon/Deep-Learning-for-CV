{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84156/607054412.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "/tmp/ipykernel_84156/607054412.py:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 0   score: 0.0   mem len: 122   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 0.0\n",
      "epis: 1   score: 1.0   mem len: 273   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 0.5\n",
      "epis: 2   score: 0.0   mem len: 395   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 0.33\n",
      "epis: 3   score: 3.0   mem len: 661   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.0\n",
      "epis: 4   score: 1.0   mem len: 831   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.0\n",
      "epis: 5   score: 2.0   mem len: 1049   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.17\n",
      "epis: 6   score: 2.0   mem len: 1265   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.29\n",
      "epis: 7   score: 1.0   mem len: 1433   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.25\n",
      "epis: 8   score: 3.0   mem len: 1679   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.44\n",
      "epis: 9   score: 1.0   mem len: 1848   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 10   score: 2.0   mem len: 2030   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.45\n",
      "epis: 11   score: 1.0   mem len: 2199   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 12   score: 2.0   mem len: 2379   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.46\n",
      "epis: 13   score: 0.0   mem len: 2501   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 14   score: 3.0   mem len: 2745   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.47\n",
      "epis: 15   score: 0.0   mem len: 2868   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 16   score: 1.0   mem len: 3036   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.35\n",
      "epis: 17   score: 1.0   mem len: 3188   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.33\n",
      "epis: 18   score: 4.0   mem len: 3448   epsilon: 1.0    steps: 260    lr: 0.0001     reward: 1.47\n",
      "epis: 19   score: 5.0   mem len: 3755   epsilon: 1.0    steps: 307    lr: 0.0001     reward: 1.65\n",
      "epis: 20   score: 2.0   mem len: 3953   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 21   score: 1.0   mem len: 4124   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.64\n",
      "epis: 22   score: 4.0   mem len: 4401   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.74\n",
      "epis: 23   score: 1.0   mem len: 4569   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.71\n",
      "epis: 24   score: 2.0   mem len: 4787   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.72\n",
      "epis: 25   score: 1.0   mem len: 4937   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.69\n",
      "epis: 26   score: 4.0   mem len: 5212   epsilon: 1.0    steps: 275    lr: 0.0001     reward: 1.78\n",
      "epis: 27   score: 0.0   mem len: 5334   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.71\n",
      "epis: 28   score: 0.0   mem len: 5457   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 29   score: 1.0   mem len: 5608   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.63\n",
      "epis: 30   score: 0.0   mem len: 5731   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 31   score: 2.0   mem len: 5950   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.59\n",
      "epis: 32   score: 1.0   mem len: 6101   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 33   score: 0.0   mem len: 6223   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 34   score: 2.0   mem len: 6439   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.54\n",
      "epis: 35   score: 3.0   mem len: 6686   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.58\n",
      "epis: 36   score: 0.0   mem len: 6808   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 37   score: 1.0   mem len: 6959   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 38   score: 2.0   mem len: 7157   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 39   score: 0.0   mem len: 7280   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 40   score: 1.0   mem len: 7430   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.49\n",
      "epis: 41   score: 2.0   mem len: 7628   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 42   score: 1.0   mem len: 7797   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 43   score: 1.0   mem len: 7968   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.48\n",
      "epis: 44   score: 0.0   mem len: 8091   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 45   score: 1.0   mem len: 8242   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 46   score: 3.0   mem len: 8486   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.47\n",
      "epis: 47   score: 4.0   mem len: 8763   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.52\n",
      "epis: 48   score: 1.0   mem len: 8914   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 49   score: 1.0   mem len: 9083   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 50   score: 1.0   mem len: 9254   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.49\n",
      "epis: 51   score: 0.0   mem len: 9377   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 52   score: 4.0   mem len: 9655   epsilon: 1.0    steps: 278    lr: 0.0001     reward: 1.51\n",
      "epis: 53   score: 1.0   mem len: 9824   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 54   score: 0.0   mem len: 9947   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 55   score: 2.0   mem len: 10127   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.48\n",
      "epis: 56   score: 1.0   mem len: 10298   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.47\n",
      "epis: 57   score: 3.0   mem len: 10531   epsilon: 1.0    steps: 233    lr: 0.0001     reward: 1.5\n",
      "epis: 58   score: 1.0   mem len: 10701   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.49\n",
      "epis: 59   score: 2.0   mem len: 10919   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.5\n",
      "epis: 60   score: 0.0   mem len: 11042   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 61   score: 1.0   mem len: 11212   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.47\n",
      "epis: 62   score: 2.0   mem len: 11429   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.48\n",
      "epis: 63   score: 1.0   mem len: 11580   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.47\n",
      "epis: 64   score: 2.0   mem len: 11778   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 65   score: 2.0   mem len: 11976   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 66   score: 0.0   mem len: 12099   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 67   score: 0.0   mem len: 12222   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 68   score: 2.0   mem len: 12419   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 69   score: 4.0   mem len: 12686   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.49\n",
      "epis: 70   score: 2.0   mem len: 12883   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.49\n",
      "epis: 71   score: 0.0   mem len: 13005   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 72   score: 2.0   mem len: 13187   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.48\n",
      "epis: 73   score: 1.0   mem len: 13355   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.47\n",
      "epis: 74   score: 1.0   mem len: 13507   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.47\n",
      "epis: 75   score: 0.0   mem len: 13630   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 76   score: 4.0   mem len: 13925   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.48\n",
      "epis: 77   score: 0.0   mem len: 14048   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 78   score: 1.0   mem len: 14199   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 79   score: 3.0   mem len: 14448   epsilon: 1.0    steps: 249    lr: 0.0001     reward: 1.48\n",
      "epis: 80   score: 3.0   mem len: 14695   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.49\n",
      "epis: 81   score: 0.0   mem len: 14817   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 82   score: 2.0   mem len: 15036   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.48\n",
      "epis: 83   score: 2.0   mem len: 15216   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 84   score: 0.0   mem len: 15339   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 85   score: 0.0   mem len: 15462   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 86   score: 0.0   mem len: 15584   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 87   score: 1.0   mem len: 15734   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.43\n",
      "epis: 88   score: 2.0   mem len: 15949   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.44\n",
      "epis: 89   score: 0.0   mem len: 16072   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 90   score: 2.0   mem len: 16272   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.43\n",
      "epis: 91   score: 1.0   mem len: 16424   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.42\n",
      "epis: 92   score: 1.0   mem len: 16595   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.42\n",
      "epis: 93   score: 1.0   mem len: 16763   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.41\n",
      "epis: 94   score: 1.0   mem len: 16933   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.41\n",
      "epis: 95   score: 5.0   mem len: 17279   epsilon: 1.0    steps: 346    lr: 0.0001     reward: 1.45\n",
      "epis: 96   score: 1.0   mem len: 17430   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 97   score: 2.0   mem len: 17628   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 98   score: 2.0   mem len: 17825   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 99   score: 0.0   mem len: 17947   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 100   score: 0.0   mem len: 18069   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 101   score: 0.0   mem len: 18192   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 102   score: 2.0   mem len: 18410   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.45\n",
      "epis: 103   score: 2.0   mem len: 18629   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.44\n",
      "epis: 104   score: 1.0   mem len: 18797   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 105   score: 2.0   mem len: 18977   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.44\n",
      "epis: 106   score: 1.0   mem len: 19148   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.43\n",
      "epis: 107   score: 2.0   mem len: 19364   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.44\n",
      "epis: 108   score: 1.0   mem len: 19532   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.42\n",
      "epis: 109   score: 0.0   mem len: 19655   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 110   score: 1.0   mem len: 19824   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 111   score: 1.0   mem len: 19975   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 112   score: 6.0   mem len: 20365   epsilon: 1.0    steps: 390    lr: 0.0001     reward: 1.44\n",
      "epis: 113   score: 0.0   mem len: 20487   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 114   score: 2.0   mem len: 20706   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.43\n",
      "epis: 115   score: 1.0   mem len: 20874   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 116   score: 4.0   mem len: 21171   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.47\n",
      "epis: 117   score: 2.0   mem len: 21368   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.48\n",
      "epis: 118   score: 0.0   mem len: 21491   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 119   score: 2.0   mem len: 21670   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.41\n",
      "epis: 120   score: 0.0   mem len: 21793   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 121   score: 0.0   mem len: 21915   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 122   score: 2.0   mem len: 22113   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 123   score: 2.0   mem len: 22311   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 124   score: 0.0   mem len: 22434   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 125   score: 2.0   mem len: 22632   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 126   score: 2.0   mem len: 22830   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 127   score: 0.0   mem len: 22952   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 128   score: 2.0   mem len: 23150   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 129   score: 2.0   mem len: 23348   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 130   score: 1.0   mem len: 23518   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.38\n",
      "epis: 131   score: 2.0   mem len: 23719   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.38\n",
      "epis: 132   score: 2.0   mem len: 23917   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 133   score: 0.0   mem len: 24040   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 134   score: 0.0   mem len: 24163   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 135   score: 4.0   mem len: 24483   epsilon: 1.0    steps: 320    lr: 0.0001     reward: 1.38\n",
      "epis: 136   score: 5.0   mem len: 24806   epsilon: 1.0    steps: 323    lr: 0.0001     reward: 1.43\n",
      "epis: 137   score: 5.0   mem len: 25144   epsilon: 1.0    steps: 338    lr: 0.0001     reward: 1.47\n",
      "epis: 138   score: 2.0   mem len: 25364   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.47\n",
      "epis: 139   score: 2.0   mem len: 25562   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 140   score: 2.0   mem len: 25780   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.5\n",
      "epis: 141   score: 2.0   mem len: 25995   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.5\n",
      "epis: 142   score: 0.0   mem len: 26118   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 143   score: 1.0   mem len: 26269   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.49\n",
      "epis: 144   score: 0.0   mem len: 26392   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 145   score: 0.0   mem len: 26514   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 146   score: 2.0   mem len: 26712   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 147   score: 1.0   mem len: 26880   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 148   score: 3.0   mem len: 27107   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.46\n",
      "epis: 149   score: 2.0   mem len: 27287   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.47\n",
      "epis: 150   score: 1.0   mem len: 27456   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 151   score: 0.0   mem len: 27578   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 152   score: 0.0   mem len: 27700   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 153   score: 5.0   mem len: 28024   epsilon: 1.0    steps: 324    lr: 0.0001     reward: 1.47\n",
      "epis: 154   score: 1.0   mem len: 28196   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.48\n",
      "epis: 155   score: 2.0   mem len: 28395   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.48\n",
      "epis: 156   score: 2.0   mem len: 28593   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 157   score: 1.0   mem len: 28743   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.47\n",
      "epis: 158   score: 2.0   mem len: 28944   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.48\n",
      "epis: 159   score: 3.0   mem len: 29172   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.49\n",
      "epis: 160   score: 2.0   mem len: 29371   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.51\n",
      "epis: 161   score: 1.0   mem len: 29539   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.51\n",
      "epis: 162   score: 0.0   mem len: 29661   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 163   score: 2.0   mem len: 29858   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.5\n",
      "epis: 164   score: 1.0   mem len: 30026   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.49\n",
      "epis: 165   score: 2.0   mem len: 30244   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.49\n",
      "epis: 166   score: 0.0   mem len: 30367   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 167   score: 1.0   mem len: 30538   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.5\n",
      "epis: 168   score: 4.0   mem len: 30817   epsilon: 1.0    steps: 279    lr: 0.0001     reward: 1.52\n",
      "epis: 169   score: 1.0   mem len: 30967   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.49\n",
      "epis: 170   score: 2.0   mem len: 31183   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.49\n",
      "epis: 171   score: 1.0   mem len: 31351   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 172   score: 6.0   mem len: 31682   epsilon: 1.0    steps: 331    lr: 0.0001     reward: 1.54\n",
      "epis: 173   score: 1.0   mem len: 31850   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.54\n",
      "epis: 174   score: 0.0   mem len: 31972   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 175   score: 1.0   mem len: 32122   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.54\n",
      "epis: 176   score: 2.0   mem len: 32340   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.52\n",
      "epis: 177   score: 1.0   mem len: 32490   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.53\n",
      "epis: 178   score: 1.0   mem len: 32659   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 179   score: 3.0   mem len: 32905   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.53\n",
      "epis: 180   score: 4.0   mem len: 33183   epsilon: 1.0    steps: 278    lr: 0.0001     reward: 1.54\n",
      "epis: 181   score: 1.0   mem len: 33333   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.55\n",
      "epis: 182   score: 2.0   mem len: 33531   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 183   score: 1.0   mem len: 33681   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.54\n",
      "epis: 184   score: 1.0   mem len: 33853   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.55\n",
      "epis: 185   score: 1.0   mem len: 34005   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.56\n",
      "epis: 186   score: 3.0   mem len: 34249   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.59\n",
      "epis: 187   score: 1.0   mem len: 34400   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 188   score: 2.0   mem len: 34598   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 189   score: 0.0   mem len: 34721   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 190   score: 2.0   mem len: 34937   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.59\n",
      "epis: 191   score: 4.0   mem len: 35218   epsilon: 1.0    steps: 281    lr: 0.0001     reward: 1.62\n",
      "epis: 192   score: 0.0   mem len: 35341   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 193   score: 1.0   mem len: 35510   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 194   score: 2.0   mem len: 35729   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.62\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84156/607054412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mframe_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_next_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mterminal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlife\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lives'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/utils.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reflect'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"epis:\", e, \"  score:\", score, \"  mem len:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 4), \"   steps:\", step,\n",
    "                  \"   lr:\", round(agent.optimizer.param_groups[0]['lr'], 7), \"    reward:\", round(np.mean(evaluation_reward), 2))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_double_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_double_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
