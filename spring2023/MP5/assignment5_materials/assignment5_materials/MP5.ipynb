{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44971/607054412.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "/tmp/ipykernel_44971/607054412.py:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 0   score: 0.0   mem len: 122   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 0.0\n",
      "epis: 1   score: 2.0   mem len: 339   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.0\n",
      "epis: 2   score: 3.0   mem len: 567   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.67\n",
      "epis: 3   score: 2.0   mem len: 766   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.75\n",
      "epis: 4   score: 1.0   mem len: 917   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 5   score: 1.0   mem len: 1089   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.5\n",
      "epis: 6   score: 2.0   mem len: 1271   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.57\n",
      "epis: 7   score: 1.0   mem len: 1439   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 8   score: 0.0   mem len: 1562   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 9   score: 0.0   mem len: 1685   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 10   score: 1.0   mem len: 1836   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.18\n",
      "epis: 11   score: 2.0   mem len: 2033   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.25\n",
      "epis: 12   score: 1.0   mem len: 2184   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.23\n",
      "epis: 13   score: 3.0   mem len: 2430   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.36\n",
      "epis: 14   score: 1.0   mem len: 2599   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.33\n",
      "epis: 15   score: 1.0   mem len: 2770   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.31\n",
      "epis: 16   score: 0.0   mem len: 2892   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.24\n",
      "epis: 17   score: 3.0   mem len: 3117   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.33\n",
      "epis: 18   score: 3.0   mem len: 3387   epsilon: 1.0    steps: 270    lr: 0.0001     reward: 1.42\n",
      "epis: 19   score: 1.0   mem len: 3537   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.4\n",
      "epis: 20   score: 1.0   mem len: 3688   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 21   score: 4.0   mem len: 3944   epsilon: 1.0    steps: 256    lr: 0.0001     reward: 1.5\n",
      "epis: 22   score: 1.0   mem len: 4113   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 23   score: 0.0   mem len: 4235   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 24   score: 1.0   mem len: 4386   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 25   score: 1.0   mem len: 4555   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 26   score: 1.0   mem len: 4725   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.37\n",
      "epis: 27   score: 1.0   mem len: 4896   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.36\n",
      "epis: 28   score: 2.0   mem len: 5098   epsilon: 1.0    steps: 202    lr: 0.0001     reward: 1.38\n",
      "epis: 29   score: 1.0   mem len: 5266   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.37\n",
      "epis: 30   score: 0.0   mem len: 5388   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 31   score: 1.0   mem len: 5556   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.31\n",
      "epis: 32   score: 0.0   mem len: 5679   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 33   score: 3.0   mem len: 5925   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.32\n",
      "epis: 34   score: 2.0   mem len: 6125   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.34\n",
      "epis: 35   score: 0.0   mem len: 6248   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 36   score: 0.0   mem len: 6371   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 37   score: 0.0   mem len: 6493   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.24\n",
      "epis: 38   score: 0.0   mem len: 6615   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.21\n",
      "epis: 39   score: 3.0   mem len: 6879   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.25\n",
      "epis: 40   score: 2.0   mem len: 7078   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.27\n",
      "epis: 41   score: 2.0   mem len: 7260   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.29\n",
      "epis: 42   score: 0.0   mem len: 7383   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.26\n",
      "epis: 43   score: 1.0   mem len: 7555   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.25\n",
      "epis: 44   score: 2.0   mem len: 7774   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.27\n",
      "epis: 45   score: 1.0   mem len: 7924   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.26\n",
      "epis: 46   score: 2.0   mem len: 8142   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.28\n",
      "epis: 47   score: 2.0   mem len: 8340   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 48   score: 2.0   mem len: 8537   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.31\n",
      "epis: 49   score: 0.0   mem len: 8660   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 50   score: 4.0   mem len: 8937   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.33\n",
      "epis: 51   score: 2.0   mem len: 9116   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.35\n",
      "epis: 52   score: 4.0   mem len: 9414   epsilon: 1.0    steps: 298    lr: 0.0001     reward: 1.4\n",
      "epis: 53   score: 2.0   mem len: 9629   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.41\n",
      "epis: 54   score: 1.0   mem len: 9798   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 55   score: 0.0   mem len: 9921   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 56   score: 0.0   mem len: 10044   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 57   score: 0.0   mem len: 10167   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 58   score: 1.0   mem len: 10339   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.32\n",
      "epis: 59   score: 2.0   mem len: 10537   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 60   score: 0.0   mem len: 10659   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 61   score: 4.0   mem len: 10935   epsilon: 1.0    steps: 276    lr: 0.0001     reward: 1.35\n",
      "epis: 62   score: 1.0   mem len: 11107   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.35\n",
      "epis: 63   score: 0.0   mem len: 11229   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 64   score: 3.0   mem len: 11496   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.35\n",
      "epis: 65   score: 2.0   mem len: 11694   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 66   score: 0.0   mem len: 11816   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 67   score: 1.0   mem len: 11967   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 68   score: 3.0   mem len: 12229   epsilon: 1.0    steps: 262    lr: 0.0001     reward: 1.36\n",
      "epis: 69   score: 2.0   mem len: 12427   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 70   score: 0.0   mem len: 12550   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 71   score: 1.0   mem len: 12719   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 72   score: 3.0   mem len: 12985   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.37\n",
      "epis: 73   score: 0.0   mem len: 13108   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 74   score: 2.0   mem len: 13306   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 75   score: 0.0   mem len: 13429   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 76   score: 1.0   mem len: 13581   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.34\n",
      "epis: 77   score: 0.0   mem len: 13703   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 78   score: 1.0   mem len: 13871   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.32\n",
      "epis: 79   score: 3.0   mem len: 14118   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.34\n",
      "epis: 80   score: 0.0   mem len: 14241   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 81   score: 0.0   mem len: 14363   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 82   score: 1.0   mem len: 14513   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.3\n",
      "epis: 83   score: 4.0   mem len: 14771   epsilon: 1.0    steps: 258    lr: 0.0001     reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 84   score: 0.0   mem len: 14894   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 85   score: 1.0   mem len: 15045   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.31\n",
      "epis: 86   score: 1.0   mem len: 15197   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.31\n",
      "epis: 87   score: 4.0   mem len: 15494   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.34\n",
      "epis: 88   score: 2.0   mem len: 15713   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.35\n",
      "epis: 89   score: 1.0   mem len: 15864   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 90   score: 1.0   mem len: 16014   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.34\n",
      "epis: 91   score: 1.0   mem len: 16183   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 92   score: 0.0   mem len: 16306   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 93   score: 2.0   mem len: 16524   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.33\n",
      "epis: 94   score: 1.0   mem len: 16675   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 95   score: 3.0   mem len: 16941   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.34\n",
      "epis: 96   score: 3.0   mem len: 17191   epsilon: 1.0    steps: 250    lr: 0.0001     reward: 1.36\n",
      "epis: 97   score: 0.0   mem len: 17314   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 98   score: 1.0   mem len: 17483   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 99   score: 1.0   mem len: 17633   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.34\n",
      "epis: 100   score: 1.0   mem len: 17805   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.35\n",
      "epis: 101   score: 0.0   mem len: 17928   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 102   score: 0.0   mem len: 18050   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 103   score: 1.0   mem len: 18219   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.29\n",
      "epis: 104   score: 2.0   mem len: 18417   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.3\n",
      "epis: 105   score: 0.0   mem len: 18539   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 106   score: 2.0   mem len: 18737   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 107   score: 0.0   mem len: 18860   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 108   score: 4.0   mem len: 19156   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.32\n",
      "epis: 109   score: 0.0   mem len: 19278   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 110   score: 0.0   mem len: 19401   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 111   score: 2.0   mem len: 19623   epsilon: 1.0    steps: 222    lr: 0.0001     reward: 1.31\n",
      "epis: 112   score: 2.0   mem len: 19822   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.32\n",
      "epis: 113   score: 2.0   mem len: 20039   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.31\n",
      "epis: 114   score: 1.0   mem len: 20208   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 115   score: 0.0   mem len: 20331   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.3\n",
      "epis: 116   score: 2.0   mem len: 20549   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 117   score: 1.0   mem len: 20720   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.3\n",
      "epis: 118   score: 2.0   mem len: 20918   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 119   score: 3.0   mem len: 21185   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.31\n",
      "epis: 120   score: 2.0   mem len: 21382   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.32\n",
      "epis: 121   score: 1.0   mem len: 21533   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.29\n",
      "epis: 122   score: 3.0   mem len: 21778   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.31\n",
      "epis: 123   score: 2.0   mem len: 21976   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 124   score: 2.0   mem len: 22191   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.34\n",
      "epis: 125   score: 5.0   mem len: 22499   epsilon: 1.0    steps: 308    lr: 0.0001     reward: 1.38\n",
      "epis: 126   score: 0.0   mem len: 22621   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 127   score: 2.0   mem len: 22819   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 128   score: 1.0   mem len: 22989   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.37\n",
      "epis: 129   score: 3.0   mem len: 23235   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.39\n",
      "epis: 130   score: 1.0   mem len: 23403   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.4\n",
      "epis: 131   score: 1.0   mem len: 23571   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.4\n",
      "epis: 132   score: 0.0   mem len: 23694   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 133   score: 1.0   mem len: 23845   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 134   score: 0.0   mem len: 23967   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 135   score: 0.0   mem len: 24089   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 136   score: 3.0   mem len: 24337   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.39\n",
      "epis: 137   score: 1.0   mem len: 24488   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 138   score: 0.0   mem len: 24610   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 139   score: 1.0   mem len: 24781   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.38\n",
      "epis: 140   score: 4.0   mem len: 25053   epsilon: 1.0    steps: 272    lr: 0.0001     reward: 1.4\n",
      "epis: 141   score: 0.0   mem len: 25175   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 142   score: 1.0   mem len: 25344   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 143   score: 1.0   mem len: 25513   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 144   score: 0.0   mem len: 25636   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 145   score: 1.0   mem len: 25787   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 146   score: 0.0   mem len: 25909   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 147   score: 4.0   mem len: 26225   epsilon: 1.0    steps: 316    lr: 0.0001     reward: 1.37\n",
      "epis: 148   score: 0.0   mem len: 26347   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 149   score: 4.0   mem len: 26627   epsilon: 1.0    steps: 280    lr: 0.0001     reward: 1.39\n",
      "epis: 150   score: 0.0   mem len: 26749   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 151   score: 3.0   mem len: 26996   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.36\n",
      "epis: 152   score: 1.0   mem len: 27147   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 153   score: 2.0   mem len: 27345   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 154   score: 2.0   mem len: 27543   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 155   score: 1.0   mem len: 27712   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 156   score: 1.0   mem len: 27864   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.36\n",
      "epis: 157   score: 4.0   mem len: 28139   epsilon: 1.0    steps: 275    lr: 0.0001     reward: 1.4\n",
      "epis: 158   score: 1.0   mem len: 28310   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.4\n",
      "epis: 159   score: 2.0   mem len: 28527   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.4\n",
      "epis: 160   score: 0.0   mem len: 28650   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 161   score: 0.0   mem len: 28773   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 162   score: 2.0   mem len: 28974   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.37\n",
      "epis: 163   score: 3.0   mem len: 29222   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.4\n",
      "epis: 164   score: 2.0   mem len: 29421   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.39\n",
      "epis: 165   score: 2.0   mem len: 29637   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.39\n",
      "epis: 166   score: 0.0   mem len: 29760   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 167   score: 1.0   mem len: 29929   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 168   score: 2.0   mem len: 30126   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.38\n",
      "epis: 169   score: 2.0   mem len: 30324   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 170   score: 4.0   mem len: 30600   epsilon: 1.0    steps: 276    lr: 0.0001     reward: 1.42\n",
      "epis: 171   score: 0.0   mem len: 30723   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 172   score: 1.0   mem len: 30892   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 173   score: 2.0   mem len: 31110   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 174   score: 0.0   mem len: 31233   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 175   score: 1.0   mem len: 31384   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 176   score: 2.0   mem len: 31602   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 177   score: 1.0   mem len: 31771   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 178   score: 0.0   mem len: 31894   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 179   score: 1.0   mem len: 32045   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 180   score: 0.0   mem len: 32167   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 181   score: 0.0   mem len: 32289   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 182   score: 0.0   mem len: 32412   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 183   score: 3.0   mem len: 32660   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.37\n",
      "epis: 184   score: 2.0   mem len: 32878   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.39\n",
      "epis: 185   score: 4.0   mem len: 33136   epsilon: 1.0    steps: 258    lr: 0.0001     reward: 1.42\n",
      "epis: 186   score: 0.0   mem len: 33259   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 187   score: 1.0   mem len: 33430   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.38\n",
      "epis: 188   score: 0.0   mem len: 33553   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 189   score: 0.0   mem len: 33675   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 190   score: 2.0   mem len: 33875   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.36\n",
      "epis: 191   score: 3.0   mem len: 34143   epsilon: 1.0    steps: 268    lr: 0.0001     reward: 1.38\n",
      "epis: 192   score: 0.0   mem len: 34265   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 193   score: 2.0   mem len: 34445   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.38\n",
      "epis: 194   score: 1.0   mem len: 34614   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 195   score: 1.0   mem len: 34785   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.36\n",
      "epis: 196   score: 0.0   mem len: 34907   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 197   score: 4.0   mem len: 35221   epsilon: 1.0    steps: 314    lr: 0.0001     reward: 1.37\n",
      "epis: 198   score: 1.0   mem len: 35390   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 199   score: 0.0   mem len: 35512   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 200   score: 3.0   mem len: 35761   epsilon: 1.0    steps: 249    lr: 0.0001     reward: 1.38\n",
      "epis: 201   score: 1.0   mem len: 35912   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 202   score: 3.0   mem len: 36138   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.42\n",
      "epis: 203   score: 8.0   mem len: 36601   epsilon: 1.0    steps: 463    lr: 0.0001     reward: 1.49\n",
      "epis: 204   score: 2.0   mem len: 36817   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.49\n",
      "epis: 205   score: 2.0   mem len: 37015   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 206   score: 1.0   mem len: 37186   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.5\n",
      "epis: 207   score: 1.0   mem len: 37355   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.51\n",
      "epis: 208   score: 0.0   mem len: 37477   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 209   score: 0.0   mem len: 37599   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 210   score: 4.0   mem len: 37913   epsilon: 1.0    steps: 314    lr: 0.0001     reward: 1.51\n",
      "epis: 211   score: 3.0   mem len: 38178   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.52\n",
      "epis: 212   score: 1.0   mem len: 38329   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 213   score: 2.0   mem len: 38547   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.51\n",
      "epis: 214   score: 2.0   mem len: 38726   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.52\n",
      "epis: 215   score: 0.0   mem len: 38849   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 216   score: 3.0   mem len: 39076   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.53\n",
      "epis: 217   score: 1.0   mem len: 39245   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 218   score: 0.0   mem len: 39367   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 219   score: 2.0   mem len: 39565   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 220   score: 1.0   mem len: 39736   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.49\n",
      "epis: 221   score: 0.0   mem len: 39858   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 222   score: 1.0   mem len: 40029   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.46\n",
      "epis: 223   score: 1.0   mem len: 40180   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.45\n",
      "epis: 224   score: 0.0   mem len: 40303   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 225   score: 2.0   mem len: 40521   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.4\n",
      "epis: 226   score: 2.0   mem len: 40738   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.42\n",
      "epis: 227   score: 0.0   mem len: 40860   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 228   score: 2.0   mem len: 41077   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.41\n",
      "epis: 229   score: 2.0   mem len: 41274   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.4\n",
      "epis: 230   score: 4.0   mem len: 41534   epsilon: 1.0    steps: 260    lr: 0.0001     reward: 1.43\n",
      "epis: 231   score: 2.0   mem len: 41731   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 232   score: 0.0   mem len: 41853   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 233   score: 0.0   mem len: 41976   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 234   score: 2.0   mem len: 42191   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.45\n",
      "epis: 235   score: 0.0   mem len: 42314   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 236   score: 1.0   mem len: 42486   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.43\n",
      "epis: 237   score: 1.0   mem len: 42655   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 238   score: 0.0   mem len: 42778   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 239   score: 0.0   mem len: 42900   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 240   score: 1.0   mem len: 43051   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 241   score: 0.0   mem len: 43174   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 242   score: 1.0   mem len: 43344   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.39\n",
      "epis: 243   score: 0.0   mem len: 43466   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 244   score: 3.0   mem len: 43713   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.41\n",
      "epis: 245   score: 1.0   mem len: 43883   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.41\n",
      "epis: 246   score: 1.0   mem len: 44051   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.42\n",
      "epis: 247   score: 2.0   mem len: 44249   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.4\n",
      "epis: 248   score: 3.0   mem len: 44496   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 249   score: 1.0   mem len: 44668   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.4\n",
      "epis: 250   score: 0.0   mem len: 44791   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 251   score: 0.0   mem len: 44914   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 252   score: 2.0   mem len: 45112   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 253   score: 0.0   mem len: 45235   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 254   score: 2.0   mem len: 45433   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 255   score: 0.0   mem len: 45556   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 256   score: 1.0   mem len: 45724   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.35\n",
      "epis: 257   score: 0.0   mem len: 45847   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 258   score: 0.0   mem len: 45970   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.3\n",
      "epis: 259   score: 0.0   mem len: 46093   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 260   score: 2.0   mem len: 46291   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.3\n",
      "epis: 261   score: 1.0   mem len: 46463   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.31\n",
      "epis: 262   score: 0.0   mem len: 46586   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 263   score: 1.0   mem len: 46736   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.27\n",
      "epis: 264   score: 4.0   mem len: 47013   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.29\n",
      "epis: 265   score: 1.0   mem len: 47182   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.28\n",
      "epis: 266   score: 4.0   mem len: 47433   epsilon: 1.0    steps: 251    lr: 0.0001     reward: 1.32\n",
      "epis: 267   score: 0.0   mem len: 47556   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 268   score: 3.0   mem len: 47808   epsilon: 1.0    steps: 252    lr: 0.0001     reward: 1.32\n",
      "epis: 269   score: 2.0   mem len: 48025   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.32\n",
      "epis: 270   score: 1.0   mem len: 48175   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.29\n",
      "epis: 271   score: 2.0   mem len: 48372   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.31\n",
      "epis: 272   score: 0.0   mem len: 48494   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 273   score: 0.0   mem len: 48616   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.28\n",
      "epis: 274   score: 0.0   mem len: 48738   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.28\n",
      "epis: 275   score: 2.0   mem len: 48955   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.29\n",
      "epis: 276   score: 1.0   mem len: 49125   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.28\n",
      "epis: 277   score: 0.0   mem len: 49248   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 278   score: 0.0   mem len: 49370   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.27\n",
      "epis: 279   score: 3.0   mem len: 49615   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.29\n",
      "epis: 280   score: 0.0   mem len: 49737   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 281   score: 0.0   mem len: 49860   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 282   score: 3.0   mem len: 50070   epsilon: 1.0    steps: 210    lr: 0.0001     reward: 1.32\n",
      "epis: 283   score: 2.0   mem len: 50285   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.31\n",
      "epis: 284   score: 2.0   mem len: 50482   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.31\n",
      "epis: 285   score: 1.0   mem len: 50654   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.28\n",
      "epis: 286   score: 0.0   mem len: 50777   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 287   score: 1.0   mem len: 50927   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.28\n",
      "epis: 288   score: 2.0   mem len: 51148   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.3\n",
      "epis: 289   score: 0.0   mem len: 51270   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 290   score: 1.0   mem len: 51439   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.29\n",
      "epis: 291   score: 2.0   mem len: 51637   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 292   score: 6.0   mem len: 51976   epsilon: 1.0    steps: 339    lr: 0.0001     reward: 1.34\n",
      "epis: 293   score: 0.0   mem len: 52099   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 294   score: 0.0   mem len: 52221   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 295   score: 2.0   mem len: 52439   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 296   score: 0.0   mem len: 52562   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 297   score: 3.0   mem len: 52790   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.31\n",
      "epis: 298   score: 0.0   mem len: 52912   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 299   score: 1.0   mem len: 53082   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.31\n",
      "epis: 300   score: 0.0   mem len: 53205   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 301   score: 0.0   mem len: 53327   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.27\n",
      "epis: 302   score: 2.0   mem len: 53525   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.26\n",
      "epis: 303   score: 0.0   mem len: 53647   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.18\n",
      "epis: 304   score: 1.0   mem len: 53798   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.17\n",
      "epis: 305   score: 4.0   mem len: 54115   epsilon: 1.0    steps: 317    lr: 0.0001     reward: 1.19\n",
      "epis: 306   score: 5.0   mem len: 54465   epsilon: 1.0    steps: 350    lr: 0.0001     reward: 1.23\n",
      "epis: 307   score: 3.0   mem len: 54730   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.25\n",
      "epis: 308   score: 2.0   mem len: 54914   epsilon: 1.0    steps: 184    lr: 0.0001     reward: 1.27\n",
      "epis: 309   score: 1.0   mem len: 55086   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.28\n",
      "epis: 310   score: 1.0   mem len: 55255   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.25\n",
      "epis: 311   score: 1.0   mem len: 55405   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.23\n",
      "epis: 312   score: 4.0   mem len: 55663   epsilon: 1.0    steps: 258    lr: 0.0001     reward: 1.26\n",
      "epis: 313   score: 1.0   mem len: 55814   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.25\n",
      "epis: 314   score: 1.0   mem len: 55984   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.24\n",
      "epis: 315   score: 2.0   mem len: 56181   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.26\n",
      "epis: 316   score: 5.0   mem len: 56508   epsilon: 1.0    steps: 327    lr: 0.0001     reward: 1.28\n",
      "epis: 317   score: 2.0   mem len: 56724   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.29\n",
      "epis: 318   score: 3.0   mem len: 56971   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.32\n",
      "epis: 319   score: 2.0   mem len: 57169   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.32\n",
      "epis: 320   score: 2.0   mem len: 57367   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 321   score: 4.0   mem len: 57644   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.37\n",
      "epis: 322   score: 2.0   mem len: 57861   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.38\n",
      "epis: 323   score: 1.0   mem len: 58030   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 324   score: 1.0   mem len: 58199   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 325   score: 0.0   mem len: 58322   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 326   score: 3.0   mem len: 58568   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.38\n",
      "epis: 327   score: 2.0   mem len: 58765   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.4\n",
      "epis: 328   score: 2.0   mem len: 58983   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.4\n",
      "epis: 329   score: 1.0   mem len: 59152   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 330   score: 2.0   mem len: 59350   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 331   score: 2.0   mem len: 59568   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 332   score: 2.0   mem len: 59786   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.39\n",
      "epis: 333   score: 3.0   mem len: 60050   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.42\n",
      "epis: 334   score: 3.0   mem len: 60297   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.43\n",
      "epis: 335   score: 0.0   mem len: 60419   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 336   score: 1.0   mem len: 60572   epsilon: 1.0    steps: 153    lr: 0.0001     reward: 1.43\n",
      "epis: 337   score: 2.0   mem len: 60792   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.44\n",
      "epis: 338   score: 0.0   mem len: 60915   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 339   score: 0.0   mem len: 61037   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 340   score: 2.0   mem len: 61235   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 341   score: 1.0   mem len: 61406   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.46\n",
      "epis: 342   score: 1.0   mem len: 61556   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 343   score: 1.0   mem len: 61725   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 344   score: 0.0   mem len: 61848   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 345   score: 0.0   mem len: 61971   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 346   score: 0.0   mem len: 62094   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 347   score: 0.0   mem len: 62217   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 348   score: 4.0   mem len: 62491   epsilon: 1.0    steps: 274    lr: 0.0001     reward: 1.41\n",
      "epis: 349   score: 0.0   mem len: 62614   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 350   score: 2.0   mem len: 62812   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.42\n",
      "epis: 351   score: 2.0   mem len: 63010   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.44\n",
      "epis: 352   score: 2.0   mem len: 63207   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 353   score: 3.0   mem len: 63452   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.47\n",
      "epis: 354   score: 1.0   mem len: 63603   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 355   score: 3.0   mem len: 63853   epsilon: 1.0    steps: 250    lr: 0.0001     reward: 1.49\n",
      "epis: 356   score: 2.0   mem len: 64073   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.5\n",
      "epis: 357   score: 2.0   mem len: 64271   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 358   score: 0.0   mem len: 64394   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 359   score: 0.0   mem len: 64517   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 360   score: 1.0   mem len: 64689   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.51\n",
      "epis: 361   score: 0.0   mem len: 64812   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 362   score: 2.0   mem len: 65029   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.52\n",
      "epis: 363   score: 0.0   mem len: 65152   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 364   score: 1.0   mem len: 65302   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.48\n",
      "epis: 365   score: 1.0   mem len: 65452   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.48\n",
      "epis: 366   score: 1.0   mem len: 65621   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 367   score: 2.0   mem len: 65819   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 368   score: 3.0   mem len: 66062   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.47\n",
      "epis: 369   score: 3.0   mem len: 66293   epsilon: 1.0    steps: 231    lr: 0.0001     reward: 1.48\n",
      "epis: 370   score: 0.0   mem len: 66416   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 371   score: 4.0   mem len: 66711   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.49\n",
      "epis: 372   score: 3.0   mem len: 66937   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.52\n",
      "epis: 373   score: 1.0   mem len: 67088   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 374   score: 4.0   mem len: 67383   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.57\n",
      "epis: 375   score: 1.0   mem len: 67534   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 376   score: 1.0   mem len: 67705   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.56\n",
      "epis: 377   score: 3.0   mem len: 67973   epsilon: 1.0    steps: 268    lr: 0.0001     reward: 1.59\n",
      "epis: 378   score: 2.0   mem len: 68154   epsilon: 1.0    steps: 181    lr: 0.0001     reward: 1.61\n",
      "epis: 379   score: 0.0   mem len: 68276   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 380   score: 1.0   mem len: 68445   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 381   score: 2.0   mem len: 68643   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.61\n",
      "epis: 382   score: 1.0   mem len: 68795   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.59\n",
      "epis: 383   score: 1.0   mem len: 68946   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 384   score: 1.0   mem len: 69118   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 385   score: 1.0   mem len: 69269   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 386   score: 3.0   mem len: 69514   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.6\n",
      "epis: 387   score: 3.0   mem len: 69743   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.62\n",
      "epis: 388   score: 0.0   mem len: 69866   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 389   score: 1.0   mem len: 70036   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.61\n",
      "epis: 390   score: 0.0   mem len: 70159   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 391   score: 0.0   mem len: 70282   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 392   score: 1.0   mem len: 70432   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.53\n",
      "epis: 393   score: 1.0   mem len: 70601   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 394   score: 1.0   mem len: 70751   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.55\n",
      "epis: 395   score: 2.0   mem len: 70969   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.55\n",
      "epis: 396   score: 9.0   mem len: 71465   epsilon: 1.0    steps: 496    lr: 0.0001     reward: 1.64\n",
      "epis: 397   score: 3.0   mem len: 71709   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.64\n",
      "epis: 398   score: 3.0   mem len: 71935   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.67\n",
      "epis: 399   score: 0.0   mem len: 72058   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 400   score: 2.0   mem len: 72280   epsilon: 1.0    steps: 222    lr: 0.0001     reward: 1.68\n",
      "epis: 401   score: 0.0   mem len: 72403   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 402   score: 2.0   mem len: 72600   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.68\n",
      "epis: 403   score: 3.0   mem len: 72847   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.71\n",
      "epis: 404   score: 3.0   mem len: 73093   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.73\n",
      "epis: 405   score: 0.0   mem len: 73215   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.69\n",
      "epis: 406   score: 4.0   mem len: 73495   epsilon: 1.0    steps: 280    lr: 0.0001     reward: 1.68\n",
      "epis: 407   score: 0.0   mem len: 73617   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.65\n",
      "epis: 408   score: 4.0   mem len: 73893   epsilon: 1.0    steps: 276    lr: 0.0001     reward: 1.67\n",
      "epis: 409   score: 2.0   mem len: 74094   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.68\n",
      "epis: 410   score: 1.0   mem len: 74264   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.68\n",
      "epis: 411   score: 4.0   mem len: 74563   epsilon: 1.0    steps: 299    lr: 0.0001     reward: 1.71\n",
      "epis: 412   score: 1.0   mem len: 74713   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 413   score: 2.0   mem len: 74915   epsilon: 1.0    steps: 202    lr: 0.0001     reward: 1.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 414   score: 2.0   mem len: 75112   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 415   score: 1.0   mem len: 75281   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.69\n",
      "epis: 416   score: 2.0   mem len: 75479   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.66\n",
      "epis: 417   score: 3.0   mem len: 75728   epsilon: 1.0    steps: 249    lr: 0.0001     reward: 1.67\n",
      "epis: 418   score: 0.0   mem len: 75850   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 419   score: 0.0   mem len: 75973   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 420   score: 2.0   mem len: 76171   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 421   score: 0.0   mem len: 76294   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 422   score: 0.0   mem len: 76416   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 423   score: 3.0   mem len: 76642   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.58\n",
      "epis: 424   score: 1.0   mem len: 76793   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 425   score: 0.0   mem len: 76916   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 426   score: 1.0   mem len: 77086   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.56\n",
      "epis: 427   score: 5.0   mem len: 77424   epsilon: 1.0    steps: 338    lr: 0.0001     reward: 1.59\n",
      "epis: 428   score: 3.0   mem len: 77689   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.6\n",
      "epis: 429   score: 3.0   mem len: 77934   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.62\n",
      "epis: 430   score: 0.0   mem len: 78057   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 431   score: 2.0   mem len: 78273   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.6\n",
      "epis: 432   score: 1.0   mem len: 78424   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 433   score: 2.0   mem len: 78643   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.58\n",
      "epis: 434   score: 0.0   mem len: 78765   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 435   score: 1.0   mem len: 78934   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 436   score: 3.0   mem len: 79182   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.58\n",
      "epis: 437   score: 4.0   mem len: 79469   epsilon: 1.0    steps: 287    lr: 0.0001     reward: 1.6\n",
      "epis: 438   score: 0.0   mem len: 79592   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 439   score: 2.0   mem len: 79790   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 440   score: 0.0   mem len: 79912   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 441   score: 3.0   mem len: 80141   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.62\n",
      "epis: 442   score: 2.0   mem len: 80359   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.63\n",
      "epis: 443   score: 1.0   mem len: 80531   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.63\n",
      "epis: 444   score: 0.0   mem len: 80653   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 445   score: 0.0   mem len: 80776   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 446   score: 0.0   mem len: 80899   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 447   score: 2.0   mem len: 81117   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.65\n",
      "epis: 448   score: 2.0   mem len: 81334   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.63\n",
      "epis: 449   score: 1.0   mem len: 81504   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.64\n",
      "epis: 450   score: 2.0   mem len: 81724   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.64\n",
      "epis: 451   score: 0.0   mem len: 81847   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 452   score: 0.0   mem len: 81970   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 453   score: 2.0   mem len: 82187   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.59\n",
      "epis: 454   score: 1.0   mem len: 82358   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.59\n",
      "epis: 455   score: 1.0   mem len: 82508   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.57\n",
      "epis: 456   score: 2.0   mem len: 82729   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.57\n",
      "epis: 457   score: 3.0   mem len: 82973   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.58\n",
      "epis: 458   score: 0.0   mem len: 83096   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 459   score: 3.0   mem len: 83358   epsilon: 1.0    steps: 262    lr: 0.0001     reward: 1.61\n",
      "epis: 460   score: 3.0   mem len: 83602   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.63\n",
      "epis: 461   score: 1.0   mem len: 83772   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.64\n",
      "epis: 462   score: 2.0   mem len: 83988   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.64\n",
      "epis: 463   score: 0.0   mem len: 84111   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 464   score: 5.0   mem len: 84413   epsilon: 1.0    steps: 302    lr: 0.0001     reward: 1.68\n",
      "epis: 465   score: 0.0   mem len: 84536   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 466   score: 1.0   mem len: 84706   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.67\n",
      "epis: 467   score: 2.0   mem len: 84888   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.67\n",
      "epis: 468   score: 0.0   mem len: 85010   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 469   score: 3.0   mem len: 85276   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.64\n",
      "epis: 470   score: 1.0   mem len: 85447   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.65\n",
      "epis: 471   score: 3.0   mem len: 85673   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.64\n",
      "epis: 472   score: 1.0   mem len: 85844   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.62\n",
      "epis: 473   score: 1.0   mem len: 86012   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.62\n",
      "epis: 474   score: 0.0   mem len: 86135   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 475   score: 2.0   mem len: 86333   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 476   score: 1.0   mem len: 86502   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 477   score: 1.0   mem len: 86673   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.57\n",
      "epis: 478   score: 2.0   mem len: 86871   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.57\n",
      "epis: 479   score: 4.0   mem len: 87132   epsilon: 1.0    steps: 261    lr: 0.0001     reward: 1.61\n",
      "epis: 480   score: 0.0   mem len: 87255   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 481   score: 2.0   mem len: 87452   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.6\n",
      "epis: 482   score: 0.0   mem len: 87575   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 483   score: 2.0   mem len: 87793   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.6\n",
      "epis: 484   score: 6.0   mem len: 88153   epsilon: 1.0    steps: 360    lr: 0.0001     reward: 1.65\n",
      "epis: 485   score: 3.0   mem len: 88417   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.67\n",
      "epis: 486   score: 0.0   mem len: 88540   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 487   score: 3.0   mem len: 88807   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.64\n",
      "epis: 488   score: 3.0   mem len: 89016   epsilon: 1.0    steps: 209    lr: 0.0001     reward: 1.67\n",
      "epis: 489   score: 0.0   mem len: 89139   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 490   score: 0.0   mem len: 89262   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 491   score: 1.0   mem len: 89432   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.67\n",
      "epis: 492   score: 2.0   mem len: 89630   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.68\n",
      "epis: 493   score: 1.0   mem len: 89800   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.68\n",
      "epis: 494   score: 1.0   mem len: 89951   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.68\n",
      "epis: 495   score: 4.0   mem len: 90224   epsilon: 1.0    steps: 273    lr: 0.0001     reward: 1.7\n",
      "epis: 496   score: 2.0   mem len: 90408   epsilon: 1.0    steps: 184    lr: 0.0001     reward: 1.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 497   score: 3.0   mem len: 90654   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.63\n",
      "epis: 498   score: 1.0   mem len: 90806   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.61\n",
      "epis: 499   score: 2.0   mem len: 91004   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.63\n",
      "epis: 500   score: 5.0   mem len: 91311   epsilon: 1.0    steps: 307    lr: 0.0001     reward: 1.66\n",
      "epis: 501   score: 0.0   mem len: 91434   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 502   score: 2.0   mem len: 91653   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.66\n",
      "epis: 503   score: 3.0   mem len: 91878   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.66\n",
      "epis: 504   score: 1.0   mem len: 92046   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.64\n",
      "epis: 505   score: 2.0   mem len: 92244   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.66\n",
      "epis: 506   score: 0.0   mem len: 92367   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 507   score: 0.0   mem len: 92489   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 508   score: 2.0   mem len: 92687   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 509   score: 5.0   mem len: 93030   epsilon: 1.0    steps: 343    lr: 0.0001     reward: 1.63\n",
      "epis: 510   score: 2.0   mem len: 93228   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.64\n",
      "epis: 511   score: 1.0   mem len: 93397   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 512   score: 3.0   mem len: 93644   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.63\n",
      "epis: 513   score: 9.0   mem len: 94031   epsilon: 1.0    steps: 387    lr: 0.0001     reward: 1.7\n",
      "epis: 514   score: 3.0   mem len: 94279   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.71\n",
      "epis: 515   score: 1.0   mem len: 94448   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.71\n",
      "epis: 516   score: 2.0   mem len: 94646   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 517   score: 1.0   mem len: 94818   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.69\n",
      "epis: 518   score: 1.0   mem len: 94988   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.7\n",
      "epis: 519   score: 4.0   mem len: 95282   epsilon: 1.0    steps: 294    lr: 0.0001     reward: 1.74\n",
      "epis: 520   score: 0.0   mem len: 95404   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.72\n",
      "epis: 521   score: 3.0   mem len: 95669   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.75\n",
      "epis: 522   score: 0.0   mem len: 95791   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.75\n",
      "epis: 523   score: 0.0   mem len: 95913   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.72\n",
      "epis: 524   score: 0.0   mem len: 96035   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.71\n",
      "epis: 525   score: 6.0   mem len: 96358   epsilon: 1.0    steps: 323    lr: 0.0001     reward: 1.77\n",
      "epis: 526   score: 4.0   mem len: 96655   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.8\n",
      "epis: 527   score: 3.0   mem len: 96882   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.78\n",
      "epis: 528   score: 5.0   mem len: 97197   epsilon: 1.0    steps: 315    lr: 0.0001     reward: 1.8\n",
      "epis: 529   score: 3.0   mem len: 97468   epsilon: 1.0    steps: 271    lr: 0.0001     reward: 1.8\n",
      "epis: 530   score: 1.0   mem len: 97639   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.81\n",
      "epis: 531   score: 3.0   mem len: 97865   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.82\n",
      "epis: 532   score: 2.0   mem len: 98065   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.83\n",
      "epis: 533   score: 1.0   mem len: 98235   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.82\n",
      "epis: 534   score: 0.0   mem len: 98358   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 535   score: 2.0   mem len: 98573   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.83\n",
      "epis: 536   score: 1.0   mem len: 98744   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.81\n",
      "epis: 537   score: 1.0   mem len: 98894   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.78\n",
      "epis: 538   score: 5.0   mem len: 99218   epsilon: 1.0    steps: 324    lr: 0.0001     reward: 1.83\n",
      "epis: 539   score: 0.0   mem len: 99341   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.81\n",
      "epis: 540   score: 0.0   mem len: 99464   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.81\n",
      "epis: 541   score: 1.0   mem len: 99615   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.79\n",
      "epis: 542   score: 2.0   mem len: 99833   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/memory.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array(sample)\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent.py:91: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  next_state_values[mask] = self.policy_net(non_final_next_states).max(1)[0].cuda()[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 543   score: 2.0   mem len: 100031   epsilon: 0.9999    steps: 198    lr: 0.0001     reward: 1.8\n",
      "epis: 544   score: 1.0   mem len: 100202   epsilon: 0.9996    steps: 171    lr: 0.0001     reward: 1.81\n",
      "epis: 545   score: 0.0   mem len: 100325   epsilon: 0.9994    steps: 123    lr: 0.0001     reward: 1.81\n",
      "epis: 546   score: 1.0   mem len: 100475   epsilon: 0.9991    steps: 150    lr: 0.0001     reward: 1.82\n",
      "epis: 547   score: 1.0   mem len: 100625   epsilon: 0.9988    steps: 150    lr: 0.0001     reward: 1.81\n",
      "epis: 548   score: 2.0   mem len: 100824   epsilon: 0.9984    steps: 199    lr: 0.0001     reward: 1.81\n",
      "epis: 549   score: 0.0   mem len: 100947   epsilon: 0.9981    steps: 123    lr: 0.0001     reward: 1.8\n",
      "epis: 550   score: 1.0   mem len: 101116   epsilon: 0.9978    steps: 169    lr: 0.0001     reward: 1.79\n",
      "epis: 551   score: 1.0   mem len: 101266   epsilon: 0.9975    steps: 150    lr: 0.0001     reward: 1.8\n",
      "epis: 552   score: 4.0   mem len: 101564   epsilon: 0.9969    steps: 298    lr: 0.0001     reward: 1.84\n",
      "epis: 553   score: 2.0   mem len: 101762   epsilon: 0.9965    steps: 198    lr: 0.0001     reward: 1.84\n",
      "epis: 554   score: 0.0   mem len: 101885   epsilon: 0.9963    steps: 123    lr: 0.0001     reward: 1.83\n",
      "epis: 555   score: 0.0   mem len: 102008   epsilon: 0.996    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 556   score: 5.0   mem len: 102334   epsilon: 0.9954    steps: 326    lr: 0.0001     reward: 1.85\n",
      "epis: 557   score: 2.0   mem len: 102532   epsilon: 0.995    steps: 198    lr: 0.0001     reward: 1.84\n",
      "epis: 558   score: 0.0   mem len: 102654   epsilon: 0.9947    steps: 122    lr: 0.0001     reward: 1.84\n",
      "epis: 559   score: 2.0   mem len: 102852   epsilon: 0.9944    steps: 198    lr: 0.0001     reward: 1.83\n",
      "epis: 560   score: 2.0   mem len: 103073   epsilon: 0.9939    steps: 221    lr: 0.0001     reward: 1.82\n",
      "epis: 561   score: 1.0   mem len: 103242   epsilon: 0.9936    steps: 169    lr: 0.0001     reward: 1.82\n",
      "epis: 562   score: 6.0   mem len: 103599   epsilon: 0.9929    steps: 357    lr: 0.0001     reward: 1.86\n",
      "epis: 563   score: 1.0   mem len: 103767   epsilon: 0.9925    steps: 168    lr: 0.0001     reward: 1.87\n",
      "epis: 564   score: 0.0   mem len: 103890   epsilon: 0.9923    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 565   score: 3.0   mem len: 104134   epsilon: 0.9918    steps: 244    lr: 0.0001     reward: 1.85\n",
      "epis: 566   score: 2.0   mem len: 104353   epsilon: 0.9914    steps: 219    lr: 0.0001     reward: 1.86\n",
      "epis: 567   score: 2.0   mem len: 104551   epsilon: 0.991    steps: 198    lr: 0.0001     reward: 1.86\n",
      "epis: 568   score: 0.0   mem len: 104674   epsilon: 0.9907    steps: 123    lr: 0.0001     reward: 1.86\n",
      "epis: 569   score: 1.0   mem len: 104824   epsilon: 0.9904    steps: 150    lr: 0.0001     reward: 1.84\n",
      "epis: 570   score: 0.0   mem len: 104946   epsilon: 0.9902    steps: 122    lr: 0.0001     reward: 1.83\n",
      "epis: 571   score: 3.0   mem len: 105193   epsilon: 0.9897    steps: 247    lr: 0.0001     reward: 1.83\n",
      "epis: 572   score: 0.0   mem len: 105316   epsilon: 0.9895    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 573   score: 1.0   mem len: 105486   epsilon: 0.9891    steps: 170    lr: 0.0001     reward: 1.82\n",
      "epis: 574   score: 0.0   mem len: 105608   epsilon: 0.9889    steps: 122    lr: 0.0001     reward: 1.82\n",
      "epis: 575   score: 4.0   mem len: 105889   epsilon: 0.9883    steps: 281    lr: 0.0001     reward: 1.84\n",
      "epis: 576   score: 3.0   mem len: 106138   epsilon: 0.9878    steps: 249    lr: 0.0001     reward: 1.86\n",
      "epis: 577   score: 3.0   mem len: 106366   epsilon: 0.9874    steps: 228    lr: 0.0001     reward: 1.88\n",
      "epis: 578   score: 1.0   mem len: 106517   epsilon: 0.9871    steps: 151    lr: 0.0001     reward: 1.87\n",
      "epis: 579   score: 1.0   mem len: 106668   epsilon: 0.9868    steps: 151    lr: 0.0001     reward: 1.84\n",
      "epis: 580   score: 2.0   mem len: 106850   epsilon: 0.9864    steps: 182    lr: 0.0001     reward: 1.86\n",
      "epis: 581   score: 1.0   mem len: 107021   epsilon: 0.9861    steps: 171    lr: 0.0001     reward: 1.85\n",
      "epis: 582   score: 1.0   mem len: 107172   epsilon: 0.9858    steps: 151    lr: 0.0001     reward: 1.86\n",
      "epis: 583   score: 0.0   mem len: 107295   epsilon: 0.9856    steps: 123    lr: 0.0001     reward: 1.84\n",
      "epis: 584   score: 2.0   mem len: 107493   epsilon: 0.9852    steps: 198    lr: 0.0001     reward: 1.8\n",
      "epis: 585   score: 3.0   mem len: 107718   epsilon: 0.9847    steps: 225    lr: 0.0001     reward: 1.8\n",
      "epis: 586   score: 0.0   mem len: 107840   epsilon: 0.9845    steps: 122    lr: 0.0001     reward: 1.8\n",
      "epis: 587   score: 3.0   mem len: 108105   epsilon: 0.984    steps: 265    lr: 0.0001     reward: 1.8\n",
      "epis: 588   score: 1.0   mem len: 108256   epsilon: 0.9837    steps: 151    lr: 0.0001     reward: 1.78\n",
      "epis: 589   score: 1.0   mem len: 108406   epsilon: 0.9834    steps: 150    lr: 0.0001     reward: 1.79\n",
      "epis: 590   score: 2.0   mem len: 108603   epsilon: 0.983    steps: 197    lr: 0.0001     reward: 1.81\n",
      "epis: 591   score: 2.0   mem len: 108801   epsilon: 0.9826    steps: 198    lr: 0.0001     reward: 1.82\n",
      "epis: 592   score: 1.0   mem len: 108953   epsilon: 0.9823    steps: 152    lr: 0.0001     reward: 1.81\n",
      "epis: 593   score: 3.0   mem len: 109202   epsilon: 0.9818    steps: 249    lr: 0.0001     reward: 1.83\n",
      "epis: 594   score: 0.0   mem len: 109325   epsilon: 0.9815    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 595   score: 1.0   mem len: 109476   epsilon: 0.9812    steps: 151    lr: 0.0001     reward: 1.79\n",
      "epis: 596   score: 1.0   mem len: 109645   epsilon: 0.9809    steps: 169    lr: 0.0001     reward: 1.78\n",
      "epis: 597   score: 1.0   mem len: 109814   epsilon: 0.9806    steps: 169    lr: 0.0001     reward: 1.76\n",
      "epis: 598   score: 2.0   mem len: 110012   epsilon: 0.9802    steps: 198    lr: 0.0001     reward: 1.77\n",
      "epis: 599   score: 1.0   mem len: 110184   epsilon: 0.9798    steps: 172    lr: 0.0001     reward: 1.76\n",
      "epis: 600   score: 2.0   mem len: 110382   epsilon: 0.9794    steps: 198    lr: 0.0001     reward: 1.73\n",
      "epis: 601   score: 4.0   mem len: 110645   epsilon: 0.9789    steps: 263    lr: 0.0001     reward: 1.77\n",
      "epis: 602   score: 0.0   mem len: 110767   epsilon: 0.9787    steps: 122    lr: 0.0001     reward: 1.75\n",
      "epis: 603   score: 1.0   mem len: 110936   epsilon: 0.9783    steps: 169    lr: 0.0001     reward: 1.73\n",
      "epis: 604   score: 3.0   mem len: 111164   epsilon: 0.9779    steps: 228    lr: 0.0001     reward: 1.75\n",
      "epis: 605   score: 0.0   mem len: 111286   epsilon: 0.9777    steps: 122    lr: 0.0001     reward: 1.73\n",
      "epis: 606   score: 1.0   mem len: 111457   epsilon: 0.9773    steps: 171    lr: 0.0001     reward: 1.74\n",
      "epis: 607   score: 1.0   mem len: 111628   epsilon: 0.977    steps: 171    lr: 0.0001     reward: 1.75\n",
      "epis: 608   score: 4.0   mem len: 111923   epsilon: 0.9764    steps: 295    lr: 0.0001     reward: 1.77\n",
      "epis: 609   score: 2.0   mem len: 112139   epsilon: 0.976    steps: 216    lr: 0.0001     reward: 1.74\n",
      "epis: 610   score: 1.0   mem len: 112308   epsilon: 0.9756    steps: 169    lr: 0.0001     reward: 1.73\n",
      "epis: 611   score: 1.0   mem len: 112479   epsilon: 0.9753    steps: 171    lr: 0.0001     reward: 1.73\n",
      "epis: 612   score: 5.0   mem len: 112807   epsilon: 0.9746    steps: 328    lr: 0.0001     reward: 1.75\n",
      "epis: 613   score: 1.0   mem len: 112979   epsilon: 0.9743    steps: 172    lr: 0.0001     reward: 1.67\n",
      "epis: 614   score: 3.0   mem len: 113251   epsilon: 0.9738    steps: 272    lr: 0.0001     reward: 1.67\n",
      "epis: 615   score: 2.0   mem len: 113451   epsilon: 0.9734    steps: 200    lr: 0.0001     reward: 1.68\n",
      "epis: 616   score: 2.0   mem len: 113667   epsilon: 0.9729    steps: 216    lr: 0.0001     reward: 1.68\n",
      "epis: 617   score: 1.0   mem len: 113835   epsilon: 0.9726    steps: 168    lr: 0.0001     reward: 1.68\n",
      "epis: 618   score: 0.0   mem len: 113958   epsilon: 0.9724    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 619   score: 0.0   mem len: 114080   epsilon: 0.9721    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 620   score: 2.0   mem len: 114298   epsilon: 0.9717    steps: 218    lr: 0.0001     reward: 1.65\n",
      "epis: 621   score: 3.0   mem len: 114545   epsilon: 0.9712    steps: 247    lr: 0.0001     reward: 1.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 622   score: 0.0   mem len: 114667   epsilon: 0.971    steps: 122    lr: 0.0001     reward: 1.65\n",
      "epis: 623   score: 1.0   mem len: 114818   epsilon: 0.9707    steps: 151    lr: 0.0001     reward: 1.66\n",
      "epis: 624   score: 4.0   mem len: 115112   epsilon: 0.9701    steps: 294    lr: 0.0001     reward: 1.7\n",
      "epis: 625   score: 2.0   mem len: 115294   epsilon: 0.9697    steps: 182    lr: 0.0001     reward: 1.66\n",
      "epis: 626   score: 2.0   mem len: 115476   epsilon: 0.9694    steps: 182    lr: 0.0001     reward: 1.64\n",
      "epis: 627   score: 0.0   mem len: 115599   epsilon: 0.9691    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 628   score: 2.0   mem len: 115799   epsilon: 0.9687    steps: 200    lr: 0.0001     reward: 1.58\n",
      "epis: 629   score: 2.0   mem len: 115979   epsilon: 0.9684    steps: 180    lr: 0.0001     reward: 1.57\n",
      "epis: 630   score: 1.0   mem len: 116151   epsilon: 0.968    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 631   score: 0.0   mem len: 116273   epsilon: 0.9678    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 632   score: 1.0   mem len: 116443   epsilon: 0.9674    steps: 170    lr: 0.0001     reward: 1.53\n",
      "epis: 633   score: 5.0   mem len: 116733   epsilon: 0.9669    steps: 290    lr: 0.0001     reward: 1.57\n",
      "epis: 634   score: 0.0   mem len: 116856   epsilon: 0.9666    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 635   score: 2.0   mem len: 117053   epsilon: 0.9662    steps: 197    lr: 0.0001     reward: 1.57\n",
      "epis: 636   score: 3.0   mem len: 117282   epsilon: 0.9658    steps: 229    lr: 0.0001     reward: 1.59\n",
      "epis: 637   score: 3.0   mem len: 117507   epsilon: 0.9653    steps: 225    lr: 0.0001     reward: 1.61\n",
      "epis: 638   score: 7.0   mem len: 117917   epsilon: 0.9645    steps: 410    lr: 0.0001     reward: 1.63\n",
      "epis: 639   score: 2.0   mem len: 118133   epsilon: 0.9641    steps: 216    lr: 0.0001     reward: 1.65\n",
      "epis: 640   score: 1.0   mem len: 118302   epsilon: 0.9638    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 641   score: 2.0   mem len: 118500   epsilon: 0.9634    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 642   score: 4.0   mem len: 118795   epsilon: 0.9628    steps: 295    lr: 0.0001     reward: 1.69\n",
      "epis: 643   score: 2.0   mem len: 118993   epsilon: 0.9624    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 644   score: 1.0   mem len: 119144   epsilon: 0.9621    steps: 151    lr: 0.0001     reward: 1.69\n",
      "epis: 645   score: 0.0   mem len: 119267   epsilon: 0.9618    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 646   score: 0.0   mem len: 119390   epsilon: 0.9616    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 647   score: 1.0   mem len: 119558   epsilon: 0.9613    steps: 168    lr: 0.0001     reward: 1.68\n",
      "epis: 648   score: 0.0   mem len: 119680   epsilon: 0.961    steps: 122    lr: 0.0001     reward: 1.66\n",
      "epis: 649   score: 0.0   mem len: 119803   epsilon: 0.9608    steps: 123    lr: 0.0001     reward: 1.66\n",
      "epis: 650   score: 1.0   mem len: 119972   epsilon: 0.9605    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 651   score: 1.0   mem len: 120141   epsilon: 0.9601    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 652   score: 1.0   mem len: 120311   epsilon: 0.9598    steps: 170    lr: 0.0001     reward: 1.63\n",
      "epis: 653   score: 1.0   mem len: 120462   epsilon: 0.9595    steps: 151    lr: 0.0001     reward: 1.62\n",
      "epis: 654   score: 4.0   mem len: 120759   epsilon: 0.9589    steps: 297    lr: 0.0001     reward: 1.66\n",
      "epis: 655   score: 1.0   mem len: 120930   epsilon: 0.9586    steps: 171    lr: 0.0001     reward: 1.67\n",
      "epis: 656   score: 6.0   mem len: 121270   epsilon: 0.9579    steps: 340    lr: 0.0001     reward: 1.68\n",
      "epis: 657   score: 1.0   mem len: 121439   epsilon: 0.9575    steps: 169    lr: 0.0001     reward: 1.67\n",
      "epis: 658   score: 1.0   mem len: 121589   epsilon: 0.9573    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 659   score: 1.0   mem len: 121758   epsilon: 0.9569    steps: 169    lr: 0.0001     reward: 1.67\n",
      "epis: 660   score: 2.0   mem len: 121956   epsilon: 0.9565    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 661   score: 1.0   mem len: 122126   epsilon: 0.9562    steps: 170    lr: 0.0001     reward: 1.67\n",
      "epis: 662   score: 2.0   mem len: 122344   epsilon: 0.9558    steps: 218    lr: 0.0001     reward: 1.63\n",
      "epis: 663   score: 0.0   mem len: 122466   epsilon: 0.9555    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 664   score: 0.0   mem len: 122589   epsilon: 0.9553    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 665   score: 0.0   mem len: 122712   epsilon: 0.955    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 666   score: 1.0   mem len: 122881   epsilon: 0.9547    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 667   score: 2.0   mem len: 123082   epsilon: 0.9543    steps: 201    lr: 0.0001     reward: 1.58\n",
      "epis: 668   score: 1.0   mem len: 123251   epsilon: 0.954    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 669   score: 4.0   mem len: 123547   epsilon: 0.9534    steps: 296    lr: 0.0001     reward: 1.62\n",
      "epis: 670   score: 0.0   mem len: 123670   epsilon: 0.9531    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 671   score: 0.0   mem len: 123793   epsilon: 0.9529    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 672   score: 0.0   mem len: 123916   epsilon: 0.9526    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 673   score: 2.0   mem len: 124134   epsilon: 0.9522    steps: 218    lr: 0.0001     reward: 1.6\n",
      "epis: 674   score: 2.0   mem len: 124352   epsilon: 0.9518    steps: 218    lr: 0.0001     reward: 1.62\n",
      "epis: 675   score: 2.0   mem len: 124550   epsilon: 0.9514    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 676   score: 2.0   mem len: 124768   epsilon: 0.951    steps: 218    lr: 0.0001     reward: 1.59\n",
      "epis: 677   score: 0.0   mem len: 124891   epsilon: 0.9507    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 678   score: 3.0   mem len: 125116   epsilon: 0.9503    steps: 225    lr: 0.0001     reward: 1.58\n",
      "epis: 679   score: 1.0   mem len: 125266   epsilon: 0.95    steps: 150    lr: 0.0001     reward: 1.58\n",
      "epis: 680   score: 2.0   mem len: 125464   epsilon: 0.9496    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 681   score: 0.0   mem len: 125586   epsilon: 0.9493    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 682   score: 1.0   mem len: 125756   epsilon: 0.949    steps: 170    lr: 0.0001     reward: 1.57\n",
      "epis: 683   score: 1.0   mem len: 125928   epsilon: 0.9487    steps: 172    lr: 0.0001     reward: 1.58\n",
      "epis: 684   score: 2.0   mem len: 126129   epsilon: 0.9483    steps: 201    lr: 0.0001     reward: 1.58\n",
      "epis: 685   score: 0.0   mem len: 126252   epsilon: 0.948    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 686   score: 1.0   mem len: 126420   epsilon: 0.9477    steps: 168    lr: 0.0001     reward: 1.56\n",
      "epis: 687   score: 3.0   mem len: 126686   epsilon: 0.9472    steps: 266    lr: 0.0001     reward: 1.56\n",
      "epis: 688   score: 1.0   mem len: 126837   epsilon: 0.9469    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 689   score: 0.0   mem len: 126959   epsilon: 0.9466    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 690   score: 2.0   mem len: 127156   epsilon: 0.9462    steps: 197    lr: 0.0001     reward: 1.55\n",
      "epis: 691   score: 2.0   mem len: 127356   epsilon: 0.9458    steps: 200    lr: 0.0001     reward: 1.55\n",
      "epis: 692   score: 1.0   mem len: 127507   epsilon: 0.9455    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 693   score: 0.0   mem len: 127629   epsilon: 0.9453    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 694   score: 2.0   mem len: 127828   epsilon: 0.9449    steps: 199    lr: 0.0001     reward: 1.54\n",
      "epis: 695   score: 1.0   mem len: 127997   epsilon: 0.9446    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 696   score: 1.0   mem len: 128148   epsilon: 0.9443    steps: 151    lr: 0.0001     reward: 1.54\n",
      "epis: 697   score: 0.0   mem len: 128270   epsilon: 0.944    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 698   score: 1.0   mem len: 128441   epsilon: 0.9437    steps: 171    lr: 0.0001     reward: 1.52\n",
      "epis: 699   score: 0.0   mem len: 128564   epsilon: 0.9434    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 700   score: 0.0   mem len: 128687   epsilon: 0.9432    steps: 123    lr: 0.0001     reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 701   score: 2.0   mem len: 128905   epsilon: 0.9428    steps: 218    lr: 0.0001     reward: 1.47\n",
      "epis: 702   score: 2.0   mem len: 129103   epsilon: 0.9424    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 703   score: 0.0   mem len: 129225   epsilon: 0.9421    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 704   score: 2.0   mem len: 129423   epsilon: 0.9417    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 705   score: 2.0   mem len: 129639   epsilon: 0.9413    steps: 216    lr: 0.0001     reward: 1.49\n",
      "epis: 706   score: 3.0   mem len: 129868   epsilon: 0.9409    steps: 229    lr: 0.0001     reward: 1.51\n",
      "epis: 707   score: 0.0   mem len: 129991   epsilon: 0.9406    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 708   score: 2.0   mem len: 130189   epsilon: 0.9402    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 709   score: 1.0   mem len: 130340   epsilon: 0.9399    steps: 151    lr: 0.0001     reward: 1.47\n",
      "epis: 710   score: 1.0   mem len: 130509   epsilon: 0.9396    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 711   score: 1.0   mem len: 130659   epsilon: 0.9393    steps: 150    lr: 0.0001     reward: 1.47\n",
      "epis: 712   score: 2.0   mem len: 130877   epsilon: 0.9389    steps: 218    lr: 0.0001     reward: 1.44\n",
      "epis: 713   score: 0.0   mem len: 131000   epsilon: 0.9386    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 714   score: 3.0   mem len: 131225   epsilon: 0.9382    steps: 225    lr: 0.0001     reward: 1.43\n",
      "epis: 715   score: 2.0   mem len: 131443   epsilon: 0.9377    steps: 218    lr: 0.0001     reward: 1.43\n",
      "epis: 716   score: 1.0   mem len: 131614   epsilon: 0.9374    steps: 171    lr: 0.0001     reward: 1.42\n",
      "epis: 717   score: 1.0   mem len: 131783   epsilon: 0.9371    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 718   score: 0.0   mem len: 131905   epsilon: 0.9368    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 719   score: 0.0   mem len: 132028   epsilon: 0.9366    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 720   score: 3.0   mem len: 132258   epsilon: 0.9361    steps: 230    lr: 0.0001     reward: 1.43\n",
      "epis: 721   score: 5.0   mem len: 132582   epsilon: 0.9355    steps: 324    lr: 0.0001     reward: 1.45\n",
      "epis: 722   score: 1.0   mem len: 132751   epsilon: 0.9352    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 723   score: 2.0   mem len: 132949   epsilon: 0.9348    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 724   score: 2.0   mem len: 133146   epsilon: 0.9344    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 725   score: 0.0   mem len: 133269   epsilon: 0.9341    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 726   score: 0.0   mem len: 133392   epsilon: 0.9339    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 727   score: 2.0   mem len: 133572   epsilon: 0.9335    steps: 180    lr: 0.0001     reward: 1.43\n",
      "epis: 728   score: 0.0   mem len: 133695   epsilon: 0.9333    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 729   score: 0.0   mem len: 133817   epsilon: 0.933    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 730   score: 1.0   mem len: 133986   epsilon: 0.9327    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 731   score: 5.0   mem len: 134277   epsilon: 0.9321    steps: 291    lr: 0.0001     reward: 1.44\n",
      "epis: 732   score: 2.0   mem len: 134463   epsilon: 0.9318    steps: 186    lr: 0.0001     reward: 1.45\n",
      "epis: 733   score: 0.0   mem len: 134586   epsilon: 0.9315    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 734   score: 0.0   mem len: 134709   epsilon: 0.9313    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 735   score: 1.0   mem len: 134878   epsilon: 0.9309    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 736   score: 1.0   mem len: 135049   epsilon: 0.9306    steps: 171    lr: 0.0001     reward: 1.37\n",
      "epis: 737   score: 0.0   mem len: 135171   epsilon: 0.9304    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 738   score: 2.0   mem len: 135369   epsilon: 0.93    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 739   score: 0.0   mem len: 135492   epsilon: 0.9297    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 740   score: 0.0   mem len: 135614   epsilon: 0.9295    steps: 122    lr: 0.0001     reward: 1.26\n",
      "epis: 741   score: 0.0   mem len: 135737   epsilon: 0.9292    steps: 123    lr: 0.0001     reward: 1.24\n",
      "epis: 742   score: 0.0   mem len: 135860   epsilon: 0.929    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 743   score: 2.0   mem len: 136077   epsilon: 0.9286    steps: 217    lr: 0.0001     reward: 1.2\n",
      "epis: 744   score: 1.0   mem len: 136248   epsilon: 0.9282    steps: 171    lr: 0.0001     reward: 1.2\n",
      "epis: 745   score: 0.0   mem len: 136371   epsilon: 0.928    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 746   score: 0.0   mem len: 136494   epsilon: 0.9277    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 747   score: 0.0   mem len: 136617   epsilon: 0.9275    steps: 123    lr: 0.0001     reward: 1.19\n",
      "epis: 748   score: 1.0   mem len: 136786   epsilon: 0.9272    steps: 169    lr: 0.0001     reward: 1.2\n",
      "epis: 749   score: 1.0   mem len: 136954   epsilon: 0.9268    steps: 168    lr: 0.0001     reward: 1.21\n",
      "epis: 750   score: 2.0   mem len: 137153   epsilon: 0.9264    steps: 199    lr: 0.0001     reward: 1.22\n",
      "epis: 751   score: 0.0   mem len: 137276   epsilon: 0.9262    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 752   score: 0.0   mem len: 137399   epsilon: 0.9259    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 753   score: 0.0   mem len: 137522   epsilon: 0.9257    steps: 123    lr: 0.0001     reward: 1.19\n",
      "epis: 754   score: 1.0   mem len: 137693   epsilon: 0.9254    steps: 171    lr: 0.0001     reward: 1.16\n",
      "epis: 755   score: 0.0   mem len: 137816   epsilon: 0.9251    steps: 123    lr: 0.0001     reward: 1.15\n",
      "epis: 756   score: 1.0   mem len: 137966   epsilon: 0.9248    steps: 150    lr: 0.0001     reward: 1.1\n",
      "epis: 757   score: 0.0   mem len: 138089   epsilon: 0.9246    steps: 123    lr: 0.0001     reward: 1.09\n",
      "epis: 758   score: 2.0   mem len: 138269   epsilon: 0.9242    steps: 180    lr: 0.0001     reward: 1.1\n",
      "epis: 759   score: 3.0   mem len: 138495   epsilon: 0.9238    steps: 226    lr: 0.0001     reward: 1.12\n",
      "epis: 760   score: 0.0   mem len: 138617   epsilon: 0.9235    steps: 122    lr: 0.0001     reward: 1.1\n",
      "epis: 761   score: 4.0   mem len: 138932   epsilon: 0.9229    steps: 315    lr: 0.0001     reward: 1.13\n",
      "epis: 762   score: 0.0   mem len: 139055   epsilon: 0.9227    steps: 123    lr: 0.0001     reward: 1.11\n",
      "epis: 763   score: 2.0   mem len: 139275   epsilon: 0.9222    steps: 220    lr: 0.0001     reward: 1.13\n",
      "epis: 764   score: 3.0   mem len: 139507   epsilon: 0.9218    steps: 232    lr: 0.0001     reward: 1.16\n",
      "epis: 765   score: 1.0   mem len: 139658   epsilon: 0.9215    steps: 151    lr: 0.0001     reward: 1.17\n",
      "epis: 766   score: 4.0   mem len: 139953   epsilon: 0.9209    steps: 295    lr: 0.0001     reward: 1.2\n",
      "epis: 767   score: 1.0   mem len: 140103   epsilon: 0.9206    steps: 150    lr: 0.0001     reward: 1.19\n",
      "epis: 768   score: 0.0   mem len: 140226   epsilon: 0.9204    steps: 123    lr: 0.0001     reward: 1.18\n",
      "epis: 769   score: 0.0   mem len: 140349   epsilon: 0.9201    steps: 123    lr: 0.0001     reward: 1.14\n",
      "epis: 770   score: 1.0   mem len: 140518   epsilon: 0.9198    steps: 169    lr: 0.0001     reward: 1.15\n",
      "epis: 771   score: 0.0   mem len: 140640   epsilon: 0.9195    steps: 122    lr: 0.0001     reward: 1.15\n",
      "epis: 772   score: 1.0   mem len: 140809   epsilon: 0.9192    steps: 169    lr: 0.0001     reward: 1.16\n",
      "epis: 773   score: 0.0   mem len: 140932   epsilon: 0.919    steps: 123    lr: 0.0001     reward: 1.14\n",
      "epis: 774   score: 0.0   mem len: 141055   epsilon: 0.9187    steps: 123    lr: 0.0001     reward: 1.12\n",
      "epis: 775   score: 0.0   mem len: 141177   epsilon: 0.9185    steps: 122    lr: 0.0001     reward: 1.1\n",
      "epis: 776   score: 2.0   mem len: 141375   epsilon: 0.9181    steps: 198    lr: 0.0001     reward: 1.1\n",
      "epis: 777   score: 1.0   mem len: 141546   epsilon: 0.9177    steps: 171    lr: 0.0001     reward: 1.11\n",
      "epis: 778   score: 2.0   mem len: 141763   epsilon: 0.9173    steps: 217    lr: 0.0001     reward: 1.1\n",
      "epis: 779   score: 1.0   mem len: 141932   epsilon: 0.917    steps: 169    lr: 0.0001     reward: 1.1\n",
      "epis: 780   score: 0.0   mem len: 142055   epsilon: 0.9167    steps: 123    lr: 0.0001     reward: 1.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 781   score: 1.0   mem len: 142224   epsilon: 0.9164    steps: 169    lr: 0.0001     reward: 1.09\n",
      "epis: 782   score: 0.0   mem len: 142346   epsilon: 0.9162    steps: 122    lr: 0.0001     reward: 1.08\n",
      "epis: 783   score: 0.0   mem len: 142469   epsilon: 0.9159    steps: 123    lr: 0.0001     reward: 1.07\n",
      "epis: 784   score: 0.0   mem len: 142592   epsilon: 0.9157    steps: 123    lr: 0.0001     reward: 1.05\n",
      "epis: 785   score: 4.0   mem len: 142889   epsilon: 0.9151    steps: 297    lr: 0.0001     reward: 1.09\n",
      "epis: 786   score: 0.0   mem len: 143012   epsilon: 0.9148    steps: 123    lr: 0.0001     reward: 1.08\n",
      "epis: 787   score: 2.0   mem len: 143210   epsilon: 0.9144    steps: 198    lr: 0.0001     reward: 1.07\n",
      "epis: 788   score: 2.0   mem len: 143407   epsilon: 0.9141    steps: 197    lr: 0.0001     reward: 1.08\n",
      "epis: 789   score: 0.0   mem len: 143530   epsilon: 0.9138    steps: 123    lr: 0.0001     reward: 1.08\n",
      "epis: 790   score: 2.0   mem len: 143746   epsilon: 0.9134    steps: 216    lr: 0.0001     reward: 1.08\n",
      "epis: 791   score: 2.0   mem len: 143962   epsilon: 0.913    steps: 216    lr: 0.0001     reward: 1.08\n",
      "epis: 792   score: 2.0   mem len: 144162   epsilon: 0.9126    steps: 200    lr: 0.0001     reward: 1.09\n",
      "epis: 793   score: 0.0   mem len: 144284   epsilon: 0.9123    steps: 122    lr: 0.0001     reward: 1.09\n",
      "epis: 794   score: 3.0   mem len: 144549   epsilon: 0.9118    steps: 265    lr: 0.0001     reward: 1.1\n",
      "epis: 795   score: 4.0   mem len: 144823   epsilon: 0.9112    steps: 274    lr: 0.0001     reward: 1.13\n",
      "epis: 796   score: 2.0   mem len: 145042   epsilon: 0.9108    steps: 219    lr: 0.0001     reward: 1.14\n",
      "epis: 797   score: 2.0   mem len: 145240   epsilon: 0.9104    steps: 198    lr: 0.0001     reward: 1.16\n",
      "epis: 798   score: 3.0   mem len: 145507   epsilon: 0.9099    steps: 267    lr: 0.0001     reward: 1.18\n",
      "epis: 799   score: 0.0   mem len: 145630   epsilon: 0.9097    steps: 123    lr: 0.0001     reward: 1.18\n",
      "epis: 800   score: 0.0   mem len: 145752   epsilon: 0.9094    steps: 122    lr: 0.0001     reward: 1.18\n",
      "epis: 801   score: 3.0   mem len: 145982   epsilon: 0.909    steps: 230    lr: 0.0001     reward: 1.19\n",
      "epis: 802   score: 5.0   mem len: 146306   epsilon: 0.9083    steps: 324    lr: 0.0001     reward: 1.22\n",
      "epis: 803   score: 0.0   mem len: 146429   epsilon: 0.9081    steps: 123    lr: 0.0001     reward: 1.22\n",
      "epis: 804   score: 4.0   mem len: 146738   epsilon: 0.9075    steps: 309    lr: 0.0001     reward: 1.24\n",
      "epis: 805   score: 2.0   mem len: 146935   epsilon: 0.9071    steps: 197    lr: 0.0001     reward: 1.24\n",
      "epis: 806   score: 2.0   mem len: 147133   epsilon: 0.9067    steps: 198    lr: 0.0001     reward: 1.23\n",
      "epis: 807   score: 0.0   mem len: 147256   epsilon: 0.9064    steps: 123    lr: 0.0001     reward: 1.23\n",
      "epis: 808   score: 0.0   mem len: 147379   epsilon: 0.9062    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 809   score: 1.0   mem len: 147530   epsilon: 0.9059    steps: 151    lr: 0.0001     reward: 1.21\n",
      "epis: 810   score: 2.0   mem len: 147729   epsilon: 0.9055    steps: 199    lr: 0.0001     reward: 1.22\n",
      "epis: 811   score: 2.0   mem len: 147927   epsilon: 0.9051    steps: 198    lr: 0.0001     reward: 1.23\n",
      "epis: 812   score: 0.0   mem len: 148050   epsilon: 0.9049    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 813   score: 0.0   mem len: 148172   epsilon: 0.9046    steps: 122    lr: 0.0001     reward: 1.21\n",
      "epis: 814   score: 5.0   mem len: 148485   epsilon: 0.904    steps: 313    lr: 0.0001     reward: 1.23\n",
      "epis: 815   score: 3.0   mem len: 148749   epsilon: 0.9035    steps: 264    lr: 0.0001     reward: 1.24\n",
      "epis: 816   score: 3.0   mem len: 148975   epsilon: 0.903    steps: 226    lr: 0.0001     reward: 1.26\n",
      "epis: 817   score: 0.0   mem len: 149098   epsilon: 0.9028    steps: 123    lr: 0.0001     reward: 1.25\n",
      "epis: 818   score: 0.0   mem len: 149221   epsilon: 0.9025    steps: 123    lr: 0.0001     reward: 1.25\n",
      "epis: 819   score: 4.0   mem len: 149488   epsilon: 0.902    steps: 267    lr: 0.0001     reward: 1.29\n",
      "epis: 820   score: 1.0   mem len: 149639   epsilon: 0.9017    steps: 151    lr: 0.0001     reward: 1.27\n",
      "epis: 821   score: 4.0   mem len: 149894   epsilon: 0.9012    steps: 255    lr: 0.0001     reward: 1.26\n",
      "epis: 822   score: 1.0   mem len: 150062   epsilon: 0.9009    steps: 168    lr: 0.0001     reward: 1.26\n",
      "epis: 823   score: 1.0   mem len: 150212   epsilon: 0.9006    steps: 150    lr: 0.0001     reward: 1.25\n",
      "epis: 824   score: 1.0   mem len: 150363   epsilon: 0.9003    steps: 151    lr: 0.0001     reward: 1.24\n",
      "epis: 825   score: 1.0   mem len: 150532   epsilon: 0.8999    steps: 169    lr: 0.0001     reward: 1.25\n",
      "epis: 826   score: 3.0   mem len: 150758   epsilon: 0.8995    steps: 226    lr: 0.0001     reward: 1.28\n",
      "epis: 827   score: 0.0   mem len: 150880   epsilon: 0.8993    steps: 122    lr: 0.0001     reward: 1.26\n",
      "epis: 828   score: 2.0   mem len: 151078   epsilon: 0.8989    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 829   score: 2.0   mem len: 151296   epsilon: 0.8984    steps: 218    lr: 0.0001     reward: 1.3\n",
      "epis: 830   score: 2.0   mem len: 151494   epsilon: 0.898    steps: 198    lr: 0.0001     reward: 1.31\n",
      "epis: 831   score: 2.0   mem len: 151692   epsilon: 0.8976    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 832   score: 0.0   mem len: 151814   epsilon: 0.8974    steps: 122    lr: 0.0001     reward: 1.26\n",
      "epis: 833   score: 2.0   mem len: 152032   epsilon: 0.897    steps: 218    lr: 0.0001     reward: 1.28\n",
      "epis: 834   score: 1.0   mem len: 152183   epsilon: 0.8967    steps: 151    lr: 0.0001     reward: 1.29\n",
      "epis: 835   score: 4.0   mem len: 152480   epsilon: 0.8961    steps: 297    lr: 0.0001     reward: 1.32\n",
      "epis: 836   score: 1.0   mem len: 152650   epsilon: 0.8958    steps: 170    lr: 0.0001     reward: 1.32\n",
      "epis: 837   score: 2.0   mem len: 152868   epsilon: 0.8953    steps: 218    lr: 0.0001     reward: 1.34\n",
      "epis: 838   score: 4.0   mem len: 153144   epsilon: 0.8948    steps: 276    lr: 0.0001     reward: 1.36\n",
      "epis: 839   score: 0.0   mem len: 153267   epsilon: 0.8945    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 840   score: 1.0   mem len: 153436   epsilon: 0.8942    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 841   score: 2.0   mem len: 153654   epsilon: 0.8938    steps: 218    lr: 0.0001     reward: 1.39\n",
      "epis: 842   score: 2.0   mem len: 153872   epsilon: 0.8933    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 843   score: 0.0   mem len: 153995   epsilon: 0.8931    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 844   score: 3.0   mem len: 154221   epsilon: 0.8926    steps: 226    lr: 0.0001     reward: 1.41\n",
      "epis: 845   score: 1.0   mem len: 154372   epsilon: 0.8923    steps: 151    lr: 0.0001     reward: 1.42\n",
      "epis: 846   score: 4.0   mem len: 154688   epsilon: 0.8917    steps: 316    lr: 0.0001     reward: 1.46\n",
      "epis: 847   score: 1.0   mem len: 154857   epsilon: 0.8914    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 848   score: 1.0   mem len: 155029   epsilon: 0.891    steps: 172    lr: 0.0001     reward: 1.47\n",
      "epis: 849   score: 1.0   mem len: 155198   epsilon: 0.8907    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 850   score: 0.0   mem len: 155320   epsilon: 0.8905    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 851   score: 3.0   mem len: 155567   epsilon: 0.89    steps: 247    lr: 0.0001     reward: 1.48\n",
      "epis: 852   score: 2.0   mem len: 155787   epsilon: 0.8895    steps: 220    lr: 0.0001     reward: 1.5\n",
      "epis: 853   score: 3.0   mem len: 156034   epsilon: 0.8891    steps: 247    lr: 0.0001     reward: 1.53\n",
      "epis: 854   score: 1.0   mem len: 156203   epsilon: 0.8887    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 855   score: 4.0   mem len: 156479   epsilon: 0.8882    steps: 276    lr: 0.0001     reward: 1.57\n",
      "epis: 856   score: 2.0   mem len: 156677   epsilon: 0.8878    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 857   score: 2.0   mem len: 156877   epsilon: 0.8874    steps: 200    lr: 0.0001     reward: 1.6\n",
      "epis: 858   score: 3.0   mem len: 157125   epsilon: 0.8869    steps: 248    lr: 0.0001     reward: 1.61\n",
      "epis: 859   score: 2.0   mem len: 157327   epsilon: 0.8865    steps: 202    lr: 0.0001     reward: 1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 860   score: 3.0   mem len: 157574   epsilon: 0.886    steps: 247    lr: 0.0001     reward: 1.63\n",
      "epis: 861   score: 0.0   mem len: 157697   epsilon: 0.8858    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 862   score: 0.0   mem len: 157819   epsilon: 0.8855    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 863   score: 2.0   mem len: 158017   epsilon: 0.8851    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 864   score: 3.0   mem len: 158242   epsilon: 0.8847    steps: 225    lr: 0.0001     reward: 1.59\n",
      "epis: 865   score: 2.0   mem len: 158440   epsilon: 0.8843    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 866   score: 2.0   mem len: 158637   epsilon: 0.8839    steps: 197    lr: 0.0001     reward: 1.58\n",
      "epis: 867   score: 0.0   mem len: 158759   epsilon: 0.8837    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 868   score: 3.0   mem len: 158985   epsilon: 0.8832    steps: 226    lr: 0.0001     reward: 1.6\n",
      "epis: 869   score: 1.0   mem len: 159154   epsilon: 0.8829    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 870   score: 0.0   mem len: 159276   epsilon: 0.8826    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 871   score: 1.0   mem len: 159445   epsilon: 0.8823    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 872   score: 2.0   mem len: 159661   epsilon: 0.8819    steps: 216    lr: 0.0001     reward: 1.62\n",
      "epis: 873   score: 3.0   mem len: 159908   epsilon: 0.8814    steps: 247    lr: 0.0001     reward: 1.65\n",
      "epis: 874   score: 2.0   mem len: 160106   epsilon: 0.881    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 875   score: 3.0   mem len: 160354   epsilon: 0.8805    steps: 248    lr: 0.0001     reward: 1.7\n",
      "epis: 876   score: 1.0   mem len: 160523   epsilon: 0.8802    steps: 169    lr: 0.0001     reward: 1.69\n",
      "epis: 877   score: 1.0   mem len: 160691   epsilon: 0.8798    steps: 168    lr: 0.0001     reward: 1.69\n",
      "epis: 878   score: 1.0   mem len: 160860   epsilon: 0.8795    steps: 169    lr: 0.0001     reward: 1.68\n",
      "epis: 879   score: 1.0   mem len: 161028   epsilon: 0.8792    steps: 168    lr: 0.0001     reward: 1.68\n",
      "epis: 880   score: 2.0   mem len: 161226   epsilon: 0.8788    steps: 198    lr: 0.0001     reward: 1.7\n",
      "epis: 881   score: 2.0   mem len: 161447   epsilon: 0.8783    steps: 221    lr: 0.0001     reward: 1.71\n",
      "epis: 882   score: 1.0   mem len: 161618   epsilon: 0.878    steps: 171    lr: 0.0001     reward: 1.72\n",
      "epis: 883   score: 2.0   mem len: 161836   epsilon: 0.8776    steps: 218    lr: 0.0001     reward: 1.74\n",
      "epis: 884   score: 1.0   mem len: 162005   epsilon: 0.8772    steps: 169    lr: 0.0001     reward: 1.75\n",
      "epis: 885   score: 3.0   mem len: 162270   epsilon: 0.8767    steps: 265    lr: 0.0001     reward: 1.74\n",
      "epis: 886   score: 2.0   mem len: 162468   epsilon: 0.8763    steps: 198    lr: 0.0001     reward: 1.76\n",
      "epis: 887   score: 1.0   mem len: 162619   epsilon: 0.876    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 888   score: 0.0   mem len: 162742   epsilon: 0.8758    steps: 123    lr: 0.0001     reward: 1.73\n",
      "epis: 889   score: 2.0   mem len: 162940   epsilon: 0.8754    steps: 198    lr: 0.0001     reward: 1.75\n",
      "epis: 890   score: 1.0   mem len: 163108   epsilon: 0.875    steps: 168    lr: 0.0001     reward: 1.74\n",
      "epis: 891   score: 1.0   mem len: 163259   epsilon: 0.8747    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 892   score: 2.0   mem len: 163480   epsilon: 0.8743    steps: 221    lr: 0.0001     reward: 1.73\n",
      "epis: 893   score: 2.0   mem len: 163678   epsilon: 0.8739    steps: 198    lr: 0.0001     reward: 1.75\n",
      "epis: 894   score: 1.0   mem len: 163829   epsilon: 0.8736    steps: 151    lr: 0.0001     reward: 1.73\n",
      "epis: 895   score: 2.0   mem len: 164030   epsilon: 0.8732    steps: 201    lr: 0.0001     reward: 1.71\n",
      "epis: 896   score: 2.0   mem len: 164228   epsilon: 0.8728    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 897   score: 2.0   mem len: 164427   epsilon: 0.8724    steps: 199    lr: 0.0001     reward: 1.71\n",
      "epis: 898   score: 0.0   mem len: 164549   epsilon: 0.8722    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 899   score: 0.0   mem len: 164671   epsilon: 0.8719    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 900   score: 3.0   mem len: 164937   epsilon: 0.8714    steps: 266    lr: 0.0001     reward: 1.71\n",
      "epis: 901   score: 3.0   mem len: 165185   epsilon: 0.8709    steps: 248    lr: 0.0001     reward: 1.71\n",
      "epis: 902   score: 3.0   mem len: 165432   epsilon: 0.8704    steps: 247    lr: 0.0001     reward: 1.69\n",
      "epis: 903   score: 0.0   mem len: 165555   epsilon: 0.8702    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 904   score: 0.0   mem len: 165677   epsilon: 0.87    steps: 122    lr: 0.0001     reward: 1.65\n",
      "epis: 905   score: 3.0   mem len: 165926   epsilon: 0.8695    steps: 249    lr: 0.0001     reward: 1.66\n",
      "epis: 906   score: 0.0   mem len: 166048   epsilon: 0.8692    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 907   score: 0.0   mem len: 166170   epsilon: 0.869    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 908   score: 1.0   mem len: 166340   epsilon: 0.8686    steps: 170    lr: 0.0001     reward: 1.65\n",
      "epis: 909   score: 0.0   mem len: 166462   epsilon: 0.8684    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 910   score: 2.0   mem len: 166678   epsilon: 0.868    steps: 216    lr: 0.0001     reward: 1.64\n",
      "epis: 911   score: 1.0   mem len: 166847   epsilon: 0.8676    steps: 169    lr: 0.0001     reward: 1.63\n",
      "epis: 912   score: 2.0   mem len: 167045   epsilon: 0.8672    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 913   score: 3.0   mem len: 167310   epsilon: 0.8667    steps: 265    lr: 0.0001     reward: 1.68\n",
      "epis: 914   score: 0.0   mem len: 167433   epsilon: 0.8665    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 915   score: 1.0   mem len: 167602   epsilon: 0.8661    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 916   score: 0.0   mem len: 167724   epsilon: 0.8659    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 917   score: 1.0   mem len: 167893   epsilon: 0.8656    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 918   score: 3.0   mem len: 168139   epsilon: 0.8651    steps: 246    lr: 0.0001     reward: 1.62\n",
      "epis: 919   score: 4.0   mem len: 168416   epsilon: 0.8645    steps: 277    lr: 0.0001     reward: 1.62\n",
      "epis: 920   score: 0.0   mem len: 168539   epsilon: 0.8643    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 921   score: 2.0   mem len: 168741   epsilon: 0.8639    steps: 202    lr: 0.0001     reward: 1.59\n",
      "epis: 922   score: 1.0   mem len: 168911   epsilon: 0.8636    steps: 170    lr: 0.0001     reward: 1.59\n",
      "epis: 923   score: 1.0   mem len: 169063   epsilon: 0.8633    steps: 152    lr: 0.0001     reward: 1.59\n",
      "epis: 924   score: 0.0   mem len: 169185   epsilon: 0.863    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 925   score: 1.0   mem len: 169353   epsilon: 0.8627    steps: 168    lr: 0.0001     reward: 1.58\n",
      "epis: 926   score: 2.0   mem len: 169573   epsilon: 0.8622    steps: 220    lr: 0.0001     reward: 1.57\n",
      "epis: 927   score: 1.0   mem len: 169723   epsilon: 0.8619    steps: 150    lr: 0.0001     reward: 1.58\n",
      "epis: 928   score: 1.0   mem len: 169874   epsilon: 0.8616    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 929   score: 1.0   mem len: 170025   epsilon: 0.8613    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 930   score: 2.0   mem len: 170223   epsilon: 0.861    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 931   score: 0.0   mem len: 170346   epsilon: 0.8607    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 932   score: 3.0   mem len: 170595   epsilon: 0.8602    steps: 249    lr: 0.0001     reward: 1.57\n",
      "epis: 933   score: 1.0   mem len: 170746   epsilon: 0.8599    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 934   score: 2.0   mem len: 170965   epsilon: 0.8595    steps: 219    lr: 0.0001     reward: 1.57\n",
      "epis: 935   score: 2.0   mem len: 171163   epsilon: 0.8591    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 936   score: 5.0   mem len: 171526   epsilon: 0.8584    steps: 363    lr: 0.0001     reward: 1.59\n",
      "epis: 937   score: 1.0   mem len: 171697   epsilon: 0.858    steps: 171    lr: 0.0001     reward: 1.58\n",
      "epis: 938   score: 1.0   mem len: 171866   epsilon: 0.8577    steps: 169    lr: 0.0001     reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 939   score: 4.0   mem len: 172162   epsilon: 0.8571    steps: 296    lr: 0.0001     reward: 1.59\n",
      "epis: 940   score: 0.0   mem len: 172285   epsilon: 0.8569    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 941   score: 2.0   mem len: 172483   epsilon: 0.8565    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 942   score: 1.0   mem len: 172655   epsilon: 0.8561    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 943   score: 0.0   mem len: 172778   epsilon: 0.8559    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 944   score: 1.0   mem len: 172929   epsilon: 0.8556    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 945   score: 2.0   mem len: 173127   epsilon: 0.8552    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 946   score: 1.0   mem len: 173296   epsilon: 0.8549    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 947   score: 2.0   mem len: 173494   epsilon: 0.8545    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 948   score: 2.0   mem len: 173691   epsilon: 0.8541    steps: 197    lr: 0.0001     reward: 1.55\n",
      "epis: 949   score: 2.0   mem len: 173889   epsilon: 0.8537    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 950   score: 0.0   mem len: 174011   epsilon: 0.8535    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 951   score: 3.0   mem len: 174237   epsilon: 0.853    steps: 226    lr: 0.0001     reward: 1.56\n",
      "epis: 952   score: 3.0   mem len: 174483   epsilon: 0.8525    steps: 246    lr: 0.0001     reward: 1.57\n",
      "epis: 953   score: 2.0   mem len: 174683   epsilon: 0.8521    steps: 200    lr: 0.0001     reward: 1.56\n",
      "epis: 954   score: 0.0   mem len: 174806   epsilon: 0.8519    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 955   score: 0.0   mem len: 174929   epsilon: 0.8516    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 956   score: 2.0   mem len: 175127   epsilon: 0.8512    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 957   score: 1.0   mem len: 175277   epsilon: 0.8509    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 958   score: 1.0   mem len: 175445   epsilon: 0.8506    steps: 168    lr: 0.0001     reward: 1.48\n",
      "epis: 959   score: 0.0   mem len: 175568   epsilon: 0.8504    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 960   score: 0.0   mem len: 175690   epsilon: 0.8501    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 961   score: 1.0   mem len: 175862   epsilon: 0.8498    steps: 172    lr: 0.0001     reward: 1.44\n",
      "epis: 962   score: 2.0   mem len: 176081   epsilon: 0.8494    steps: 219    lr: 0.0001     reward: 1.46\n",
      "epis: 963   score: 4.0   mem len: 176377   epsilon: 0.8488    steps: 296    lr: 0.0001     reward: 1.48\n",
      "epis: 964   score: 4.0   mem len: 176675   epsilon: 0.8482    steps: 298    lr: 0.0001     reward: 1.49\n",
      "epis: 965   score: 0.0   mem len: 176798   epsilon: 0.8479    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 966   score: 3.0   mem len: 177047   epsilon: 0.8474    steps: 249    lr: 0.0001     reward: 1.48\n",
      "epis: 967   score: 0.0   mem len: 177169   epsilon: 0.8472    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 968   score: 1.0   mem len: 177338   epsilon: 0.8469    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 969   score: 0.0   mem len: 177460   epsilon: 0.8466    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 970   score: 0.0   mem len: 177583   epsilon: 0.8464    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 971   score: 2.0   mem len: 177763   epsilon: 0.846    steps: 180    lr: 0.0001     reward: 1.46\n",
      "epis: 972   score: 0.0   mem len: 177886   epsilon: 0.8458    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 973   score: 0.0   mem len: 178008   epsilon: 0.8455    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 974   score: 3.0   mem len: 178255   epsilon: 0.8451    steps: 247    lr: 0.0001     reward: 1.42\n",
      "epis: 975   score: 4.0   mem len: 178529   epsilon: 0.8445    steps: 274    lr: 0.0001     reward: 1.43\n",
      "epis: 976   score: 0.0   mem len: 178652   epsilon: 0.8443    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 977   score: 4.0   mem len: 178929   epsilon: 0.8437    steps: 277    lr: 0.0001     reward: 1.45\n",
      "epis: 978   score: 2.0   mem len: 179127   epsilon: 0.8433    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 979   score: 2.0   mem len: 179324   epsilon: 0.8429    steps: 197    lr: 0.0001     reward: 1.47\n",
      "epis: 980   score: 1.0   mem len: 179475   epsilon: 0.8426    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 981   score: 2.0   mem len: 179673   epsilon: 0.8422    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 982   score: 2.0   mem len: 179854   epsilon: 0.8419    steps: 181    lr: 0.0001     reward: 1.47\n",
      "epis: 983   score: 0.0   mem len: 179977   epsilon: 0.8416    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 984   score: 2.0   mem len: 180175   epsilon: 0.8413    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 985   score: 1.0   mem len: 180326   epsilon: 0.841    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 986   score: 3.0   mem len: 180592   epsilon: 0.8404    steps: 266    lr: 0.0001     reward: 1.45\n",
      "epis: 987   score: 3.0   mem len: 180818   epsilon: 0.84    steps: 226    lr: 0.0001     reward: 1.47\n",
      "epis: 988   score: 4.0   mem len: 181095   epsilon: 0.8394    steps: 277    lr: 0.0001     reward: 1.51\n",
      "epis: 989   score: 1.0   mem len: 181246   epsilon: 0.8391    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 990   score: 2.0   mem len: 181445   epsilon: 0.8387    steps: 199    lr: 0.0001     reward: 1.51\n",
      "epis: 991   score: 6.0   mem len: 181804   epsilon: 0.838    steps: 359    lr: 0.0001     reward: 1.56\n",
      "epis: 992   score: 3.0   mem len: 182069   epsilon: 0.8375    steps: 265    lr: 0.0001     reward: 1.57\n",
      "epis: 993   score: 2.0   mem len: 182288   epsilon: 0.8371    steps: 219    lr: 0.0001     reward: 1.57\n",
      "epis: 994   score: 3.0   mem len: 182534   epsilon: 0.8366    steps: 246    lr: 0.0001     reward: 1.59\n",
      "epis: 995   score: 1.0   mem len: 182706   epsilon: 0.8362    steps: 172    lr: 0.0001     reward: 1.58\n",
      "epis: 996   score: 0.0   mem len: 182829   epsilon: 0.836    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 997   score: 0.0   mem len: 182951   epsilon: 0.8358    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 998   score: 0.0   mem len: 183074   epsilon: 0.8355    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 999   score: 0.0   mem len: 183196   epsilon: 0.8353    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 1000   score: 1.0   mem len: 183365   epsilon: 0.8349    steps: 169    lr: 0.0001     reward: 1.52\n",
      "epis: 1001   score: 1.0   mem len: 183515   epsilon: 0.8346    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 1002   score: 0.0   mem len: 183637   epsilon: 0.8344    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 1003   score: 3.0   mem len: 183903   epsilon: 0.8339    steps: 266    lr: 0.0001     reward: 1.5\n",
      "epis: 1004   score: 3.0   mem len: 184129   epsilon: 0.8334    steps: 226    lr: 0.0001     reward: 1.53\n",
      "epis: 1005   score: 1.0   mem len: 184280   epsilon: 0.8331    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 1006   score: 1.0   mem len: 184450   epsilon: 0.8328    steps: 170    lr: 0.0001     reward: 1.52\n",
      "epis: 1007   score: 0.0   mem len: 184573   epsilon: 0.8325    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 1008   score: 4.0   mem len: 184850   epsilon: 0.832    steps: 277    lr: 0.0001     reward: 1.55\n",
      "epis: 1009   score: 0.0   mem len: 184973   epsilon: 0.8318    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 1010   score: 1.0   mem len: 185142   epsilon: 0.8314    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 1011   score: 3.0   mem len: 185407   epsilon: 0.8309    steps: 265    lr: 0.0001     reward: 1.56\n",
      "epis: 1012   score: 0.0   mem len: 185529   epsilon: 0.8307    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 1013   score: 2.0   mem len: 185745   epsilon: 0.8302    steps: 216    lr: 0.0001     reward: 1.53\n",
      "epis: 1014   score: 2.0   mem len: 185943   epsilon: 0.8298    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 1015   score: 3.0   mem len: 186192   epsilon: 0.8293    steps: 249    lr: 0.0001     reward: 1.57\n",
      "epis: 1016   score: 6.0   mem len: 186584   epsilon: 0.8286    steps: 392    lr: 0.0001     reward: 1.63\n",
      "epis: 1017   score: 3.0   mem len: 186834   epsilon: 0.8281    steps: 250    lr: 0.0001     reward: 1.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1018   score: 1.0   mem len: 187003   epsilon: 0.8277    steps: 169    lr: 0.0001     reward: 1.63\n",
      "epis: 1019   score: 2.0   mem len: 187220   epsilon: 0.8273    steps: 217    lr: 0.0001     reward: 1.61\n",
      "epis: 1020   score: 0.0   mem len: 187343   epsilon: 0.8271    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 1021   score: 2.0   mem len: 187562   epsilon: 0.8266    steps: 219    lr: 0.0001     reward: 1.61\n",
      "epis: 1022   score: 5.0   mem len: 187898   epsilon: 0.826    steps: 336    lr: 0.0001     reward: 1.65\n",
      "epis: 1023   score: 3.0   mem len: 188124   epsilon: 0.8255    steps: 226    lr: 0.0001     reward: 1.67\n",
      "epis: 1024   score: 4.0   mem len: 188441   epsilon: 0.8249    steps: 317    lr: 0.0001     reward: 1.71\n",
      "epis: 1025   score: 0.0   mem len: 188564   epsilon: 0.8246    steps: 123    lr: 0.0001     reward: 1.7\n",
      "epis: 1026   score: 3.0   mem len: 188812   epsilon: 0.8242    steps: 248    lr: 0.0001     reward: 1.71\n",
      "epis: 1027   score: 2.0   mem len: 188994   epsilon: 0.8238    steps: 182    lr: 0.0001     reward: 1.72\n",
      "epis: 1028   score: 3.0   mem len: 189240   epsilon: 0.8233    steps: 246    lr: 0.0001     reward: 1.74\n",
      "epis: 1029   score: 0.0   mem len: 189362   epsilon: 0.8231    steps: 122    lr: 0.0001     reward: 1.73\n",
      "epis: 1030   score: 1.0   mem len: 189513   epsilon: 0.8228    steps: 151    lr: 0.0001     reward: 1.72\n",
      "epis: 1031   score: 0.0   mem len: 189636   epsilon: 0.8225    steps: 123    lr: 0.0001     reward: 1.72\n",
      "epis: 1032   score: 2.0   mem len: 189834   epsilon: 0.8221    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 1033   score: 1.0   mem len: 190003   epsilon: 0.8218    steps: 169    lr: 0.0001     reward: 1.71\n",
      "epis: 1034   score: 1.0   mem len: 190154   epsilon: 0.8215    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 1035   score: 2.0   mem len: 190352   epsilon: 0.8211    steps: 198    lr: 0.0001     reward: 1.7\n",
      "epis: 1036   score: 3.0   mem len: 190580   epsilon: 0.8206    steps: 228    lr: 0.0001     reward: 1.68\n",
      "epis: 1037   score: 2.0   mem len: 190778   epsilon: 0.8203    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 1038   score: 3.0   mem len: 191004   epsilon: 0.8198    steps: 226    lr: 0.0001     reward: 1.71\n",
      "epis: 1039   score: 2.0   mem len: 191202   epsilon: 0.8194    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 1040   score: 1.0   mem len: 191353   epsilon: 0.8191    steps: 151    lr: 0.0001     reward: 1.7\n",
      "epis: 1041   score: 2.0   mem len: 191550   epsilon: 0.8187    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 1042   score: 2.0   mem len: 191748   epsilon: 0.8183    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 1043   score: 0.0   mem len: 191870   epsilon: 0.8181    steps: 122    lr: 0.0001     reward: 1.71\n",
      "epis: 1044   score: 1.0   mem len: 192039   epsilon: 0.8178    steps: 169    lr: 0.0001     reward: 1.71\n",
      "epis: 1045   score: 2.0   mem len: 192237   epsilon: 0.8174    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 1046   score: 1.0   mem len: 192387   epsilon: 0.8171    steps: 150    lr: 0.0001     reward: 1.71\n",
      "epis: 1047   score: 0.0   mem len: 192510   epsilon: 0.8168    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 1048   score: 2.0   mem len: 192707   epsilon: 0.8164    steps: 197    lr: 0.0001     reward: 1.69\n",
      "epis: 1049   score: 1.0   mem len: 192857   epsilon: 0.8161    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 1050   score: 1.0   mem len: 193027   epsilon: 0.8158    steps: 170    lr: 0.0001     reward: 1.69\n",
      "epis: 1051   score: 0.0   mem len: 193149   epsilon: 0.8156    steps: 122    lr: 0.0001     reward: 1.66\n",
      "epis: 1052   score: 5.0   mem len: 193513   epsilon: 0.8148    steps: 364    lr: 0.0001     reward: 1.68\n",
      "epis: 1053   score: 1.0   mem len: 193663   epsilon: 0.8145    steps: 150    lr: 0.0001     reward: 1.67\n",
      "epis: 1054   score: 2.0   mem len: 193861   epsilon: 0.8142    steps: 198    lr: 0.0001     reward: 1.69\n",
      "epis: 1055   score: 2.0   mem len: 194061   epsilon: 0.8138    steps: 200    lr: 0.0001     reward: 1.71\n",
      "epis: 1056   score: 4.0   mem len: 194318   epsilon: 0.8132    steps: 257    lr: 0.0001     reward: 1.73\n",
      "epis: 1057   score: 0.0   mem len: 194440   epsilon: 0.813    steps: 122    lr: 0.0001     reward: 1.72\n",
      "epis: 1058   score: 4.0   mem len: 194736   epsilon: 0.8124    steps: 296    lr: 0.0001     reward: 1.75\n",
      "epis: 1059   score: 2.0   mem len: 194934   epsilon: 0.812    steps: 198    lr: 0.0001     reward: 1.77\n",
      "epis: 1060   score: 0.0   mem len: 195057   epsilon: 0.8118    steps: 123    lr: 0.0001     reward: 1.77\n",
      "epis: 1061   score: 1.0   mem len: 195225   epsilon: 0.8115    steps: 168    lr: 0.0001     reward: 1.77\n",
      "epis: 1062   score: 4.0   mem len: 195500   epsilon: 0.8109    steps: 275    lr: 0.0001     reward: 1.79\n",
      "epis: 1063   score: 3.0   mem len: 195747   epsilon: 0.8104    steps: 247    lr: 0.0001     reward: 1.78\n",
      "epis: 1064   score: 0.0   mem len: 195869   epsilon: 0.8102    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1065   score: 0.0   mem len: 195991   epsilon: 0.8099    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1066   score: 0.0   mem len: 196114   epsilon: 0.8097    steps: 123    lr: 0.0001     reward: 1.71\n",
      "epis: 1067   score: 2.0   mem len: 196312   epsilon: 0.8093    steps: 198    lr: 0.0001     reward: 1.73\n",
      "epis: 1068   score: 2.0   mem len: 196512   epsilon: 0.8089    steps: 200    lr: 0.0001     reward: 1.74\n",
      "epis: 1069   score: 0.0   mem len: 196634   epsilon: 0.8087    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1070   score: 0.0   mem len: 196756   epsilon: 0.8084    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 1071   score: 2.0   mem len: 196973   epsilon: 0.808    steps: 217    lr: 0.0001     reward: 1.74\n",
      "epis: 1072   score: 1.0   mem len: 197141   epsilon: 0.8077    steps: 168    lr: 0.0001     reward: 1.75\n",
      "epis: 1073   score: 1.0   mem len: 197310   epsilon: 0.8073    steps: 169    lr: 0.0001     reward: 1.76\n",
      "epis: 1074   score: 1.0   mem len: 197479   epsilon: 0.807    steps: 169    lr: 0.0001     reward: 1.74\n",
      "epis: 1075   score: 2.0   mem len: 197677   epsilon: 0.8066    steps: 198    lr: 0.0001     reward: 1.72\n",
      "epis: 1076   score: 5.0   mem len: 198026   epsilon: 0.8059    steps: 349    lr: 0.0001     reward: 1.77\n",
      "epis: 1077   score: 2.0   mem len: 198245   epsilon: 0.8055    steps: 219    lr: 0.0001     reward: 1.75\n",
      "epis: 1078   score: 4.0   mem len: 198505   epsilon: 0.805    steps: 260    lr: 0.0001     reward: 1.77\n",
      "epis: 1079   score: 4.0   mem len: 198803   epsilon: 0.8044    steps: 298    lr: 0.0001     reward: 1.79\n",
      "epis: 1080   score: 4.0   mem len: 199101   epsilon: 0.8038    steps: 298    lr: 0.0001     reward: 1.82\n",
      "epis: 1081   score: 3.0   mem len: 199347   epsilon: 0.8033    steps: 246    lr: 0.0001     reward: 1.83\n",
      "epis: 1082   score: 3.0   mem len: 199576   epsilon: 0.8028    steps: 229    lr: 0.0001     reward: 1.84\n",
      "epis: 1083   score: 4.0   mem len: 199849   epsilon: 0.8023    steps: 273    lr: 0.0001     reward: 1.88\n",
      "epis: 1084   score: 2.0   mem len: 200047   epsilon: 0.8019    steps: 198    lr: 4e-05     reward: 1.88\n",
      "epis: 1085   score: 2.0   mem len: 200244   epsilon: 0.8015    steps: 197    lr: 4e-05     reward: 1.89\n",
      "epis: 1086   score: 1.0   mem len: 200414   epsilon: 0.8012    steps: 170    lr: 4e-05     reward: 1.87\n",
      "epis: 1087   score: 3.0   mem len: 200661   epsilon: 0.8007    steps: 247    lr: 4e-05     reward: 1.87\n",
      "epis: 1088   score: 1.0   mem len: 200832   epsilon: 0.8004    steps: 171    lr: 4e-05     reward: 1.84\n",
      "epis: 1089   score: 2.0   mem len: 201050   epsilon: 0.7999    steps: 218    lr: 4e-05     reward: 1.85\n",
      "epis: 1090   score: 1.0   mem len: 201219   epsilon: 0.7996    steps: 169    lr: 4e-05     reward: 1.84\n",
      "epis: 1091   score: 2.0   mem len: 201436   epsilon: 0.7992    steps: 217    lr: 4e-05     reward: 1.8\n",
      "epis: 1092   score: 0.0   mem len: 201559   epsilon: 0.7989    steps: 123    lr: 4e-05     reward: 1.77\n",
      "epis: 1093   score: 1.0   mem len: 201728   epsilon: 0.7986    steps: 169    lr: 4e-05     reward: 1.76\n",
      "epis: 1094   score: 0.0   mem len: 201850   epsilon: 0.7983    steps: 122    lr: 4e-05     reward: 1.73\n",
      "epis: 1095   score: 1.0   mem len: 202000   epsilon: 0.798    steps: 150    lr: 4e-05     reward: 1.73\n",
      "epis: 1096   score: 2.0   mem len: 202198   epsilon: 0.7976    steps: 198    lr: 4e-05     reward: 1.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1097   score: 2.0   mem len: 202416   epsilon: 0.7972    steps: 218    lr: 4e-05     reward: 1.77\n",
      "epis: 1098   score: 4.0   mem len: 202691   epsilon: 0.7967    steps: 275    lr: 4e-05     reward: 1.81\n",
      "epis: 1099   score: 5.0   mem len: 203055   epsilon: 0.7959    steps: 364    lr: 4e-05     reward: 1.86\n",
      "epis: 1100   score: 1.0   mem len: 203224   epsilon: 0.7956    steps: 169    lr: 4e-05     reward: 1.86\n",
      "epis: 1101   score: 0.0   mem len: 203346   epsilon: 0.7954    steps: 122    lr: 4e-05     reward: 1.85\n",
      "epis: 1102   score: 3.0   mem len: 203611   epsilon: 0.7948    steps: 265    lr: 4e-05     reward: 1.88\n",
      "epis: 1103   score: 4.0   mem len: 203888   epsilon: 0.7943    steps: 277    lr: 4e-05     reward: 1.89\n",
      "epis: 1104   score: 2.0   mem len: 204068   epsilon: 0.7939    steps: 180    lr: 4e-05     reward: 1.88\n",
      "epis: 1105   score: 1.0   mem len: 204239   epsilon: 0.7936    steps: 171    lr: 4e-05     reward: 1.88\n",
      "epis: 1106   score: 2.0   mem len: 204436   epsilon: 0.7932    steps: 197    lr: 4e-05     reward: 1.89\n",
      "epis: 1107   score: 1.0   mem len: 204605   epsilon: 0.7929    steps: 169    lr: 4e-05     reward: 1.9\n",
      "epis: 1108   score: 3.0   mem len: 204831   epsilon: 0.7924    steps: 226    lr: 4e-05     reward: 1.89\n",
      "epis: 1109   score: 3.0   mem len: 205062   epsilon: 0.792    steps: 231    lr: 4e-05     reward: 1.92\n",
      "epis: 1110   score: 2.0   mem len: 205263   epsilon: 0.7916    steps: 201    lr: 4e-05     reward: 1.93\n",
      "epis: 1111   score: 1.0   mem len: 205414   epsilon: 0.7913    steps: 151    lr: 4e-05     reward: 1.91\n",
      "epis: 1112   score: 0.0   mem len: 205537   epsilon: 0.791    steps: 123    lr: 4e-05     reward: 1.91\n",
      "epis: 1113   score: 4.0   mem len: 205831   epsilon: 0.7905    steps: 294    lr: 4e-05     reward: 1.93\n",
      "epis: 1114   score: 4.0   mem len: 206124   epsilon: 0.7899    steps: 293    lr: 4e-05     reward: 1.95\n",
      "epis: 1115   score: 2.0   mem len: 206321   epsilon: 0.7895    steps: 197    lr: 4e-05     reward: 1.94\n",
      "epis: 1116   score: 2.0   mem len: 206539   epsilon: 0.7891    steps: 218    lr: 4e-05     reward: 1.9\n",
      "epis: 1117   score: 0.0   mem len: 206662   epsilon: 0.7888    steps: 123    lr: 4e-05     reward: 1.87\n",
      "epis: 1118   score: 2.0   mem len: 206859   epsilon: 0.7884    steps: 197    lr: 4e-05     reward: 1.88\n",
      "epis: 1119   score: 3.0   mem len: 207088   epsilon: 0.788    steps: 229    lr: 4e-05     reward: 1.89\n",
      "epis: 1120   score: 4.0   mem len: 207382   epsilon: 0.7874    steps: 294    lr: 4e-05     reward: 1.93\n",
      "epis: 1121   score: 2.0   mem len: 207580   epsilon: 0.787    steps: 198    lr: 4e-05     reward: 1.93\n",
      "epis: 1122   score: 1.0   mem len: 207748   epsilon: 0.7867    steps: 168    lr: 4e-05     reward: 1.89\n",
      "epis: 1123   score: 0.0   mem len: 207871   epsilon: 0.7864    steps: 123    lr: 4e-05     reward: 1.86\n",
      "epis: 1124   score: 2.0   mem len: 208068   epsilon: 0.786    steps: 197    lr: 4e-05     reward: 1.84\n",
      "epis: 1125   score: 1.0   mem len: 208218   epsilon: 0.7857    steps: 150    lr: 4e-05     reward: 1.85\n",
      "epis: 1126   score: 0.0   mem len: 208340   epsilon: 0.7855    steps: 122    lr: 4e-05     reward: 1.82\n",
      "epis: 1127   score: 3.0   mem len: 208567   epsilon: 0.785    steps: 227    lr: 4e-05     reward: 1.83\n",
      "epis: 1128   score: 0.0   mem len: 208690   epsilon: 0.7848    steps: 123    lr: 4e-05     reward: 1.8\n",
      "epis: 1129   score: 1.0   mem len: 208859   epsilon: 0.7845    steps: 169    lr: 4e-05     reward: 1.81\n",
      "epis: 1130   score: 5.0   mem len: 209203   epsilon: 0.7838    steps: 344    lr: 4e-05     reward: 1.85\n",
      "epis: 1131   score: 1.0   mem len: 209354   epsilon: 0.7835    steps: 151    lr: 4e-05     reward: 1.86\n",
      "epis: 1132   score: 3.0   mem len: 209600   epsilon: 0.783    steps: 246    lr: 4e-05     reward: 1.87\n",
      "epis: 1133   score: 4.0   mem len: 209881   epsilon: 0.7824    steps: 281    lr: 4e-05     reward: 1.9\n",
      "epis: 1134   score: 0.0   mem len: 210003   epsilon: 0.7822    steps: 122    lr: 4e-05     reward: 1.89\n",
      "epis: 1135   score: 5.0   mem len: 210349   epsilon: 0.7815    steps: 346    lr: 4e-05     reward: 1.92\n",
      "epis: 1136   score: 0.0   mem len: 210472   epsilon: 0.7813    steps: 123    lr: 4e-05     reward: 1.89\n",
      "epis: 1137   score: 2.0   mem len: 210671   epsilon: 0.7809    steps: 199    lr: 4e-05     reward: 1.89\n",
      "epis: 1138   score: 0.0   mem len: 210794   epsilon: 0.7806    steps: 123    lr: 4e-05     reward: 1.86\n",
      "epis: 1139   score: 4.0   mem len: 211111   epsilon: 0.78    steps: 317    lr: 4e-05     reward: 1.88\n",
      "epis: 1140   score: 3.0   mem len: 211375   epsilon: 0.7795    steps: 264    lr: 4e-05     reward: 1.9\n",
      "epis: 1141   score: 1.0   mem len: 211544   epsilon: 0.7791    steps: 169    lr: 4e-05     reward: 1.89\n",
      "epis: 1142   score: 2.0   mem len: 211742   epsilon: 0.7787    steps: 198    lr: 4e-05     reward: 1.89\n",
      "epis: 1143   score: 1.0   mem len: 211892   epsilon: 0.7785    steps: 150    lr: 4e-05     reward: 1.9\n",
      "epis: 1144   score: 1.0   mem len: 212061   epsilon: 0.7781    steps: 169    lr: 4e-05     reward: 1.9\n",
      "epis: 1145   score: 2.0   mem len: 212259   epsilon: 0.7777    steps: 198    lr: 4e-05     reward: 1.9\n",
      "epis: 1146   score: 2.0   mem len: 212475   epsilon: 0.7773    steps: 216    lr: 4e-05     reward: 1.91\n",
      "epis: 1147   score: 0.0   mem len: 212598   epsilon: 0.7771    steps: 123    lr: 4e-05     reward: 1.91\n",
      "epis: 1148   score: 3.0   mem len: 212844   epsilon: 0.7766    steps: 246    lr: 4e-05     reward: 1.92\n",
      "epis: 1149   score: 4.0   mem len: 213140   epsilon: 0.776    steps: 296    lr: 4e-05     reward: 1.95\n",
      "epis: 1150   score: 1.0   mem len: 213311   epsilon: 0.7756    steps: 171    lr: 4e-05     reward: 1.95\n",
      "epis: 1151   score: 4.0   mem len: 213590   epsilon: 0.7751    steps: 279    lr: 4e-05     reward: 1.99\n",
      "epis: 1152   score: 2.0   mem len: 213771   epsilon: 0.7747    steps: 181    lr: 4e-05     reward: 1.96\n",
      "epis: 1153   score: 5.0   mem len: 214094   epsilon: 0.7741    steps: 323    lr: 4e-05     reward: 2.0\n",
      "epis: 1154   score: 1.0   mem len: 214265   epsilon: 0.7738    steps: 171    lr: 4e-05     reward: 1.99\n",
      "epis: 1155   score: 1.0   mem len: 214416   epsilon: 0.7735    steps: 151    lr: 4e-05     reward: 1.98\n",
      "epis: 1156   score: 2.0   mem len: 214617   epsilon: 0.7731    steps: 201    lr: 4e-05     reward: 1.96\n",
      "epis: 1157   score: 2.0   mem len: 214815   epsilon: 0.7727    steps: 198    lr: 4e-05     reward: 1.98\n",
      "epis: 1158   score: 4.0   mem len: 215111   epsilon: 0.7721    steps: 296    lr: 4e-05     reward: 1.98\n",
      "epis: 1159   score: 2.0   mem len: 215329   epsilon: 0.7716    steps: 218    lr: 4e-05     reward: 1.98\n",
      "epis: 1160   score: 1.0   mem len: 215498   epsilon: 0.7713    steps: 169    lr: 4e-05     reward: 1.99\n",
      "epis: 1161   score: 6.0   mem len: 215844   epsilon: 0.7706    steps: 346    lr: 4e-05     reward: 2.04\n",
      "epis: 1162   score: 2.0   mem len: 216042   epsilon: 0.7702    steps: 198    lr: 4e-05     reward: 2.02\n",
      "epis: 1163   score: 4.0   mem len: 216317   epsilon: 0.7697    steps: 275    lr: 4e-05     reward: 2.03\n",
      "epis: 1164   score: 1.0   mem len: 216468   epsilon: 0.7694    steps: 151    lr: 4e-05     reward: 2.04\n",
      "epis: 1165   score: 2.0   mem len: 216665   epsilon: 0.769    steps: 197    lr: 4e-05     reward: 2.06\n",
      "epis: 1166   score: 0.0   mem len: 216788   epsilon: 0.7688    steps: 123    lr: 4e-05     reward: 2.06\n",
      "epis: 1167   score: 2.0   mem len: 216969   epsilon: 0.7684    steps: 181    lr: 4e-05     reward: 2.06\n",
      "epis: 1168   score: 4.0   mem len: 217227   epsilon: 0.7679    steps: 258    lr: 4e-05     reward: 2.08\n",
      "epis: 1169   score: 3.0   mem len: 217453   epsilon: 0.7674    steps: 226    lr: 4e-05     reward: 2.11\n",
      "epis: 1170   score: 5.0   mem len: 217780   epsilon: 0.7668    steps: 327    lr: 4e-05     reward: 2.16\n",
      "epis: 1171   score: 2.0   mem len: 217977   epsilon: 0.7664    steps: 197    lr: 4e-05     reward: 2.16\n",
      "epis: 1172   score: 2.0   mem len: 218157   epsilon: 0.766    steps: 180    lr: 4e-05     reward: 2.17\n",
      "epis: 1173   score: 3.0   mem len: 218382   epsilon: 0.7656    steps: 225    lr: 4e-05     reward: 2.19\n",
      "epis: 1174   score: 2.0   mem len: 218580   epsilon: 0.7652    steps: 198    lr: 4e-05     reward: 2.2\n",
      "epis: 1175   score: 2.0   mem len: 218798   epsilon: 0.7648    steps: 218    lr: 4e-05     reward: 2.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1176   score: 0.0   mem len: 218921   epsilon: 0.7645    steps: 123    lr: 4e-05     reward: 2.15\n",
      "epis: 1177   score: 2.0   mem len: 219141   epsilon: 0.7641    steps: 220    lr: 4e-05     reward: 2.15\n",
      "epis: 1178   score: 1.0   mem len: 219292   epsilon: 0.7638    steps: 151    lr: 4e-05     reward: 2.12\n",
      "epis: 1179   score: 1.0   mem len: 219460   epsilon: 0.7635    steps: 168    lr: 4e-05     reward: 2.09\n",
      "epis: 1180   score: 6.0   mem len: 219799   epsilon: 0.7628    steps: 339    lr: 4e-05     reward: 2.11\n",
      "epis: 1181   score: 0.0   mem len: 219922   epsilon: 0.7626    steps: 123    lr: 4e-05     reward: 2.08\n",
      "epis: 1182   score: 3.0   mem len: 220152   epsilon: 0.7621    steps: 230    lr: 4e-05     reward: 2.08\n",
      "epis: 1183   score: 5.0   mem len: 220455   epsilon: 0.7615    steps: 303    lr: 4e-05     reward: 2.09\n",
      "epis: 1184   score: 5.0   mem len: 220766   epsilon: 0.7609    steps: 311    lr: 4e-05     reward: 2.12\n",
      "epis: 1185   score: 1.0   mem len: 220917   epsilon: 0.7606    steps: 151    lr: 4e-05     reward: 2.11\n",
      "epis: 1186   score: 2.0   mem len: 221117   epsilon: 0.7602    steps: 200    lr: 4e-05     reward: 2.12\n",
      "epis: 1187   score: 3.0   mem len: 221362   epsilon: 0.7597    steps: 245    lr: 4e-05     reward: 2.12\n",
      "epis: 1188   score: 3.0   mem len: 221588   epsilon: 0.7593    steps: 226    lr: 4e-05     reward: 2.14\n",
      "epis: 1189   score: 3.0   mem len: 221856   epsilon: 0.7587    steps: 268    lr: 4e-05     reward: 2.15\n",
      "epis: 1190   score: 2.0   mem len: 222054   epsilon: 0.7583    steps: 198    lr: 4e-05     reward: 2.16\n",
      "epis: 1191   score: 2.0   mem len: 222233   epsilon: 0.758    steps: 179    lr: 4e-05     reward: 2.16\n",
      "epis: 1192   score: 0.0   mem len: 222356   epsilon: 0.7577    steps: 123    lr: 4e-05     reward: 2.16\n",
      "epis: 1193   score: 3.0   mem len: 222582   epsilon: 0.7573    steps: 226    lr: 4e-05     reward: 2.18\n",
      "epis: 1194   score: 2.0   mem len: 222802   epsilon: 0.7569    steps: 220    lr: 4e-05     reward: 2.2\n",
      "epis: 1195   score: 3.0   mem len: 223028   epsilon: 0.7564    steps: 226    lr: 4e-05     reward: 2.22\n",
      "epis: 1196   score: 4.0   mem len: 223303   epsilon: 0.7559    steps: 275    lr: 4e-05     reward: 2.24\n",
      "epis: 1197   score: 0.0   mem len: 223426   epsilon: 0.7556    steps: 123    lr: 4e-05     reward: 2.22\n",
      "epis: 1198   score: 1.0   mem len: 223596   epsilon: 0.7553    steps: 170    lr: 4e-05     reward: 2.19\n",
      "epis: 1199   score: 1.0   mem len: 223765   epsilon: 0.7549    steps: 169    lr: 4e-05     reward: 2.15\n",
      "epis: 1200   score: 0.0   mem len: 223888   epsilon: 0.7547    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1201   score: 3.0   mem len: 224135   epsilon: 0.7542    steps: 247    lr: 4e-05     reward: 2.17\n",
      "epis: 1202   score: 0.0   mem len: 224258   epsilon: 0.754    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1203   score: 2.0   mem len: 224438   epsilon: 0.7536    steps: 180    lr: 4e-05     reward: 2.12\n",
      "epis: 1204   score: 3.0   mem len: 224707   epsilon: 0.7531    steps: 269    lr: 4e-05     reward: 2.13\n",
      "epis: 1205   score: 4.0   mem len: 224981   epsilon: 0.7525    steps: 274    lr: 4e-05     reward: 2.16\n",
      "epis: 1206   score: 2.0   mem len: 225179   epsilon: 0.7521    steps: 198    lr: 4e-05     reward: 2.16\n",
      "epis: 1207   score: 1.0   mem len: 225348   epsilon: 0.7518    steps: 169    lr: 4e-05     reward: 2.16\n",
      "epis: 1208   score: 4.0   mem len: 225624   epsilon: 0.7513    steps: 276    lr: 4e-05     reward: 2.17\n",
      "epis: 1209   score: 3.0   mem len: 225870   epsilon: 0.7508    steps: 246    lr: 4e-05     reward: 2.17\n",
      "epis: 1210   score: 0.0   mem len: 225993   epsilon: 0.7505    steps: 123    lr: 4e-05     reward: 2.15\n",
      "epis: 1211   score: 0.0   mem len: 226116   epsilon: 0.7503    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1212   score: 0.0   mem len: 226239   epsilon: 0.75    steps: 123    lr: 4e-05     reward: 2.14\n",
      "epis: 1213   score: 3.0   mem len: 226465   epsilon: 0.7496    steps: 226    lr: 4e-05     reward: 2.13\n",
      "epis: 1214   score: 1.0   mem len: 226616   epsilon: 0.7493    steps: 151    lr: 4e-05     reward: 2.1\n",
      "epis: 1215   score: 8.0   mem len: 226957   epsilon: 0.7486    steps: 341    lr: 4e-05     reward: 2.16\n",
      "epis: 1216   score: 2.0   mem len: 227158   epsilon: 0.7482    steps: 201    lr: 4e-05     reward: 2.16\n",
      "epis: 1217   score: 2.0   mem len: 227361   epsilon: 0.7478    steps: 203    lr: 4e-05     reward: 2.18\n",
      "epis: 1218   score: 1.0   mem len: 227512   epsilon: 0.7475    steps: 151    lr: 4e-05     reward: 2.17\n",
      "epis: 1219   score: 3.0   mem len: 227759   epsilon: 0.747    steps: 247    lr: 4e-05     reward: 2.17\n",
      "epis: 1220   score: 4.0   mem len: 228053   epsilon: 0.7465    steps: 294    lr: 4e-05     reward: 2.17\n",
      "epis: 1221   score: 2.0   mem len: 228251   epsilon: 0.7461    steps: 198    lr: 4e-05     reward: 2.17\n",
      "epis: 1222   score: 5.0   mem len: 228560   epsilon: 0.7454    steps: 309    lr: 4e-05     reward: 2.21\n",
      "epis: 1223   score: 3.0   mem len: 228786   epsilon: 0.745    steps: 226    lr: 4e-05     reward: 2.24\n",
      "epis: 1224   score: 2.0   mem len: 228984   epsilon: 0.7446    steps: 198    lr: 4e-05     reward: 2.24\n",
      "epis: 1225   score: 2.0   mem len: 229168   epsilon: 0.7442    steps: 184    lr: 4e-05     reward: 2.25\n",
      "epis: 1226   score: 2.0   mem len: 229366   epsilon: 0.7439    steps: 198    lr: 4e-05     reward: 2.27\n",
      "epis: 1227   score: 3.0   mem len: 229594   epsilon: 0.7434    steps: 228    lr: 4e-05     reward: 2.27\n",
      "epis: 1228   score: 4.0   mem len: 229888   epsilon: 0.7428    steps: 294    lr: 4e-05     reward: 2.31\n",
      "epis: 1229   score: 2.0   mem len: 230088   epsilon: 0.7424    steps: 200    lr: 4e-05     reward: 2.32\n",
      "epis: 1230   score: 3.0   mem len: 230362   epsilon: 0.7419    steps: 274    lr: 4e-05     reward: 2.3\n",
      "epis: 1231   score: 3.0   mem len: 230588   epsilon: 0.7414    steps: 226    lr: 4e-05     reward: 2.32\n",
      "epis: 1232   score: 2.0   mem len: 230769   epsilon: 0.7411    steps: 181    lr: 4e-05     reward: 2.31\n",
      "epis: 1233   score: 2.0   mem len: 230951   epsilon: 0.7407    steps: 182    lr: 4e-05     reward: 2.29\n",
      "epis: 1234   score: 4.0   mem len: 231267   epsilon: 0.7401    steps: 316    lr: 4e-05     reward: 2.33\n",
      "epis: 1235   score: 2.0   mem len: 231485   epsilon: 0.7397    steps: 218    lr: 4e-05     reward: 2.3\n",
      "epis: 1236   score: 2.0   mem len: 231683   epsilon: 0.7393    steps: 198    lr: 4e-05     reward: 2.32\n",
      "epis: 1237   score: 2.0   mem len: 231881   epsilon: 0.7389    steps: 198    lr: 4e-05     reward: 2.32\n",
      "epis: 1238   score: 2.0   mem len: 232096   epsilon: 0.7384    steps: 215    lr: 4e-05     reward: 2.34\n",
      "epis: 1239   score: 1.0   mem len: 232247   epsilon: 0.7381    steps: 151    lr: 4e-05     reward: 2.31\n",
      "epis: 1240   score: 2.0   mem len: 232445   epsilon: 0.7378    steps: 198    lr: 4e-05     reward: 2.3\n",
      "epis: 1241   score: 5.0   mem len: 232746   epsilon: 0.7372    steps: 301    lr: 4e-05     reward: 2.34\n",
      "epis: 1242   score: 2.0   mem len: 232928   epsilon: 0.7368    steps: 182    lr: 4e-05     reward: 2.34\n",
      "epis: 1243   score: 2.0   mem len: 233126   epsilon: 0.7364    steps: 198    lr: 4e-05     reward: 2.35\n",
      "epis: 1244   score: 1.0   mem len: 233277   epsilon: 0.7361    steps: 151    lr: 4e-05     reward: 2.35\n",
      "epis: 1245   score: 2.0   mem len: 233477   epsilon: 0.7357    steps: 200    lr: 4e-05     reward: 2.35\n",
      "epis: 1246   score: 2.0   mem len: 233659   epsilon: 0.7354    steps: 182    lr: 4e-05     reward: 2.35\n",
      "epis: 1247   score: 1.0   mem len: 233810   epsilon: 0.7351    steps: 151    lr: 4e-05     reward: 2.36\n",
      "epis: 1248   score: 3.0   mem len: 234057   epsilon: 0.7346    steps: 247    lr: 4e-05     reward: 2.36\n",
      "epis: 1249   score: 2.0   mem len: 234280   epsilon: 0.7341    steps: 223    lr: 4e-05     reward: 2.34\n",
      "epis: 1250   score: 3.0   mem len: 234527   epsilon: 0.7336    steps: 247    lr: 4e-05     reward: 2.36\n",
      "epis: 1251   score: 3.0   mem len: 234752   epsilon: 0.7332    steps: 225    lr: 4e-05     reward: 2.35\n",
      "epis: 1252   score: 2.0   mem len: 234950   epsilon: 0.7328    steps: 198    lr: 4e-05     reward: 2.35\n",
      "epis: 1253   score: 2.0   mem len: 235150   epsilon: 0.7324    steps: 200    lr: 4e-05     reward: 2.32\n",
      "epis: 1254   score: 1.0   mem len: 235301   epsilon: 0.7321    steps: 151    lr: 4e-05     reward: 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1255   score: 1.0   mem len: 235452   epsilon: 0.7318    steps: 151    lr: 4e-05     reward: 2.32\n",
      "epis: 1256   score: 3.0   mem len: 235678   epsilon: 0.7314    steps: 226    lr: 4e-05     reward: 2.33\n",
      "epis: 1257   score: 2.0   mem len: 235878   epsilon: 0.731    steps: 200    lr: 4e-05     reward: 2.33\n",
      "epis: 1258   score: 5.0   mem len: 236200   epsilon: 0.7303    steps: 322    lr: 4e-05     reward: 2.34\n",
      "epis: 1259   score: 5.0   mem len: 236534   epsilon: 0.7297    steps: 334    lr: 4e-05     reward: 2.37\n",
      "epis: 1260   score: 1.0   mem len: 236685   epsilon: 0.7294    steps: 151    lr: 4e-05     reward: 2.37\n",
      "epis: 1261   score: 3.0   mem len: 236911   epsilon: 0.7289    steps: 226    lr: 4e-05     reward: 2.34\n",
      "epis: 1262   score: 2.0   mem len: 237111   epsilon: 0.7285    steps: 200    lr: 4e-05     reward: 2.34\n",
      "epis: 1263   score: 7.0   mem len: 237527   epsilon: 0.7277    steps: 416    lr: 4e-05     reward: 2.37\n",
      "epis: 1264   score: 3.0   mem len: 237753   epsilon: 0.7272    steps: 226    lr: 4e-05     reward: 2.39\n",
      "epis: 1265   score: 2.0   mem len: 237953   epsilon: 0.7269    steps: 200    lr: 4e-05     reward: 2.39\n",
      "epis: 1266   score: 1.0   mem len: 238122   epsilon: 0.7265    steps: 169    lr: 4e-05     reward: 2.4\n",
      "epis: 1267   score: 1.0   mem len: 238273   epsilon: 0.7262    steps: 151    lr: 4e-05     reward: 2.39\n",
      "epis: 1268   score: 2.0   mem len: 238471   epsilon: 0.7258    steps: 198    lr: 4e-05     reward: 2.37\n",
      "epis: 1269   score: 3.0   mem len: 238717   epsilon: 0.7253    steps: 246    lr: 4e-05     reward: 2.37\n",
      "epis: 1270   score: 3.0   mem len: 238985   epsilon: 0.7248    steps: 268    lr: 4e-05     reward: 2.35\n",
      "epis: 1271   score: 5.0   mem len: 239327   epsilon: 0.7241    steps: 342    lr: 4e-05     reward: 2.38\n",
      "epis: 1272   score: 5.0   mem len: 239636   epsilon: 0.7235    steps: 309    lr: 4e-05     reward: 2.41\n",
      "epis: 1273   score: 0.0   mem len: 239759   epsilon: 0.7233    steps: 123    lr: 4e-05     reward: 2.38\n",
      "epis: 1274   score: 2.0   mem len: 239957   epsilon: 0.7229    steps: 198    lr: 4e-05     reward: 2.38\n",
      "epis: 1275   score: 2.0   mem len: 240155   epsilon: 0.7225    steps: 198    lr: 4e-05     reward: 2.38\n",
      "epis: 1276   score: 2.0   mem len: 240372   epsilon: 0.7221    steps: 217    lr: 4e-05     reward: 2.4\n",
      "epis: 1277   score: 2.0   mem len: 240553   epsilon: 0.7217    steps: 181    lr: 4e-05     reward: 2.4\n",
      "epis: 1278   score: 2.0   mem len: 240754   epsilon: 0.7213    steps: 201    lr: 4e-05     reward: 2.41\n",
      "epis: 1279   score: 1.0   mem len: 240923   epsilon: 0.721    steps: 169    lr: 4e-05     reward: 2.41\n",
      "epis: 1280   score: 4.0   mem len: 241186   epsilon: 0.7204    steps: 263    lr: 4e-05     reward: 2.39\n",
      "epis: 1281   score: 4.0   mem len: 241461   epsilon: 0.7199    steps: 275    lr: 4e-05     reward: 2.43\n",
      "epis: 1282   score: 2.0   mem len: 241659   epsilon: 0.7195    steps: 198    lr: 4e-05     reward: 2.42\n",
      "epis: 1283   score: 3.0   mem len: 241885   epsilon: 0.7191    steps: 226    lr: 4e-05     reward: 2.4\n",
      "epis: 1284   score: 6.0   mem len: 242278   epsilon: 0.7183    steps: 393    lr: 4e-05     reward: 2.41\n",
      "epis: 1285   score: 5.0   mem len: 242584   epsilon: 0.7177    steps: 306    lr: 4e-05     reward: 2.45\n",
      "epis: 1286   score: 1.0   mem len: 242735   epsilon: 0.7174    steps: 151    lr: 4e-05     reward: 2.44\n",
      "epis: 1287   score: 2.0   mem len: 242933   epsilon: 0.717    steps: 198    lr: 4e-05     reward: 2.43\n",
      "epis: 1288   score: 2.0   mem len: 243131   epsilon: 0.7166    steps: 198    lr: 4e-05     reward: 2.42\n",
      "epis: 1289   score: 1.0   mem len: 243282   epsilon: 0.7163    steps: 151    lr: 4e-05     reward: 2.4\n",
      "epis: 1290   score: 4.0   mem len: 243582   epsilon: 0.7157    steps: 300    lr: 4e-05     reward: 2.42\n",
      "epis: 1291   score: 3.0   mem len: 243829   epsilon: 0.7152    steps: 247    lr: 4e-05     reward: 2.43\n",
      "epis: 1292   score: 4.0   mem len: 244105   epsilon: 0.7147    steps: 276    lr: 4e-05     reward: 2.47\n",
      "epis: 1293   score: 1.0   mem len: 244256   epsilon: 0.7144    steps: 151    lr: 4e-05     reward: 2.45\n",
      "epis: 1294   score: 2.0   mem len: 244454   epsilon: 0.714    steps: 198    lr: 4e-05     reward: 2.45\n",
      "epis: 1295   score: 2.0   mem len: 244636   epsilon: 0.7136    steps: 182    lr: 4e-05     reward: 2.44\n",
      "epis: 1296   score: 3.0   mem len: 244865   epsilon: 0.7132    steps: 229    lr: 4e-05     reward: 2.43\n",
      "epis: 1297   score: 3.0   mem len: 245091   epsilon: 0.7127    steps: 226    lr: 4e-05     reward: 2.46\n",
      "epis: 1298   score: 2.0   mem len: 245309   epsilon: 0.7123    steps: 218    lr: 4e-05     reward: 2.47\n",
      "epis: 1299   score: 2.0   mem len: 245528   epsilon: 0.7119    steps: 219    lr: 4e-05     reward: 2.48\n",
      "epis: 1300   score: 1.0   mem len: 245679   epsilon: 0.7116    steps: 151    lr: 4e-05     reward: 2.49\n",
      "epis: 1301   score: 3.0   mem len: 245905   epsilon: 0.7111    steps: 226    lr: 4e-05     reward: 2.49\n",
      "epis: 1302   score: 2.0   mem len: 246102   epsilon: 0.7107    steps: 197    lr: 4e-05     reward: 2.51\n",
      "epis: 1303   score: 0.0   mem len: 246224   epsilon: 0.7105    steps: 122    lr: 4e-05     reward: 2.49\n",
      "epis: 1304   score: 2.0   mem len: 246421   epsilon: 0.7101    steps: 197    lr: 4e-05     reward: 2.48\n",
      "epis: 1305   score: 2.0   mem len: 246637   epsilon: 0.7097    steps: 216    lr: 4e-05     reward: 2.46\n",
      "epis: 1306   score: 3.0   mem len: 246887   epsilon: 0.7092    steps: 250    lr: 4e-05     reward: 2.47\n",
      "epis: 1307   score: 3.0   mem len: 247130   epsilon: 0.7087    steps: 243    lr: 4e-05     reward: 2.49\n",
      "epis: 1308   score: 6.0   mem len: 247464   epsilon: 0.708    steps: 334    lr: 4e-05     reward: 2.51\n",
      "epis: 1309   score: 4.0   mem len: 247758   epsilon: 0.7074    steps: 294    lr: 4e-05     reward: 2.52\n",
      "epis: 1310   score: 2.0   mem len: 247956   epsilon: 0.707    steps: 198    lr: 4e-05     reward: 2.54\n",
      "epis: 1311   score: 3.0   mem len: 248226   epsilon: 0.7065    steps: 270    lr: 4e-05     reward: 2.57\n",
      "epis: 1312   score: 5.0   mem len: 248550   epsilon: 0.7059    steps: 324    lr: 4e-05     reward: 2.62\n",
      "epis: 1313   score: 9.0   mem len: 248876   epsilon: 0.7052    steps: 326    lr: 4e-05     reward: 2.68\n",
      "epis: 1314   score: 0.0   mem len: 248998   epsilon: 0.705    steps: 122    lr: 4e-05     reward: 2.67\n",
      "epis: 1315   score: 2.0   mem len: 249216   epsilon: 0.7046    steps: 218    lr: 4e-05     reward: 2.61\n",
      "epis: 1316   score: 3.0   mem len: 249484   epsilon: 0.704    steps: 268    lr: 4e-05     reward: 2.62\n",
      "epis: 1317   score: 1.0   mem len: 249634   epsilon: 0.7037    steps: 150    lr: 4e-05     reward: 2.61\n",
      "epis: 1318   score: 4.0   mem len: 249908   epsilon: 0.7032    steps: 274    lr: 4e-05     reward: 2.64\n",
      "epis: 1319   score: 1.0   mem len: 250058   epsilon: 0.7029    steps: 150    lr: 4e-05     reward: 2.62\n",
      "epis: 1320   score: 1.0   mem len: 250227   epsilon: 0.7025    steps: 169    lr: 4e-05     reward: 2.59\n",
      "epis: 1321   score: 5.0   mem len: 250569   epsilon: 0.7019    steps: 342    lr: 4e-05     reward: 2.62\n",
      "epis: 1322   score: 2.0   mem len: 250788   epsilon: 0.7014    steps: 219    lr: 4e-05     reward: 2.59\n",
      "epis: 1323   score: 3.0   mem len: 251034   epsilon: 0.701    steps: 246    lr: 4e-05     reward: 2.59\n",
      "epis: 1324   score: 4.0   mem len: 251352   epsilon: 0.7003    steps: 318    lr: 4e-05     reward: 2.61\n",
      "epis: 1325   score: 2.0   mem len: 251570   epsilon: 0.6999    steps: 218    lr: 4e-05     reward: 2.61\n",
      "epis: 1326   score: 2.0   mem len: 251787   epsilon: 0.6995    steps: 217    lr: 4e-05     reward: 2.61\n",
      "epis: 1327   score: 2.0   mem len: 252005   epsilon: 0.699    steps: 218    lr: 4e-05     reward: 2.6\n",
      "epis: 1328   score: 2.0   mem len: 252185   epsilon: 0.6987    steps: 180    lr: 4e-05     reward: 2.58\n",
      "epis: 1329   score: 0.0   mem len: 252308   epsilon: 0.6984    steps: 123    lr: 4e-05     reward: 2.56\n",
      "epis: 1330   score: 1.0   mem len: 252476   epsilon: 0.6981    steps: 168    lr: 4e-05     reward: 2.54\n",
      "epis: 1331   score: 2.0   mem len: 252674   epsilon: 0.6977    steps: 198    lr: 4e-05     reward: 2.53\n",
      "epis: 1332   score: 1.0   mem len: 252843   epsilon: 0.6974    steps: 169    lr: 4e-05     reward: 2.52\n",
      "epis: 1333   score: 6.0   mem len: 253193   epsilon: 0.6967    steps: 350    lr: 4e-05     reward: 2.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1334   score: 5.0   mem len: 253539   epsilon: 0.696    steps: 346    lr: 4e-05     reward: 2.57\n",
      "epis: 1335   score: 2.0   mem len: 253736   epsilon: 0.6956    steps: 197    lr: 4e-05     reward: 2.57\n",
      "epis: 1336   score: 5.0   mem len: 254017   epsilon: 0.695    steps: 281    lr: 4e-05     reward: 2.6\n",
      "epis: 1337   score: 2.0   mem len: 254215   epsilon: 0.6947    steps: 198    lr: 4e-05     reward: 2.6\n",
      "epis: 1338   score: 5.0   mem len: 254540   epsilon: 0.694    steps: 325    lr: 4e-05     reward: 2.63\n",
      "epis: 1339   score: 6.0   mem len: 254935   epsilon: 0.6932    steps: 395    lr: 4e-05     reward: 2.68\n",
      "epis: 1340   score: 3.0   mem len: 255182   epsilon: 0.6927    steps: 247    lr: 4e-05     reward: 2.69\n",
      "epis: 1341   score: 1.0   mem len: 255333   epsilon: 0.6924    steps: 151    lr: 4e-05     reward: 2.65\n",
      "epis: 1342   score: 4.0   mem len: 255649   epsilon: 0.6918    steps: 316    lr: 4e-05     reward: 2.67\n",
      "epis: 1343   score: 3.0   mem len: 255875   epsilon: 0.6914    steps: 226    lr: 4e-05     reward: 2.68\n",
      "epis: 1344   score: 3.0   mem len: 256101   epsilon: 0.6909    steps: 226    lr: 4e-05     reward: 2.7\n",
      "epis: 1345   score: 3.0   mem len: 256364   epsilon: 0.6904    steps: 263    lr: 4e-05     reward: 2.71\n",
      "epis: 1346   score: 2.0   mem len: 256545   epsilon: 0.69    steps: 181    lr: 4e-05     reward: 2.71\n",
      "epis: 1347   score: 1.0   mem len: 256716   epsilon: 0.6897    steps: 171    lr: 4e-05     reward: 2.71\n",
      "epis: 1348   score: 2.0   mem len: 256936   epsilon: 0.6893    steps: 220    lr: 4e-05     reward: 2.7\n",
      "epis: 1349   score: 1.0   mem len: 257087   epsilon: 0.689    steps: 151    lr: 4e-05     reward: 2.69\n",
      "epis: 1350   score: 6.0   mem len: 257441   epsilon: 0.6883    steps: 354    lr: 4e-05     reward: 2.72\n",
      "epis: 1351   score: 4.0   mem len: 257744   epsilon: 0.6877    steps: 303    lr: 4e-05     reward: 2.73\n",
      "epis: 1352   score: 3.0   mem len: 257972   epsilon: 0.6872    steps: 228    lr: 4e-05     reward: 2.74\n",
      "epis: 1353   score: 3.0   mem len: 258198   epsilon: 0.6868    steps: 226    lr: 4e-05     reward: 2.75\n",
      "epis: 1354   score: 2.0   mem len: 258380   epsilon: 0.6864    steps: 182    lr: 4e-05     reward: 2.76\n",
      "epis: 1355   score: 4.0   mem len: 258677   epsilon: 0.6858    steps: 297    lr: 4e-05     reward: 2.79\n",
      "epis: 1356   score: 2.0   mem len: 258875   epsilon: 0.6854    steps: 198    lr: 4e-05     reward: 2.78\n",
      "epis: 1357   score: 1.0   mem len: 259047   epsilon: 0.6851    steps: 172    lr: 4e-05     reward: 2.77\n",
      "epis: 1358   score: 1.0   mem len: 259219   epsilon: 0.6847    steps: 172    lr: 4e-05     reward: 2.73\n",
      "epis: 1359   score: 4.0   mem len: 259493   epsilon: 0.6842    steps: 274    lr: 4e-05     reward: 2.72\n",
      "epis: 1360   score: 2.0   mem len: 259691   epsilon: 0.6838    steps: 198    lr: 4e-05     reward: 2.73\n",
      "epis: 1361   score: 1.0   mem len: 259842   epsilon: 0.6835    steps: 151    lr: 4e-05     reward: 2.71\n",
      "epis: 1362   score: 2.0   mem len: 260060   epsilon: 0.6831    steps: 218    lr: 4e-05     reward: 2.71\n",
      "epis: 1363   score: 1.0   mem len: 260211   epsilon: 0.6828    steps: 151    lr: 4e-05     reward: 2.65\n",
      "epis: 1364   score: 3.0   mem len: 260457   epsilon: 0.6823    steps: 246    lr: 4e-05     reward: 2.65\n",
      "epis: 1365   score: 2.0   mem len: 260639   epsilon: 0.6819    steps: 182    lr: 4e-05     reward: 2.65\n",
      "epis: 1366   score: 7.0   mem len: 261014   epsilon: 0.6812    steps: 375    lr: 4e-05     reward: 2.71\n",
      "epis: 1367   score: 3.0   mem len: 261243   epsilon: 0.6807    steps: 229    lr: 4e-05     reward: 2.73\n",
      "epis: 1368   score: 2.0   mem len: 261441   epsilon: 0.6803    steps: 198    lr: 4e-05     reward: 2.73\n",
      "epis: 1369   score: 3.0   mem len: 261688   epsilon: 0.6799    steps: 247    lr: 4e-05     reward: 2.73\n",
      "epis: 1370   score: 2.0   mem len: 261870   epsilon: 0.6795    steps: 182    lr: 4e-05     reward: 2.72\n",
      "epis: 1371   score: 2.0   mem len: 262086   epsilon: 0.6791    steps: 216    lr: 4e-05     reward: 2.69\n",
      "epis: 1372   score: 1.0   mem len: 262256   epsilon: 0.6787    steps: 170    lr: 4e-05     reward: 2.65\n",
      "epis: 1373   score: 2.0   mem len: 262454   epsilon: 0.6783    steps: 198    lr: 4e-05     reward: 2.67\n",
      "epis: 1374   score: 5.0   mem len: 262779   epsilon: 0.6777    steps: 325    lr: 4e-05     reward: 2.7\n",
      "epis: 1375   score: 4.0   mem len: 263054   epsilon: 0.6772    steps: 275    lr: 4e-05     reward: 2.72\n",
      "epis: 1376   score: 2.0   mem len: 263254   epsilon: 0.6768    steps: 200    lr: 4e-05     reward: 2.72\n",
      "epis: 1377   score: 3.0   mem len: 263499   epsilon: 0.6763    steps: 245    lr: 4e-05     reward: 2.73\n",
      "epis: 1378   score: 3.0   mem len: 263742   epsilon: 0.6758    steps: 243    lr: 4e-05     reward: 2.74\n",
      "epis: 1379   score: 3.0   mem len: 263989   epsilon: 0.6753    steps: 247    lr: 4e-05     reward: 2.76\n",
      "epis: 1380   score: 2.0   mem len: 264187   epsilon: 0.6749    steps: 198    lr: 4e-05     reward: 2.74\n",
      "epis: 1381   score: 3.0   mem len: 264433   epsilon: 0.6744    steps: 246    lr: 4e-05     reward: 2.73\n",
      "epis: 1382   score: 5.0   mem len: 264757   epsilon: 0.6738    steps: 324    lr: 4e-05     reward: 2.76\n",
      "epis: 1383   score: 2.0   mem len: 264938   epsilon: 0.6734    steps: 181    lr: 4e-05     reward: 2.75\n",
      "epis: 1384   score: 3.0   mem len: 265185   epsilon: 0.6729    steps: 247    lr: 4e-05     reward: 2.72\n",
      "epis: 1385   score: 2.0   mem len: 265367   epsilon: 0.6726    steps: 182    lr: 4e-05     reward: 2.69\n",
      "epis: 1386   score: 2.0   mem len: 265549   epsilon: 0.6722    steps: 182    lr: 4e-05     reward: 2.7\n",
      "epis: 1387   score: 2.0   mem len: 265747   epsilon: 0.6718    steps: 198    lr: 4e-05     reward: 2.7\n",
      "epis: 1388   score: 3.0   mem len: 265973   epsilon: 0.6714    steps: 226    lr: 4e-05     reward: 2.71\n",
      "epis: 1389   score: 2.0   mem len: 266171   epsilon: 0.671    steps: 198    lr: 4e-05     reward: 2.72\n",
      "epis: 1390   score: 4.0   mem len: 266471   epsilon: 0.6704    steps: 300    lr: 4e-05     reward: 2.72\n",
      "epis: 1391   score: 1.0   mem len: 266622   epsilon: 0.6701    steps: 151    lr: 4e-05     reward: 2.7\n",
      "epis: 1392   score: 2.0   mem len: 266820   epsilon: 0.6697    steps: 198    lr: 4e-05     reward: 2.68\n",
      "epis: 1393   score: 2.0   mem len: 267018   epsilon: 0.6693    steps: 198    lr: 4e-05     reward: 2.69\n",
      "epis: 1394   score: 2.0   mem len: 267215   epsilon: 0.6689    steps: 197    lr: 4e-05     reward: 2.69\n",
      "epis: 1395   score: 2.0   mem len: 267412   epsilon: 0.6685    steps: 197    lr: 4e-05     reward: 2.69\n",
      "epis: 1396   score: 5.0   mem len: 267709   epsilon: 0.6679    steps: 297    lr: 4e-05     reward: 2.71\n",
      "epis: 1397   score: 2.0   mem len: 267907   epsilon: 0.6675    steps: 198    lr: 4e-05     reward: 2.7\n",
      "epis: 1398   score: 4.0   mem len: 268203   epsilon: 0.667    steps: 296    lr: 4e-05     reward: 2.72\n",
      "epis: 1399   score: 7.0   mem len: 268575   epsilon: 0.6662    steps: 372    lr: 4e-05     reward: 2.77\n",
      "epis: 1400   score: 3.0   mem len: 268803   epsilon: 0.6658    steps: 228    lr: 4e-05     reward: 2.79\n",
      "epis: 1401   score: 2.0   mem len: 269001   epsilon: 0.6654    steps: 198    lr: 4e-05     reward: 2.78\n",
      "epis: 1402   score: 4.0   mem len: 269276   epsilon: 0.6648    steps: 275    lr: 4e-05     reward: 2.8\n",
      "epis: 1403   score: 11.0   mem len: 269711   epsilon: 0.664    steps: 435    lr: 4e-05     reward: 2.91\n",
      "epis: 1404   score: 3.0   mem len: 269978   epsilon: 0.6634    steps: 267    lr: 4e-05     reward: 2.92\n",
      "epis: 1405   score: 6.0   mem len: 270351   epsilon: 0.6627    steps: 373    lr: 4e-05     reward: 2.96\n",
      "epis: 1406   score: 4.0   mem len: 270629   epsilon: 0.6622    steps: 278    lr: 4e-05     reward: 2.97\n",
      "epis: 1407   score: 3.0   mem len: 270860   epsilon: 0.6617    steps: 231    lr: 4e-05     reward: 2.97\n",
      "epis: 1408   score: 4.0   mem len: 271174   epsilon: 0.6611    steps: 314    lr: 4e-05     reward: 2.95\n",
      "epis: 1409   score: 3.0   mem len: 271421   epsilon: 0.6606    steps: 247    lr: 4e-05     reward: 2.94\n",
      "epis: 1410   score: 3.0   mem len: 271672   epsilon: 0.6601    steps: 251    lr: 4e-05     reward: 2.95\n",
      "epis: 1411   score: 3.0   mem len: 271942   epsilon: 0.6596    steps: 270    lr: 4e-05     reward: 2.95\n",
      "epis: 1412   score: 4.0   mem len: 272240   epsilon: 0.659    steps: 298    lr: 4e-05     reward: 2.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1413   score: 1.0   mem len: 272390   epsilon: 0.6587    steps: 150    lr: 4e-05     reward: 2.86\n",
      "epis: 1414   score: 4.0   mem len: 272666   epsilon: 0.6581    steps: 276    lr: 4e-05     reward: 2.9\n",
      "epis: 1415   score: 4.0   mem len: 272941   epsilon: 0.6576    steps: 275    lr: 4e-05     reward: 2.92\n",
      "epis: 1416   score: 3.0   mem len: 273191   epsilon: 0.6571    steps: 250    lr: 4e-05     reward: 2.92\n",
      "epis: 1417   score: 2.0   mem len: 273370   epsilon: 0.6567    steps: 179    lr: 4e-05     reward: 2.93\n",
      "epis: 1418   score: 2.0   mem len: 273568   epsilon: 0.6563    steps: 198    lr: 4e-05     reward: 2.91\n",
      "epis: 1419   score: 2.0   mem len: 273749   epsilon: 0.656    steps: 181    lr: 4e-05     reward: 2.92\n",
      "epis: 1420   score: 0.0   mem len: 273872   epsilon: 0.6557    steps: 123    lr: 4e-05     reward: 2.91\n",
      "epis: 1421   score: 4.0   mem len: 274148   epsilon: 0.6552    steps: 276    lr: 4e-05     reward: 2.9\n",
      "epis: 1422   score: 4.0   mem len: 274403   epsilon: 0.6547    steps: 255    lr: 4e-05     reward: 2.92\n",
      "epis: 1423   score: 3.0   mem len: 274648   epsilon: 0.6542    steps: 245    lr: 4e-05     reward: 2.92\n",
      "epis: 1424   score: 4.0   mem len: 274945   epsilon: 0.6536    steps: 297    lr: 4e-05     reward: 2.92\n",
      "epis: 1425   score: 4.0   mem len: 275222   epsilon: 0.6531    steps: 277    lr: 4e-05     reward: 2.94\n",
      "epis: 1426   score: 3.0   mem len: 275448   epsilon: 0.6526    steps: 226    lr: 4e-05     reward: 2.95\n",
      "epis: 1427   score: 3.0   mem len: 275694   epsilon: 0.6521    steps: 246    lr: 4e-05     reward: 2.96\n",
      "epis: 1428   score: 2.0   mem len: 275875   epsilon: 0.6518    steps: 181    lr: 4e-05     reward: 2.96\n",
      "epis: 1429   score: 6.0   mem len: 276231   epsilon: 0.6511    steps: 356    lr: 4e-05     reward: 3.02\n",
      "epis: 1430   score: 4.0   mem len: 276489   epsilon: 0.6505    steps: 258    lr: 4e-05     reward: 3.05\n",
      "epis: 1431   score: 5.0   mem len: 276793   epsilon: 0.6499    steps: 304    lr: 4e-05     reward: 3.08\n",
      "epis: 1432   score: 2.0   mem len: 276991   epsilon: 0.6496    steps: 198    lr: 4e-05     reward: 3.09\n",
      "epis: 1433   score: 3.0   mem len: 277217   epsilon: 0.6491    steps: 226    lr: 4e-05     reward: 3.06\n",
      "epis: 1434   score: 4.0   mem len: 277492   epsilon: 0.6486    steps: 275    lr: 4e-05     reward: 3.05\n",
      "epis: 1435   score: 2.0   mem len: 277690   epsilon: 0.6482    steps: 198    lr: 4e-05     reward: 3.05\n",
      "epis: 1436   score: 2.0   mem len: 277890   epsilon: 0.6478    steps: 200    lr: 4e-05     reward: 3.02\n",
      "epis: 1437   score: 6.0   mem len: 278263   epsilon: 0.647    steps: 373    lr: 4e-05     reward: 3.06\n",
      "epis: 1438   score: 1.0   mem len: 278432   epsilon: 0.6467    steps: 169    lr: 4e-05     reward: 3.02\n",
      "epis: 1439   score: 5.0   mem len: 278728   epsilon: 0.6461    steps: 296    lr: 4e-05     reward: 3.01\n",
      "epis: 1440   score: 5.0   mem len: 279052   epsilon: 0.6455    steps: 324    lr: 4e-05     reward: 3.03\n",
      "epis: 1441   score: 6.0   mem len: 279425   epsilon: 0.6447    steps: 373    lr: 4e-05     reward: 3.08\n",
      "epis: 1442   score: 6.0   mem len: 279759   epsilon: 0.6441    steps: 334    lr: 4e-05     reward: 3.1\n",
      "epis: 1443   score: 7.0   mem len: 280149   epsilon: 0.6433    steps: 390    lr: 4e-05     reward: 3.14\n",
      "epis: 1444   score: 0.0   mem len: 280272   epsilon: 0.6431    steps: 123    lr: 4e-05     reward: 3.11\n",
      "epis: 1445   score: 3.0   mem len: 280519   epsilon: 0.6426    steps: 247    lr: 4e-05     reward: 3.11\n",
      "epis: 1446   score: 3.0   mem len: 280744   epsilon: 0.6421    steps: 225    lr: 4e-05     reward: 3.12\n",
      "epis: 1447   score: 0.0   mem len: 280866   epsilon: 0.6419    steps: 122    lr: 4e-05     reward: 3.11\n",
      "epis: 1448   score: 3.0   mem len: 281092   epsilon: 0.6414    steps: 226    lr: 4e-05     reward: 3.12\n",
      "epis: 1449   score: 4.0   mem len: 281389   epsilon: 0.6408    steps: 297    lr: 4e-05     reward: 3.15\n",
      "epis: 1450   score: 5.0   mem len: 281718   epsilon: 0.6402    steps: 329    lr: 4e-05     reward: 3.14\n",
      "epis: 1451   score: 0.0   mem len: 281840   epsilon: 0.64    steps: 122    lr: 4e-05     reward: 3.1\n",
      "epis: 1452   score: 3.0   mem len: 282066   epsilon: 0.6395    steps: 226    lr: 4e-05     reward: 3.1\n",
      "epis: 1453   score: 2.0   mem len: 282266   epsilon: 0.6391    steps: 200    lr: 4e-05     reward: 3.09\n",
      "epis: 1454   score: 4.0   mem len: 282525   epsilon: 0.6386    steps: 259    lr: 4e-05     reward: 3.11\n",
      "epis: 1455   score: 3.0   mem len: 282754   epsilon: 0.6381    steps: 229    lr: 4e-05     reward: 3.1\n",
      "epis: 1456   score: 4.0   mem len: 283049   epsilon: 0.6376    steps: 295    lr: 4e-05     reward: 3.12\n",
      "epis: 1457   score: 5.0   mem len: 283393   epsilon: 0.6369    steps: 344    lr: 4e-05     reward: 3.16\n",
      "epis: 1458   score: 3.0   mem len: 283622   epsilon: 0.6364    steps: 229    lr: 4e-05     reward: 3.18\n",
      "epis: 1459   score: 1.0   mem len: 283773   epsilon: 0.6361    steps: 151    lr: 4e-05     reward: 3.15\n",
      "epis: 1460   score: 3.0   mem len: 284021   epsilon: 0.6356    steps: 248    lr: 4e-05     reward: 3.16\n",
      "epis: 1461   score: 4.0   mem len: 284316   epsilon: 0.6351    steps: 295    lr: 4e-05     reward: 3.19\n",
      "epis: 1462   score: 3.0   mem len: 284547   epsilon: 0.6346    steps: 231    lr: 4e-05     reward: 3.2\n",
      "epis: 1463   score: 3.0   mem len: 284776   epsilon: 0.6341    steps: 229    lr: 4e-05     reward: 3.22\n",
      "epis: 1464   score: 8.0   mem len: 285174   epsilon: 0.6334    steps: 398    lr: 4e-05     reward: 3.27\n",
      "epis: 1465   score: 2.0   mem len: 285374   epsilon: 0.633    steps: 200    lr: 4e-05     reward: 3.27\n",
      "epis: 1466   score: 3.0   mem len: 285600   epsilon: 0.6325    steps: 226    lr: 4e-05     reward: 3.23\n",
      "epis: 1467   score: 0.0   mem len: 285723   epsilon: 0.6323    steps: 123    lr: 4e-05     reward: 3.2\n",
      "epis: 1468   score: 3.0   mem len: 285949   epsilon: 0.6318    steps: 226    lr: 4e-05     reward: 3.21\n",
      "epis: 1469   score: 0.0   mem len: 286072   epsilon: 0.6316    steps: 123    lr: 4e-05     reward: 3.18\n",
      "epis: 1470   score: 2.0   mem len: 286270   epsilon: 0.6312    steps: 198    lr: 4e-05     reward: 3.18\n",
      "epis: 1471   score: 3.0   mem len: 286495   epsilon: 0.6307    steps: 225    lr: 4e-05     reward: 3.19\n",
      "epis: 1472   score: 2.0   mem len: 286692   epsilon: 0.6303    steps: 197    lr: 4e-05     reward: 3.2\n",
      "epis: 1473   score: 2.0   mem len: 286872   epsilon: 0.63    steps: 180    lr: 4e-05     reward: 3.2\n",
      "epis: 1474   score: 3.0   mem len: 287102   epsilon: 0.6295    steps: 230    lr: 4e-05     reward: 3.18\n",
      "epis: 1475   score: 6.0   mem len: 287465   epsilon: 0.6288    steps: 363    lr: 4e-05     reward: 3.2\n",
      "epis: 1476   score: 6.0   mem len: 287805   epsilon: 0.6281    steps: 340    lr: 4e-05     reward: 3.24\n",
      "epis: 1477   score: 3.0   mem len: 288036   epsilon: 0.6277    steps: 231    lr: 4e-05     reward: 3.24\n",
      "epis: 1478   score: 5.0   mem len: 288381   epsilon: 0.627    steps: 345    lr: 4e-05     reward: 3.26\n",
      "epis: 1479   score: 4.0   mem len: 288638   epsilon: 0.6265    steps: 257    lr: 4e-05     reward: 3.27\n",
      "epis: 1480   score: 3.0   mem len: 288863   epsilon: 0.626    steps: 225    lr: 4e-05     reward: 3.28\n",
      "epis: 1481   score: 4.0   mem len: 289138   epsilon: 0.6255    steps: 275    lr: 4e-05     reward: 3.29\n",
      "epis: 1482   score: 2.0   mem len: 289320   epsilon: 0.6251    steps: 182    lr: 4e-05     reward: 3.26\n",
      "epis: 1483   score: 2.0   mem len: 289517   epsilon: 0.6248    steps: 197    lr: 4e-05     reward: 3.26\n",
      "epis: 1484   score: 2.0   mem len: 289715   epsilon: 0.6244    steps: 198    lr: 4e-05     reward: 3.25\n",
      "epis: 1485   score: 0.0   mem len: 289838   epsilon: 0.6241    steps: 123    lr: 4e-05     reward: 3.23\n",
      "epis: 1486   score: 4.0   mem len: 290134   epsilon: 0.6235    steps: 296    lr: 4e-05     reward: 3.25\n",
      "epis: 1487   score: 2.0   mem len: 290334   epsilon: 0.6231    steps: 200    lr: 4e-05     reward: 3.25\n",
      "epis: 1488   score: 3.0   mem len: 290560   epsilon: 0.6227    steps: 226    lr: 4e-05     reward: 3.25\n",
      "epis: 1489   score: 2.0   mem len: 290757   epsilon: 0.6223    steps: 197    lr: 4e-05     reward: 3.25\n",
      "epis: 1490   score: 6.0   mem len: 291114   epsilon: 0.6216    steps: 357    lr: 4e-05     reward: 3.27\n",
      "epis: 1491   score: 5.0   mem len: 291429   epsilon: 0.621    steps: 315    lr: 4e-05     reward: 3.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1492   score: 3.0   mem len: 291655   epsilon: 0.6205    steps: 226    lr: 4e-05     reward: 3.32\n",
      "epis: 1493   score: 4.0   mem len: 291935   epsilon: 0.62    steps: 280    lr: 4e-05     reward: 3.34\n",
      "epis: 1494   score: 3.0   mem len: 292160   epsilon: 0.6195    steps: 225    lr: 4e-05     reward: 3.35\n",
      "epis: 1495   score: 4.0   mem len: 292453   epsilon: 0.6189    steps: 293    lr: 4e-05     reward: 3.37\n",
      "epis: 1496   score: 3.0   mem len: 292682   epsilon: 0.6185    steps: 229    lr: 4e-05     reward: 3.35\n",
      "epis: 1497   score: 2.0   mem len: 292880   epsilon: 0.6181    steps: 198    lr: 4e-05     reward: 3.35\n",
      "epis: 1498   score: 3.0   mem len: 293106   epsilon: 0.6176    steps: 226    lr: 4e-05     reward: 3.34\n",
      "epis: 1499   score: 3.0   mem len: 293332   epsilon: 0.6172    steps: 226    lr: 4e-05     reward: 3.3\n",
      "epis: 1500   score: 5.0   mem len: 293620   epsilon: 0.6166    steps: 288    lr: 4e-05     reward: 3.32\n",
      "epis: 1501   score: 1.0   mem len: 293771   epsilon: 0.6163    steps: 151    lr: 4e-05     reward: 3.31\n",
      "epis: 1502   score: 2.0   mem len: 293952   epsilon: 0.616    steps: 181    lr: 4e-05     reward: 3.29\n",
      "epis: 1503   score: 4.0   mem len: 294248   epsilon: 0.6154    steps: 296    lr: 4e-05     reward: 3.22\n",
      "epis: 1504   score: 4.0   mem len: 294523   epsilon: 0.6148    steps: 275    lr: 4e-05     reward: 3.23\n",
      "epis: 1505   score: 3.0   mem len: 294768   epsilon: 0.6144    steps: 245    lr: 4e-05     reward: 3.2\n",
      "epis: 1506   score: 1.0   mem len: 294918   epsilon: 0.6141    steps: 150    lr: 4e-05     reward: 3.17\n",
      "epis: 1507   score: 4.0   mem len: 295193   epsilon: 0.6135    steps: 275    lr: 4e-05     reward: 3.18\n",
      "epis: 1508   score: 4.0   mem len: 295451   epsilon: 0.613    steps: 258    lr: 4e-05     reward: 3.18\n",
      "epis: 1509   score: 2.0   mem len: 295648   epsilon: 0.6126    steps: 197    lr: 4e-05     reward: 3.17\n",
      "epis: 1510   score: 5.0   mem len: 295992   epsilon: 0.6119    steps: 344    lr: 4e-05     reward: 3.19\n",
      "epis: 1511   score: 3.0   mem len: 296224   epsilon: 0.6115    steps: 232    lr: 4e-05     reward: 3.19\n",
      "epis: 1512   score: 5.0   mem len: 296547   epsilon: 0.6108    steps: 323    lr: 4e-05     reward: 3.2\n",
      "epis: 1513   score: 3.0   mem len: 296773   epsilon: 0.6104    steps: 226    lr: 4e-05     reward: 3.22\n",
      "epis: 1514   score: 1.0   mem len: 296924   epsilon: 0.6101    steps: 151    lr: 4e-05     reward: 3.19\n",
      "epis: 1515   score: 3.0   mem len: 297136   epsilon: 0.6097    steps: 212    lr: 4e-05     reward: 3.18\n",
      "epis: 1516   score: 8.0   mem len: 297444   epsilon: 0.6091    steps: 308    lr: 4e-05     reward: 3.23\n",
      "epis: 1517   score: 5.0   mem len: 297759   epsilon: 0.6084    steps: 315    lr: 4e-05     reward: 3.26\n",
      "epis: 1518   score: 2.0   mem len: 297941   epsilon: 0.6081    steps: 182    lr: 4e-05     reward: 3.26\n",
      "epis: 1519   score: 2.0   mem len: 298122   epsilon: 0.6077    steps: 181    lr: 4e-05     reward: 3.26\n",
      "epis: 1520   score: 3.0   mem len: 298347   epsilon: 0.6073    steps: 225    lr: 4e-05     reward: 3.29\n",
      "epis: 1521   score: 1.0   mem len: 298497   epsilon: 0.607    steps: 150    lr: 4e-05     reward: 3.26\n",
      "epis: 1522   score: 3.0   mem len: 298743   epsilon: 0.6065    steps: 246    lr: 4e-05     reward: 3.25\n",
      "epis: 1523   score: 3.0   mem len: 298969   epsilon: 0.606    steps: 226    lr: 4e-05     reward: 3.25\n",
      "epis: 1524   score: 1.0   mem len: 299140   epsilon: 0.6057    steps: 171    lr: 4e-05     reward: 3.22\n",
      "epis: 1525   score: 3.0   mem len: 299386   epsilon: 0.6052    steps: 246    lr: 4e-05     reward: 3.21\n",
      "epis: 1526   score: 3.0   mem len: 299614   epsilon: 0.6048    steps: 228    lr: 4e-05     reward: 3.21\n",
      "epis: 1527   score: 4.0   mem len: 299889   epsilon: 0.6042    steps: 275    lr: 4e-05     reward: 3.22\n",
      "epis: 1528   score: 3.0   mem len: 300115   epsilon: 0.6038    steps: 226    lr: 1.6e-05     reward: 3.23\n",
      "epis: 1529   score: 4.0   mem len: 300412   epsilon: 0.6032    steps: 297    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1530   score: 2.0   mem len: 300594   epsilon: 0.6028    steps: 182    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1531   score: 4.0   mem len: 300869   epsilon: 0.6023    steps: 275    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1532   score: 4.0   mem len: 301164   epsilon: 0.6017    steps: 295    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1533   score: 4.0   mem len: 301459   epsilon: 0.6011    steps: 295    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1534   score: 3.0   mem len: 301685   epsilon: 0.6007    steps: 226    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1535   score: 1.0   mem len: 301854   epsilon: 0.6003    steps: 169    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1536   score: 4.0   mem len: 302152   epsilon: 0.5997    steps: 298    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1537   score: 5.0   mem len: 302455   epsilon: 0.5991    steps: 303    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1538   score: 3.0   mem len: 302701   epsilon: 0.5987    steps: 246    lr: 1.6e-05     reward: 3.22\n",
      "epis: 1539   score: 3.0   mem len: 302927   epsilon: 0.5982    steps: 226    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1540   score: 2.0   mem len: 303109   epsilon: 0.5978    steps: 182    lr: 1.6e-05     reward: 3.17\n",
      "epis: 1541   score: 4.0   mem len: 303397   epsilon: 0.5973    steps: 288    lr: 1.6e-05     reward: 3.15\n",
      "epis: 1542   score: 1.0   mem len: 303548   epsilon: 0.597    steps: 151    lr: 1.6e-05     reward: 3.1\n",
      "epis: 1543   score: 3.0   mem len: 303773   epsilon: 0.5965    steps: 225    lr: 1.6e-05     reward: 3.06\n",
      "epis: 1544   score: 2.0   mem len: 303973   epsilon: 0.5961    steps: 200    lr: 1.6e-05     reward: 3.08\n",
      "epis: 1545   score: 3.0   mem len: 304217   epsilon: 0.5956    steps: 244    lr: 1.6e-05     reward: 3.08\n",
      "epis: 1546   score: 4.0   mem len: 304492   epsilon: 0.5951    steps: 275    lr: 1.6e-05     reward: 3.09\n",
      "epis: 1547   score: 4.0   mem len: 304767   epsilon: 0.5946    steps: 275    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1548   score: 3.0   mem len: 304994   epsilon: 0.5941    steps: 227    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1549   score: 4.0   mem len: 305288   epsilon: 0.5935    steps: 294    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1550   score: 2.0   mem len: 305470   epsilon: 0.5932    steps: 182    lr: 1.6e-05     reward: 3.1\n",
      "epis: 1551   score: 4.0   mem len: 305745   epsilon: 0.5926    steps: 275    lr: 1.6e-05     reward: 3.14\n",
      "epis: 1552   score: 2.0   mem len: 305925   epsilon: 0.5923    steps: 180    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1553   score: 2.0   mem len: 306123   epsilon: 0.5919    steps: 198    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1554   score: 4.0   mem len: 306397   epsilon: 0.5913    steps: 274    lr: 1.6e-05     reward: 3.13\n",
      "epis: 1555   score: 6.0   mem len: 306740   epsilon: 0.5907    steps: 343    lr: 1.6e-05     reward: 3.16\n",
      "epis: 1556   score: 4.0   mem len: 307035   epsilon: 0.5901    steps: 295    lr: 1.6e-05     reward: 3.16\n",
      "epis: 1557   score: 4.0   mem len: 307331   epsilon: 0.5895    steps: 296    lr: 1.6e-05     reward: 3.15\n",
      "epis: 1558   score: 5.0   mem len: 307652   epsilon: 0.5888    steps: 321    lr: 1.6e-05     reward: 3.17\n",
      "epis: 1559   score: 4.0   mem len: 307927   epsilon: 0.5883    steps: 275    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1560   score: 2.0   mem len: 308125   epsilon: 0.5879    steps: 198    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1561   score: 4.0   mem len: 308400   epsilon: 0.5874    steps: 275    lr: 1.6e-05     reward: 3.19\n",
      "epis: 1562   score: 5.0   mem len: 308688   epsilon: 0.5868    steps: 288    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1563   score: 4.0   mem len: 308966   epsilon: 0.5862    steps: 278    lr: 1.6e-05     reward: 3.22\n",
      "epis: 1564   score: 4.0   mem len: 309241   epsilon: 0.5857    steps: 275    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1565   score: 2.0   mem len: 309423   epsilon: 0.5853    steps: 182    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1566   score: 3.0   mem len: 309649   epsilon: 0.5849    steps: 226    lr: 1.6e-05     reward: 3.18\n",
      "epis: 1567   score: 3.0   mem len: 309875   epsilon: 0.5844    steps: 226    lr: 1.6e-05     reward: 3.21\n",
      "epis: 1568   score: 2.0   mem len: 310076   epsilon: 0.584    steps: 201    lr: 1.6e-05     reward: 3.2\n",
      "epis: 1569   score: 3.0   mem len: 310342   epsilon: 0.5835    steps: 266    lr: 1.6e-05     reward: 3.23\n",
      "epis: 1570   score: 5.0   mem len: 310631   epsilon: 0.5829    steps: 289    lr: 1.6e-05     reward: 3.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1571   score: 2.0   mem len: 310811   epsilon: 0.5826    steps: 180    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1572   score: 5.0   mem len: 311127   epsilon: 0.582    steps: 316    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1573   score: 2.0   mem len: 311309   epsilon: 0.5816    steps: 182    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1574   score: 4.0   mem len: 311603   epsilon: 0.581    steps: 294    lr: 1.6e-05     reward: 3.29\n",
      "epis: 1575   score: 3.0   mem len: 311816   epsilon: 0.5806    steps: 213    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1576   score: 5.0   mem len: 312120   epsilon: 0.58    steps: 304    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1577   score: 5.0   mem len: 312446   epsilon: 0.5794    steps: 326    lr: 1.6e-05     reward: 3.27\n",
      "epis: 1578   score: 4.0   mem len: 312721   epsilon: 0.5788    steps: 275    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1579   score: 3.0   mem len: 312947   epsilon: 0.5784    steps: 226    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1580   score: 6.0   mem len: 313310   epsilon: 0.5776    steps: 363    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1581   score: 2.0   mem len: 313508   epsilon: 0.5773    steps: 198    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1582   score: 1.0   mem len: 313658   epsilon: 0.577    steps: 150    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1583   score: 4.0   mem len: 313933   epsilon: 0.5764    steps: 275    lr: 1.6e-05     reward: 3.27\n",
      "epis: 1584   score: 5.0   mem len: 314240   epsilon: 0.5758    steps: 307    lr: 1.6e-05     reward: 3.3\n",
      "epis: 1585   score: 1.0   mem len: 314390   epsilon: 0.5755    steps: 150    lr: 1.6e-05     reward: 3.31\n",
      "epis: 1586   score: 5.0   mem len: 314704   epsilon: 0.5749    steps: 314    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1587   score: 3.0   mem len: 314930   epsilon: 0.5744    steps: 226    lr: 1.6e-05     reward: 3.33\n",
      "epis: 1588   score: 2.0   mem len: 315111   epsilon: 0.5741    steps: 181    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1589   score: 2.0   mem len: 315291   epsilon: 0.5737    steps: 180    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1590   score: 2.0   mem len: 315492   epsilon: 0.5733    steps: 201    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1591   score: 5.0   mem len: 315818   epsilon: 0.5727    steps: 326    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1592   score: 3.0   mem len: 316031   epsilon: 0.5723    steps: 213    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1593   score: 5.0   mem len: 316339   epsilon: 0.5716    steps: 308    lr: 1.6e-05     reward: 3.29\n",
      "epis: 1594   score: 1.0   mem len: 316490   epsilon: 0.5713    steps: 151    lr: 1.6e-05     reward: 3.27\n",
      "epis: 1595   score: 3.0   mem len: 316720   epsilon: 0.5709    steps: 230    lr: 1.6e-05     reward: 3.26\n",
      "epis: 1596   score: 2.0   mem len: 316938   epsilon: 0.5705    steps: 218    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1597   score: 2.0   mem len: 317120   epsilon: 0.5701    steps: 182    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1598   score: 3.0   mem len: 317346   epsilon: 0.5697    steps: 226    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1599   score: 3.0   mem len: 317590   epsilon: 0.5692    steps: 244    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1600   score: 5.0   mem len: 317894   epsilon: 0.5686    steps: 304    lr: 1.6e-05     reward: 3.25\n",
      "epis: 1601   score: 4.0   mem len: 318169   epsilon: 0.568    steps: 275    lr: 1.6e-05     reward: 3.28\n",
      "epis: 1602   score: 4.0   mem len: 318450   epsilon: 0.5675    steps: 281    lr: 1.6e-05     reward: 3.3\n",
      "epis: 1603   score: 6.0   mem len: 318823   epsilon: 0.5667    steps: 373    lr: 1.6e-05     reward: 3.32\n",
      "epis: 1604   score: 5.0   mem len: 319144   epsilon: 0.5661    steps: 321    lr: 1.6e-05     reward: 3.33\n",
      "epis: 1605   score: 4.0   mem len: 319425   epsilon: 0.5655    steps: 281    lr: 1.6e-05     reward: 3.34\n",
      "epis: 1606   score: 3.0   mem len: 319656   epsilon: 0.5651    steps: 231    lr: 1.6e-05     reward: 3.36\n",
      "epis: 1607   score: 11.0   mem len: 320080   epsilon: 0.5642    steps: 424    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1608   score: 3.0   mem len: 320330   epsilon: 0.5637    steps: 250    lr: 1.6e-05     reward: 3.42\n",
      "epis: 1609   score: 4.0   mem len: 320608   epsilon: 0.5632    steps: 278    lr: 1.6e-05     reward: 3.44\n",
      "epis: 1610   score: 4.0   mem len: 320882   epsilon: 0.5627    steps: 274    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1611   score: 4.0   mem len: 321158   epsilon: 0.5621    steps: 276    lr: 1.6e-05     reward: 3.44\n",
      "epis: 1612   score: 1.0   mem len: 321309   epsilon: 0.5618    steps: 151    lr: 1.6e-05     reward: 3.4\n",
      "epis: 1613   score: 6.0   mem len: 321660   epsilon: 0.5611    steps: 351    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1614   score: 6.0   mem len: 322015   epsilon: 0.5604    steps: 355    lr: 1.6e-05     reward: 3.48\n",
      "epis: 1615   score: 4.0   mem len: 322290   epsilon: 0.5599    steps: 275    lr: 1.6e-05     reward: 3.49\n",
      "epis: 1616   score: 5.0   mem len: 322578   epsilon: 0.5593    steps: 288    lr: 1.6e-05     reward: 3.46\n",
      "epis: 1617   score: 2.0   mem len: 322759   epsilon: 0.5589    steps: 181    lr: 1.6e-05     reward: 3.43\n",
      "epis: 1618   score: 3.0   mem len: 322989   epsilon: 0.5585    steps: 230    lr: 1.6e-05     reward: 3.44\n",
      "epis: 1619   score: 5.0   mem len: 323295   epsilon: 0.5579    steps: 306    lr: 1.6e-05     reward: 3.47\n",
      "epis: 1620   score: 6.0   mem len: 323644   epsilon: 0.5572    steps: 349    lr: 1.6e-05     reward: 3.5\n",
      "epis: 1621   score: 5.0   mem len: 323969   epsilon: 0.5565    steps: 325    lr: 1.6e-05     reward: 3.54\n",
      "epis: 1622   score: 7.0   mem len: 324369   epsilon: 0.5557    steps: 400    lr: 1.6e-05     reward: 3.58\n",
      "epis: 1623   score: 4.0   mem len: 324663   epsilon: 0.5552    steps: 294    lr: 1.6e-05     reward: 3.59\n",
      "epis: 1624   score: 6.0   mem len: 325018   epsilon: 0.5545    steps: 355    lr: 1.6e-05     reward: 3.64\n",
      "epis: 1625   score: 4.0   mem len: 325273   epsilon: 0.554    steps: 255    lr: 1.6e-05     reward: 3.65\n",
      "epis: 1626   score: 5.0   mem len: 325582   epsilon: 0.5533    steps: 309    lr: 1.6e-05     reward: 3.67\n",
      "epis: 1627   score: 4.0   mem len: 325873   epsilon: 0.5528    steps: 291    lr: 1.6e-05     reward: 3.67\n",
      "epis: 1628   score: 4.0   mem len: 326168   epsilon: 0.5522    steps: 295    lr: 1.6e-05     reward: 3.68\n",
      "epis: 1629   score: 5.0   mem len: 326471   epsilon: 0.5516    steps: 303    lr: 1.6e-05     reward: 3.69\n",
      "epis: 1630   score: 2.0   mem len: 326653   epsilon: 0.5512    steps: 182    lr: 1.6e-05     reward: 3.69\n",
      "epis: 1631   score: 8.0   mem len: 327125   epsilon: 0.5503    steps: 472    lr: 1.6e-05     reward: 3.73\n",
      "epis: 1632   score: 5.0   mem len: 327406   epsilon: 0.5497    steps: 281    lr: 1.6e-05     reward: 3.74\n",
      "epis: 1633   score: 2.0   mem len: 327606   epsilon: 0.5493    steps: 200    lr: 1.6e-05     reward: 3.72\n",
      "epis: 1634   score: 7.0   mem len: 328013   epsilon: 0.5485    steps: 407    lr: 1.6e-05     reward: 3.76\n",
      "epis: 1635   score: 6.0   mem len: 328390   epsilon: 0.5478    steps: 377    lr: 1.6e-05     reward: 3.81\n",
      "epis: 1636   score: 9.0   mem len: 328853   epsilon: 0.5469    steps: 463    lr: 1.6e-05     reward: 3.86\n",
      "epis: 1637   score: 5.0   mem len: 329160   epsilon: 0.5463    steps: 307    lr: 1.6e-05     reward: 3.86\n",
      "epis: 1638   score: 4.0   mem len: 329425   epsilon: 0.5457    steps: 265    lr: 1.6e-05     reward: 3.87\n",
      "epis: 1639   score: 4.0   mem len: 329721   epsilon: 0.5452    steps: 296    lr: 1.6e-05     reward: 3.88\n",
      "epis: 1640   score: 4.0   mem len: 329995   epsilon: 0.5446    steps: 274    lr: 1.6e-05     reward: 3.9\n",
      "epis: 1641   score: 4.0   mem len: 330269   epsilon: 0.5441    steps: 274    lr: 1.6e-05     reward: 3.9\n",
      "epis: 1642   score: 8.0   mem len: 330693   epsilon: 0.5432    steps: 424    lr: 1.6e-05     reward: 3.97\n",
      "epis: 1643   score: 8.0   mem len: 331160   epsilon: 0.5423    steps: 467    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1644   score: 5.0   mem len: 331473   epsilon: 0.5417    steps: 313    lr: 1.6e-05     reward: 4.05\n",
      "epis: 1645   score: 2.0   mem len: 331696   epsilon: 0.5412    steps: 223    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1646   score: 4.0   mem len: 331972   epsilon: 0.5407    steps: 276    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1647   score: 4.0   mem len: 332230   epsilon: 0.5402    steps: 258    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1648   score: 3.0   mem len: 332462   epsilon: 0.5397    steps: 232    lr: 1.6e-05     reward: 4.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1649   score: 3.0   mem len: 332692   epsilon: 0.5393    steps: 230    lr: 1.6e-05     reward: 4.03\n",
      "epis: 1650   score: 3.0   mem len: 332904   epsilon: 0.5388    steps: 212    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1651   score: 3.0   mem len: 333114   epsilon: 0.5384    steps: 210    lr: 1.6e-05     reward: 4.03\n",
      "epis: 1652   score: 3.0   mem len: 333348   epsilon: 0.538    steps: 234    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1653   score: 5.0   mem len: 333672   epsilon: 0.5373    steps: 324    lr: 1.6e-05     reward: 4.07\n",
      "epis: 1654   score: 1.0   mem len: 333841   epsilon: 0.537    steps: 169    lr: 1.6e-05     reward: 4.04\n",
      "epis: 1655   score: 4.0   mem len: 334116   epsilon: 0.5364    steps: 275    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1656   score: 3.0   mem len: 334385   epsilon: 0.5359    steps: 269    lr: 1.6e-05     reward: 4.01\n",
      "epis: 1657   score: 6.0   mem len: 334697   epsilon: 0.5353    steps: 312    lr: 1.6e-05     reward: 4.03\n",
      "epis: 1658   score: 4.0   mem len: 334952   epsilon: 0.5348    steps: 255    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1659   score: 4.0   mem len: 335204   epsilon: 0.5343    steps: 252    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1660   score: 1.0   mem len: 335355   epsilon: 0.534    steps: 151    lr: 1.6e-05     reward: 4.01\n",
      "epis: 1661   score: 2.0   mem len: 335536   epsilon: 0.5336    steps: 181    lr: 1.6e-05     reward: 3.99\n",
      "epis: 1662   score: 3.0   mem len: 335762   epsilon: 0.5332    steps: 226    lr: 1.6e-05     reward: 3.97\n",
      "epis: 1663   score: 6.0   mem len: 336123   epsilon: 0.5325    steps: 361    lr: 1.6e-05     reward: 3.99\n",
      "epis: 1664   score: 5.0   mem len: 336416   epsilon: 0.5319    steps: 293    lr: 1.6e-05     reward: 4.0\n",
      "epis: 1665   score: 3.0   mem len: 336647   epsilon: 0.5314    steps: 231    lr: 1.6e-05     reward: 4.01\n",
      "epis: 1666   score: 2.0   mem len: 336845   epsilon: 0.531    steps: 198    lr: 1.6e-05     reward: 4.0\n",
      "epis: 1667   score: 3.0   mem len: 337058   epsilon: 0.5306    steps: 213    lr: 1.6e-05     reward: 4.0\n",
      "epis: 1668   score: 4.0   mem len: 337334   epsilon: 0.5301    steps: 276    lr: 1.6e-05     reward: 4.02\n",
      "epis: 1669   score: 8.0   mem len: 337748   epsilon: 0.5293    steps: 414    lr: 1.6e-05     reward: 4.07\n",
      "epis: 1670   score: 5.0   mem len: 338072   epsilon: 0.5286    steps: 324    lr: 1.6e-05     reward: 4.07\n",
      "epis: 1671   score: 6.0   mem len: 338425   epsilon: 0.5279    steps: 353    lr: 1.6e-05     reward: 4.11\n",
      "epis: 1672   score: 5.0   mem len: 338751   epsilon: 0.5273    steps: 326    lr: 1.6e-05     reward: 4.11\n",
      "epis: 1673   score: 6.0   mem len: 339105   epsilon: 0.5266    steps: 354    lr: 1.6e-05     reward: 4.15\n",
      "epis: 1674   score: 5.0   mem len: 339429   epsilon: 0.5259    steps: 324    lr: 1.6e-05     reward: 4.16\n",
      "epis: 1675   score: 6.0   mem len: 339792   epsilon: 0.5252    steps: 363    lr: 1.6e-05     reward: 4.19\n",
      "epis: 1676   score: 1.0   mem len: 339942   epsilon: 0.5249    steps: 150    lr: 1.6e-05     reward: 4.15\n",
      "epis: 1677   score: 4.0   mem len: 340216   epsilon: 0.5244    steps: 274    lr: 1.6e-05     reward: 4.14\n",
      "epis: 1678   score: 5.0   mem len: 340508   epsilon: 0.5238    steps: 292    lr: 1.6e-05     reward: 4.15\n",
      "epis: 1679   score: 7.0   mem len: 340919   epsilon: 0.523    steps: 411    lr: 1.6e-05     reward: 4.19\n",
      "epis: 1680   score: 6.0   mem len: 341241   epsilon: 0.5223    steps: 322    lr: 1.6e-05     reward: 4.19\n",
      "epis: 1681   score: 5.0   mem len: 341522   epsilon: 0.5218    steps: 281    lr: 1.6e-05     reward: 4.22\n",
      "epis: 1682   score: 3.0   mem len: 341770   epsilon: 0.5213    steps: 248    lr: 1.6e-05     reward: 4.24\n",
      "epis: 1683   score: 3.0   mem len: 342016   epsilon: 0.5208    steps: 246    lr: 1.6e-05     reward: 4.23\n",
      "epis: 1684   score: 4.0   mem len: 342290   epsilon: 0.5203    steps: 274    lr: 1.6e-05     reward: 4.22\n",
      "epis: 1685   score: 4.0   mem len: 342547   epsilon: 0.5198    steps: 257    lr: 1.6e-05     reward: 4.25\n",
      "epis: 1686   score: 4.0   mem len: 342822   epsilon: 0.5192    steps: 275    lr: 1.6e-05     reward: 4.24\n",
      "epis: 1687   score: 8.0   mem len: 343222   epsilon: 0.5184    steps: 400    lr: 1.6e-05     reward: 4.29\n",
      "epis: 1688   score: 4.0   mem len: 343484   epsilon: 0.5179    steps: 262    lr: 1.6e-05     reward: 4.31\n",
      "epis: 1689   score: 7.0   mem len: 343878   epsilon: 0.5171    steps: 394    lr: 1.6e-05     reward: 4.36\n",
      "epis: 1690   score: 3.0   mem len: 344103   epsilon: 0.5167    steps: 225    lr: 1.6e-05     reward: 4.37\n",
      "epis: 1691   score: 5.0   mem len: 344428   epsilon: 0.516    steps: 325    lr: 1.6e-05     reward: 4.37\n",
      "epis: 1692   score: 5.0   mem len: 344754   epsilon: 0.5154    steps: 326    lr: 1.6e-05     reward: 4.39\n",
      "epis: 1693   score: 6.0   mem len: 345127   epsilon: 0.5146    steps: 373    lr: 1.6e-05     reward: 4.4\n",
      "epis: 1694   score: 3.0   mem len: 345395   epsilon: 0.5141    steps: 268    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1695   score: 3.0   mem len: 345621   epsilon: 0.5137    steps: 226    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1696   score: 3.0   mem len: 345850   epsilon: 0.5132    steps: 229    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1697   score: 2.0   mem len: 346050   epsilon: 0.5128    steps: 200    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1698   score: 6.0   mem len: 346387   epsilon: 0.5122    steps: 337    lr: 1.6e-05     reward: 4.46\n",
      "epis: 1699   score: 5.0   mem len: 346678   epsilon: 0.5116    steps: 291    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1700   score: 7.0   mem len: 347076   epsilon: 0.5108    steps: 398    lr: 1.6e-05     reward: 4.5\n",
      "epis: 1701   score: 5.0   mem len: 347401   epsilon: 0.5101    steps: 325    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1702   score: 4.0   mem len: 347656   epsilon: 0.5096    steps: 255    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1703   score: 3.0   mem len: 347884   epsilon: 0.5092    steps: 228    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1704   score: 4.0   mem len: 348150   epsilon: 0.5087    steps: 266    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1705   score: 4.0   mem len: 348431   epsilon: 0.5081    steps: 281    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1706   score: 4.0   mem len: 348689   epsilon: 0.5076    steps: 258    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1707   score: 5.0   mem len: 348980   epsilon: 0.507    steps: 291    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1708   score: 4.0   mem len: 349293   epsilon: 0.5064    steps: 313    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1709   score: 3.0   mem len: 349524   epsilon: 0.5059    steps: 231    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1710   score: 6.0   mem len: 349882   epsilon: 0.5052    steps: 358    lr: 1.6e-05     reward: 4.44\n",
      "epis: 1711   score: 5.0   mem len: 350190   epsilon: 0.5046    steps: 308    lr: 1.6e-05     reward: 4.45\n",
      "epis: 1712   score: 3.0   mem len: 350436   epsilon: 0.5041    steps: 246    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1713   score: 3.0   mem len: 350665   epsilon: 0.5037    steps: 229    lr: 1.6e-05     reward: 4.44\n",
      "epis: 1714   score: 12.0   mem len: 351151   epsilon: 0.5027    steps: 486    lr: 1.6e-05     reward: 4.5\n",
      "epis: 1715   score: 5.0   mem len: 351439   epsilon: 0.5021    steps: 288    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1716   score: 7.0   mem len: 351864   epsilon: 0.5013    steps: 425    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1717   score: 3.0   mem len: 352089   epsilon: 0.5009    steps: 225    lr: 1.6e-05     reward: 4.54\n",
      "epis: 1718   score: 8.0   mem len: 352510   epsilon: 0.5    steps: 421    lr: 1.6e-05     reward: 4.59\n",
      "epis: 1719   score: 2.0   mem len: 352710   epsilon: 0.4996    steps: 200    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1720   score: 3.0   mem len: 352938   epsilon: 0.4992    steps: 228    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1721   score: 3.0   mem len: 353168   epsilon: 0.4987    steps: 230    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1722   score: 12.0   mem len: 353756   epsilon: 0.4976    steps: 588    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1723   score: 4.0   mem len: 354035   epsilon: 0.497    steps: 279    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1724   score: 5.0   mem len: 354365   epsilon: 0.4964    steps: 330    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1725   score: 7.0   mem len: 354769   epsilon: 0.4956    steps: 404    lr: 1.6e-05     reward: 4.58\n",
      "epis: 1726   score: 2.0   mem len: 354951   epsilon: 0.4952    steps: 182    lr: 1.6e-05     reward: 4.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1727   score: 4.0   mem len: 355226   epsilon: 0.4947    steps: 275    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1728   score: 1.0   mem len: 355394   epsilon: 0.4943    steps: 168    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1729   score: 4.0   mem len: 355669   epsilon: 0.4938    steps: 275    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1730   score: 2.0   mem len: 355851   epsilon: 0.4934    steps: 182    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1731   score: 5.0   mem len: 356139   epsilon: 0.4928    steps: 288    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1732   score: 7.0   mem len: 356480   epsilon: 0.4922    steps: 341    lr: 1.6e-05     reward: 4.5\n",
      "epis: 1733   score: 12.0   mem len: 356918   epsilon: 0.4913    steps: 438    lr: 1.6e-05     reward: 4.6\n",
      "epis: 1734   score: 3.0   mem len: 357150   epsilon: 0.4908    steps: 232    lr: 1.6e-05     reward: 4.56\n",
      "epis: 1735   score: 5.0   mem len: 357431   epsilon: 0.4903    steps: 281    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1736   score: 5.0   mem len: 357710   epsilon: 0.4897    steps: 279    lr: 1.6e-05     reward: 4.51\n",
      "epis: 1737   score: 7.0   mem len: 358115   epsilon: 0.4889    steps: 405    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1738   score: 4.0   mem len: 358356   epsilon: 0.4885    steps: 241    lr: 1.6e-05     reward: 4.53\n",
      "epis: 1739   score: 3.0   mem len: 358569   epsilon: 0.488    steps: 213    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1740   score: 4.0   mem len: 358865   epsilon: 0.4874    steps: 296    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1741   score: 4.0   mem len: 359165   epsilon: 0.4869    steps: 300    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1742   score: 4.0   mem len: 359404   epsilon: 0.4864    steps: 239    lr: 1.6e-05     reward: 4.48\n",
      "epis: 1743   score: 3.0   mem len: 359615   epsilon: 0.486    steps: 211    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1744   score: 3.0   mem len: 359843   epsilon: 0.4855    steps: 228    lr: 1.6e-05     reward: 4.41\n",
      "epis: 1745   score: 3.0   mem len: 360074   epsilon: 0.4851    steps: 231    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1746   score: 4.0   mem len: 360350   epsilon: 0.4845    steps: 276    lr: 1.6e-05     reward: 4.42\n",
      "epis: 1747   score: 5.0   mem len: 360630   epsilon: 0.484    steps: 280    lr: 1.6e-05     reward: 4.43\n",
      "epis: 1748   score: 6.0   mem len: 360985   epsilon: 0.4832    steps: 355    lr: 1.6e-05     reward: 4.46\n",
      "epis: 1749   score: 4.0   mem len: 361250   epsilon: 0.4827    steps: 265    lr: 1.6e-05     reward: 4.47\n",
      "epis: 1750   score: 8.0   mem len: 361691   epsilon: 0.4818    steps: 441    lr: 1.6e-05     reward: 4.52\n",
      "epis: 1751   score: 6.0   mem len: 362036   epsilon: 0.4812    steps: 345    lr: 1.6e-05     reward: 4.55\n",
      "epis: 1752   score: 6.0   mem len: 362356   epsilon: 0.4805    steps: 320    lr: 1.6e-05     reward: 4.58\n",
      "epis: 1753   score: 6.0   mem len: 362726   epsilon: 0.4798    steps: 370    lr: 1.6e-05     reward: 4.59\n",
      "epis: 1754   score: 3.0   mem len: 362972   epsilon: 0.4793    steps: 246    lr: 1.6e-05     reward: 4.61\n",
      "epis: 1755   score: 6.0   mem len: 363307   epsilon: 0.4787    steps: 335    lr: 1.6e-05     reward: 4.63\n",
      "epis: 1756   score: 2.0   mem len: 363489   epsilon: 0.4783    steps: 182    lr: 1.6e-05     reward: 4.62\n",
      "epis: 1757   score: 4.0   mem len: 363748   epsilon: 0.4778    steps: 259    lr: 1.6e-05     reward: 4.6\n",
      "epis: 1758   score: 4.0   mem len: 364028   epsilon: 0.4772    steps: 280    lr: 1.6e-05     reward: 4.6\n",
      "epis: 1759   score: 3.0   mem len: 364241   epsilon: 0.4768    steps: 213    lr: 1.6e-05     reward: 4.59\n",
      "epis: 1760   score: 6.0   mem len: 364578   epsilon: 0.4761    steps: 337    lr: 1.6e-05     reward: 4.64\n",
      "epis: 1761   score: 4.0   mem len: 364857   epsilon: 0.4756    steps: 279    lr: 1.6e-05     reward: 4.66\n",
      "epis: 1762   score: 4.0   mem len: 365131   epsilon: 0.475    steps: 274    lr: 1.6e-05     reward: 4.67\n",
      "epis: 1763   score: 6.0   mem len: 365468   epsilon: 0.4744    steps: 337    lr: 1.6e-05     reward: 4.67\n",
      "epis: 1764   score: 6.0   mem len: 365792   epsilon: 0.4737    steps: 324    lr: 1.6e-05     reward: 4.68\n",
      "epis: 1765   score: 5.0   mem len: 366074   epsilon: 0.4732    steps: 282    lr: 1.6e-05     reward: 4.7\n",
      "epis: 1766   score: 5.0   mem len: 366402   epsilon: 0.4725    steps: 328    lr: 1.6e-05     reward: 4.73\n",
      "epis: 1767   score: 8.0   mem len: 366807   epsilon: 0.4717    steps: 405    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1768   score: 3.0   mem len: 367020   epsilon: 0.4713    steps: 213    lr: 1.6e-05     reward: 4.77\n",
      "epis: 1769   score: 3.0   mem len: 367231   epsilon: 0.4709    steps: 211    lr: 1.6e-05     reward: 4.72\n",
      "epis: 1770   score: 3.0   mem len: 367456   epsilon: 0.4704    steps: 225    lr: 1.6e-05     reward: 4.7\n",
      "epis: 1771   score: 7.0   mem len: 367868   epsilon: 0.4696    steps: 412    lr: 1.6e-05     reward: 4.71\n",
      "epis: 1772   score: 7.0   mem len: 368114   epsilon: 0.4691    steps: 246    lr: 1.6e-05     reward: 4.73\n",
      "epis: 1773   score: 4.0   mem len: 368373   epsilon: 0.4686    steps: 259    lr: 1.6e-05     reward: 4.71\n",
      "epis: 1774   score: 4.0   mem len: 368652   epsilon: 0.4681    steps: 279    lr: 1.6e-05     reward: 4.7\n",
      "epis: 1775   score: 5.0   mem len: 368957   epsilon: 0.4675    steps: 305    lr: 1.6e-05     reward: 4.69\n",
      "epis: 1776   score: 4.0   mem len: 369232   epsilon: 0.4669    steps: 275    lr: 1.6e-05     reward: 4.72\n",
      "epis: 1777   score: 6.0   mem len: 369585   epsilon: 0.4662    steps: 353    lr: 1.6e-05     reward: 4.74\n",
      "epis: 1778   score: 8.0   mem len: 370005   epsilon: 0.4654    steps: 420    lr: 1.6e-05     reward: 4.77\n",
      "epis: 1779   score: 2.0   mem len: 370223   epsilon: 0.465    steps: 218    lr: 1.6e-05     reward: 4.72\n",
      "epis: 1780   score: 3.0   mem len: 370470   epsilon: 0.4645    steps: 247    lr: 1.6e-05     reward: 4.69\n",
      "epis: 1781   score: 9.0   mem len: 370777   epsilon: 0.4639    steps: 307    lr: 1.6e-05     reward: 4.73\n",
      "epis: 1782   score: 6.0   mem len: 371130   epsilon: 0.4632    steps: 353    lr: 1.6e-05     reward: 4.76\n",
      "epis: 1783   score: 7.0   mem len: 371537   epsilon: 0.4624    steps: 407    lr: 1.6e-05     reward: 4.8\n",
      "epis: 1784   score: 8.0   mem len: 371960   epsilon: 0.4615    steps: 423    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1785   score: 8.0   mem len: 372396   epsilon: 0.4607    steps: 436    lr: 1.6e-05     reward: 4.88\n",
      "epis: 1786   score: 2.0   mem len: 372596   epsilon: 0.4603    steps: 200    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1787   score: 6.0   mem len: 372939   epsilon: 0.4596    steps: 343    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1788   score: 3.0   mem len: 373150   epsilon: 0.4592    steps: 211    lr: 1.6e-05     reward: 4.83\n",
      "epis: 1789   score: 2.0   mem len: 373332   epsilon: 0.4588    steps: 182    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1790   score: 3.0   mem len: 373557   epsilon: 0.4584    steps: 225    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1791   score: 4.0   mem len: 373857   epsilon: 0.4578    steps: 300    lr: 1.6e-05     reward: 4.77\n",
      "epis: 1792   score: 7.0   mem len: 374248   epsilon: 0.457    steps: 391    lr: 1.6e-05     reward: 4.79\n",
      "epis: 1793   score: 5.0   mem len: 374592   epsilon: 0.4563    steps: 344    lr: 1.6e-05     reward: 4.78\n",
      "epis: 1794   score: 5.0   mem len: 374919   epsilon: 0.4557    steps: 327    lr: 1.6e-05     reward: 4.8\n",
      "epis: 1795   score: 6.0   mem len: 375252   epsilon: 0.455    steps: 333    lr: 1.6e-05     reward: 4.83\n",
      "epis: 1796   score: 7.0   mem len: 375660   epsilon: 0.4542    steps: 408    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1797   score: 4.0   mem len: 375936   epsilon: 0.4536    steps: 276    lr: 1.6e-05     reward: 4.89\n",
      "epis: 1798   score: 4.0   mem len: 376233   epsilon: 0.4531    steps: 297    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1799   score: 4.0   mem len: 376512   epsilon: 0.4525    steps: 279    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1800   score: 4.0   mem len: 376771   epsilon: 0.452    steps: 259    lr: 1.6e-05     reward: 4.83\n",
      "epis: 1801   score: 4.0   mem len: 377030   epsilon: 0.4515    steps: 259    lr: 1.6e-05     reward: 4.82\n",
      "epis: 1802   score: 4.0   mem len: 377272   epsilon: 0.451    steps: 242    lr: 1.6e-05     reward: 4.82\n",
      "epis: 1803   score: 3.0   mem len: 377485   epsilon: 0.4506    steps: 213    lr: 1.6e-05     reward: 4.82\n",
      "epis: 1804   score: 7.0   mem len: 377881   epsilon: 0.4498    steps: 396    lr: 1.6e-05     reward: 4.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1805   score: 4.0   mem len: 378158   epsilon: 0.4492    steps: 277    lr: 1.6e-05     reward: 4.85\n",
      "epis: 1806   score: 5.0   mem len: 378482   epsilon: 0.4486    steps: 324    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1807   score: 3.0   mem len: 378708   epsilon: 0.4482    steps: 226    lr: 1.6e-05     reward: 4.84\n",
      "epis: 1808   score: 6.0   mem len: 379040   epsilon: 0.4475    steps: 332    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1809   score: 4.0   mem len: 379322   epsilon: 0.4469    steps: 282    lr: 1.6e-05     reward: 4.87\n",
      "epis: 1810   score: 5.0   mem len: 379664   epsilon: 0.4463    steps: 342    lr: 1.6e-05     reward: 4.86\n",
      "epis: 1811   score: 3.0   mem len: 379873   epsilon: 0.4458    steps: 209    lr: 1.6e-05     reward: 4.84\n"
     ]
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"epis:\", e, \"  score:\", score, \"  mem len:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 4), \"   steps:\", step,\n",
    "                  \"   lr:\", round(agent.optimizer.param_groups[0]['lr'], 7), \"    reward:\", round(np.mean(evaluation_reward), 2))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
