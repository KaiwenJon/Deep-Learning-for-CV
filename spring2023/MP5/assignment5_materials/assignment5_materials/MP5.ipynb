{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309641/1376515593.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "/tmp/ipykernel_309641/1376515593.py:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 0   score: 2.0   mem len: 218   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 2.0\n",
      "epis: 1   score: 1.0   mem len: 369   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 2   score: 2.0   mem len: 567   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 3   score: 1.0   mem len: 739   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.5\n",
      "epis: 4   score: 1.0   mem len: 889   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.4\n",
      "epis: 5   score: 1.0   mem len: 1039   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.33\n",
      "epis: 6   score: 0.0   mem len: 1162   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.14\n",
      "epis: 7   score: 0.0   mem len: 1284   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.0\n",
      "epis: 8   score: 0.0   mem len: 1407   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 0.89\n",
      "epis: 9   score: 2.0   mem len: 1625   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.0\n",
      "epis: 10   score: 2.0   mem len: 1822   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.09\n",
      "epis: 11   score: 2.0   mem len: 2019   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.17\n",
      "epis: 12   score: 0.0   mem len: 2141   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.08\n",
      "epis: 13   score: 2.0   mem len: 2359   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.14\n",
      "epis: 14   score: 2.0   mem len: 2560   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.2\n",
      "epis: 15   score: 3.0   mem len: 2803   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.31\n",
      "epis: 16   score: 2.0   mem len: 3001   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.35\n",
      "epis: 17   score: 2.0   mem len: 3199   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 18   score: 2.0   mem len: 3400   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.42\n",
      "epis: 19   score: 1.0   mem len: 3569   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 20   score: 1.0   mem len: 3737   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.38\n",
      "epis: 21   score: 8.0   mem len: 4106   epsilon: 1.0    steps: 369    lr: 0.0001     reward: 1.68\n",
      "epis: 22   score: 2.0   mem len: 4304   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.7\n",
      "epis: 23   score: 2.0   mem len: 4502   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 24   score: 0.0   mem len: 4625   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 25   score: 2.0   mem len: 4841   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.65\n",
      "epis: 26   score: 2.0   mem len: 5039   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 27   score: 0.0   mem len: 5162   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 28   score: 1.0   mem len: 5332   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.59\n",
      "epis: 29   score: 1.0   mem len: 5501   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 30   score: 0.0   mem len: 5624   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 31   score: 3.0   mem len: 5876   epsilon: 1.0    steps: 252    lr: 0.0001     reward: 1.56\n",
      "epis: 32   score: 2.0   mem len: 6058   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.58\n",
      "epis: 33   score: 0.0   mem len: 6180   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 34   score: 1.0   mem len: 6350   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.51\n",
      "epis: 35   score: 1.0   mem len: 6519   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 36   score: 0.0   mem len: 6641   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.46\n",
      "epis: 37   score: 2.0   mem len: 6838   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.47\n",
      "epis: 38   score: 2.0   mem len: 7036   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 39   score: 1.0   mem len: 7208   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.48\n",
      "epis: 40   score: 3.0   mem len: 7475   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.51\n",
      "epis: 41   score: 3.0   mem len: 7703   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.55\n",
      "epis: 42   score: 3.0   mem len: 7931   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.58\n",
      "epis: 43   score: 0.0   mem len: 8054   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 44   score: 1.0   mem len: 8225   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.53\n",
      "epis: 45   score: 3.0   mem len: 8475   epsilon: 1.0    steps: 250    lr: 0.0001     reward: 1.57\n",
      "epis: 46   score: 2.0   mem len: 8675   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.57\n",
      "epis: 47   score: 1.0   mem len: 8844   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 48   score: 1.0   mem len: 9013   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.55\n",
      "epis: 49   score: 4.0   mem len: 9309   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.6\n",
      "epis: 50   score: 2.0   mem len: 9528   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.61\n",
      "epis: 51   score: 1.0   mem len: 9679   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 52   score: 0.0   mem len: 9801   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 53   score: 2.0   mem len: 10001   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.57\n",
      "epis: 54   score: 2.0   mem len: 10199   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 55   score: 0.0   mem len: 10321   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 56   score: 2.0   mem len: 10519   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 57   score: 2.0   mem len: 10717   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.57\n",
      "epis: 58   score: 1.0   mem len: 10886   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 59   score: 2.0   mem len: 11104   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.57\n",
      "epis: 60   score: 1.0   mem len: 11273   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 61   score: 3.0   mem len: 11499   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.58\n",
      "epis: 62   score: 1.0   mem len: 11671   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 63   score: 0.0   mem len: 11793   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 64   score: 2.0   mem len: 12010   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.55\n",
      "epis: 65   score: 2.0   mem len: 12208   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 66   score: 1.0   mem len: 12379   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.55\n",
      "epis: 67   score: 4.0   mem len: 12657   epsilon: 1.0    steps: 278    lr: 0.0001     reward: 1.59\n",
      "epis: 68   score: 2.0   mem len: 12855   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 69   score: 4.0   mem len: 13132   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.63\n",
      "epis: 70   score: 1.0   mem len: 13301   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 71   score: 1.0   mem len: 13452   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.61\n",
      "epis: 72   score: 3.0   mem len: 13683   epsilon: 1.0    steps: 231    lr: 0.0001     reward: 1.63\n",
      "epis: 73   score: 0.0   mem len: 13805   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 74   score: 1.0   mem len: 13955   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.6\n",
      "epis: 75   score: 1.0   mem len: 14127   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.59\n",
      "epis: 76   score: 2.0   mem len: 14344   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.6\n",
      "epis: 77   score: 0.0   mem len: 14467   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 78   score: 2.0   mem len: 14690   epsilon: 1.0    steps: 223    lr: 0.0001     reward: 1.58\n",
      "epis: 79   score: 2.0   mem len: 14910   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.59\n",
      "epis: 80   score: 2.0   mem len: 15108   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 81   score: 0.0   mem len: 15231   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 82   score: 0.0   mem len: 15353   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 83   score: 2.0   mem len: 15554   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 84   score: 0.0   mem len: 15676   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 85   score: 1.0   mem len: 15845   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 86   score: 0.0   mem len: 15968   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 87   score: 2.0   mem len: 16189   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.52\n",
      "epis: 88   score: 0.0   mem len: 16312   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 89   score: 1.0   mem len: 16480   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 90   score: 0.0   mem len: 16602   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 91   score: 0.0   mem len: 16725   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 92   score: 3.0   mem len: 16996   epsilon: 1.0    steps: 271    lr: 0.0001     reward: 1.48\n",
      "epis: 93   score: 1.0   mem len: 17147   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 94   score: 1.0   mem len: 17297   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.47\n",
      "epis: 95   score: 0.0   mem len: 17420   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 96   score: 2.0   mem len: 17618   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 97   score: 3.0   mem len: 17845   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.48\n",
      "epis: 98   score: 0.0   mem len: 17967   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.46\n",
      "epis: 99   score: 1.0   mem len: 18118   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 100   score: 3.0   mem len: 18385   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.47\n",
      "epis: 101   score: 4.0   mem len: 18677   epsilon: 1.0    steps: 292    lr: 0.0001     reward: 1.5\n",
      "epis: 102   score: 0.0   mem len: 18799   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 103   score: 2.0   mem len: 18997   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 104   score: 1.0   mem len: 19167   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.49\n",
      "epis: 105   score: 3.0   mem len: 19397   epsilon: 1.0    steps: 230    lr: 0.0001     reward: 1.51\n",
      "epis: 106   score: 4.0   mem len: 19692   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.55\n",
      "epis: 107   score: 3.0   mem len: 19939   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.58\n",
      "epis: 108   score: 2.0   mem len: 20136   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.6\n",
      "epis: 109   score: 1.0   mem len: 20287   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 110   score: 2.0   mem len: 20505   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.59\n",
      "epis: 111   score: 2.0   mem len: 20723   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.59\n",
      "epis: 112   score: 0.0   mem len: 20846   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 113   score: 3.0   mem len: 21090   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.6\n",
      "epis: 114   score: 6.0   mem len: 21443   epsilon: 1.0    steps: 353    lr: 0.0001     reward: 1.64\n",
      "epis: 115   score: 2.0   mem len: 21662   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.63\n",
      "epis: 116   score: 0.0   mem len: 21785   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 117   score: 1.0   mem len: 21954   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.6\n",
      "epis: 118   score: 2.0   mem len: 22151   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.6\n",
      "epis: 119   score: 0.0   mem len: 22273   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 120   score: 0.0   mem len: 22395   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 121   score: 1.0   mem len: 22564   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.51\n",
      "epis: 122   score: 1.0   mem len: 22732   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 123   score: 0.0   mem len: 22854   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 124   score: 1.0   mem len: 23024   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.49\n",
      "epis: 125   score: 3.0   mem len: 23267   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.5\n",
      "epis: 126   score: 2.0   mem len: 23485   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.5\n",
      "epis: 127   score: 0.0   mem len: 23607   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 128   score: 1.0   mem len: 23759   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.5\n",
      "epis: 129   score: 0.0   mem len: 23882   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 130   score: 2.0   mem len: 24080   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 131   score: 1.0   mem len: 24231   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.49\n",
      "epis: 132   score: 3.0   mem len: 24463   epsilon: 1.0    steps: 232    lr: 0.0001     reward: 1.5\n",
      "epis: 133   score: 2.0   mem len: 24680   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.52\n",
      "epis: 134   score: 1.0   mem len: 24831   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 135   score: 1.0   mem len: 25002   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.52\n",
      "epis: 136   score: 2.0   mem len: 25221   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.54\n",
      "epis: 137   score: 1.0   mem len: 25372   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 138   score: 2.0   mem len: 25589   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.53\n",
      "epis: 139   score: 2.0   mem len: 25807   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.54\n",
      "epis: 140   score: 0.0   mem len: 25930   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 141   score: 3.0   mem len: 26196   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.51\n",
      "epis: 142   score: 2.0   mem len: 26397   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.5\n",
      "epis: 143   score: 3.0   mem len: 26623   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.53\n",
      "epis: 144   score: 4.0   mem len: 26924   epsilon: 1.0    steps: 301    lr: 0.0001     reward: 1.56\n",
      "epis: 145   score: 1.0   mem len: 27093   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 146   score: 0.0   mem len: 27215   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 147   score: 0.0   mem len: 27338   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 148   score: 0.0   mem len: 27460   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 149   score: 0.0   mem len: 27583   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 150   score: 0.0   mem len: 27705   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 151   score: 0.0   mem len: 27828   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 152   score: 0.0   mem len: 27951   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 153   score: 0.0   mem len: 28073   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 154   score: 2.0   mem len: 28271   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 155   score: 3.0   mem len: 28496   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.44\n",
      "epis: 156   score: 5.0   mem len: 28839   epsilon: 1.0    steps: 343    lr: 0.0001     reward: 1.47\n",
      "epis: 157   score: 0.0   mem len: 28962   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 158   score: 0.0   mem len: 29085   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 159   score: 2.0   mem len: 29264   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.44\n",
      "epis: 160   score: 1.0   mem len: 29433   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.44\n",
      "epis: 161   score: 2.0   mem len: 29632   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.43\n",
      "epis: 162   score: 0.0   mem len: 29755   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 163   score: 0.0   mem len: 29877   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 164   score: 2.0   mem len: 30095   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.42\n",
      "epis: 165   score: 2.0   mem len: 30292   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.42\n",
      "epis: 166   score: 4.0   mem len: 30584   epsilon: 1.0    steps: 292    lr: 0.0001     reward: 1.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 167   score: 3.0   mem len: 30814   epsilon: 1.0    steps: 230    lr: 0.0001     reward: 1.44\n",
      "epis: 168   score: 1.0   mem len: 30982   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.43\n",
      "epis: 169   score: 1.0   mem len: 31134   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.4\n",
      "epis: 170   score: 0.0   mem len: 31257   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 171   score: 2.0   mem len: 31454   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.4\n",
      "epis: 172   score: 3.0   mem len: 31700   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.4\n",
      "epis: 173   score: 2.0   mem len: 31897   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.42\n",
      "epis: 174   score: 0.0   mem len: 32020   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 175   score: 0.0   mem len: 32143   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 176   score: 1.0   mem len: 32294   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 177   score: 1.0   mem len: 32466   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.4\n",
      "epis: 178   score: 1.0   mem len: 32635   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 179   score: 4.0   mem len: 32933   epsilon: 1.0    steps: 298    lr: 0.0001     reward: 1.41\n",
      "epis: 180   score: 2.0   mem len: 33114   epsilon: 1.0    steps: 181    lr: 0.0001     reward: 1.41\n",
      "epis: 181   score: 2.0   mem len: 33333   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.43\n",
      "epis: 182   score: 1.0   mem len: 33502   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.44\n",
      "epis: 183   score: 1.0   mem len: 33673   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.43\n",
      "epis: 184   score: 3.0   mem len: 33898   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.46\n",
      "epis: 185   score: 1.0   mem len: 34069   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.46\n",
      "epis: 186   score: 2.0   mem len: 34266   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.48\n",
      "epis: 187   score: 1.0   mem len: 34438   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.47\n",
      "epis: 188   score: 3.0   mem len: 34686   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.5\n",
      "epis: 189   score: 0.0   mem len: 34809   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 190   score: 0.0   mem len: 34931   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 191   score: 2.0   mem len: 35147   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.51\n",
      "epis: 192   score: 3.0   mem len: 35414   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.51\n",
      "epis: 193   score: 0.0   mem len: 35536   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 194   score: 3.0   mem len: 35783   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.52\n",
      "epis: 195   score: 2.0   mem len: 36000   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.54\n",
      "epis: 196   score: 2.0   mem len: 36197   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.54\n",
      "epis: 197   score: 4.0   mem len: 36493   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.55\n",
      "epis: 198   score: 0.0   mem len: 36615   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 199   score: 0.0   mem len: 36738   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 200   score: 1.0   mem len: 36906   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.52\n",
      "epis: 201   score: 0.0   mem len: 37028   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 202   score: 2.0   mem len: 37244   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.5\n",
      "epis: 203   score: 2.0   mem len: 37443   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.5\n",
      "epis: 204   score: 4.0   mem len: 37720   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.53\n",
      "epis: 205   score: 3.0   mem len: 37946   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.53\n",
      "epis: 206   score: 1.0   mem len: 38117   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.5\n",
      "epis: 207   score: 1.0   mem len: 38287   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.48\n",
      "epis: 208   score: 1.0   mem len: 38456   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 209   score: 1.0   mem len: 38625   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 210   score: 0.0   mem len: 38748   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 211   score: 2.0   mem len: 38946   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 212   score: 1.0   mem len: 39097   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 213   score: 1.0   mem len: 39248   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 214   score: 4.0   mem len: 39527   epsilon: 1.0    steps: 279    lr: 0.0001     reward: 1.42\n",
      "epis: 215   score: 0.0   mem len: 39649   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 216   score: 1.0   mem len: 39799   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.41\n",
      "epis: 217   score: 0.0   mem len: 39921   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 218   score: 0.0   mem len: 40043   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 219   score: 3.0   mem len: 40274   epsilon: 1.0    steps: 231    lr: 0.0001     reward: 1.41\n",
      "epis: 220   score: 1.0   mem len: 40425   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.42\n",
      "epis: 221   score: 0.0   mem len: 40548   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 222   score: 1.0   mem len: 40717   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.41\n",
      "epis: 223   score: 2.0   mem len: 40935   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.43\n",
      "epis: 224   score: 0.0   mem len: 41057   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 225   score: 0.0   mem len: 41180   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 226   score: 0.0   mem len: 41302   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 227   score: 1.0   mem len: 41473   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.38\n",
      "epis: 228   score: 2.0   mem len: 41693   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.39\n",
      "epis: 229   score: 0.0   mem len: 41816   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 230   score: 3.0   mem len: 42082   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.4\n",
      "epis: 231   score: 6.0   mem len: 42427   epsilon: 1.0    steps: 345    lr: 0.0001     reward: 1.45\n",
      "epis: 232   score: 0.0   mem len: 42549   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 233   score: 4.0   mem len: 42806   epsilon: 1.0    steps: 257    lr: 0.0001     reward: 1.44\n",
      "epis: 234   score: 3.0   mem len: 43054   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.46\n",
      "epis: 235   score: 0.0   mem len: 43177   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 236   score: 6.0   mem len: 43518   epsilon: 1.0    steps: 341    lr: 0.0001     reward: 1.49\n",
      "epis: 237   score: 2.0   mem len: 43715   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.5\n",
      "epis: 238   score: 2.0   mem len: 43895   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.5\n",
      "epis: 239   score: 2.0   mem len: 44112   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.5\n",
      "epis: 240   score: 3.0   mem len: 44356   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.53\n",
      "epis: 241   score: 1.0   mem len: 44524   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.51\n",
      "epis: 242   score: 0.0   mem len: 44647   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 243   score: 2.0   mem len: 44865   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.48\n",
      "epis: 244   score: 1.0   mem len: 45035   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.45\n",
      "epis: 245   score: 1.0   mem len: 45204   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 246   score: 3.0   mem len: 45450   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.48\n",
      "epis: 247   score: 2.0   mem len: 45632   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.5\n",
      "epis: 248   score: 0.0   mem len: 45755   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 249   score: 2.0   mem len: 45952   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 250   score: 1.0   mem len: 46102   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.53\n",
      "epis: 251   score: 0.0   mem len: 46225   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 252   score: 0.0   mem len: 46348   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 253   score: 1.0   mem len: 46498   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.54\n",
      "epis: 254   score: 2.0   mem len: 46678   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.54\n",
      "epis: 255   score: 0.0   mem len: 46800   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 256   score: 3.0   mem len: 47029   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.49\n",
      "epis: 257   score: 1.0   mem len: 47197   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 258   score: 1.0   mem len: 47369   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.51\n",
      "epis: 259   score: 0.0   mem len: 47491   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 260   score: 5.0   mem len: 47854   epsilon: 1.0    steps: 363    lr: 0.0001     reward: 1.53\n",
      "epis: 261   score: 1.0   mem len: 48004   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.52\n",
      "epis: 262   score: 1.0   mem len: 48173   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 263   score: 4.0   mem len: 48451   epsilon: 1.0    steps: 278    lr: 0.0001     reward: 1.57\n",
      "epis: 264   score: 1.0   mem len: 48621   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.56\n",
      "epis: 265   score: 0.0   mem len: 48743   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 266   score: 3.0   mem len: 49010   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.53\n",
      "epis: 267   score: 0.0   mem len: 49133   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 268   score: 3.0   mem len: 49358   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.52\n",
      "epis: 269   score: 2.0   mem len: 49556   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.53\n",
      "epis: 270   score: 0.0   mem len: 49678   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 271   score: 2.0   mem len: 49865   epsilon: 1.0    steps: 187    lr: 0.0001     reward: 1.53\n",
      "epis: 272   score: 1.0   mem len: 50034   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.51\n",
      "epis: 273   score: 1.0   mem len: 50203   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 274   score: 2.0   mem len: 50424   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.52\n",
      "epis: 275   score: 0.0   mem len: 50547   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 276   score: 0.0   mem len: 50670   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 277   score: 0.0   mem len: 50793   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 278   score: 0.0   mem len: 50915   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 279   score: 0.0   mem len: 51038   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 280   score: 2.0   mem len: 51236   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 281   score: 1.0   mem len: 51405   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.44\n",
      "epis: 282   score: 3.0   mem len: 51632   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.46\n",
      "epis: 283   score: 3.0   mem len: 51877   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.48\n",
      "epis: 284   score: 5.0   mem len: 52201   epsilon: 1.0    steps: 324    lr: 0.0001     reward: 1.5\n",
      "epis: 285   score: 0.0   mem len: 52324   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 286   score: 2.0   mem len: 52540   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.49\n",
      "epis: 287   score: 2.0   mem len: 52737   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.5\n",
      "epis: 288   score: 1.0   mem len: 52905   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.48\n",
      "epis: 289   score: 1.0   mem len: 53076   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.49\n",
      "epis: 290   score: 0.0   mem len: 53199   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 291   score: 2.0   mem len: 53396   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.49\n",
      "epis: 292   score: 2.0   mem len: 53593   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.48\n",
      "epis: 293   score: 2.0   mem len: 53791   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 294   score: 4.0   mem len: 54108   epsilon: 1.0    steps: 317    lr: 0.0001     reward: 1.51\n",
      "epis: 295   score: 2.0   mem len: 54308   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.51\n",
      "epis: 296   score: 2.0   mem len: 54508   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.51\n",
      "epis: 297   score: 0.0   mem len: 54631   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 298   score: 0.0   mem len: 54754   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 299   score: 1.0   mem len: 54905   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 300   score: 1.0   mem len: 55076   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.48\n",
      "epis: 301   score: 5.0   mem len: 55372   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.53\n",
      "epis: 302   score: 0.0   mem len: 55495   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 303   score: 1.0   mem len: 55664   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 304   score: 2.0   mem len: 55865   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.48\n",
      "epis: 305   score: 0.0   mem len: 55988   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 306   score: 10.0   mem len: 56435   epsilon: 1.0    steps: 447    lr: 0.0001     reward: 1.54\n",
      "epis: 307   score: 1.0   mem len: 56586   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.54\n",
      "epis: 308   score: 0.0   mem len: 56708   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 309   score: 0.0   mem len: 56831   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 310   score: 4.0   mem len: 57090   epsilon: 1.0    steps: 259    lr: 0.0001     reward: 1.56\n",
      "epis: 311   score: 0.0   mem len: 57212   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 312   score: 1.0   mem len: 57381   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 313   score: 3.0   mem len: 57629   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.56\n",
      "epis: 314   score: 2.0   mem len: 57826   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.54\n",
      "epis: 315   score: 2.0   mem len: 58026   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.56\n",
      "epis: 316   score: 0.0   mem len: 58148   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 317   score: 1.0   mem len: 58299   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 318   score: 0.0   mem len: 58422   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 319   score: 1.0   mem len: 58573   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.54\n",
      "epis: 320   score: 1.0   mem len: 58724   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.54\n",
      "epis: 321   score: 1.0   mem len: 58894   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.55\n",
      "epis: 322   score: 0.0   mem len: 59017   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 323   score: 4.0   mem len: 59295   epsilon: 1.0    steps: 278    lr: 0.0001     reward: 1.56\n",
      "epis: 324   score: 1.0   mem len: 59445   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.57\n",
      "epis: 325   score: 2.0   mem len: 59661   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.59\n",
      "epis: 326   score: 1.0   mem len: 59811   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.6\n",
      "epis: 327   score: 1.0   mem len: 59983   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.6\n",
      "epis: 328   score: 0.0   mem len: 60106   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 329   score: 1.0   mem len: 60275   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 330   score: 1.0   mem len: 60446   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.57\n",
      "epis: 331   score: 0.0   mem len: 60569   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 332   score: 2.0   mem len: 60767   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 333   score: 0.0   mem len: 60890   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 334   score: 2.0   mem len: 61109   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.48\n",
      "epis: 335   score: 3.0   mem len: 61356   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.51\n",
      "epis: 336   score: 0.0   mem len: 61479   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 337   score: 1.0   mem len: 61651   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.44\n",
      "epis: 338   score: 2.0   mem len: 61867   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.44\n",
      "epis: 339   score: 0.0   mem len: 61990   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 340   score: 2.0   mem len: 62188   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 341   score: 0.0   mem len: 62311   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 342   score: 0.0   mem len: 62434   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 343   score: 0.0   mem len: 62557   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 344   score: 3.0   mem len: 62803   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.4\n",
      "epis: 345   score: 2.0   mem len: 63001   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 346   score: 1.0   mem len: 63170   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 347   score: 1.0   mem len: 63338   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.38\n",
      "epis: 348   score: 3.0   mem len: 63585   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.41\n",
      "epis: 349   score: 0.0   mem len: 63707   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 350   score: 0.0   mem len: 63830   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 351   score: 3.0   mem len: 64077   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.41\n",
      "epis: 352   score: 0.0   mem len: 64199   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 353   score: 3.0   mem len: 64448   epsilon: 1.0    steps: 249    lr: 0.0001     reward: 1.43\n",
      "epis: 354   score: 3.0   mem len: 64695   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.44\n",
      "epis: 355   score: 0.0   mem len: 64818   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 356   score: 1.0   mem len: 64986   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.42\n",
      "epis: 357   score: 5.0   mem len: 65324   epsilon: 1.0    steps: 338    lr: 0.0001     reward: 1.46\n",
      "epis: 358   score: 0.0   mem len: 65447   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 359   score: 4.0   mem len: 65726   epsilon: 1.0    steps: 279    lr: 0.0001     reward: 1.49\n",
      "epis: 360   score: 1.0   mem len: 65877   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.45\n",
      "epis: 361   score: 0.0   mem len: 65999   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 362   score: 5.0   mem len: 66286   epsilon: 1.0    steps: 287    lr: 0.0001     reward: 1.48\n",
      "epis: 363   score: 0.0   mem len: 66408   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 364   score: 0.0   mem len: 66531   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 365   score: 1.0   mem len: 66682   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 366   score: 2.0   mem len: 66880   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.43\n",
      "epis: 367   score: 3.0   mem len: 67105   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.46\n",
      "epis: 368   score: 0.0   mem len: 67227   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 369   score: 3.0   mem len: 67453   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.44\n",
      "epis: 370   score: 0.0   mem len: 67575   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 371   score: 1.0   mem len: 67744   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 372   score: 2.0   mem len: 67945   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.44\n",
      "epis: 373   score: 2.0   mem len: 68143   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 374   score: 1.0   mem len: 68294   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 375   score: 3.0   mem len: 68522   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.47\n",
      "epis: 376   score: 1.0   mem len: 68692   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.48\n",
      "epis: 377   score: 1.0   mem len: 68861   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 378   score: 4.0   mem len: 69156   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.53\n",
      "epis: 379   score: 2.0   mem len: 69378   epsilon: 1.0    steps: 222    lr: 0.0001     reward: 1.55\n",
      "epis: 380   score: 2.0   mem len: 69560   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.55\n",
      "epis: 381   score: 0.0   mem len: 69683   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 382   score: 3.0   mem len: 69930   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.54\n",
      "epis: 383   score: 1.0   mem len: 70081   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 384   score: 2.0   mem len: 70281   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.49\n",
      "epis: 385   score: 1.0   mem len: 70432   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 386   score: 2.0   mem len: 70649   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.5\n",
      "epis: 387   score: 3.0   mem len: 70896   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.51\n",
      "epis: 388   score: 0.0   mem len: 71019   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 389   score: 1.0   mem len: 71170   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 390   score: 1.0   mem len: 71321   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 391   score: 1.0   mem len: 71471   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 392   score: 0.0   mem len: 71594   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 393   score: 1.0   mem len: 71745   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.47\n",
      "epis: 394   score: 1.0   mem len: 71896   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 395   score: 3.0   mem len: 72141   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.45\n",
      "epis: 396   score: 2.0   mem len: 72338   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 397   score: 4.0   mem len: 72617   epsilon: 1.0    steps: 279    lr: 0.0001     reward: 1.49\n",
      "epis: 398   score: 0.0   mem len: 72740   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 399   score: 0.0   mem len: 72863   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 400   score: 1.0   mem len: 73032   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 401   score: 0.0   mem len: 73155   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 402   score: 0.0   mem len: 73278   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 403   score: 2.0   mem len: 73475   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 404   score: 0.0   mem len: 73598   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 405   score: 1.0   mem len: 73768   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.43\n",
      "epis: 406   score: 1.0   mem len: 73937   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 407   score: 2.0   mem len: 74155   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.35\n",
      "epis: 408   score: 0.0   mem len: 74277   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 409   score: 2.0   mem len: 74474   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.37\n",
      "epis: 410   score: 0.0   mem len: 74597   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 411   score: 1.0   mem len: 74765   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.34\n",
      "epis: 412   score: 0.0   mem len: 74888   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 413   score: 2.0   mem len: 75106   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 414   score: 1.0   mem len: 75274   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.31\n",
      "epis: 415   score: 0.0   mem len: 75397   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 416   score: 0.0   mem len: 75520   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 417   score: 4.0   mem len: 75817   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.32\n",
      "epis: 418   score: 4.0   mem len: 76096   epsilon: 1.0    steps: 279    lr: 0.0001     reward: 1.36\n",
      "epis: 419   score: 3.0   mem len: 76321   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.38\n",
      "epis: 420   score: 0.0   mem len: 76444   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 421   score: 0.0   mem len: 76566   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 422   score: 0.0   mem len: 76689   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 423   score: 0.0   mem len: 76811   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 424   score: 0.0   mem len: 76934   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 425   score: 0.0   mem len: 77056   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 426   score: 0.0   mem len: 77179   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 427   score: 0.0   mem len: 77301   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.27\n",
      "epis: 428   score: 0.0   mem len: 77424   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 429   score: 2.0   mem len: 77622   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.28\n",
      "epis: 430   score: 0.0   mem len: 77745   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 431   score: 2.0   mem len: 77964   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.29\n",
      "epis: 432   score: 2.0   mem len: 78162   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.29\n",
      "epis: 433   score: 0.0   mem len: 78284   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 434   score: 3.0   mem len: 78549   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.3\n",
      "epis: 435   score: 3.0   mem len: 78795   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.3\n",
      "epis: 436   score: 0.0   mem len: 78918   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.3\n",
      "epis: 437   score: 2.0   mem len: 79135   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.31\n",
      "epis: 438   score: 0.0   mem len: 79258   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 439   score: 0.0   mem len: 79380   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 440   score: 4.0   mem len: 79675   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.31\n",
      "epis: 441   score: 1.0   mem len: 79826   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.32\n",
      "epis: 442   score: 0.0   mem len: 79949   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 443   score: 3.0   mem len: 80216   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.35\n",
      "epis: 444   score: 1.0   mem len: 80366   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.33\n",
      "epis: 445   score: 1.0   mem len: 80536   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.32\n",
      "epis: 446   score: 3.0   mem len: 80762   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.34\n",
      "epis: 447   score: 0.0   mem len: 80885   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 448   score: 4.0   mem len: 81146   epsilon: 1.0    steps: 261    lr: 0.0001     reward: 1.34\n",
      "epis: 449   score: 1.0   mem len: 81316   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.35\n",
      "epis: 450   score: 0.0   mem len: 81439   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 451   score: 0.0   mem len: 81561   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 452   score: 1.0   mem len: 81711   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.33\n",
      "epis: 453   score: 2.0   mem len: 81911   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.32\n",
      "epis: 454   score: 2.0   mem len: 82109   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.31\n",
      "epis: 455   score: 3.0   mem len: 82374   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.34\n",
      "epis: 456   score: 0.0   mem len: 82497   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 457   score: 2.0   mem len: 82712   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.3\n",
      "epis: 458   score: 2.0   mem len: 82909   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.32\n",
      "epis: 459   score: 2.0   mem len: 83111   epsilon: 1.0    steps: 202    lr: 0.0001     reward: 1.3\n",
      "epis: 460   score: 2.0   mem len: 83293   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.31\n",
      "epis: 461   score: 1.0   mem len: 83444   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.32\n",
      "epis: 462   score: 3.0   mem len: 83691   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.3\n",
      "epis: 463   score: 0.0   mem len: 83813   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 464   score: 1.0   mem len: 83964   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.31\n",
      "epis: 465   score: 0.0   mem len: 84086   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 466   score: 2.0   mem len: 84304   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.3\n",
      "epis: 467   score: 1.0   mem len: 84475   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.28\n",
      "epis: 468   score: 1.0   mem len: 84646   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.29\n",
      "epis: 469   score: 1.0   mem len: 84815   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.27\n",
      "epis: 470   score: 1.0   mem len: 84966   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.28\n",
      "epis: 471   score: 3.0   mem len: 85194   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.3\n",
      "epis: 472   score: 2.0   mem len: 85394   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.3\n",
      "epis: 473   score: 3.0   mem len: 85639   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.31\n",
      "epis: 474   score: 3.0   mem len: 85887   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.33\n",
      "epis: 475   score: 0.0   mem len: 86009   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 476   score: 1.0   mem len: 86178   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.3\n",
      "epis: 477   score: 0.0   mem len: 86301   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 478   score: 3.0   mem len: 86530   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.28\n",
      "epis: 479   score: 2.0   mem len: 86732   epsilon: 1.0    steps: 202    lr: 0.0001     reward: 1.28\n",
      "epis: 480   score: 2.0   mem len: 86952   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.28\n",
      "epis: 481   score: 0.0   mem len: 87075   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.28\n",
      "epis: 482   score: 2.0   mem len: 87276   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.27\n",
      "epis: 483   score: 0.0   mem len: 87398   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.26\n",
      "epis: 484   score: 1.0   mem len: 87550   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.25\n",
      "epis: 485   score: 2.0   mem len: 87748   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.26\n",
      "epis: 486   score: 1.0   mem len: 87917   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.25\n",
      "epis: 487   score: 1.0   mem len: 88068   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.23\n",
      "epis: 488   score: 1.0   mem len: 88218   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.24\n",
      "epis: 489   score: 2.0   mem len: 88398   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.25\n",
      "epis: 490   score: 1.0   mem len: 88549   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.25\n",
      "epis: 491   score: 2.0   mem len: 88749   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.26\n",
      "epis: 492   score: 5.0   mem len: 89115   epsilon: 1.0    steps: 366    lr: 0.0001     reward: 1.31\n",
      "epis: 493   score: 1.0   mem len: 89284   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 494   score: 1.0   mem len: 89456   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.31\n",
      "epis: 495   score: 1.0   mem len: 89625   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.29\n",
      "epis: 496   score: 1.0   mem len: 89794   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.28\n",
      "epis: 497   score: 2.0   mem len: 90010   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.26\n",
      "epis: 498   score: 0.0   mem len: 90133   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 499   score: 3.0   mem len: 90381   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.29\n",
      "epis: 500   score: 1.0   mem len: 90531   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.29\n",
      "epis: 501   score: 0.0   mem len: 90653   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.29\n",
      "epis: 502   score: 3.0   mem len: 90900   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.32\n",
      "epis: 503   score: 1.0   mem len: 91069   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 504   score: 1.0   mem len: 91240   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.32\n",
      "epis: 505   score: 0.0   mem len: 91363   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 506   score: 4.0   mem len: 91622   epsilon: 1.0    steps: 259    lr: 0.0001     reward: 1.34\n",
      "epis: 507   score: 3.0   mem len: 91868   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.35\n",
      "epis: 508   score: 0.0   mem len: 91991   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 509   score: 3.0   mem len: 92237   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.36\n",
      "epis: 510   score: 2.0   mem len: 92435   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 511   score: 2.0   mem len: 92633   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 512   score: 0.0   mem len: 92755   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 513   score: 0.0   mem len: 92878   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 514   score: 1.0   mem len: 93047   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 515   score: 0.0   mem len: 93170   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 516   score: 1.0   mem len: 93321   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 517   score: 0.0   mem len: 93444   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 518   score: 0.0   mem len: 93567   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.3\n",
      "epis: 519   score: 0.0   mem len: 93690   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 520   score: 1.0   mem len: 93841   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.28\n",
      "epis: 521   score: 2.0   mem len: 94039   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.3\n",
      "epis: 522   score: 1.0   mem len: 94190   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.31\n",
      "epis: 523   score: 1.0   mem len: 94341   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.32\n",
      "epis: 524   score: 2.0   mem len: 94557   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.34\n",
      "epis: 525   score: 0.0   mem len: 94680   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 526   score: 4.0   mem len: 94955   epsilon: 1.0    steps: 275    lr: 0.0001     reward: 1.38\n",
      "epis: 527   score: 0.0   mem len: 95078   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 528   score: 3.0   mem len: 95326   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.41\n",
      "epis: 529   score: 1.0   mem len: 95495   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 530   score: 5.0   mem len: 95820   epsilon: 1.0    steps: 325    lr: 0.0001     reward: 1.45\n",
      "epis: 531   score: 0.0   mem len: 95943   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 532   score: 1.0   mem len: 96115   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.42\n",
      "epis: 533   score: 2.0   mem len: 96336   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.44\n",
      "epis: 534   score: 2.0   mem len: 96516   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.43\n",
      "epis: 535   score: 1.0   mem len: 96684   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.41\n",
      "epis: 536   score: 0.0   mem len: 96807   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 537   score: 0.0   mem len: 96929   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 538   score: 0.0   mem len: 97051   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 539   score: 3.0   mem len: 97277   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.42\n",
      "epis: 540   score: 3.0   mem len: 97508   epsilon: 1.0    steps: 231    lr: 0.0001     reward: 1.41\n",
      "epis: 541   score: 3.0   mem len: 97753   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.43\n",
      "epis: 542   score: 2.0   mem len: 97970   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.45\n",
      "epis: 543   score: 1.0   mem len: 98139   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 544   score: 2.0   mem len: 98337   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.44\n",
      "epis: 545   score: 2.0   mem len: 98552   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.45\n",
      "epis: 546   score: 2.0   mem len: 98769   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.44\n",
      "epis: 547   score: 0.0   mem len: 98892   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 548   score: 1.0   mem len: 99043   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.41\n",
      "epis: 549   score: 0.0   mem len: 99166   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 550   score: 2.0   mem len: 99364   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.42\n",
      "epis: 551   score: 2.0   mem len: 99562   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.44\n",
      "epis: 552   score: 2.0   mem len: 99760   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 553   score: 2.0   mem len: 99940   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/memory.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array(sample)\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent.py:76: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 554   score: 2.0   mem len: 100137   epsilon: 0.9997    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 555   score: 0.0   mem len: 100260   epsilon: 0.9995    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 556   score: 1.0   mem len: 100411   epsilon: 0.9992    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 557   score: 4.0   mem len: 100687   epsilon: 0.9986    steps: 276    lr: 0.0001     reward: 1.45\n",
      "epis: 558   score: 3.0   mem len: 100955   epsilon: 0.9981    steps: 268    lr: 0.0001     reward: 1.46\n",
      "epis: 559   score: 0.0   mem len: 101078   epsilon: 0.9979    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 560   score: 0.0   mem len: 101201   epsilon: 0.9976    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 561   score: 2.0   mem len: 101419   epsilon: 0.9972    steps: 218    lr: 0.0001     reward: 1.43\n",
      "epis: 562   score: 1.0   mem len: 101570   epsilon: 0.9969    steps: 151    lr: 0.0001     reward: 1.41\n",
      "epis: 563   score: 2.0   mem len: 101767   epsilon: 0.9965    steps: 197    lr: 0.0001     reward: 1.43\n",
      "epis: 564   score: 1.0   mem len: 101935   epsilon: 0.9962    steps: 168    lr: 0.0001     reward: 1.43\n",
      "epis: 565   score: 2.0   mem len: 102132   epsilon: 0.9958    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 566   score: 1.0   mem len: 102301   epsilon: 0.9954    steps: 169    lr: 0.0001     reward: 1.44\n",
      "epis: 567   score: 1.0   mem len: 102469   epsilon: 0.9951    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 568   score: 0.0   mem len: 102592   epsilon: 0.9949    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 569   score: 0.0   mem len: 102715   epsilon: 0.9946    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 570   score: 5.0   mem len: 103059   epsilon: 0.9939    steps: 344    lr: 0.0001     reward: 1.46\n",
      "epis: 571   score: 4.0   mem len: 103334   epsilon: 0.9934    steps: 275    lr: 0.0001     reward: 1.47\n",
      "epis: 572   score: 2.0   mem len: 103531   epsilon: 0.993    steps: 197    lr: 0.0001     reward: 1.47\n",
      "epis: 573   score: 0.0   mem len: 103654   epsilon: 0.9928    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 574   score: 3.0   mem len: 103902   epsilon: 0.9923    steps: 248    lr: 0.0001     reward: 1.44\n",
      "epis: 575   score: 1.0   mem len: 104073   epsilon: 0.9919    steps: 171    lr: 0.0001     reward: 1.45\n",
      "epis: 576   score: 0.0   mem len: 104195   epsilon: 0.9917    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 577   score: 0.0   mem len: 104317   epsilon: 0.9915    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 578   score: 0.0   mem len: 104440   epsilon: 0.9912    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 579   score: 1.0   mem len: 104609   epsilon: 0.9909    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 580   score: 0.0   mem len: 104732   epsilon: 0.9906    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 581   score: 1.0   mem len: 104904   epsilon: 0.9903    steps: 172    lr: 0.0001     reward: 1.39\n",
      "epis: 582   score: 1.0   mem len: 105074   epsilon: 0.99    steps: 170    lr: 0.0001     reward: 1.38\n",
      "epis: 583   score: 1.0   mem len: 105226   epsilon: 0.9897    steps: 152    lr: 0.0001     reward: 1.39\n",
      "epis: 584   score: 1.0   mem len: 105395   epsilon: 0.9893    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 585   score: 1.0   mem len: 105546   epsilon: 0.989    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 586   score: 0.0   mem len: 105669   epsilon: 0.9888    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 587   score: 0.0   mem len: 105792   epsilon: 0.9885    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 588   score: 2.0   mem len: 105992   epsilon: 0.9881    steps: 200    lr: 0.0001     reward: 1.37\n",
      "epis: 589   score: 0.0   mem len: 106114   epsilon: 0.9879    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 590   score: 3.0   mem len: 106359   epsilon: 0.9874    steps: 245    lr: 0.0001     reward: 1.37\n",
      "epis: 591   score: 3.0   mem len: 106625   epsilon: 0.9869    steps: 266    lr: 0.0001     reward: 1.38\n",
      "epis: 592   score: 0.0   mem len: 106748   epsilon: 0.9866    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 593   score: 3.0   mem len: 106977   epsilon: 0.9862    steps: 229    lr: 0.0001     reward: 1.35\n",
      "epis: 594   score: 1.0   mem len: 107146   epsilon: 0.9858    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 595   score: 1.0   mem len: 107315   epsilon: 0.9855    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 596   score: 2.0   mem len: 107513   epsilon: 0.9851    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 597   score: 1.0   mem len: 107684   epsilon: 0.9848    steps: 171    lr: 0.0001     reward: 1.35\n",
      "epis: 598   score: 1.0   mem len: 107834   epsilon: 0.9845    steps: 150    lr: 0.0001     reward: 1.36\n",
      "epis: 599   score: 1.0   mem len: 107984   epsilon: 0.9842    steps: 150    lr: 0.0001     reward: 1.34\n",
      "epis: 600   score: 2.0   mem len: 108184   epsilon: 0.9838    steps: 200    lr: 0.0001     reward: 1.35\n",
      "epis: 601   score: 2.0   mem len: 108382   epsilon: 0.9834    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 602   score: 0.0   mem len: 108505   epsilon: 0.9832    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 603   score: 4.0   mem len: 108819   epsilon: 0.9825    steps: 314    lr: 0.0001     reward: 1.37\n",
      "epis: 604   score: 1.0   mem len: 108970   epsilon: 0.9822    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 605   score: 1.0   mem len: 109139   epsilon: 0.9819    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 606   score: 4.0   mem len: 109434   epsilon: 0.9813    steps: 295    lr: 0.0001     reward: 1.38\n",
      "epis: 607   score: 1.0   mem len: 109603   epsilon: 0.981    steps: 169    lr: 0.0001     reward: 1.36\n",
      "epis: 608   score: 2.0   mem len: 109803   epsilon: 0.9806    steps: 200    lr: 0.0001     reward: 1.38\n",
      "epis: 609   score: 3.0   mem len: 110073   epsilon: 0.9801    steps: 270    lr: 0.0001     reward: 1.38\n",
      "epis: 610   score: 6.0   mem len: 110401   epsilon: 0.9794    steps: 328    lr: 0.0001     reward: 1.42\n",
      "epis: 611   score: 0.0   mem len: 110524   epsilon: 0.9792    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 612   score: 3.0   mem len: 110736   epsilon: 0.9787    steps: 212    lr: 0.0001     reward: 1.43\n",
      "epis: 613   score: 3.0   mem len: 110981   epsilon: 0.9783    steps: 245    lr: 0.0001     reward: 1.46\n",
      "epis: 614   score: 0.0   mem len: 111103   epsilon: 0.978    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 615   score: 1.0   mem len: 111272   epsilon: 0.9777    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 616   score: 1.0   mem len: 111423   epsilon: 0.9774    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 617   score: 3.0   mem len: 111688   epsilon: 0.9769    steps: 265    lr: 0.0001     reward: 1.49\n",
      "epis: 618   score: 1.0   mem len: 111858   epsilon: 0.9765    steps: 170    lr: 0.0001     reward: 1.5\n",
      "epis: 619   score: 4.0   mem len: 112116   epsilon: 0.976    steps: 258    lr: 0.0001     reward: 1.54\n",
      "epis: 620   score: 4.0   mem len: 112371   epsilon: 0.9755    steps: 255    lr: 0.0001     reward: 1.57\n",
      "epis: 621   score: 1.0   mem len: 112522   epsilon: 0.9752    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 622   score: 3.0   mem len: 112769   epsilon: 0.9747    steps: 247    lr: 0.0001     reward: 1.58\n",
      "epis: 623   score: 4.0   mem len: 113086   epsilon: 0.9741    steps: 317    lr: 0.0001     reward: 1.61\n",
      "epis: 624   score: 3.0   mem len: 113314   epsilon: 0.9736    steps: 228    lr: 0.0001     reward: 1.62\n",
      "epis: 625   score: 0.0   mem len: 113437   epsilon: 0.9734    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 626   score: 1.0   mem len: 113608   epsilon: 0.9731    steps: 171    lr: 0.0001     reward: 1.59\n",
      "epis: 627   score: 1.0   mem len: 113759   epsilon: 0.9728    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 628   score: 2.0   mem len: 113957   epsilon: 0.9724    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 629   score: 0.0   mem len: 114080   epsilon: 0.9721    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 630   score: 3.0   mem len: 114324   epsilon: 0.9716    steps: 244    lr: 0.0001     reward: 1.56\n",
      "epis: 631   score: 0.0   mem len: 114446   epsilon: 0.9714    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 632   score: 0.0   mem len: 114569   epsilon: 0.9712    steps: 123    lr: 0.0001     reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 633   score: 2.0   mem len: 114790   epsilon: 0.9707    steps: 221    lr: 0.0001     reward: 1.55\n",
      "epis: 634   score: 3.0   mem len: 115025   epsilon: 0.9702    steps: 235    lr: 0.0001     reward: 1.56\n",
      "epis: 635   score: 1.0   mem len: 115193   epsilon: 0.9699    steps: 168    lr: 0.0001     reward: 1.56\n",
      "epis: 636   score: 0.0   mem len: 115316   epsilon: 0.9697    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 637   score: 3.0   mem len: 115584   epsilon: 0.9691    steps: 268    lr: 0.0001     reward: 1.59\n",
      "epis: 638   score: 1.0   mem len: 115752   epsilon: 0.9688    steps: 168    lr: 0.0001     reward: 1.6\n",
      "epis: 639   score: 2.0   mem len: 115949   epsilon: 0.9684    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 640   score: 0.0   mem len: 116071   epsilon: 0.9682    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 641   score: 1.0   mem len: 116242   epsilon: 0.9678    steps: 171    lr: 0.0001     reward: 1.54\n",
      "epis: 642   score: 1.0   mem len: 116412   epsilon: 0.9675    steps: 170    lr: 0.0001     reward: 1.53\n",
      "epis: 643   score: 0.0   mem len: 116535   epsilon: 0.9673    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 644   score: 0.0   mem len: 116658   epsilon: 0.967    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 645   score: 0.0   mem len: 116781   epsilon: 0.9668    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 646   score: 0.0   mem len: 116904   epsilon: 0.9665    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 647   score: 2.0   mem len: 117083   epsilon: 0.9662    steps: 179    lr: 0.0001     reward: 1.48\n",
      "epis: 648   score: 1.0   mem len: 117255   epsilon: 0.9658    steps: 172    lr: 0.0001     reward: 1.48\n",
      "epis: 649   score: 3.0   mem len: 117524   epsilon: 0.9653    steps: 269    lr: 0.0001     reward: 1.51\n",
      "epis: 650   score: 0.0   mem len: 117647   epsilon: 0.9651    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 651   score: 1.0   mem len: 117797   epsilon: 0.9648    steps: 150    lr: 0.0001     reward: 1.48\n",
      "epis: 652   score: 4.0   mem len: 118093   epsilon: 0.9642    steps: 296    lr: 0.0001     reward: 1.5\n",
      "epis: 653   score: 1.0   mem len: 118263   epsilon: 0.9638    steps: 170    lr: 0.0001     reward: 1.49\n",
      "epis: 654   score: 0.0   mem len: 118386   epsilon: 0.9636    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 655   score: 2.0   mem len: 118604   epsilon: 0.9632    steps: 218    lr: 0.0001     reward: 1.49\n",
      "epis: 656   score: 5.0   mem len: 118912   epsilon: 0.9626    steps: 308    lr: 0.0001     reward: 1.53\n",
      "epis: 657   score: 3.0   mem len: 119182   epsilon: 0.962    steps: 270    lr: 0.0001     reward: 1.52\n",
      "epis: 658   score: 1.0   mem len: 119351   epsilon: 0.9617    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 659   score: 2.0   mem len: 119549   epsilon: 0.9613    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 660   score: 1.0   mem len: 119718   epsilon: 0.961    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 661   score: 2.0   mem len: 119934   epsilon: 0.9605    steps: 216    lr: 0.0001     reward: 1.53\n",
      "epis: 662   score: 0.0   mem len: 120056   epsilon: 0.9603    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 663   score: 1.0   mem len: 120207   epsilon: 0.96    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 664   score: 2.0   mem len: 120405   epsilon: 0.9596    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 665   score: 2.0   mem len: 120602   epsilon: 0.9592    steps: 197    lr: 0.0001     reward: 1.52\n",
      "epis: 666   score: 1.0   mem len: 120772   epsilon: 0.9589    steps: 170    lr: 0.0001     reward: 1.52\n",
      "epis: 667   score: 2.0   mem len: 120970   epsilon: 0.9585    steps: 198    lr: 0.0001     reward: 1.53\n",
      "epis: 668   score: 1.0   mem len: 121120   epsilon: 0.9582    steps: 150    lr: 0.0001     reward: 1.54\n",
      "epis: 669   score: 2.0   mem len: 121302   epsilon: 0.9578    steps: 182    lr: 0.0001     reward: 1.56\n",
      "epis: 670   score: 0.0   mem len: 121424   epsilon: 0.9576    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 671   score: 2.0   mem len: 121642   epsilon: 0.9571    steps: 218    lr: 0.0001     reward: 1.49\n",
      "epis: 672   score: 0.0   mem len: 121765   epsilon: 0.9569    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 673   score: 0.0   mem len: 121888   epsilon: 0.9567    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 674   score: 0.0   mem len: 122010   epsilon: 0.9564    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 675   score: 3.0   mem len: 122261   epsilon: 0.9559    steps: 251    lr: 0.0001     reward: 1.46\n",
      "epis: 676   score: 1.0   mem len: 122429   epsilon: 0.9556    steps: 168    lr: 0.0001     reward: 1.47\n",
      "epis: 677   score: 0.0   mem len: 122551   epsilon: 0.9553    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 678   score: 2.0   mem len: 122767   epsilon: 0.9549    steps: 216    lr: 0.0001     reward: 1.49\n",
      "epis: 679   score: 0.0   mem len: 122890   epsilon: 0.9547    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 680   score: 1.0   mem len: 123059   epsilon: 0.9543    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 681   score: 0.0   mem len: 123182   epsilon: 0.9541    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 682   score: 1.0   mem len: 123332   epsilon: 0.9538    steps: 150    lr: 0.0001     reward: 1.48\n",
      "epis: 683   score: 1.0   mem len: 123503   epsilon: 0.9535    steps: 171    lr: 0.0001     reward: 1.48\n",
      "epis: 684   score: 1.0   mem len: 123672   epsilon: 0.9531    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 685   score: 4.0   mem len: 123964   epsilon: 0.9525    steps: 292    lr: 0.0001     reward: 1.51\n",
      "epis: 686   score: 0.0   mem len: 124087   epsilon: 0.9523    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 687   score: 0.0   mem len: 124210   epsilon: 0.9521    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 688   score: 1.0   mem len: 124361   epsilon: 0.9518    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 689   score: 1.0   mem len: 124532   epsilon: 0.9514    steps: 171    lr: 0.0001     reward: 1.51\n",
      "epis: 690   score: 2.0   mem len: 124730   epsilon: 0.951    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 691   score: 3.0   mem len: 124942   epsilon: 0.9506    steps: 212    lr: 0.0001     reward: 1.5\n",
      "epis: 692   score: 2.0   mem len: 125140   epsilon: 0.9502    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 693   score: 3.0   mem len: 125389   epsilon: 0.9497    steps: 249    lr: 0.0001     reward: 1.52\n",
      "epis: 694   score: 1.0   mem len: 125557   epsilon: 0.9494    steps: 168    lr: 0.0001     reward: 1.52\n",
      "epis: 695   score: 0.0   mem len: 125680   epsilon: 0.9492    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 696   score: 0.0   mem len: 125803   epsilon: 0.9489    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 697   score: 0.0   mem len: 125926   epsilon: 0.9487    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 698   score: 0.0   mem len: 126049   epsilon: 0.9484    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 699   score: 0.0   mem len: 126172   epsilon: 0.9482    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 700   score: 1.0   mem len: 126342   epsilon: 0.9478    steps: 170    lr: 0.0001     reward: 1.45\n",
      "epis: 701   score: 1.0   mem len: 126493   epsilon: 0.9475    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 702   score: 2.0   mem len: 126692   epsilon: 0.9471    steps: 199    lr: 0.0001     reward: 1.46\n",
      "epis: 703   score: 2.0   mem len: 126889   epsilon: 0.9468    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 704   score: 0.0   mem len: 127011   epsilon: 0.9465    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 705   score: 2.0   mem len: 127226   epsilon: 0.9461    steps: 215    lr: 0.0001     reward: 1.44\n",
      "epis: 706   score: 1.0   mem len: 127395   epsilon: 0.9458    steps: 169    lr: 0.0001     reward: 1.41\n",
      "epis: 707   score: 0.0   mem len: 127518   epsilon: 0.9455    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 708   score: 1.0   mem len: 127687   epsilon: 0.9452    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 709   score: 1.0   mem len: 127859   epsilon: 0.9448    steps: 172    lr: 0.0001     reward: 1.37\n",
      "epis: 710   score: 0.0   mem len: 127982   epsilon: 0.9446    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 711   score: 0.0   mem len: 128105   epsilon: 0.9444    steps: 123    lr: 0.0001     reward: 1.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 712   score: 3.0   mem len: 128316   epsilon: 0.9439    steps: 211    lr: 0.0001     reward: 1.31\n",
      "epis: 713   score: 3.0   mem len: 128545   epsilon: 0.9435    steps: 229    lr: 0.0001     reward: 1.31\n",
      "epis: 714   score: 1.0   mem len: 128695   epsilon: 0.9432    steps: 150    lr: 0.0001     reward: 1.32\n",
      "epis: 715   score: 2.0   mem len: 128913   epsilon: 0.9428    steps: 218    lr: 0.0001     reward: 1.33\n",
      "epis: 716   score: 2.0   mem len: 129111   epsilon: 0.9424    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 717   score: 1.0   mem len: 129280   epsilon: 0.942    steps: 169    lr: 0.0001     reward: 1.32\n",
      "epis: 718   score: 1.0   mem len: 129431   epsilon: 0.9417    steps: 151    lr: 0.0001     reward: 1.32\n",
      "epis: 719   score: 2.0   mem len: 129648   epsilon: 0.9413    steps: 217    lr: 0.0001     reward: 1.3\n",
      "epis: 720   score: 1.0   mem len: 129816   epsilon: 0.941    steps: 168    lr: 0.0001     reward: 1.27\n",
      "epis: 721   score: 0.0   mem len: 129939   epsilon: 0.9407    steps: 123    lr: 0.0001     reward: 1.26\n",
      "epis: 722   score: 1.0   mem len: 130107   epsilon: 0.9404    steps: 168    lr: 0.0001     reward: 1.24\n",
      "epis: 723   score: 1.0   mem len: 130257   epsilon: 0.9401    steps: 150    lr: 0.0001     reward: 1.21\n",
      "epis: 724   score: 3.0   mem len: 130522   epsilon: 0.9396    steps: 265    lr: 0.0001     reward: 1.21\n",
      "epis: 725   score: 1.0   mem len: 130692   epsilon: 0.9392    steps: 170    lr: 0.0001     reward: 1.22\n",
      "epis: 726   score: 1.0   mem len: 130843   epsilon: 0.9389    steps: 151    lr: 0.0001     reward: 1.22\n",
      "epis: 727   score: 2.0   mem len: 131043   epsilon: 0.9385    steps: 200    lr: 0.0001     reward: 1.23\n",
      "epis: 728   score: 0.0   mem len: 131166   epsilon: 0.9383    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 729   score: 1.0   mem len: 131337   epsilon: 0.938    steps: 171    lr: 0.0001     reward: 1.22\n",
      "epis: 730   score: 2.0   mem len: 131555   epsilon: 0.9375    steps: 218    lr: 0.0001     reward: 1.21\n",
      "epis: 731   score: 3.0   mem len: 131802   epsilon: 0.937    steps: 247    lr: 0.0001     reward: 1.24\n",
      "epis: 732   score: 1.0   mem len: 131972   epsilon: 0.9367    steps: 170    lr: 0.0001     reward: 1.25\n",
      "epis: 733   score: 1.0   mem len: 132123   epsilon: 0.9364    steps: 151    lr: 0.0001     reward: 1.24\n",
      "epis: 734   score: 2.0   mem len: 132339   epsilon: 0.936    steps: 216    lr: 0.0001     reward: 1.23\n",
      "epis: 735   score: 0.0   mem len: 132461   epsilon: 0.9357    steps: 122    lr: 0.0001     reward: 1.22\n",
      "epis: 736   score: 2.0   mem len: 132659   epsilon: 0.9353    steps: 198    lr: 0.0001     reward: 1.24\n",
      "epis: 737   score: 2.0   mem len: 132878   epsilon: 0.9349    steps: 219    lr: 0.0001     reward: 1.23\n",
      "epis: 738   score: 3.0   mem len: 133140   epsilon: 0.9344    steps: 262    lr: 0.0001     reward: 1.25\n",
      "epis: 739   score: 0.0   mem len: 133263   epsilon: 0.9341    steps: 123    lr: 0.0001     reward: 1.23\n",
      "epis: 740   score: 0.0   mem len: 133385   epsilon: 0.9339    steps: 122    lr: 0.0001     reward: 1.23\n",
      "epis: 741   score: 0.0   mem len: 133508   epsilon: 0.9337    steps: 123    lr: 0.0001     reward: 1.22\n",
      "epis: 742   score: 2.0   mem len: 133730   epsilon: 0.9332    steps: 222    lr: 0.0001     reward: 1.23\n",
      "epis: 743   score: 0.0   mem len: 133853   epsilon: 0.933    steps: 123    lr: 0.0001     reward: 1.23\n",
      "epis: 744   score: 2.0   mem len: 134071   epsilon: 0.9325    steps: 218    lr: 0.0001     reward: 1.25\n",
      "epis: 745   score: 5.0   mem len: 134401   epsilon: 0.9319    steps: 330    lr: 0.0001     reward: 1.3\n",
      "epis: 746   score: 2.0   mem len: 134619   epsilon: 0.9315    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 747   score: 1.0   mem len: 134788   epsilon: 0.9311    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 748   score: 0.0   mem len: 134910   epsilon: 0.9309    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 749   score: 0.0   mem len: 135033   epsilon: 0.9306    steps: 123    lr: 0.0001     reward: 1.27\n",
      "epis: 750   score: 0.0   mem len: 135155   epsilon: 0.9304    steps: 122    lr: 0.0001     reward: 1.27\n",
      "epis: 751   score: 1.0   mem len: 135327   epsilon: 0.9301    steps: 172    lr: 0.0001     reward: 1.27\n",
      "epis: 752   score: 1.0   mem len: 135496   epsilon: 0.9297    steps: 169    lr: 0.0001     reward: 1.24\n",
      "epis: 753   score: 1.0   mem len: 135667   epsilon: 0.9294    steps: 171    lr: 0.0001     reward: 1.24\n",
      "epis: 754   score: 2.0   mem len: 135864   epsilon: 0.929    steps: 197    lr: 0.0001     reward: 1.26\n",
      "epis: 755   score: 1.0   mem len: 136032   epsilon: 0.9287    steps: 168    lr: 0.0001     reward: 1.25\n",
      "epis: 756   score: 0.0   mem len: 136155   epsilon: 0.9284    steps: 123    lr: 0.0001     reward: 1.2\n",
      "epis: 757   score: 0.0   mem len: 136278   epsilon: 0.9282    steps: 123    lr: 0.0001     reward: 1.17\n",
      "epis: 758   score: 0.0   mem len: 136401   epsilon: 0.9279    steps: 123    lr: 0.0001     reward: 1.16\n",
      "epis: 759   score: 0.0   mem len: 136523   epsilon: 0.9277    steps: 122    lr: 0.0001     reward: 1.14\n",
      "epis: 760   score: 3.0   mem len: 136733   epsilon: 0.9273    steps: 210    lr: 0.0001     reward: 1.16\n",
      "epis: 761   score: 3.0   mem len: 136979   epsilon: 0.9268    steps: 246    lr: 0.0001     reward: 1.17\n",
      "epis: 762   score: 0.0   mem len: 137102   epsilon: 0.9265    steps: 123    lr: 0.0001     reward: 1.17\n",
      "epis: 763   score: 0.0   mem len: 137224   epsilon: 0.9263    steps: 122    lr: 0.0001     reward: 1.16\n",
      "epis: 764   score: 0.0   mem len: 137347   epsilon: 0.9261    steps: 123    lr: 0.0001     reward: 1.14\n",
      "epis: 765   score: 1.0   mem len: 137498   epsilon: 0.9258    steps: 151    lr: 0.0001     reward: 1.13\n",
      "epis: 766   score: 2.0   mem len: 137716   epsilon: 0.9253    steps: 218    lr: 0.0001     reward: 1.14\n",
      "epis: 767   score: 0.0   mem len: 137838   epsilon: 0.9251    steps: 122    lr: 0.0001     reward: 1.12\n",
      "epis: 768   score: 2.0   mem len: 138035   epsilon: 0.9247    steps: 197    lr: 0.0001     reward: 1.13\n",
      "epis: 769   score: 1.0   mem len: 138186   epsilon: 0.9244    steps: 151    lr: 0.0001     reward: 1.12\n",
      "epis: 770   score: 0.0   mem len: 138309   epsilon: 0.9241    steps: 123    lr: 0.0001     reward: 1.12\n",
      "epis: 771   score: 0.0   mem len: 138432   epsilon: 0.9239    steps: 123    lr: 0.0001     reward: 1.1\n",
      "epis: 772   score: 4.0   mem len: 138749   epsilon: 0.9233    steps: 317    lr: 0.0001     reward: 1.14\n",
      "epis: 773   score: 2.0   mem len: 138971   epsilon: 0.9228    steps: 222    lr: 0.0001     reward: 1.16\n",
      "epis: 774   score: 0.0   mem len: 139094   epsilon: 0.9226    steps: 123    lr: 0.0001     reward: 1.16\n",
      "epis: 775   score: 0.0   mem len: 139217   epsilon: 0.9223    steps: 123    lr: 0.0001     reward: 1.13\n",
      "epis: 776   score: 0.0   mem len: 139340   epsilon: 0.9221    steps: 123    lr: 0.0001     reward: 1.12\n",
      "epis: 777   score: 0.0   mem len: 139462   epsilon: 0.9219    steps: 122    lr: 0.0001     reward: 1.12\n",
      "epis: 778   score: 4.0   mem len: 139739   epsilon: 0.9213    steps: 277    lr: 0.0001     reward: 1.14\n",
      "epis: 779   score: 5.0   mem len: 140084   epsilon: 0.9206    steps: 345    lr: 0.0001     reward: 1.19\n",
      "epis: 780   score: 2.0   mem len: 140281   epsilon: 0.9202    steps: 197    lr: 0.0001     reward: 1.2\n",
      "epis: 781   score: 4.0   mem len: 140576   epsilon: 0.9197    steps: 295    lr: 0.0001     reward: 1.24\n",
      "epis: 782   score: 0.0   mem len: 140699   epsilon: 0.9194    steps: 123    lr: 0.0001     reward: 1.23\n",
      "epis: 783   score: 1.0   mem len: 140868   epsilon: 0.9191    steps: 169    lr: 0.0001     reward: 1.23\n",
      "epis: 784   score: 1.0   mem len: 141019   epsilon: 0.9188    steps: 151    lr: 0.0001     reward: 1.23\n",
      "epis: 785   score: 0.0   mem len: 141142   epsilon: 0.9185    steps: 123    lr: 0.0001     reward: 1.19\n",
      "epis: 786   score: 0.0   mem len: 141265   epsilon: 0.9183    steps: 123    lr: 0.0001     reward: 1.19\n",
      "epis: 787   score: 2.0   mem len: 141483   epsilon: 0.9179    steps: 218    lr: 0.0001     reward: 1.21\n",
      "epis: 788   score: 3.0   mem len: 141751   epsilon: 0.9173    steps: 268    lr: 0.0001     reward: 1.23\n",
      "epis: 789   score: 4.0   mem len: 141999   epsilon: 0.9168    steps: 248    lr: 0.0001     reward: 1.26\n",
      "epis: 790   score: 3.0   mem len: 142266   epsilon: 0.9163    steps: 267    lr: 0.0001     reward: 1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 791   score: 2.0   mem len: 142463   epsilon: 0.9159    steps: 197    lr: 0.0001     reward: 1.26\n",
      "epis: 792   score: 4.0   mem len: 142735   epsilon: 0.9154    steps: 272    lr: 0.0001     reward: 1.28\n",
      "epis: 793   score: 1.0   mem len: 142905   epsilon: 0.915    steps: 170    lr: 0.0001     reward: 1.26\n",
      "epis: 794   score: 2.0   mem len: 143103   epsilon: 0.9147    steps: 198    lr: 0.0001     reward: 1.27\n",
      "epis: 795   score: 4.0   mem len: 143381   epsilon: 0.9141    steps: 278    lr: 0.0001     reward: 1.31\n",
      "epis: 796   score: 1.0   mem len: 143532   epsilon: 0.9138    steps: 151    lr: 0.0001     reward: 1.32\n",
      "epis: 797   score: 0.0   mem len: 143654   epsilon: 0.9136    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 798   score: 0.0   mem len: 143776   epsilon: 0.9133    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 799   score: 1.0   mem len: 143946   epsilon: 0.913    steps: 170    lr: 0.0001     reward: 1.33\n",
      "epis: 800   score: 2.0   mem len: 144144   epsilon: 0.9126    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 801   score: 3.0   mem len: 144387   epsilon: 0.9121    steps: 243    lr: 0.0001     reward: 1.36\n",
      "epis: 802   score: 0.0   mem len: 144510   epsilon: 0.9119    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 803   score: 0.0   mem len: 144633   epsilon: 0.9116    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 804   score: 2.0   mem len: 144851   epsilon: 0.9112    steps: 218    lr: 0.0001     reward: 1.34\n",
      "epis: 805   score: 4.0   mem len: 145127   epsilon: 0.9106    steps: 276    lr: 0.0001     reward: 1.36\n",
      "epis: 806   score: 4.0   mem len: 145419   epsilon: 0.9101    steps: 292    lr: 0.0001     reward: 1.39\n",
      "epis: 807   score: 2.0   mem len: 145617   epsilon: 0.9097    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 808   score: 0.0   mem len: 145740   epsilon: 0.9094    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 809   score: 3.0   mem len: 145965   epsilon: 0.909    steps: 225    lr: 0.0001     reward: 1.42\n",
      "epis: 810   score: 1.0   mem len: 146134   epsilon: 0.9087    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 811   score: 3.0   mem len: 146381   epsilon: 0.9082    steps: 247    lr: 0.0001     reward: 1.46\n",
      "epis: 812   score: 4.0   mem len: 146698   epsilon: 0.9075    steps: 317    lr: 0.0001     reward: 1.47\n",
      "epis: 813   score: 3.0   mem len: 146943   epsilon: 0.9071    steps: 245    lr: 0.0001     reward: 1.47\n",
      "epis: 814   score: 4.0   mem len: 147236   epsilon: 0.9065    steps: 293    lr: 0.0001     reward: 1.5\n",
      "epis: 815   score: 0.0   mem len: 147358   epsilon: 0.9062    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 816   score: 2.0   mem len: 147576   epsilon: 0.9058    steps: 218    lr: 0.0001     reward: 1.48\n",
      "epis: 817   score: 2.0   mem len: 147775   epsilon: 0.9054    steps: 199    lr: 0.0001     reward: 1.49\n",
      "epis: 818   score: 0.0   mem len: 147898   epsilon: 0.9052    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 819   score: 2.0   mem len: 148098   epsilon: 0.9048    steps: 200    lr: 0.0001     reward: 1.48\n",
      "epis: 820   score: 1.0   mem len: 148249   epsilon: 0.9045    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 821   score: 3.0   mem len: 148513   epsilon: 0.9039    steps: 264    lr: 0.0001     reward: 1.51\n",
      "epis: 822   score: 1.0   mem len: 148664   epsilon: 0.9036    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 823   score: 1.0   mem len: 148836   epsilon: 0.9033    steps: 172    lr: 0.0001     reward: 1.51\n",
      "epis: 824   score: 2.0   mem len: 149035   epsilon: 0.9029    steps: 199    lr: 0.0001     reward: 1.5\n",
      "epis: 825   score: 1.0   mem len: 149205   epsilon: 0.9026    steps: 170    lr: 0.0001     reward: 1.5\n",
      "epis: 826   score: 4.0   mem len: 149485   epsilon: 0.902    steps: 280    lr: 0.0001     reward: 1.53\n",
      "epis: 827   score: 4.0   mem len: 149759   epsilon: 0.9015    steps: 274    lr: 0.0001     reward: 1.55\n",
      "epis: 828   score: 0.0   mem len: 149882   epsilon: 0.9012    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 829   score: 3.0   mem len: 150149   epsilon: 0.9007    steps: 267    lr: 0.0001     reward: 1.57\n",
      "epis: 830   score: 6.0   mem len: 150509   epsilon: 0.9    steps: 360    lr: 0.0001     reward: 1.61\n",
      "epis: 831   score: 3.0   mem len: 150740   epsilon: 0.8995    steps: 231    lr: 0.0001     reward: 1.61\n",
      "epis: 832   score: 1.0   mem len: 150911   epsilon: 0.8992    steps: 171    lr: 0.0001     reward: 1.61\n",
      "epis: 833   score: 3.0   mem len: 151155   epsilon: 0.8987    steps: 244    lr: 0.0001     reward: 1.63\n",
      "epis: 834   score: 0.0   mem len: 151277   epsilon: 0.8985    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 835   score: 0.0   mem len: 151400   epsilon: 0.8982    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 836   score: 1.0   mem len: 151551   epsilon: 0.8979    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 837   score: 2.0   mem len: 151770   epsilon: 0.8975    steps: 219    lr: 0.0001     reward: 1.6\n",
      "epis: 838   score: 5.0   mem len: 152097   epsilon: 0.8968    steps: 327    lr: 0.0001     reward: 1.62\n",
      "epis: 839   score: 1.0   mem len: 152265   epsilon: 0.8965    steps: 168    lr: 0.0001     reward: 1.63\n",
      "epis: 840   score: 2.0   mem len: 152463   epsilon: 0.8961    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 841   score: 0.0   mem len: 152586   epsilon: 0.8959    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 842   score: 1.0   mem len: 152758   epsilon: 0.8955    steps: 172    lr: 0.0001     reward: 1.64\n",
      "epis: 843   score: 3.0   mem len: 153005   epsilon: 0.895    steps: 247    lr: 0.0001     reward: 1.67\n",
      "epis: 844   score: 6.0   mem len: 153383   epsilon: 0.8943    steps: 378    lr: 0.0001     reward: 1.71\n",
      "epis: 845   score: 1.0   mem len: 153552   epsilon: 0.894    steps: 169    lr: 0.0001     reward: 1.67\n",
      "epis: 846   score: 1.0   mem len: 153721   epsilon: 0.8936    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 847   score: 0.0   mem len: 153844   epsilon: 0.8934    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 848   score: 0.0   mem len: 153967   epsilon: 0.8931    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 849   score: 2.0   mem len: 154184   epsilon: 0.8927    steps: 217    lr: 0.0001     reward: 1.67\n",
      "epis: 850   score: 2.0   mem len: 154402   epsilon: 0.8923    steps: 218    lr: 0.0001     reward: 1.69\n",
      "epis: 851   score: 0.0   mem len: 154525   epsilon: 0.892    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 852   score: 1.0   mem len: 154695   epsilon: 0.8917    steps: 170    lr: 0.0001     reward: 1.68\n",
      "epis: 853   score: 2.0   mem len: 154910   epsilon: 0.8913    steps: 215    lr: 0.0001     reward: 1.69\n",
      "epis: 854   score: 4.0   mem len: 155169   epsilon: 0.8908    steps: 259    lr: 0.0001     reward: 1.71\n",
      "epis: 855   score: 4.0   mem len: 155428   epsilon: 0.8903    steps: 259    lr: 0.0001     reward: 1.74\n",
      "epis: 856   score: 4.0   mem len: 155688   epsilon: 0.8897    steps: 260    lr: 0.0001     reward: 1.78\n",
      "epis: 857   score: 1.0   mem len: 155858   epsilon: 0.8894    steps: 170    lr: 0.0001     reward: 1.79\n",
      "epis: 858   score: 0.0   mem len: 155981   epsilon: 0.8892    steps: 123    lr: 0.0001     reward: 1.79\n",
      "epis: 859   score: 4.0   mem len: 156274   epsilon: 0.8886    steps: 293    lr: 0.0001     reward: 1.83\n",
      "epis: 860   score: 2.0   mem len: 156472   epsilon: 0.8882    steps: 198    lr: 0.0001     reward: 1.82\n",
      "epis: 861   score: 1.0   mem len: 156623   epsilon: 0.8879    steps: 151    lr: 0.0001     reward: 1.8\n",
      "epis: 862   score: 3.0   mem len: 156869   epsilon: 0.8874    steps: 246    lr: 0.0001     reward: 1.83\n",
      "epis: 863   score: 2.0   mem len: 157049   epsilon: 0.887    steps: 180    lr: 0.0001     reward: 1.85\n",
      "epis: 864   score: 3.0   mem len: 157275   epsilon: 0.8866    steps: 226    lr: 0.0001     reward: 1.88\n",
      "epis: 865   score: 1.0   mem len: 157445   epsilon: 0.8863    steps: 170    lr: 0.0001     reward: 1.88\n",
      "epis: 866   score: 2.0   mem len: 157666   epsilon: 0.8858    steps: 221    lr: 0.0001     reward: 1.88\n",
      "epis: 867   score: 0.0   mem len: 157789   epsilon: 0.8856    steps: 123    lr: 0.0001     reward: 1.88\n",
      "epis: 868   score: 1.0   mem len: 157957   epsilon: 0.8852    steps: 168    lr: 0.0001     reward: 1.87\n",
      "epis: 869   score: 3.0   mem len: 158205   epsilon: 0.8848    steps: 248    lr: 0.0001     reward: 1.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 870   score: 0.0   mem len: 158327   epsilon: 0.8845    steps: 122    lr: 0.0001     reward: 1.89\n",
      "epis: 871   score: 2.0   mem len: 158543   epsilon: 0.8841    steps: 216    lr: 0.0001     reward: 1.91\n",
      "epis: 872   score: 2.0   mem len: 158761   epsilon: 0.8837    steps: 218    lr: 0.0001     reward: 1.89\n",
      "epis: 873   score: 1.0   mem len: 158933   epsilon: 0.8833    steps: 172    lr: 0.0001     reward: 1.88\n",
      "epis: 874   score: 2.0   mem len: 159132   epsilon: 0.8829    steps: 199    lr: 0.0001     reward: 1.9\n",
      "epis: 875   score: 2.0   mem len: 159313   epsilon: 0.8826    steps: 181    lr: 0.0001     reward: 1.92\n",
      "epis: 876   score: 6.0   mem len: 159668   epsilon: 0.8819    steps: 355    lr: 0.0001     reward: 1.98\n",
      "epis: 877   score: 4.0   mem len: 159919   epsilon: 0.8814    steps: 251    lr: 0.0001     reward: 2.02\n",
      "epis: 878   score: 3.0   mem len: 160183   epsilon: 0.8808    steps: 264    lr: 0.0001     reward: 2.01\n",
      "epis: 879   score: 3.0   mem len: 160451   epsilon: 0.8803    steps: 268    lr: 0.0001     reward: 1.99\n",
      "epis: 880   score: 2.0   mem len: 160649   epsilon: 0.8799    steps: 198    lr: 0.0001     reward: 1.99\n",
      "epis: 881   score: 1.0   mem len: 160800   epsilon: 0.8796    steps: 151    lr: 0.0001     reward: 1.96\n",
      "epis: 882   score: 1.0   mem len: 160951   epsilon: 0.8793    steps: 151    lr: 0.0001     reward: 1.97\n",
      "epis: 883   score: 1.0   mem len: 161119   epsilon: 0.879    steps: 168    lr: 0.0001     reward: 1.97\n",
      "epis: 884   score: 3.0   mem len: 161364   epsilon: 0.8785    steps: 245    lr: 0.0001     reward: 1.99\n",
      "epis: 885   score: 1.0   mem len: 161534   epsilon: 0.8782    steps: 170    lr: 0.0001     reward: 2.0\n",
      "epis: 886   score: 1.0   mem len: 161686   epsilon: 0.8779    steps: 152    lr: 0.0001     reward: 2.01\n",
      "epis: 887   score: 4.0   mem len: 161964   epsilon: 0.8773    steps: 278    lr: 0.0001     reward: 2.03\n",
      "epis: 888   score: 3.0   mem len: 162210   epsilon: 0.8768    steps: 246    lr: 0.0001     reward: 2.03\n",
      "epis: 889   score: 1.0   mem len: 162361   epsilon: 0.8765    steps: 151    lr: 0.0001     reward: 2.0\n",
      "epis: 890   score: 3.0   mem len: 162612   epsilon: 0.876    steps: 251    lr: 0.0001     reward: 2.0\n",
      "epis: 891   score: 0.0   mem len: 162735   epsilon: 0.8758    steps: 123    lr: 0.0001     reward: 1.98\n",
      "epis: 892   score: 6.0   mem len: 163093   epsilon: 0.8751    steps: 358    lr: 0.0001     reward: 2.0\n",
      "epis: 893   score: 3.0   mem len: 163340   epsilon: 0.8746    steps: 247    lr: 0.0001     reward: 2.02\n",
      "epis: 894   score: 3.0   mem len: 163568   epsilon: 0.8741    steps: 228    lr: 0.0001     reward: 2.03\n",
      "epis: 895   score: 3.0   mem len: 163817   epsilon: 0.8736    steps: 249    lr: 0.0001     reward: 2.02\n",
      "epis: 896   score: 3.0   mem len: 164045   epsilon: 0.8732    steps: 228    lr: 0.0001     reward: 2.04\n",
      "epis: 897   score: 2.0   mem len: 164243   epsilon: 0.8728    steps: 198    lr: 0.0001     reward: 2.06\n",
      "epis: 898   score: 1.0   mem len: 164412   epsilon: 0.8725    steps: 169    lr: 0.0001     reward: 2.07\n",
      "epis: 899   score: 1.0   mem len: 164582   epsilon: 0.8721    steps: 170    lr: 0.0001     reward: 2.07\n",
      "epis: 900   score: 4.0   mem len: 164860   epsilon: 0.8716    steps: 278    lr: 0.0001     reward: 2.09\n",
      "epis: 901   score: 1.0   mem len: 165029   epsilon: 0.8712    steps: 169    lr: 0.0001     reward: 2.07\n",
      "epis: 902   score: 1.0   mem len: 165180   epsilon: 0.8709    steps: 151    lr: 0.0001     reward: 2.08\n",
      "epis: 903   score: 2.0   mem len: 165398   epsilon: 0.8705    steps: 218    lr: 0.0001     reward: 2.1\n",
      "epis: 904   score: 1.0   mem len: 165566   epsilon: 0.8702    steps: 168    lr: 0.0001     reward: 2.09\n",
      "epis: 905   score: 3.0   mem len: 165809   epsilon: 0.8697    steps: 243    lr: 0.0001     reward: 2.08\n",
      "epis: 906   score: 4.0   mem len: 166068   epsilon: 0.8692    steps: 259    lr: 0.0001     reward: 2.08\n",
      "epis: 907   score: 4.0   mem len: 166323   epsilon: 0.8687    steps: 255    lr: 0.0001     reward: 2.1\n",
      "epis: 908   score: 3.0   mem len: 166568   epsilon: 0.8682    steps: 245    lr: 0.0001     reward: 2.13\n",
      "epis: 909   score: 2.0   mem len: 166787   epsilon: 0.8678    steps: 219    lr: 0.0001     reward: 2.12\n",
      "epis: 910   score: 3.0   mem len: 167012   epsilon: 0.8673    steps: 225    lr: 0.0001     reward: 2.14\n",
      "epis: 911   score: 1.0   mem len: 167163   epsilon: 0.867    steps: 151    lr: 0.0001     reward: 2.12\n",
      "epis: 912   score: 1.0   mem len: 167332   epsilon: 0.8667    steps: 169    lr: 0.0001     reward: 2.09\n",
      "epis: 913   score: 0.0   mem len: 167455   epsilon: 0.8664    steps: 123    lr: 0.0001     reward: 2.06\n",
      "epis: 914   score: 2.0   mem len: 167673   epsilon: 0.866    steps: 218    lr: 0.0001     reward: 2.04\n",
      "epis: 915   score: 4.0   mem len: 167954   epsilon: 0.8654    steps: 281    lr: 0.0001     reward: 2.08\n",
      "epis: 916   score: 3.0   mem len: 168221   epsilon: 0.8649    steps: 267    lr: 0.0001     reward: 2.09\n",
      "epis: 917   score: 1.0   mem len: 168374   epsilon: 0.8646    steps: 153    lr: 0.0001     reward: 2.08\n",
      "epis: 918   score: 1.0   mem len: 168542   epsilon: 0.8643    steps: 168    lr: 0.0001     reward: 2.09\n",
      "epis: 919   score: 3.0   mem len: 168772   epsilon: 0.8638    steps: 230    lr: 0.0001     reward: 2.1\n",
      "epis: 920   score: 3.0   mem len: 169042   epsilon: 0.8633    steps: 270    lr: 0.0001     reward: 2.12\n",
      "epis: 921   score: 1.0   mem len: 169192   epsilon: 0.863    steps: 150    lr: 0.0001     reward: 2.1\n",
      "epis: 922   score: 1.0   mem len: 169344   epsilon: 0.8627    steps: 152    lr: 0.0001     reward: 2.1\n",
      "epis: 923   score: 1.0   mem len: 169497   epsilon: 0.8624    steps: 153    lr: 0.0001     reward: 2.1\n",
      "epis: 924   score: 1.0   mem len: 169647   epsilon: 0.8621    steps: 150    lr: 0.0001     reward: 2.09\n",
      "epis: 925   score: 0.0   mem len: 169770   epsilon: 0.8619    steps: 123    lr: 0.0001     reward: 2.08\n",
      "epis: 926   score: 1.0   mem len: 169940   epsilon: 0.8615    steps: 170    lr: 0.0001     reward: 2.05\n",
      "epis: 927   score: 1.0   mem len: 170109   epsilon: 0.8612    steps: 169    lr: 0.0001     reward: 2.02\n",
      "epis: 928   score: 4.0   mem len: 170376   epsilon: 0.8607    steps: 267    lr: 0.0001     reward: 2.06\n",
      "epis: 929   score: 4.0   mem len: 170670   epsilon: 0.8601    steps: 294    lr: 0.0001     reward: 2.07\n",
      "epis: 930   score: 2.0   mem len: 170868   epsilon: 0.8597    steps: 198    lr: 0.0001     reward: 2.03\n",
      "epis: 931   score: 1.0   mem len: 171037   epsilon: 0.8593    steps: 169    lr: 0.0001     reward: 2.01\n",
      "epis: 932   score: 1.0   mem len: 171188   epsilon: 0.859    steps: 151    lr: 0.0001     reward: 2.01\n",
      "epis: 933   score: 0.0   mem len: 171311   epsilon: 0.8588    steps: 123    lr: 0.0001     reward: 1.98\n",
      "epis: 934   score: 2.0   mem len: 171528   epsilon: 0.8584    steps: 217    lr: 0.0001     reward: 2.0\n",
      "epis: 935   score: 2.0   mem len: 171707   epsilon: 0.858    steps: 179    lr: 0.0001     reward: 2.02\n",
      "epis: 936   score: 0.0   mem len: 171829   epsilon: 0.8578    steps: 122    lr: 0.0001     reward: 2.01\n",
      "epis: 937   score: 1.0   mem len: 171998   epsilon: 0.8574    steps: 169    lr: 0.0001     reward: 2.0\n",
      "epis: 938   score: 4.0   mem len: 172295   epsilon: 0.8569    steps: 297    lr: 0.0001     reward: 1.99\n",
      "epis: 939   score: 5.0   mem len: 172645   epsilon: 0.8562    steps: 350    lr: 0.0001     reward: 2.03\n",
      "epis: 940   score: 3.0   mem len: 172892   epsilon: 0.8557    steps: 247    lr: 0.0001     reward: 2.04\n",
      "epis: 941   score: 1.0   mem len: 173043   epsilon: 0.8554    steps: 151    lr: 0.0001     reward: 2.05\n",
      "epis: 942   score: 2.0   mem len: 173240   epsilon: 0.855    steps: 197    lr: 0.0001     reward: 2.06\n",
      "epis: 943   score: 3.0   mem len: 173489   epsilon: 0.8545    steps: 249    lr: 0.0001     reward: 2.06\n",
      "epis: 944   score: 2.0   mem len: 173708   epsilon: 0.8541    steps: 219    lr: 0.0001     reward: 2.02\n",
      "epis: 945   score: 0.0   mem len: 173831   epsilon: 0.8538    steps: 123    lr: 0.0001     reward: 2.01\n",
      "epis: 946   score: 1.0   mem len: 174001   epsilon: 0.8535    steps: 170    lr: 0.0001     reward: 2.01\n",
      "epis: 947   score: 2.0   mem len: 174198   epsilon: 0.8531    steps: 197    lr: 0.0001     reward: 2.03\n",
      "epis: 948   score: 1.0   mem len: 174349   epsilon: 0.8528    steps: 151    lr: 0.0001     reward: 2.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 949   score: 5.0   mem len: 174655   epsilon: 0.8522    steps: 306    lr: 0.0001     reward: 2.07\n",
      "epis: 950   score: 3.0   mem len: 174881   epsilon: 0.8517    steps: 226    lr: 0.0001     reward: 2.08\n",
      "epis: 951   score: 6.0   mem len: 175292   epsilon: 0.8509    steps: 411    lr: 0.0001     reward: 2.14\n",
      "epis: 952   score: 3.0   mem len: 175539   epsilon: 0.8504    steps: 247    lr: 0.0001     reward: 2.16\n",
      "epis: 953   score: 2.0   mem len: 175737   epsilon: 0.85    steps: 198    lr: 0.0001     reward: 2.16\n",
      "epis: 954   score: 0.0   mem len: 175860   epsilon: 0.8498    steps: 123    lr: 0.0001     reward: 2.12\n",
      "epis: 955   score: 3.0   mem len: 176086   epsilon: 0.8493    steps: 226    lr: 0.0001     reward: 2.11\n",
      "epis: 956   score: 1.0   mem len: 176255   epsilon: 0.849    steps: 169    lr: 0.0001     reward: 2.08\n",
      "epis: 957   score: 0.0   mem len: 176378   epsilon: 0.8488    steps: 123    lr: 0.0001     reward: 2.07\n",
      "epis: 958   score: 2.0   mem len: 176577   epsilon: 0.8484    steps: 199    lr: 0.0001     reward: 2.09\n",
      "epis: 959   score: 3.0   mem len: 176803   epsilon: 0.8479    steps: 226    lr: 0.0001     reward: 2.08\n",
      "epis: 960   score: 3.0   mem len: 177069   epsilon: 0.8474    steps: 266    lr: 0.0001     reward: 2.09\n",
      "epis: 961   score: 1.0   mem len: 177240   epsilon: 0.8471    steps: 171    lr: 0.0001     reward: 2.09\n",
      "epis: 962   score: 3.0   mem len: 177486   epsilon: 0.8466    steps: 246    lr: 0.0001     reward: 2.09\n",
      "epis: 963   score: 2.0   mem len: 177665   epsilon: 0.8462    steps: 179    lr: 0.0001     reward: 2.09\n",
      "epis: 964   score: 4.0   mem len: 177919   epsilon: 0.8457    steps: 254    lr: 0.0001     reward: 2.1\n",
      "epis: 965   score: 0.0   mem len: 178041   epsilon: 0.8455    steps: 122    lr: 0.0001     reward: 2.09\n",
      "epis: 966   score: 4.0   mem len: 178337   epsilon: 0.8449    steps: 296    lr: 0.0001     reward: 2.11\n",
      "epis: 967   score: 5.0   mem len: 178663   epsilon: 0.8442    steps: 326    lr: 0.0001     reward: 2.16\n",
      "epis: 968   score: 0.0   mem len: 178786   epsilon: 0.844    steps: 123    lr: 0.0001     reward: 2.15\n",
      "epis: 969   score: 1.0   mem len: 178936   epsilon: 0.8437    steps: 150    lr: 0.0001     reward: 2.13\n",
      "epis: 970   score: 0.0   mem len: 179059   epsilon: 0.8435    steps: 123    lr: 0.0001     reward: 2.13\n",
      "epis: 971   score: 4.0   mem len: 179350   epsilon: 0.8429    steps: 291    lr: 0.0001     reward: 2.15\n",
      "epis: 972   score: 2.0   mem len: 179532   epsilon: 0.8425    steps: 182    lr: 0.0001     reward: 2.15\n",
      "epis: 973   score: 4.0   mem len: 179811   epsilon: 0.842    steps: 279    lr: 0.0001     reward: 2.18\n",
      "epis: 974   score: 1.0   mem len: 179962   epsilon: 0.8417    steps: 151    lr: 0.0001     reward: 2.17\n",
      "epis: 975   score: 5.0   mem len: 180329   epsilon: 0.8409    steps: 367    lr: 0.0001     reward: 2.2\n",
      "epis: 976   score: 2.0   mem len: 180509   epsilon: 0.8406    steps: 180    lr: 0.0001     reward: 2.16\n",
      "epis: 977   score: 6.0   mem len: 180864   epsilon: 0.8399    steps: 355    lr: 0.0001     reward: 2.18\n",
      "epis: 978   score: 1.0   mem len: 181015   epsilon: 0.8396    steps: 151    lr: 0.0001     reward: 2.16\n",
      "epis: 979   score: 3.0   mem len: 181260   epsilon: 0.8391    steps: 245    lr: 0.0001     reward: 2.16\n",
      "epis: 980   score: 1.0   mem len: 181430   epsilon: 0.8388    steps: 170    lr: 0.0001     reward: 2.15\n",
      "epis: 981   score: 2.0   mem len: 181648   epsilon: 0.8383    steps: 218    lr: 0.0001     reward: 2.16\n",
      "epis: 982   score: 3.0   mem len: 181859   epsilon: 0.8379    steps: 211    lr: 0.0001     reward: 2.18\n",
      "epis: 983   score: 0.0   mem len: 181981   epsilon: 0.8377    steps: 122    lr: 0.0001     reward: 2.17\n",
      "epis: 984   score: 2.0   mem len: 182181   epsilon: 0.8373    steps: 200    lr: 0.0001     reward: 2.16\n",
      "epis: 985   score: 3.0   mem len: 182412   epsilon: 0.8368    steps: 231    lr: 0.0001     reward: 2.18\n",
      "epis: 986   score: 2.0   mem len: 182610   epsilon: 0.8364    steps: 198    lr: 0.0001     reward: 2.19\n",
      "epis: 987   score: 2.0   mem len: 182828   epsilon: 0.836    steps: 218    lr: 0.0001     reward: 2.17\n",
      "epis: 988   score: 0.0   mem len: 182951   epsilon: 0.8358    steps: 123    lr: 0.0001     reward: 2.14\n",
      "epis: 989   score: 5.0   mem len: 183274   epsilon: 0.8351    steps: 323    lr: 0.0001     reward: 2.18\n",
      "epis: 990   score: 4.0   mem len: 183568   epsilon: 0.8345    steps: 294    lr: 0.0001     reward: 2.19\n",
      "epis: 991   score: 2.0   mem len: 183768   epsilon: 0.8341    steps: 200    lr: 0.0001     reward: 2.21\n",
      "epis: 992   score: 3.0   mem len: 183998   epsilon: 0.8337    steps: 230    lr: 0.0001     reward: 2.18\n",
      "epis: 993   score: 4.0   mem len: 184257   epsilon: 0.8332    steps: 259    lr: 0.0001     reward: 2.19\n",
      "epis: 994   score: 0.0   mem len: 184380   epsilon: 0.8329    steps: 123    lr: 0.0001     reward: 2.16\n",
      "epis: 995   score: 1.0   mem len: 184530   epsilon: 0.8326    steps: 150    lr: 0.0001     reward: 2.14\n",
      "epis: 996   score: 0.0   mem len: 184652   epsilon: 0.8324    steps: 122    lr: 0.0001     reward: 2.11\n",
      "epis: 997   score: 1.0   mem len: 184823   epsilon: 0.832    steps: 171    lr: 0.0001     reward: 2.1\n",
      "epis: 998   score: 4.0   mem len: 185140   epsilon: 0.8314    steps: 317    lr: 0.0001     reward: 2.13\n",
      "epis: 999   score: 1.0   mem len: 185311   epsilon: 0.8311    steps: 171    lr: 0.0001     reward: 2.13\n",
      "epis: 1000   score: 2.0   mem len: 185529   epsilon: 0.8307    steps: 218    lr: 0.0001     reward: 2.11\n",
      "epis: 1001   score: 0.0   mem len: 185652   epsilon: 0.8304    steps: 123    lr: 0.0001     reward: 2.1\n",
      "epis: 1002   score: 8.0   mem len: 186063   epsilon: 0.8296    steps: 411    lr: 0.0001     reward: 2.17\n",
      "epis: 1003   score: 3.0   mem len: 186293   epsilon: 0.8291    steps: 230    lr: 0.0001     reward: 2.18\n",
      "epis: 1004   score: 0.0   mem len: 186415   epsilon: 0.8289    steps: 122    lr: 0.0001     reward: 2.17\n",
      "epis: 1005   score: 4.0   mem len: 186690   epsilon: 0.8284    steps: 275    lr: 0.0001     reward: 2.18\n",
      "epis: 1006   score: 2.0   mem len: 186905   epsilon: 0.8279    steps: 215    lr: 0.0001     reward: 2.16\n",
      "epis: 1007   score: 5.0   mem len: 187234   epsilon: 0.8273    steps: 329    lr: 0.0001     reward: 2.17\n",
      "epis: 1008   score: 4.0   mem len: 187510   epsilon: 0.8267    steps: 276    lr: 0.0001     reward: 2.18\n",
      "epis: 1009   score: 2.0   mem len: 187692   epsilon: 0.8264    steps: 182    lr: 0.0001     reward: 2.18\n",
      "epis: 1010   score: 0.0   mem len: 187815   epsilon: 0.8261    steps: 123    lr: 0.0001     reward: 2.15\n",
      "epis: 1011   score: 2.0   mem len: 188013   epsilon: 0.8257    steps: 198    lr: 0.0001     reward: 2.16\n",
      "epis: 1012   score: 3.0   mem len: 188277   epsilon: 0.8252    steps: 264    lr: 0.0001     reward: 2.18\n",
      "epis: 1013   score: 0.0   mem len: 188399   epsilon: 0.825    steps: 122    lr: 0.0001     reward: 2.18\n",
      "epis: 1014   score: 5.0   mem len: 188744   epsilon: 0.8243    steps: 345    lr: 0.0001     reward: 2.21\n",
      "epis: 1015   score: 0.0   mem len: 188867   epsilon: 0.824    steps: 123    lr: 0.0001     reward: 2.17\n",
      "epis: 1016   score: 5.0   mem len: 189234   epsilon: 0.8233    steps: 367    lr: 0.0001     reward: 2.19\n",
      "epis: 1017   score: 2.0   mem len: 189434   epsilon: 0.8229    steps: 200    lr: 0.0001     reward: 2.2\n",
      "epis: 1018   score: 0.0   mem len: 189557   epsilon: 0.8227    steps: 123    lr: 0.0001     reward: 2.19\n",
      "epis: 1019   score: 1.0   mem len: 189708   epsilon: 0.8224    steps: 151    lr: 0.0001     reward: 2.17\n",
      "epis: 1020   score: 0.0   mem len: 189831   epsilon: 0.8221    steps: 123    lr: 0.0001     reward: 2.14\n",
      "epis: 1021   score: 2.0   mem len: 190012   epsilon: 0.8218    steps: 181    lr: 0.0001     reward: 2.15\n",
      "epis: 1022   score: 2.0   mem len: 190210   epsilon: 0.8214    steps: 198    lr: 0.0001     reward: 2.16\n",
      "epis: 1023   score: 2.0   mem len: 190408   epsilon: 0.821    steps: 198    lr: 0.0001     reward: 2.17\n",
      "epis: 1024   score: 5.0   mem len: 190751   epsilon: 0.8203    steps: 343    lr: 0.0001     reward: 2.21\n",
      "epis: 1025   score: 1.0   mem len: 190902   epsilon: 0.82    steps: 151    lr: 0.0001     reward: 2.22\n",
      "epis: 1026   score: 2.0   mem len: 191121   epsilon: 0.8196    steps: 219    lr: 0.0001     reward: 2.23\n",
      "epis: 1027   score: 3.0   mem len: 191352   epsilon: 0.8191    steps: 231    lr: 0.0001     reward: 2.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1028   score: 5.0   mem len: 191693   epsilon: 0.8184    steps: 341    lr: 0.0001     reward: 2.26\n",
      "epis: 1029   score: 2.0   mem len: 191891   epsilon: 0.8181    steps: 198    lr: 0.0001     reward: 2.24\n",
      "epis: 1030   score: 1.0   mem len: 192059   epsilon: 0.8177    steps: 168    lr: 0.0001     reward: 2.23\n",
      "epis: 1031   score: 6.0   mem len: 192380   epsilon: 0.8171    steps: 321    lr: 0.0001     reward: 2.28\n",
      "epis: 1032   score: 2.0   mem len: 192579   epsilon: 0.8167    steps: 199    lr: 0.0001     reward: 2.29\n",
      "epis: 1033   score: 2.0   mem len: 192777   epsilon: 0.8163    steps: 198    lr: 0.0001     reward: 2.31\n",
      "epis: 1034   score: 0.0   mem len: 192900   epsilon: 0.8161    steps: 123    lr: 0.0001     reward: 2.29\n",
      "epis: 1035   score: 3.0   mem len: 193146   epsilon: 0.8156    steps: 246    lr: 0.0001     reward: 2.3\n",
      "epis: 1036   score: 0.0   mem len: 193269   epsilon: 0.8153    steps: 123    lr: 0.0001     reward: 2.3\n",
      "epis: 1037   score: 1.0   mem len: 193420   epsilon: 0.815    steps: 151    lr: 0.0001     reward: 2.3\n",
      "epis: 1038   score: 9.0   mem len: 193850   epsilon: 0.8142    steps: 430    lr: 0.0001     reward: 2.35\n",
      "epis: 1039   score: 3.0   mem len: 194096   epsilon: 0.8137    steps: 246    lr: 0.0001     reward: 2.33\n",
      "epis: 1040   score: 5.0   mem len: 194441   epsilon: 0.813    steps: 345    lr: 0.0001     reward: 2.35\n",
      "epis: 1041   score: 4.0   mem len: 194734   epsilon: 0.8124    steps: 293    lr: 0.0001     reward: 2.38\n",
      "epis: 1042   score: 5.0   mem len: 195077   epsilon: 0.8117    steps: 343    lr: 0.0001     reward: 2.41\n",
      "epis: 1043   score: 0.0   mem len: 195200   epsilon: 0.8115    steps: 123    lr: 0.0001     reward: 2.38\n",
      "epis: 1044   score: 3.0   mem len: 195428   epsilon: 0.8111    steps: 228    lr: 0.0001     reward: 2.39\n",
      "epis: 1045   score: 2.0   mem len: 195630   epsilon: 0.8107    steps: 202    lr: 0.0001     reward: 2.41\n",
      "epis: 1046   score: 1.0   mem len: 195780   epsilon: 0.8104    steps: 150    lr: 0.0001     reward: 2.41\n",
      "epis: 1047   score: 4.0   mem len: 196078   epsilon: 0.8098    steps: 298    lr: 0.0001     reward: 2.43\n",
      "epis: 1048   score: 5.0   mem len: 196407   epsilon: 0.8091    steps: 329    lr: 0.0001     reward: 2.47\n",
      "epis: 1049   score: 1.0   mem len: 196558   epsilon: 0.8088    steps: 151    lr: 0.0001     reward: 2.43\n",
      "epis: 1050   score: 3.0   mem len: 196783   epsilon: 0.8084    steps: 225    lr: 0.0001     reward: 2.43\n",
      "epis: 1051   score: 0.0   mem len: 196906   epsilon: 0.8081    steps: 123    lr: 0.0001     reward: 2.37\n",
      "epis: 1052   score: 2.0   mem len: 197104   epsilon: 0.8077    steps: 198    lr: 0.0001     reward: 2.36\n",
      "epis: 1053   score: 4.0   mem len: 197376   epsilon: 0.8072    steps: 272    lr: 0.0001     reward: 2.38\n",
      "epis: 1054   score: 2.0   mem len: 197594   epsilon: 0.8068    steps: 218    lr: 0.0001     reward: 2.4\n",
      "epis: 1055   score: 2.0   mem len: 197811   epsilon: 0.8063    steps: 217    lr: 0.0001     reward: 2.39\n",
      "epis: 1056   score: 2.0   mem len: 198009   epsilon: 0.8059    steps: 198    lr: 0.0001     reward: 2.4\n",
      "epis: 1057   score: 3.0   mem len: 198257   epsilon: 0.8054    steps: 248    lr: 0.0001     reward: 2.43\n",
      "epis: 1058   score: 1.0   mem len: 198428   epsilon: 0.8051    steps: 171    lr: 0.0001     reward: 2.42\n",
      "epis: 1059   score: 5.0   mem len: 198751   epsilon: 0.8045    steps: 323    lr: 0.0001     reward: 2.44\n",
      "epis: 1060   score: 1.0   mem len: 198904   epsilon: 0.8042    steps: 153    lr: 0.0001     reward: 2.42\n",
      "epis: 1061   score: 2.0   mem len: 199101   epsilon: 0.8038    steps: 197    lr: 0.0001     reward: 2.43\n",
      "epis: 1062   score: 4.0   mem len: 199400   epsilon: 0.8032    steps: 299    lr: 0.0001     reward: 2.44\n",
      "epis: 1063   score: 1.0   mem len: 199569   epsilon: 0.8029    steps: 169    lr: 0.0001     reward: 2.43\n",
      "epis: 1064   score: 1.0   mem len: 199720   epsilon: 0.8026    steps: 151    lr: 0.0001     reward: 2.4\n",
      "epis: 1065   score: 3.0   mem len: 199951   epsilon: 0.8021    steps: 231    lr: 0.0001     reward: 2.43\n",
      "epis: 1066   score: 1.0   mem len: 200120   epsilon: 0.8018    steps: 169    lr: 4e-05     reward: 2.4\n",
      "epis: 1067   score: 2.0   mem len: 200318   epsilon: 0.8014    steps: 198    lr: 4e-05     reward: 2.37\n",
      "epis: 1068   score: 3.0   mem len: 200543   epsilon: 0.8009    steps: 225    lr: 4e-05     reward: 2.4\n",
      "epis: 1069   score: 0.0   mem len: 200666   epsilon: 0.8007    steps: 123    lr: 4e-05     reward: 2.39\n",
      "epis: 1070   score: 0.0   mem len: 200789   epsilon: 0.8004    steps: 123    lr: 4e-05     reward: 2.39\n",
      "epis: 1071   score: 4.0   mem len: 201085   epsilon: 0.7998    steps: 296    lr: 4e-05     reward: 2.39\n",
      "epis: 1072   score: 2.0   mem len: 201304   epsilon: 0.7994    steps: 219    lr: 4e-05     reward: 2.39\n",
      "epis: 1073   score: 4.0   mem len: 201620   epsilon: 0.7988    steps: 316    lr: 4e-05     reward: 2.39\n",
      "epis: 1074   score: 3.0   mem len: 201886   epsilon: 0.7983    steps: 266    lr: 4e-05     reward: 2.41\n",
      "epis: 1075   score: 2.0   mem len: 202104   epsilon: 0.7978    steps: 218    lr: 4e-05     reward: 2.38\n",
      "epis: 1076   score: 4.0   mem len: 202417   epsilon: 0.7972    steps: 313    lr: 4e-05     reward: 2.4\n",
      "epis: 1077   score: 2.0   mem len: 202599   epsilon: 0.7969    steps: 182    lr: 4e-05     reward: 2.36\n",
      "epis: 1078   score: 2.0   mem len: 202817   epsilon: 0.7964    steps: 218    lr: 4e-05     reward: 2.37\n",
      "epis: 1079   score: 2.0   mem len: 203014   epsilon: 0.796    steps: 197    lr: 4e-05     reward: 2.36\n",
      "epis: 1080   score: 5.0   mem len: 203376   epsilon: 0.7953    steps: 362    lr: 4e-05     reward: 2.4\n",
      "epis: 1081   score: 3.0   mem len: 203622   epsilon: 0.7948    steps: 246    lr: 4e-05     reward: 2.41\n",
      "epis: 1082   score: 4.0   mem len: 203875   epsilon: 0.7943    steps: 253    lr: 4e-05     reward: 2.42\n",
      "epis: 1083   score: 4.0   mem len: 204133   epsilon: 0.7938    steps: 258    lr: 4e-05     reward: 2.46\n",
      "epis: 1084   score: 3.0   mem len: 204361   epsilon: 0.7934    steps: 228    lr: 4e-05     reward: 2.47\n",
      "epis: 1085   score: 2.0   mem len: 204561   epsilon: 0.793    steps: 200    lr: 4e-05     reward: 2.46\n",
      "epis: 1086   score: 2.0   mem len: 204778   epsilon: 0.7925    steps: 217    lr: 4e-05     reward: 2.46\n",
      "epis: 1087   score: 4.0   mem len: 205044   epsilon: 0.792    steps: 266    lr: 4e-05     reward: 2.48\n",
      "epis: 1088   score: 2.0   mem len: 205244   epsilon: 0.7916    steps: 200    lr: 4e-05     reward: 2.5\n",
      "epis: 1089   score: 3.0   mem len: 205512   epsilon: 0.7911    steps: 268    lr: 4e-05     reward: 2.48\n",
      "epis: 1090   score: 4.0   mem len: 205789   epsilon: 0.7905    steps: 277    lr: 4e-05     reward: 2.48\n",
      "epis: 1091   score: 2.0   mem len: 205970   epsilon: 0.7902    steps: 181    lr: 4e-05     reward: 2.48\n",
      "epis: 1092   score: 4.0   mem len: 206246   epsilon: 0.7896    steps: 276    lr: 4e-05     reward: 2.49\n",
      "epis: 1093   score: 5.0   mem len: 206571   epsilon: 0.789    steps: 325    lr: 4e-05     reward: 2.5\n",
      "epis: 1094   score: 3.0   mem len: 206797   epsilon: 0.7885    steps: 226    lr: 4e-05     reward: 2.53\n",
      "epis: 1095   score: 0.0   mem len: 206919   epsilon: 0.7883    steps: 122    lr: 4e-05     reward: 2.52\n",
      "epis: 1096   score: 0.0   mem len: 207042   epsilon: 0.7881    steps: 123    lr: 4e-05     reward: 2.52\n",
      "epis: 1097   score: 7.0   mem len: 207446   epsilon: 0.7873    steps: 404    lr: 4e-05     reward: 2.58\n",
      "epis: 1098   score: 1.0   mem len: 207597   epsilon: 0.787    steps: 151    lr: 4e-05     reward: 2.55\n",
      "epis: 1099   score: 0.0   mem len: 207719   epsilon: 0.7867    steps: 122    lr: 4e-05     reward: 2.54\n",
      "epis: 1100   score: 3.0   mem len: 207952   epsilon: 0.7863    steps: 233    lr: 4e-05     reward: 2.55\n",
      "epis: 1101   score: 4.0   mem len: 208196   epsilon: 0.7858    steps: 244    lr: 4e-05     reward: 2.59\n",
      "epis: 1102   score: 5.0   mem len: 208497   epsilon: 0.7852    steps: 301    lr: 4e-05     reward: 2.56\n",
      "epis: 1103   score: 3.0   mem len: 208744   epsilon: 0.7847    steps: 247    lr: 4e-05     reward: 2.56\n",
      "epis: 1104   score: 6.0   mem len: 209108   epsilon: 0.784    steps: 364    lr: 4e-05     reward: 2.62\n",
      "epis: 1105   score: 2.0   mem len: 209308   epsilon: 0.7836    steps: 200    lr: 4e-05     reward: 2.6\n",
      "epis: 1106   score: 1.0   mem len: 209480   epsilon: 0.7832    steps: 172    lr: 4e-05     reward: 2.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1107   score: 6.0   mem len: 209831   epsilon: 0.7825    steps: 351    lr: 4e-05     reward: 2.6\n",
      "epis: 1108   score: 3.0   mem len: 210062   epsilon: 0.7821    steps: 231    lr: 4e-05     reward: 2.59\n",
      "epis: 1109   score: 2.0   mem len: 210243   epsilon: 0.7817    steps: 181    lr: 4e-05     reward: 2.59\n",
      "epis: 1110   score: 4.0   mem len: 210521   epsilon: 0.7812    steps: 278    lr: 4e-05     reward: 2.63\n",
      "epis: 1111   score: 4.0   mem len: 210813   epsilon: 0.7806    steps: 292    lr: 4e-05     reward: 2.65\n",
      "epis: 1112   score: 3.0   mem len: 211038   epsilon: 0.7801    steps: 225    lr: 4e-05     reward: 2.65\n",
      "epis: 1113   score: 8.0   mem len: 211506   epsilon: 0.7792    steps: 468    lr: 4e-05     reward: 2.73\n",
      "epis: 1114   score: 2.0   mem len: 211723   epsilon: 0.7788    steps: 217    lr: 4e-05     reward: 2.7\n",
      "epis: 1115   score: 7.0   mem len: 212115   epsilon: 0.778    steps: 392    lr: 4e-05     reward: 2.77\n",
      "epis: 1116   score: 1.0   mem len: 212285   epsilon: 0.7777    steps: 170    lr: 4e-05     reward: 2.73\n",
      "epis: 1117   score: 4.0   mem len: 212580   epsilon: 0.7771    steps: 295    lr: 4e-05     reward: 2.75\n",
      "epis: 1118   score: 3.0   mem len: 212826   epsilon: 0.7766    steps: 246    lr: 4e-05     reward: 2.78\n",
      "epis: 1119   score: 4.0   mem len: 213140   epsilon: 0.776    steps: 314    lr: 4e-05     reward: 2.81\n",
      "epis: 1120   score: 5.0   mem len: 213447   epsilon: 0.7754    steps: 307    lr: 4e-05     reward: 2.86\n",
      "epis: 1121   score: 2.0   mem len: 213629   epsilon: 0.775    steps: 182    lr: 4e-05     reward: 2.86\n",
      "epis: 1122   score: 2.0   mem len: 213829   epsilon: 0.7746    steps: 200    lr: 4e-05     reward: 2.86\n",
      "epis: 1123   score: 1.0   mem len: 213998   epsilon: 0.7743    steps: 169    lr: 4e-05     reward: 2.85\n",
      "epis: 1124   score: 3.0   mem len: 214228   epsilon: 0.7738    steps: 230    lr: 4e-05     reward: 2.83\n",
      "epis: 1125   score: 3.0   mem len: 214454   epsilon: 0.7734    steps: 226    lr: 4e-05     reward: 2.85\n",
      "epis: 1126   score: 4.0   mem len: 214731   epsilon: 0.7728    steps: 277    lr: 4e-05     reward: 2.87\n",
      "epis: 1127   score: 2.0   mem len: 214928   epsilon: 0.7724    steps: 197    lr: 4e-05     reward: 2.86\n",
      "epis: 1128   score: 3.0   mem len: 215138   epsilon: 0.772    steps: 210    lr: 4e-05     reward: 2.84\n",
      "epis: 1129   score: 4.0   mem len: 215439   epsilon: 0.7714    steps: 301    lr: 4e-05     reward: 2.86\n",
      "epis: 1130   score: 5.0   mem len: 215747   epsilon: 0.7708    steps: 308    lr: 4e-05     reward: 2.9\n",
      "epis: 1131   score: 6.0   mem len: 216100   epsilon: 0.7701    steps: 353    lr: 4e-05     reward: 2.9\n",
      "epis: 1132   score: 7.0   mem len: 216522   epsilon: 0.7693    steps: 422    lr: 4e-05     reward: 2.95\n",
      "epis: 1133   score: 1.0   mem len: 216673   epsilon: 0.769    steps: 151    lr: 4e-05     reward: 2.94\n",
      "epis: 1134   score: 2.0   mem len: 216873   epsilon: 0.7686    steps: 200    lr: 4e-05     reward: 2.96\n",
      "epis: 1135   score: 2.0   mem len: 217074   epsilon: 0.7682    steps: 201    lr: 4e-05     reward: 2.95\n",
      "epis: 1136   score: 2.0   mem len: 217295   epsilon: 0.7678    steps: 221    lr: 4e-05     reward: 2.97\n",
      "epis: 1137   score: 2.0   mem len: 217475   epsilon: 0.7674    steps: 180    lr: 4e-05     reward: 2.98\n",
      "epis: 1138   score: 3.0   mem len: 217722   epsilon: 0.7669    steps: 247    lr: 4e-05     reward: 2.92\n",
      "epis: 1139   score: 2.0   mem len: 217920   epsilon: 0.7665    steps: 198    lr: 4e-05     reward: 2.91\n",
      "epis: 1140   score: 3.0   mem len: 218172   epsilon: 0.766    steps: 252    lr: 4e-05     reward: 2.89\n",
      "epis: 1141   score: 4.0   mem len: 218446   epsilon: 0.7655    steps: 274    lr: 4e-05     reward: 2.89\n",
      "epis: 1142   score: 5.0   mem len: 218791   epsilon: 0.7648    steps: 345    lr: 4e-05     reward: 2.89\n",
      "epis: 1143   score: 2.0   mem len: 218991   epsilon: 0.7644    steps: 200    lr: 4e-05     reward: 2.91\n",
      "epis: 1144   score: 4.0   mem len: 219290   epsilon: 0.7638    steps: 299    lr: 4e-05     reward: 2.92\n",
      "epis: 1145   score: 5.0   mem len: 219609   epsilon: 0.7632    steps: 319    lr: 4e-05     reward: 2.95\n",
      "epis: 1146   score: 0.0   mem len: 219732   epsilon: 0.7629    steps: 123    lr: 4e-05     reward: 2.94\n",
      "epis: 1147   score: 4.0   mem len: 220009   epsilon: 0.7624    steps: 277    lr: 4e-05     reward: 2.94\n",
      "epis: 1148   score: 5.0   mem len: 220304   epsilon: 0.7618    steps: 295    lr: 4e-05     reward: 2.94\n",
      "epis: 1149   score: 5.0   mem len: 220610   epsilon: 0.7612    steps: 306    lr: 4e-05     reward: 2.98\n",
      "epis: 1150   score: 2.0   mem len: 220827   epsilon: 0.7608    steps: 217    lr: 4e-05     reward: 2.97\n",
      "epis: 1151   score: 4.0   mem len: 221089   epsilon: 0.7602    steps: 262    lr: 4e-05     reward: 3.01\n",
      "epis: 1152   score: 2.0   mem len: 221287   epsilon: 0.7598    steps: 198    lr: 4e-05     reward: 3.01\n",
      "epis: 1153   score: 4.0   mem len: 221547   epsilon: 0.7593    steps: 260    lr: 4e-05     reward: 3.01\n",
      "epis: 1154   score: 3.0   mem len: 221757   epsilon: 0.7589    steps: 210    lr: 4e-05     reward: 3.02\n",
      "epis: 1155   score: 1.0   mem len: 221908   epsilon: 0.7586    steps: 151    lr: 4e-05     reward: 3.01\n",
      "epis: 1156   score: 3.0   mem len: 222152   epsilon: 0.7581    steps: 244    lr: 4e-05     reward: 3.02\n",
      "epis: 1157   score: 7.0   mem len: 222523   epsilon: 0.7574    steps: 371    lr: 4e-05     reward: 3.06\n",
      "epis: 1158   score: 2.0   mem len: 222723   epsilon: 0.757    steps: 200    lr: 4e-05     reward: 3.07\n",
      "epis: 1159   score: 6.0   mem len: 223080   epsilon: 0.7563    steps: 357    lr: 4e-05     reward: 3.08\n",
      "epis: 1160   score: 6.0   mem len: 223437   epsilon: 0.7556    steps: 357    lr: 4e-05     reward: 3.13\n",
      "epis: 1161   score: 1.0   mem len: 223588   epsilon: 0.7553    steps: 151    lr: 4e-05     reward: 3.12\n",
      "epis: 1162   score: 3.0   mem len: 223857   epsilon: 0.7548    steps: 269    lr: 4e-05     reward: 3.11\n",
      "epis: 1163   score: 3.0   mem len: 224126   epsilon: 0.7542    steps: 269    lr: 4e-05     reward: 3.13\n",
      "epis: 1164   score: 3.0   mem len: 224393   epsilon: 0.7537    steps: 267    lr: 4e-05     reward: 3.15\n",
      "epis: 1165   score: 2.0   mem len: 224591   epsilon: 0.7533    steps: 198    lr: 4e-05     reward: 3.14\n",
      "epis: 1166   score: 8.0   mem len: 225018   epsilon: 0.7525    steps: 427    lr: 4e-05     reward: 3.21\n",
      "epis: 1167   score: 6.0   mem len: 225341   epsilon: 0.7518    steps: 323    lr: 4e-05     reward: 3.25\n",
      "epis: 1168   score: 1.0   mem len: 225510   epsilon: 0.7515    steps: 169    lr: 4e-05     reward: 3.23\n",
      "epis: 1169   score: 10.0   mem len: 225938   epsilon: 0.7506    steps: 428    lr: 4e-05     reward: 3.33\n",
      "epis: 1170   score: 6.0   mem len: 226352   epsilon: 0.7498    steps: 414    lr: 4e-05     reward: 3.39\n",
      "epis: 1171   score: 4.0   mem len: 226627   epsilon: 0.7493    steps: 275    lr: 4e-05     reward: 3.39\n",
      "epis: 1172   score: 8.0   mem len: 227082   epsilon: 0.7484    steps: 455    lr: 4e-05     reward: 3.45\n",
      "epis: 1173   score: 4.0   mem len: 227339   epsilon: 0.7479    steps: 257    lr: 4e-05     reward: 3.45\n",
      "epis: 1174   score: 4.0   mem len: 227616   epsilon: 0.7473    steps: 277    lr: 4e-05     reward: 3.46\n",
      "epis: 1175   score: 3.0   mem len: 227846   epsilon: 0.7469    steps: 230    lr: 4e-05     reward: 3.47\n",
      "epis: 1176   score: 5.0   mem len: 228212   epsilon: 0.7461    steps: 366    lr: 4e-05     reward: 3.48\n",
      "epis: 1177   score: 4.0   mem len: 228487   epsilon: 0.7456    steps: 275    lr: 4e-05     reward: 3.5\n",
      "epis: 1178   score: 2.0   mem len: 228684   epsilon: 0.7452    steps: 197    lr: 4e-05     reward: 3.5\n",
      "epis: 1179   score: 5.0   mem len: 229011   epsilon: 0.7446    steps: 327    lr: 4e-05     reward: 3.53\n",
      "epis: 1180   score: 1.0   mem len: 229162   epsilon: 0.7443    steps: 151    lr: 4e-05     reward: 3.49\n",
      "epis: 1181   score: 5.0   mem len: 229510   epsilon: 0.7436    steps: 348    lr: 4e-05     reward: 3.51\n",
      "epis: 1182   score: 6.0   mem len: 229885   epsilon: 0.7428    steps: 375    lr: 4e-05     reward: 3.53\n",
      "epis: 1183   score: 4.0   mem len: 230126   epsilon: 0.7423    steps: 241    lr: 4e-05     reward: 3.53\n",
      "epis: 1184   score: 3.0   mem len: 230335   epsilon: 0.7419    steps: 209    lr: 4e-05     reward: 3.53\n",
      "epis: 1185   score: 6.0   mem len: 230707   epsilon: 0.7412    steps: 372    lr: 4e-05     reward: 3.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1186   score: 2.0   mem len: 230906   epsilon: 0.7408    steps: 199    lr: 4e-05     reward: 3.57\n",
      "epis: 1187   score: 2.0   mem len: 231088   epsilon: 0.7404    steps: 182    lr: 4e-05     reward: 3.55\n",
      "epis: 1188   score: 8.0   mem len: 231520   epsilon: 0.7396    steps: 432    lr: 4e-05     reward: 3.61\n",
      "epis: 1189   score: 4.0   mem len: 231812   epsilon: 0.739    steps: 292    lr: 4e-05     reward: 3.62\n",
      "epis: 1190   score: 1.0   mem len: 231981   epsilon: 0.7387    steps: 169    lr: 4e-05     reward: 3.59\n",
      "epis: 1191   score: 2.0   mem len: 232163   epsilon: 0.7383    steps: 182    lr: 4e-05     reward: 3.59\n",
      "epis: 1192   score: 5.0   mem len: 232485   epsilon: 0.7377    steps: 322    lr: 4e-05     reward: 3.6\n",
      "epis: 1193   score: 2.0   mem len: 232683   epsilon: 0.7373    steps: 198    lr: 4e-05     reward: 3.57\n",
      "epis: 1194   score: 5.0   mem len: 233009   epsilon: 0.7366    steps: 326    lr: 4e-05     reward: 3.59\n",
      "epis: 1195   score: 4.0   mem len: 233266   epsilon: 0.7361    steps: 257    lr: 4e-05     reward: 3.63\n",
      "epis: 1196   score: 2.0   mem len: 233463   epsilon: 0.7357    steps: 197    lr: 4e-05     reward: 3.65\n",
      "epis: 1197   score: 9.0   mem len: 233831   epsilon: 0.735    steps: 368    lr: 4e-05     reward: 3.67\n",
      "epis: 1198   score: 1.0   mem len: 233982   epsilon: 0.7347    steps: 151    lr: 4e-05     reward: 3.67\n",
      "epis: 1199   score: 4.0   mem len: 234276   epsilon: 0.7341    steps: 294    lr: 4e-05     reward: 3.71\n",
      "epis: 1200   score: 4.0   mem len: 234553   epsilon: 0.7336    steps: 277    lr: 4e-05     reward: 3.72\n",
      "epis: 1201   score: 10.0   mem len: 235006   epsilon: 0.7327    steps: 453    lr: 4e-05     reward: 3.78\n",
      "epis: 1202   score: 3.0   mem len: 235257   epsilon: 0.7322    steps: 251    lr: 4e-05     reward: 3.76\n",
      "epis: 1203   score: 7.0   mem len: 235692   epsilon: 0.7313    steps: 435    lr: 4e-05     reward: 3.8\n",
      "epis: 1204   score: 5.0   mem len: 236060   epsilon: 0.7306    steps: 368    lr: 4e-05     reward: 3.79\n",
      "epis: 1205   score: 5.0   mem len: 236386   epsilon: 0.73    steps: 326    lr: 4e-05     reward: 3.82\n",
      "epis: 1206   score: 7.0   mem len: 236649   epsilon: 0.7294    steps: 263    lr: 4e-05     reward: 3.88\n",
      "epis: 1207   score: 2.0   mem len: 236851   epsilon: 0.729    steps: 202    lr: 4e-05     reward: 3.84\n",
      "epis: 1208   score: 3.0   mem len: 237097   epsilon: 0.7285    steps: 246    lr: 4e-05     reward: 3.84\n",
      "epis: 1209   score: 6.0   mem len: 237471   epsilon: 0.7278    steps: 374    lr: 4e-05     reward: 3.88\n",
      "epis: 1210   score: 2.0   mem len: 237669   epsilon: 0.7274    steps: 198    lr: 4e-05     reward: 3.86\n",
      "epis: 1211   score: 3.0   mem len: 237878   epsilon: 0.727    steps: 209    lr: 4e-05     reward: 3.85\n",
      "epis: 1212   score: 4.0   mem len: 238137   epsilon: 0.7265    steps: 259    lr: 4e-05     reward: 3.86\n",
      "epis: 1213   score: 4.0   mem len: 238414   epsilon: 0.7259    steps: 277    lr: 4e-05     reward: 3.82\n",
      "epis: 1214   score: 4.0   mem len: 238731   epsilon: 0.7253    steps: 317    lr: 4e-05     reward: 3.84\n",
      "epis: 1215   score: 3.0   mem len: 238944   epsilon: 0.7249    steps: 213    lr: 4e-05     reward: 3.8\n",
      "epis: 1216   score: 5.0   mem len: 239251   epsilon: 0.7243    steps: 307    lr: 4e-05     reward: 3.84\n",
      "epis: 1217   score: 3.0   mem len: 239476   epsilon: 0.7238    steps: 225    lr: 4e-05     reward: 3.83\n",
      "epis: 1218   score: 4.0   mem len: 239755   epsilon: 0.7233    steps: 279    lr: 4e-05     reward: 3.84\n",
      "epis: 1219   score: 2.0   mem len: 239954   epsilon: 0.7229    steps: 199    lr: 4e-05     reward: 3.82\n",
      "epis: 1220   score: 3.0   mem len: 240198   epsilon: 0.7224    steps: 244    lr: 4e-05     reward: 3.8\n",
      "epis: 1221   score: 9.0   mem len: 240682   epsilon: 0.7214    steps: 484    lr: 4e-05     reward: 3.87\n",
      "epis: 1222   score: 0.0   mem len: 240805   epsilon: 0.7212    steps: 123    lr: 4e-05     reward: 3.85\n",
      "epis: 1223   score: 2.0   mem len: 241027   epsilon: 0.7208    steps: 222    lr: 4e-05     reward: 3.86\n",
      "epis: 1224   score: 6.0   mem len: 241389   epsilon: 0.72    steps: 362    lr: 4e-05     reward: 3.89\n",
      "epis: 1225   score: 5.0   mem len: 241714   epsilon: 0.7194    steps: 325    lr: 4e-05     reward: 3.91\n",
      "epis: 1226   score: 5.0   mem len: 242031   epsilon: 0.7188    steps: 317    lr: 4e-05     reward: 3.92\n",
      "epis: 1227   score: 3.0   mem len: 242276   epsilon: 0.7183    steps: 245    lr: 4e-05     reward: 3.93\n",
      "epis: 1228   score: 1.0   mem len: 242447   epsilon: 0.718    steps: 171    lr: 4e-05     reward: 3.91\n",
      "epis: 1229   score: 5.0   mem len: 242755   epsilon: 0.7173    steps: 308    lr: 4e-05     reward: 3.92\n",
      "epis: 1230   score: 3.0   mem len: 243004   epsilon: 0.7169    steps: 249    lr: 4e-05     reward: 3.9\n",
      "epis: 1231   score: 5.0   mem len: 243333   epsilon: 0.7162    steps: 329    lr: 4e-05     reward: 3.89\n",
      "epis: 1232   score: 6.0   mem len: 243710   epsilon: 0.7155    steps: 377    lr: 4e-05     reward: 3.88\n",
      "epis: 1233   score: 3.0   mem len: 243957   epsilon: 0.715    steps: 247    lr: 4e-05     reward: 3.9\n",
      "epis: 1234   score: 4.0   mem len: 244275   epsilon: 0.7143    steps: 318    lr: 4e-05     reward: 3.92\n",
      "epis: 1235   score: 3.0   mem len: 244504   epsilon: 0.7139    steps: 229    lr: 4e-05     reward: 3.93\n",
      "epis: 1236   score: 6.0   mem len: 244877   epsilon: 0.7131    steps: 373    lr: 4e-05     reward: 3.97\n",
      "epis: 1237   score: 5.0   mem len: 245197   epsilon: 0.7125    steps: 320    lr: 4e-05     reward: 4.0\n",
      "epis: 1238   score: 3.0   mem len: 245442   epsilon: 0.712    steps: 245    lr: 4e-05     reward: 4.0\n",
      "epis: 1239   score: 2.0   mem len: 245641   epsilon: 0.7116    steps: 199    lr: 4e-05     reward: 4.0\n",
      "epis: 1240   score: 3.0   mem len: 245868   epsilon: 0.7112    steps: 227    lr: 4e-05     reward: 4.0\n",
      "epis: 1241   score: 5.0   mem len: 246176   epsilon: 0.7106    steps: 308    lr: 4e-05     reward: 4.01\n",
      "epis: 1242   score: 3.0   mem len: 246421   epsilon: 0.7101    steps: 245    lr: 4e-05     reward: 3.99\n",
      "epis: 1243   score: 3.0   mem len: 246634   epsilon: 0.7097    steps: 213    lr: 4e-05     reward: 4.0\n",
      "epis: 1244   score: 1.0   mem len: 246803   epsilon: 0.7093    steps: 169    lr: 4e-05     reward: 3.97\n",
      "epis: 1245   score: 3.0   mem len: 247051   epsilon: 0.7088    steps: 248    lr: 4e-05     reward: 3.95\n",
      "epis: 1246   score: 1.0   mem len: 247219   epsilon: 0.7085    steps: 168    lr: 4e-05     reward: 3.96\n",
      "epis: 1247   score: 4.0   mem len: 247494   epsilon: 0.708    steps: 275    lr: 4e-05     reward: 3.96\n",
      "epis: 1248   score: 6.0   mem len: 247885   epsilon: 0.7072    steps: 391    lr: 4e-05     reward: 3.97\n",
      "epis: 1249   score: 5.0   mem len: 248201   epsilon: 0.7066    steps: 316    lr: 4e-05     reward: 3.97\n",
      "epis: 1250   score: 6.0   mem len: 248505   epsilon: 0.706    steps: 304    lr: 4e-05     reward: 4.01\n",
      "epis: 1251   score: 4.0   mem len: 248800   epsilon: 0.7054    steps: 295    lr: 4e-05     reward: 4.01\n",
      "epis: 1252   score: 5.0   mem len: 249125   epsilon: 0.7047    steps: 325    lr: 4e-05     reward: 4.04\n",
      "epis: 1253   score: 5.0   mem len: 249432   epsilon: 0.7041    steps: 307    lr: 4e-05     reward: 4.05\n",
      "epis: 1254   score: 4.0   mem len: 249709   epsilon: 0.7036    steps: 277    lr: 4e-05     reward: 4.06\n",
      "epis: 1255   score: 6.0   mem len: 250058   epsilon: 0.7029    steps: 349    lr: 4e-05     reward: 4.11\n",
      "epis: 1256   score: 6.0   mem len: 250455   epsilon: 0.7021    steps: 397    lr: 4e-05     reward: 4.14\n",
      "epis: 1257   score: 2.0   mem len: 250652   epsilon: 0.7017    steps: 197    lr: 4e-05     reward: 4.09\n",
      "epis: 1258   score: 5.0   mem len: 250961   epsilon: 0.7011    steps: 309    lr: 4e-05     reward: 4.12\n",
      "epis: 1259   score: 5.0   mem len: 251304   epsilon: 0.7004    steps: 343    lr: 4e-05     reward: 4.11\n",
      "epis: 1260   score: 3.0   mem len: 251552   epsilon: 0.6999    steps: 248    lr: 4e-05     reward: 4.08\n",
      "epis: 1261   score: 6.0   mem len: 251907   epsilon: 0.6992    steps: 355    lr: 4e-05     reward: 4.13\n",
      "epis: 1262   score: 5.0   mem len: 252214   epsilon: 0.6986    steps: 307    lr: 4e-05     reward: 4.15\n",
      "epis: 1263   score: 3.0   mem len: 252424   epsilon: 0.6982    steps: 210    lr: 4e-05     reward: 4.15\n",
      "epis: 1264   score: 4.0   mem len: 252697   epsilon: 0.6977    steps: 273    lr: 4e-05     reward: 4.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1265   score: 2.0   mem len: 252894   epsilon: 0.6973    steps: 197    lr: 4e-05     reward: 4.16\n",
      "epis: 1266   score: 4.0   mem len: 253137   epsilon: 0.6968    steps: 243    lr: 4e-05     reward: 4.12\n",
      "epis: 1267   score: 6.0   mem len: 253494   epsilon: 0.6961    steps: 357    lr: 4e-05     reward: 4.12\n",
      "epis: 1268   score: 3.0   mem len: 253723   epsilon: 0.6956    steps: 229    lr: 4e-05     reward: 4.14\n",
      "epis: 1269   score: 3.0   mem len: 253971   epsilon: 0.6951    steps: 248    lr: 4e-05     reward: 4.07\n",
      "epis: 1270   score: 4.0   mem len: 254246   epsilon: 0.6946    steps: 275    lr: 4e-05     reward: 4.05\n",
      "epis: 1271   score: 4.0   mem len: 254539   epsilon: 0.694    steps: 293    lr: 4e-05     reward: 4.05\n",
      "epis: 1272   score: 3.0   mem len: 254788   epsilon: 0.6935    steps: 249    lr: 4e-05     reward: 4.0\n",
      "epis: 1273   score: 2.0   mem len: 254986   epsilon: 0.6931    steps: 198    lr: 4e-05     reward: 3.98\n",
      "epis: 1274   score: 3.0   mem len: 255230   epsilon: 0.6926    steps: 244    lr: 4e-05     reward: 3.97\n",
      "epis: 1275   score: 6.0   mem len: 255601   epsilon: 0.6919    steps: 371    lr: 4e-05     reward: 4.0\n",
      "epis: 1276   score: 6.0   mem len: 255920   epsilon: 0.6913    steps: 319    lr: 4e-05     reward: 4.01\n",
      "epis: 1277   score: 9.0   mem len: 256414   epsilon: 0.6903    steps: 494    lr: 4e-05     reward: 4.06\n",
      "epis: 1278   score: 4.0   mem len: 256654   epsilon: 0.6898    steps: 240    lr: 4e-05     reward: 4.08\n",
      "epis: 1279   score: 1.0   mem len: 256805   epsilon: 0.6895    steps: 151    lr: 4e-05     reward: 4.04\n",
      "epis: 1280   score: 1.0   mem len: 256974   epsilon: 0.6892    steps: 169    lr: 4e-05     reward: 4.04\n",
      "epis: 1281   score: 6.0   mem len: 257332   epsilon: 0.6885    steps: 358    lr: 4e-05     reward: 4.05\n",
      "epis: 1282   score: 3.0   mem len: 257580   epsilon: 0.688    steps: 248    lr: 4e-05     reward: 4.02\n",
      "epis: 1283   score: 1.0   mem len: 257731   epsilon: 0.6877    steps: 151    lr: 4e-05     reward: 3.99\n",
      "epis: 1284   score: 6.0   mem len: 258081   epsilon: 0.687    steps: 350    lr: 4e-05     reward: 4.02\n",
      "epis: 1285   score: 8.0   mem len: 258489   epsilon: 0.6862    steps: 408    lr: 4e-05     reward: 4.04\n",
      "epis: 1286   score: 3.0   mem len: 258717   epsilon: 0.6857    steps: 228    lr: 4e-05     reward: 4.05\n",
      "epis: 1287   score: 6.0   mem len: 259109   epsilon: 0.685    steps: 392    lr: 4e-05     reward: 4.09\n",
      "epis: 1288   score: 2.0   mem len: 259329   epsilon: 0.6845    steps: 220    lr: 4e-05     reward: 4.03\n",
      "epis: 1289   score: 2.0   mem len: 259526   epsilon: 0.6841    steps: 197    lr: 4e-05     reward: 4.01\n",
      "epis: 1290   score: 2.0   mem len: 259724   epsilon: 0.6837    steps: 198    lr: 4e-05     reward: 4.02\n",
      "epis: 1291   score: 9.0   mem len: 260176   epsilon: 0.6828    steps: 452    lr: 4e-05     reward: 4.09\n",
      "epis: 1292   score: 6.0   mem len: 260555   epsilon: 0.6821    steps: 379    lr: 4e-05     reward: 4.1\n"
     ]
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"epis:\", e, \"  score:\", score, \"  mem len:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 4), \"   steps:\", step,\n",
    "                  \"   lr:\", round(agent.optimizer.param_groups[0]['lr'], 7), \"    reward:\", round(np.mean(evaluation_reward), 2))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
