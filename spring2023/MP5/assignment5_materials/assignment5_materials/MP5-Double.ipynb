{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243996/1886764261.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "/tmp/ipykernel_243996/1886764261.py:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 0   score: 1.0   mem len: 151   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.0\n",
      "epis: 1   score: 1.0   mem len: 322   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.0\n",
      "epis: 2   score: 0.0   mem len: 445   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 0.67\n",
      "epis: 3   score: 3.0   mem len: 691   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.25\n",
      "epis: 4   score: 0.0   mem len: 814   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.0\n",
      "epis: 5   score: 0.0   mem len: 937   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 0.83\n",
      "epis: 6   score: 3.0   mem len: 1163   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.14\n",
      "epis: 7   score: 1.0   mem len: 1332   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.12\n",
      "epis: 8   score: 3.0   mem len: 1598   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.33\n",
      "epis: 9   score: 0.0   mem len: 1720   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.2\n",
      "epis: 10   score: 1.0   mem len: 1871   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.18\n",
      "epis: 11   score: 6.0   mem len: 2224   epsilon: 1.0    steps: 353    lr: 0.0001     reward: 1.58\n",
      "epis: 12   score: 2.0   mem len: 2443   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.62\n",
      "epis: 13   score: 1.0   mem len: 2613   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.57\n",
      "epis: 14   score: 3.0   mem len: 2858   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.67\n",
      "epis: 15   score: 3.0   mem len: 3101   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.75\n",
      "epis: 16   score: 1.0   mem len: 3272   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.71\n",
      "epis: 17   score: 2.0   mem len: 3451   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.72\n",
      "epis: 18   score: 1.0   mem len: 3620   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.68\n",
      "epis: 19   score: 0.0   mem len: 3742   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 20   score: 2.0   mem len: 3940   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 21   score: 0.0   mem len: 4063   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 22   score: 2.0   mem len: 4244   epsilon: 1.0    steps: 181    lr: 0.0001     reward: 1.57\n",
      "epis: 23   score: 0.0   mem len: 4366   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 24   score: 0.0   mem len: 4488   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 25   score: 0.0   mem len: 4610   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 26   score: 1.0   mem len: 4779   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 27   score: 1.0   mem len: 4930   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.36\n",
      "epis: 28   score: 2.0   mem len: 5148   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.38\n",
      "epis: 29   score: 1.0   mem len: 5317   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 30   score: 0.0   mem len: 5440   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 31   score: 1.0   mem len: 5609   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.31\n",
      "epis: 32   score: 3.0   mem len: 5875   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.36\n",
      "epis: 33   score: 1.0   mem len: 6026   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.35\n",
      "epis: 34   score: 0.0   mem len: 6148   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 35   score: 1.0   mem len: 6298   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.31\n",
      "epis: 36   score: 2.0   mem len: 6518   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.32\n",
      "epis: 37   score: 1.0   mem len: 6669   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.32\n",
      "epis: 38   score: 2.0   mem len: 6886   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.33\n",
      "epis: 39   score: 5.0   mem len: 7210   epsilon: 1.0    steps: 324    lr: 0.0001     reward: 1.42\n",
      "epis: 40   score: 2.0   mem len: 7409   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.44\n",
      "epis: 41   score: 1.0   mem len: 7560   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 42   score: 0.0   mem len: 7683   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 43   score: 3.0   mem len: 7952   epsilon: 1.0    steps: 269    lr: 0.0001     reward: 1.43\n",
      "epis: 44   score: 2.0   mem len: 8149   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 45   score: 7.0   mem len: 8537   epsilon: 1.0    steps: 388    lr: 0.0001     reward: 1.57\n",
      "epis: 46   score: 2.0   mem len: 8752   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.57\n",
      "epis: 47   score: 1.0   mem len: 8920   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.56\n",
      "epis: 48   score: 2.0   mem len: 9117   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.57\n",
      "epis: 49   score: 2.0   mem len: 9336   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.58\n",
      "epis: 50   score: 3.0   mem len: 9579   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.61\n",
      "epis: 51   score: 1.0   mem len: 9750   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.6\n",
      "epis: 52   score: 2.0   mem len: 9950   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.6\n",
      "epis: 53   score: 4.0   mem len: 10245   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.65\n",
      "epis: 54   score: 6.0   mem len: 10641   epsilon: 1.0    steps: 396    lr: 0.0001     reward: 1.73\n",
      "epis: 55   score: 0.0   mem len: 10764   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.7\n",
      "epis: 56   score: 2.0   mem len: 10961   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.7\n",
      "epis: 57   score: 0.0   mem len: 11084   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 58   score: 1.0   mem len: 11253   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 59   score: 2.0   mem len: 11470   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.67\n",
      "epis: 60   score: 2.0   mem len: 11687   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.67\n",
      "epis: 61   score: 2.0   mem len: 11884   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.68\n",
      "epis: 62   score: 0.0   mem len: 12007   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 63   score: 0.0   mem len: 12129   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 64   score: 7.0   mem len: 12574   epsilon: 1.0    steps: 445    lr: 0.0001     reward: 1.71\n",
      "epis: 65   score: 0.0   mem len: 12697   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 66   score: 0.0   mem len: 12819   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.66\n",
      "epis: 67   score: 1.0   mem len: 12988   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.65\n",
      "epis: 68   score: 2.0   mem len: 13185   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.65\n",
      "epis: 69   score: 3.0   mem len: 13431   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.67\n",
      "epis: 70   score: 6.0   mem len: 13809   epsilon: 1.0    steps: 378    lr: 0.0001     reward: 1.73\n",
      "epis: 71   score: 1.0   mem len: 13979   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.72\n",
      "epis: 72   score: 3.0   mem len: 14243   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.74\n",
      "epis: 73   score: 3.0   mem len: 14469   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.76\n",
      "epis: 74   score: 1.0   mem len: 14620   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 75   score: 2.0   mem len: 14840   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.75\n",
      "epis: 76   score: 0.0   mem len: 14963   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.73\n",
      "epis: 77   score: 2.0   mem len: 15161   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.73\n",
      "epis: 78   score: 3.0   mem len: 15430   epsilon: 1.0    steps: 269    lr: 0.0001     reward: 1.75\n",
      "epis: 79   score: 4.0   mem len: 15726   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.78\n",
      "epis: 80   score: 3.0   mem len: 15952   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.79\n",
      "epis: 81   score: 3.0   mem len: 16218   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.8\n",
      "epis: 82   score: 2.0   mem len: 16435   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.81\n",
      "epis: 83   score: 2.0   mem len: 16651   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 84   score: 2.0   mem len: 16851   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.81\n",
      "epis: 85   score: 2.0   mem len: 17070   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.81\n",
      "epis: 86   score: 1.0   mem len: 17239   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.8\n",
      "epis: 87   score: 0.0   mem len: 17362   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.78\n",
      "epis: 88   score: 4.0   mem len: 17603   epsilon: 1.0    steps: 241    lr: 0.0001     reward: 1.81\n",
      "epis: 89   score: 2.0   mem len: 17819   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.81\n",
      "epis: 90   score: 1.0   mem len: 17990   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.8\n",
      "epis: 91   score: 0.0   mem len: 18112   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.78\n",
      "epis: 92   score: 2.0   mem len: 18331   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.78\n",
      "epis: 93   score: 1.0   mem len: 18482   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.78\n",
      "epis: 94   score: 1.0   mem len: 18654   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.77\n",
      "epis: 95   score: 1.0   mem len: 18822   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.76\n",
      "epis: 96   score: 0.0   mem len: 18945   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.74\n",
      "epis: 97   score: 1.0   mem len: 19114   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.73\n",
      "epis: 98   score: 4.0   mem len: 19401   epsilon: 1.0    steps: 287    lr: 0.0001     reward: 1.76\n",
      "epis: 99   score: 8.0   mem len: 19824   epsilon: 1.0    steps: 423    lr: 0.0001     reward: 1.82\n",
      "epis: 100   score: 2.0   mem len: 20022   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.83\n",
      "epis: 101   score: 1.0   mem len: 20173   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.83\n",
      "epis: 102   score: 0.0   mem len: 20296   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.83\n",
      "epis: 103   score: 2.0   mem len: 20512   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.82\n",
      "epis: 104   score: 0.0   mem len: 20634   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.82\n",
      "epis: 105   score: 1.0   mem len: 20803   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.83\n",
      "epis: 106   score: 0.0   mem len: 20925   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.8\n",
      "epis: 107   score: 3.0   mem len: 21188   epsilon: 1.0    steps: 263    lr: 0.0001     reward: 1.82\n",
      "epis: 108   score: 2.0   mem len: 21388   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.81\n",
      "epis: 109   score: 2.0   mem len: 21589   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.83\n",
      "epis: 110   score: 3.0   mem len: 21854   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.85\n",
      "epis: 111   score: 0.0   mem len: 21977   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.79\n",
      "epis: 112   score: 1.0   mem len: 22146   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.78\n",
      "epis: 113   score: 0.0   mem len: 22269   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.77\n",
      "epis: 114   score: 1.0   mem len: 22420   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.75\n",
      "epis: 115   score: 5.0   mem len: 22761   epsilon: 1.0    steps: 341    lr: 0.0001     reward: 1.77\n",
      "epis: 116   score: 2.0   mem len: 22977   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.78\n",
      "epis: 117   score: 1.0   mem len: 23149   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.77\n",
      "epis: 118   score: 0.0   mem len: 23271   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.76\n",
      "epis: 119   score: 0.0   mem len: 23394   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.76\n",
      "epis: 120   score: 2.0   mem len: 23592   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.76\n",
      "epis: 121   score: 1.0   mem len: 23761   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.77\n",
      "epis: 122   score: 1.0   mem len: 23930   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.76\n",
      "epis: 123   score: 1.0   mem len: 24099   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.77\n",
      "epis: 124   score: 2.0   mem len: 24297   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.79\n",
      "epis: 125   score: 3.0   mem len: 24565   epsilon: 1.0    steps: 268    lr: 0.0001     reward: 1.82\n",
      "epis: 126   score: 1.0   mem len: 24734   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.82\n",
      "epis: 127   score: 0.0   mem len: 24856   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.81\n",
      "epis: 128   score: 1.0   mem len: 25025   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.8\n",
      "epis: 129   score: 2.0   mem len: 25223   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.81\n",
      "epis: 130   score: 5.0   mem len: 25569   epsilon: 1.0    steps: 346    lr: 0.0001     reward: 1.86\n",
      "epis: 131   score: 1.0   mem len: 25741   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.86\n",
      "epis: 132   score: 1.0   mem len: 25909   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.84\n",
      "epis: 133   score: 4.0   mem len: 26220   epsilon: 1.0    steps: 311    lr: 0.0001     reward: 1.87\n",
      "epis: 134   score: 0.0   mem len: 26342   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.87\n",
      "epis: 135   score: 1.0   mem len: 26511   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.87\n",
      "epis: 136   score: 0.0   mem len: 26634   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.85\n",
      "epis: 137   score: 2.0   mem len: 26832   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.86\n",
      "epis: 138   score: 2.0   mem len: 27032   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.86\n",
      "epis: 139   score: 2.0   mem len: 27230   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.83\n",
      "epis: 140   score: 3.0   mem len: 27478   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.84\n",
      "epis: 141   score: 0.0   mem len: 27601   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.83\n",
      "epis: 142   score: 2.0   mem len: 27820   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.85\n",
      "epis: 143   score: 2.0   mem len: 28017   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.84\n",
      "epis: 144   score: 0.0   mem len: 28140   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 145   score: 1.0   mem len: 28309   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.76\n",
      "epis: 146   score: 2.0   mem len: 28508   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.76\n",
      "epis: 147   score: 0.0   mem len: 28631   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.75\n",
      "epis: 148   score: 0.0   mem len: 28753   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.73\n",
      "epis: 149   score: 1.0   mem len: 28922   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.72\n",
      "epis: 150   score: 2.0   mem len: 29119   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.71\n",
      "epis: 151   score: 0.0   mem len: 29242   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.7\n",
      "epis: 152   score: 1.0   mem len: 29411   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.69\n",
      "epis: 153   score: 2.0   mem len: 29609   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.67\n",
      "epis: 154   score: 0.0   mem len: 29732   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 155   score: 2.0   mem len: 29930   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.63\n",
      "epis: 156   score: 0.0   mem len: 30052   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 157   score: 1.0   mem len: 30221   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 158   score: 1.0   mem len: 30391   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.62\n",
      "epis: 159   score: 1.0   mem len: 30559   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.61\n",
      "epis: 160   score: 2.0   mem len: 30741   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.61\n",
      "epis: 161   score: 3.0   mem len: 31012   epsilon: 1.0    steps: 271    lr: 0.0001     reward: 1.62\n",
      "epis: 162   score: 2.0   mem len: 31211   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.64\n",
      "epis: 163   score: 2.0   mem len: 31409   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.66\n",
      "epis: 164   score: 0.0   mem len: 31531   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 165   score: 0.0   mem len: 31654   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 166   score: 1.0   mem len: 31804   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 167   score: 1.0   mem len: 31955   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 168   score: 0.0   mem len: 32077   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 169   score: 2.0   mem len: 32296   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.57\n",
      "epis: 170   score: 1.0   mem len: 32465   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.52\n",
      "epis: 171   score: 2.0   mem len: 32680   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.53\n",
      "epis: 172   score: 0.0   mem len: 32803   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 173   score: 2.0   mem len: 33023   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.49\n",
      "epis: 174   score: 2.0   mem len: 33220   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.5\n",
      "epis: 175   score: 3.0   mem len: 33445   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.51\n",
      "epis: 176   score: 2.0   mem len: 33642   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.53\n",
      "epis: 177   score: 3.0   mem len: 33868   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.54\n",
      "epis: 178   score: 0.0   mem len: 33990   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 179   score: 0.0   mem len: 34112   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 180   score: 2.0   mem len: 34329   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.46\n",
      "epis: 181   score: 0.0   mem len: 34451   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 182   score: 1.0   mem len: 34619   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.42\n",
      "epis: 183   score: 2.0   mem len: 34819   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.42\n",
      "epis: 184   score: 2.0   mem len: 34999   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.42\n",
      "epis: 185   score: 0.0   mem len: 35122   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 186   score: 0.0   mem len: 35245   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 187   score: 0.0   mem len: 35367   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 188   score: 0.0   mem len: 35489   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 189   score: 2.0   mem len: 35687   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.35\n",
      "epis: 190   score: 3.0   mem len: 35952   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.37\n",
      "epis: 191   score: 2.0   mem len: 36150   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 192   score: 0.0   mem len: 36273   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 193   score: 1.0   mem len: 36424   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 194   score: 3.0   mem len: 36671   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.39\n",
      "epis: 195   score: 1.0   mem len: 36839   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.39\n",
      "epis: 196   score: 2.0   mem len: 37037   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 197   score: 2.0   mem len: 37216   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.42\n",
      "epis: 198   score: 3.0   mem len: 37447   epsilon: 1.0    steps: 231    lr: 0.0001     reward: 1.41\n",
      "epis: 199   score: 0.0   mem len: 37570   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 200   score: 5.0   mem len: 37878   epsilon: 1.0    steps: 308    lr: 0.0001     reward: 1.36\n",
      "epis: 201   score: 6.0   mem len: 38252   epsilon: 1.0    steps: 374    lr: 0.0001     reward: 1.41\n",
      "epis: 202   score: 0.0   mem len: 38375   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 203   score: 1.0   mem len: 38526   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 204   score: 0.0   mem len: 38649   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 205   score: 3.0   mem len: 38896   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.42\n",
      "epis: 206   score: 1.0   mem len: 39065   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 207   score: 4.0   mem len: 39362   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.44\n",
      "epis: 208   score: 1.0   mem len: 39533   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.43\n",
      "epis: 209   score: 0.0   mem len: 39656   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 210   score: 1.0   mem len: 39807   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 211   score: 0.0   mem len: 39929   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 212   score: 0.0   mem len: 40051   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 213   score: 2.0   mem len: 40250   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.4\n",
      "epis: 214   score: 6.0   mem len: 40584   epsilon: 1.0    steps: 334    lr: 0.0001     reward: 1.45\n",
      "epis: 215   score: 2.0   mem len: 40802   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.42\n",
      "epis: 216   score: 1.0   mem len: 40971   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.41\n",
      "epis: 217   score: 3.0   mem len: 41198   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.43\n",
      "epis: 218   score: 0.0   mem len: 41321   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 219   score: 0.0   mem len: 41443   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 220   score: 6.0   mem len: 41819   epsilon: 1.0    steps: 376    lr: 0.0001     reward: 1.47\n",
      "epis: 221   score: 1.0   mem len: 41987   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.47\n",
      "epis: 222   score: 3.0   mem len: 42219   epsilon: 1.0    steps: 232    lr: 0.0001     reward: 1.49\n",
      "epis: 223   score: 6.0   mem len: 42615   epsilon: 1.0    steps: 396    lr: 0.0001     reward: 1.54\n",
      "epis: 224   score: 3.0   mem len: 42881   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.55\n",
      "epis: 225   score: 2.0   mem len: 43081   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.54\n",
      "epis: 226   score: 0.0   mem len: 43203   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 227   score: 2.0   mem len: 43424   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.55\n",
      "epis: 228   score: 2.0   mem len: 43643   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.56\n",
      "epis: 229   score: 0.0   mem len: 43765   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 230   score: 0.0   mem len: 43888   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 231   score: 0.0   mem len: 44011   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 232   score: 0.0   mem len: 44133   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 233   score: 1.0   mem len: 44301   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 234   score: 1.0   mem len: 44453   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.45\n",
      "epis: 235   score: 3.0   mem len: 44700   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.47\n",
      "epis: 236   score: 0.0   mem len: 44823   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 237   score: 1.0   mem len: 44973   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 238   score: 2.0   mem len: 45171   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 239   score: 1.0   mem len: 45321   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.45\n",
      "epis: 240   score: 1.0   mem len: 45472   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 241   score: 2.0   mem len: 45654   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.45\n",
      "epis: 242   score: 1.0   mem len: 45805   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 243   score: 0.0   mem len: 45927   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 244   score: 2.0   mem len: 46125   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.44\n",
      "epis: 245   score: 0.0   mem len: 46248   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 246   score: 1.0   mem len: 46416   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.42\n",
      "epis: 247   score: 0.0   mem len: 46539   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 248   score: 1.0   mem len: 46709   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 249   score: 2.0   mem len: 46928   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.44\n",
      "epis: 250   score: 3.0   mem len: 47155   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.45\n",
      "epis: 251   score: 1.0   mem len: 47305   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 252   score: 1.0   mem len: 47474   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 253   score: 1.0   mem len: 47643   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 254   score: 1.0   mem len: 47812   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 255   score: 0.0   mem len: 47935   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 256   score: 2.0   mem len: 48135   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.46\n",
      "epis: 257   score: 2.0   mem len: 48333   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 258   score: 1.0   mem len: 48503   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.47\n",
      "epis: 259   score: 0.0   mem len: 48626   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 260   score: 1.0   mem len: 48795   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 261   score: 1.0   mem len: 48967   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.43\n",
      "epis: 262   score: 3.0   mem len: 49217   epsilon: 1.0    steps: 250    lr: 0.0001     reward: 1.44\n",
      "epis: 263   score: 0.0   mem len: 49339   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 264   score: 1.0   mem len: 49507   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.43\n",
      "epis: 265   score: 0.0   mem len: 49630   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 266   score: 2.0   mem len: 49810   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.44\n",
      "epis: 267   score: 2.0   mem len: 50009   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.45\n",
      "epis: 268   score: 0.0   mem len: 50131   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 269   score: 2.0   mem len: 50329   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 270   score: 0.0   mem len: 50452   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 271   score: 4.0   mem len: 50729   epsilon: 1.0    steps: 277    lr: 0.0001     reward: 1.46\n",
      "epis: 272   score: 0.0   mem len: 50852   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 273   score: 1.0   mem len: 51005   epsilon: 1.0    steps: 153    lr: 0.0001     reward: 1.45\n",
      "epis: 274   score: 2.0   mem len: 51220   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.45\n",
      "epis: 275   score: 2.0   mem len: 51404   epsilon: 1.0    steps: 184    lr: 0.0001     reward: 1.44\n",
      "epis: 276   score: 0.0   mem len: 51526   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 277   score: 1.0   mem len: 51696   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.4\n",
      "epis: 278   score: 2.0   mem len: 51893   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.42\n",
      "epis: 279   score: 1.0   mem len: 52061   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.43\n",
      "epis: 280   score: 0.0   mem len: 52184   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 281   score: 3.0   mem len: 52430   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.44\n",
      "epis: 282   score: 5.0   mem len: 52771   epsilon: 1.0    steps: 341    lr: 0.0001     reward: 1.48\n",
      "epis: 283   score: 2.0   mem len: 52969   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 284   score: 0.0   mem len: 53092   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 285   score: 1.0   mem len: 53261   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 286   score: 3.0   mem len: 53530   epsilon: 1.0    steps: 269    lr: 0.0001     reward: 1.5\n",
      "epis: 287   score: 2.0   mem len: 53730   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.52\n",
      "epis: 288   score: 4.0   mem len: 54003   epsilon: 1.0    steps: 273    lr: 0.0001     reward: 1.56\n",
      "epis: 289   score: 0.0   mem len: 54125   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 290   score: 2.0   mem len: 54341   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.53\n",
      "epis: 291   score: 2.0   mem len: 54521   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.53\n",
      "epis: 292   score: 0.0   mem len: 54644   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 293   score: 3.0   mem len: 54870   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.55\n",
      "epis: 294   score: 1.0   mem len: 55021   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 295   score: 2.0   mem len: 55239   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.54\n",
      "epis: 296   score: 1.0   mem len: 55390   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 297   score: 0.0   mem len: 55512   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 298   score: 4.0   mem len: 55806   epsilon: 1.0    steps: 294    lr: 0.0001     reward: 1.52\n",
      "epis: 299   score: 3.0   mem len: 56070   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.55\n",
      "epis: 300   score: 1.0   mem len: 56240   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.51\n",
      "epis: 301   score: 2.0   mem len: 56439   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.47\n",
      "epis: 302   score: 1.0   mem len: 56590   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 303   score: 2.0   mem len: 56788   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 304   score: 2.0   mem len: 56986   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 305   score: 1.0   mem len: 57156   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.49\n",
      "epis: 306   score: 1.0   mem len: 57328   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.49\n",
      "epis: 307   score: 1.0   mem len: 57480   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.46\n",
      "epis: 308   score: 0.0   mem len: 57602   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 309   score: 2.0   mem len: 57818   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.47\n",
      "epis: 310   score: 0.0   mem len: 57941   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 311   score: 1.0   mem len: 58112   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.47\n",
      "epis: 312   score: 1.0   mem len: 58281   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 313   score: 5.0   mem len: 58610   epsilon: 1.0    steps: 329    lr: 0.0001     reward: 1.51\n",
      "epis: 314   score: 0.0   mem len: 58733   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 315   score: 0.0   mem len: 58855   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 316   score: 0.0   mem len: 58977   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 317   score: 3.0   mem len: 59209   epsilon: 1.0    steps: 232    lr: 0.0001     reward: 1.42\n",
      "epis: 318   score: 1.0   mem len: 59359   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.43\n",
      "epis: 319   score: 4.0   mem len: 59639   epsilon: 1.0    steps: 280    lr: 0.0001     reward: 1.47\n",
      "epis: 320   score: 0.0   mem len: 59762   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 321   score: 5.0   mem len: 60101   epsilon: 1.0    steps: 339    lr: 0.0001     reward: 1.45\n",
      "epis: 322   score: 0.0   mem len: 60223   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 323   score: 3.0   mem len: 60490   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.39\n",
      "epis: 324   score: 2.0   mem len: 60708   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.38\n",
      "epis: 325   score: 0.0   mem len: 60831   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 326   score: 0.0   mem len: 60954   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 327   score: 2.0   mem len: 61133   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.36\n",
      "epis: 328   score: 0.0   mem len: 61255   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 329   score: 1.0   mem len: 61425   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.35\n",
      "epis: 330   score: 0.0   mem len: 61547   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 331   score: 2.0   mem len: 61770   epsilon: 1.0    steps: 223    lr: 0.0001     reward: 1.37\n",
      "epis: 332   score: 0.0   mem len: 61892   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 333   score: 2.0   mem len: 62090   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 334   score: 2.0   mem len: 62287   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.39\n",
      "epis: 335   score: 0.0   mem len: 62410   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 336   score: 1.0   mem len: 62561   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 337   score: 5.0   mem len: 62887   epsilon: 1.0    steps: 326    lr: 0.0001     reward: 1.41\n",
      "epis: 338   score: 1.0   mem len: 63056   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 339   score: 1.0   mem len: 63206   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.4\n",
      "epis: 340   score: 0.0   mem len: 63329   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 341   score: 3.0   mem len: 63556   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.4\n",
      "epis: 342   score: 2.0   mem len: 63774   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 343   score: 2.0   mem len: 63992   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.43\n",
      "epis: 344   score: 2.0   mem len: 64190   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.43\n",
      "epis: 345   score: 3.0   mem len: 64456   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.46\n",
      "epis: 346   score: 1.0   mem len: 64625   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 347   score: 2.0   mem len: 64842   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.48\n",
      "epis: 348   score: 0.0   mem len: 64965   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 349   score: 1.0   mem len: 65134   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 350   score: 1.0   mem len: 65285   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 351   score: 1.0   mem len: 65453   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 352   score: 0.0   mem len: 65576   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 353   score: 1.0   mem len: 65727   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 354   score: 1.0   mem len: 65878   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 355   score: 2.0   mem len: 66077   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.45\n",
      "epis: 356   score: 3.0   mem len: 66343   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.46\n",
      "epis: 357   score: 0.0   mem len: 66465   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.44\n",
      "epis: 358   score: 1.0   mem len: 66633   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 359   score: 1.0   mem len: 66805   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.45\n",
      "epis: 360   score: 2.0   mem len: 67003   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 361   score: 1.0   mem len: 67171   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.46\n",
      "epis: 362   score: 2.0   mem len: 67392   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.45\n",
      "epis: 363   score: 3.0   mem len: 67618   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.48\n",
      "epis: 364   score: 1.0   mem len: 67786   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.48\n",
      "epis: 365   score: 0.0   mem len: 67908   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 366   score: 2.0   mem len: 68125   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.48\n",
      "epis: 367   score: 1.0   mem len: 68296   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.47\n",
      "epis: 368   score: 2.0   mem len: 68494   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 369   score: 1.0   mem len: 68645   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 370   score: 3.0   mem len: 68893   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.51\n",
      "epis: 371   score: 2.0   mem len: 69091   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 372   score: 3.0   mem len: 69360   epsilon: 1.0    steps: 269    lr: 0.0001     reward: 1.52\n",
      "epis: 373   score: 0.0   mem len: 69483   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 374   score: 1.0   mem len: 69651   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 375   score: 1.0   mem len: 69801   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.49\n",
      "epis: 376   score: 1.0   mem len: 69969   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 377   score: 1.0   mem len: 70141   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.5\n",
      "epis: 378   score: 1.0   mem len: 70309   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.49\n",
      "epis: 379   score: 1.0   mem len: 70478   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 380   score: 0.0   mem len: 70600   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 381   score: 0.0   mem len: 70722   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.46\n",
      "epis: 382   score: 2.0   mem len: 70919   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.43\n",
      "epis: 383   score: 3.0   mem len: 71190   epsilon: 1.0    steps: 271    lr: 0.0001     reward: 1.44\n",
      "epis: 384   score: 2.0   mem len: 71388   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 385   score: 2.0   mem len: 71586   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 386   score: 0.0   mem len: 71709   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 387   score: 1.0   mem len: 71877   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.43\n",
      "epis: 388   score: 1.0   mem len: 72046   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 389   score: 2.0   mem len: 72244   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.42\n",
      "epis: 390   score: 0.0   mem len: 72367   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 391   score: 2.0   mem len: 72566   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.4\n",
      "epis: 392   score: 0.0   mem len: 72688   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 393   score: 3.0   mem len: 72935   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.4\n",
      "epis: 394   score: 1.0   mem len: 73104   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 395   score: 0.0   mem len: 73227   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 396   score: 3.0   mem len: 73475   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.4\n",
      "epis: 397   score: 1.0   mem len: 73626   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.41\n",
      "epis: 398   score: 2.0   mem len: 73829   epsilon: 1.0    steps: 203    lr: 0.0001     reward: 1.39\n",
      "epis: 399   score: 0.0   mem len: 73952   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 400   score: 3.0   mem len: 74198   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.38\n",
      "epis: 401   score: 1.0   mem len: 74348   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.37\n",
      "epis: 402   score: 2.0   mem len: 74565   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.38\n",
      "epis: 403   score: 1.0   mem len: 74734   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 404   score: 0.0   mem len: 74857   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 405   score: 0.0   mem len: 74979   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 406   score: 7.0   mem len: 75388   epsilon: 1.0    steps: 409    lr: 0.0001     reward: 1.4\n",
      "epis: 407   score: 1.0   mem len: 75539   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 408   score: 0.0   mem len: 75661   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 409   score: 2.0   mem len: 75858   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.4\n",
      "epis: 410   score: 5.0   mem len: 76167   epsilon: 1.0    steps: 309    lr: 0.0001     reward: 1.45\n",
      "epis: 411   score: 1.0   mem len: 76336   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 412   score: 2.0   mem len: 76534   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 413   score: 0.0   mem len: 76656   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 414   score: 1.0   mem len: 76806   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.42\n",
      "epis: 415   score: 5.0   mem len: 77153   epsilon: 1.0    steps: 347    lr: 0.0001     reward: 1.47\n",
      "epis: 416   score: 0.0   mem len: 77276   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 417   score: 4.0   mem len: 77573   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.48\n",
      "epis: 418   score: 1.0   mem len: 77741   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.48\n",
      "epis: 419   score: 0.0   mem len: 77864   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 420   score: 4.0   mem len: 78157   epsilon: 1.0    steps: 293    lr: 0.0001     reward: 1.48\n",
      "epis: 421   score: 0.0   mem len: 78280   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 422   score: 0.0   mem len: 78403   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 423   score: 0.0   mem len: 78526   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 424   score: 0.0   mem len: 78649   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 425   score: 2.0   mem len: 78867   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.4\n",
      "epis: 426   score: 2.0   mem len: 79047   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.42\n",
      "epis: 427   score: 2.0   mem len: 79247   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.42\n",
      "epis: 428   score: 0.0   mem len: 79369   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 429   score: 0.0   mem len: 79491   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 430   score: 4.0   mem len: 79750   epsilon: 1.0    steps: 259    lr: 0.0001     reward: 1.45\n",
      "epis: 431   score: 0.0   mem len: 79873   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 432   score: 1.0   mem len: 80041   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 433   score: 1.0   mem len: 80191   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.43\n",
      "epis: 434   score: 0.0   mem len: 80313   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 435   score: 2.0   mem len: 80532   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.43\n",
      "epis: 436   score: 2.0   mem len: 80748   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.44\n",
      "epis: 437   score: 2.0   mem len: 80966   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.41\n",
      "epis: 438   score: 2.0   mem len: 81164   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.42\n",
      "epis: 439   score: 1.0   mem len: 81335   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.42\n",
      "epis: 440   score: 0.0   mem len: 81457   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 441   score: 4.0   mem len: 81751   epsilon: 1.0    steps: 294    lr: 0.0001     reward: 1.43\n",
      "epis: 442   score: 1.0   mem len: 81923   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.42\n",
      "epis: 443   score: 2.0   mem len: 82143   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.42\n",
      "epis: 444   score: 0.0   mem len: 82265   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 445   score: 1.0   mem len: 82433   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.38\n",
      "epis: 446   score: 3.0   mem len: 82699   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.4\n",
      "epis: 447   score: 1.0   mem len: 82870   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.39\n",
      "epis: 448   score: 4.0   mem len: 83187   epsilon: 1.0    steps: 317    lr: 0.0001     reward: 1.43\n",
      "epis: 449   score: 1.0   mem len: 83338   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 450   score: 1.0   mem len: 83510   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.43\n",
      "epis: 451   score: 3.0   mem len: 83737   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.45\n",
      "epis: 452   score: 0.0   mem len: 83859   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 453   score: 1.0   mem len: 84030   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.45\n",
      "epis: 454   score: 2.0   mem len: 84212   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.46\n",
      "epis: 455   score: 2.0   mem len: 84432   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.46\n",
      "epis: 456   score: 4.0   mem len: 84727   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.47\n",
      "epis: 457   score: 3.0   mem len: 84972   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.5\n",
      "epis: 458   score: 2.0   mem len: 85172   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.51\n",
      "epis: 459   score: 2.0   mem len: 85369   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.52\n",
      "epis: 460   score: 4.0   mem len: 85609   epsilon: 1.0    steps: 240    lr: 0.0001     reward: 1.54\n",
      "epis: 461   score: 1.0   mem len: 85760   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.54\n",
      "epis: 462   score: 0.0   mem len: 85882   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 463   score: 3.0   mem len: 86128   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.52\n",
      "epis: 464   score: 1.0   mem len: 86297   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.52\n",
      "epis: 465   score: 3.0   mem len: 86542   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.55\n",
      "epis: 466   score: 1.0   mem len: 86712   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.54\n",
      "epis: 467   score: 0.0   mem len: 86834   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 468   score: 1.0   mem len: 86984   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.52\n",
      "epis: 469   score: 5.0   mem len: 87311   epsilon: 1.0    steps: 327    lr: 0.0001     reward: 1.56\n",
      "epis: 470   score: 0.0   mem len: 87434   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 471   score: 1.0   mem len: 87585   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 472   score: 0.0   mem len: 87708   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 473   score: 4.0   mem len: 88004   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.53\n",
      "epis: 474   score: 0.0   mem len: 88126   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 475   score: 2.0   mem len: 88305   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.53\n",
      "epis: 476   score: 1.0   mem len: 88475   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.53\n",
      "epis: 477   score: 0.0   mem len: 88598   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 478   score: 2.0   mem len: 88798   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.53\n",
      "epis: 479   score: 0.0   mem len: 88920   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 480   score: 3.0   mem len: 89187   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.55\n",
      "epis: 481   score: 7.0   mem len: 89486   epsilon: 1.0    steps: 299    lr: 0.0001     reward: 1.62\n",
      "epis: 482   score: 0.0   mem len: 89609   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 483   score: 3.0   mem len: 89876   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.6\n",
      "epis: 484   score: 2.0   mem len: 90076   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.6\n",
      "epis: 485   score: 0.0   mem len: 90198   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 486   score: 1.0   mem len: 90349   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 487   score: 0.0   mem len: 90472   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 488   score: 1.0   mem len: 90641   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 489   score: 2.0   mem len: 90839   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 490   score: 1.0   mem len: 90989   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.59\n",
      "epis: 491   score: 2.0   mem len: 91187   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 492   score: 0.0   mem len: 91309   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 493   score: 0.0   mem len: 91432   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 494   score: 1.0   mem len: 91582   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.56\n",
      "epis: 495   score: 2.0   mem len: 91779   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.58\n",
      "epis: 496   score: 1.0   mem len: 91947   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 497   score: 1.0   mem len: 92097   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.56\n",
      "epis: 498   score: 0.0   mem len: 92220   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 499   score: 2.0   mem len: 92418   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 500   score: 1.0   mem len: 92587   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 501   score: 4.0   mem len: 92882   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.57\n",
      "epis: 502   score: 0.0   mem len: 93005   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 503   score: 2.0   mem len: 93203   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 504   score: 0.0   mem len: 93326   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 505   score: 1.0   mem len: 93495   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 506   score: 2.0   mem len: 93677   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.52\n",
      "epis: 507   score: 1.0   mem len: 93846   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.52\n",
      "epis: 508   score: 0.0   mem len: 93969   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 509   score: 2.0   mem len: 94167   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 510   score: 1.0   mem len: 94335   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.48\n",
      "epis: 511   score: 2.0   mem len: 94553   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.49\n",
      "epis: 512   score: 0.0   mem len: 94676   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 513   score: 0.0   mem len: 94799   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 514   score: 0.0   mem len: 94921   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.46\n",
      "epis: 515   score: 0.0   mem len: 95044   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 516   score: 1.0   mem len: 95213   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 517   score: 3.0   mem len: 95461   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.41\n",
      "epis: 518   score: 0.0   mem len: 95584   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 519   score: 1.0   mem len: 95734   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.41\n",
      "epis: 520   score: 2.0   mem len: 95955   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.39\n",
      "epis: 521   score: 0.0   mem len: 96078   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 522   score: 1.0   mem len: 96249   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.4\n",
      "epis: 523   score: 2.0   mem len: 96467   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.42\n",
      "epis: 524   score: 0.0   mem len: 96590   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 525   score: 1.0   mem len: 96761   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.41\n",
      "epis: 526   score: 2.0   mem len: 96961   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.41\n",
      "epis: 527   score: 2.0   mem len: 97160   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.41\n",
      "epis: 528   score: 3.0   mem len: 97403   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.44\n",
      "epis: 529   score: 3.0   mem len: 97654   epsilon: 1.0    steps: 251    lr: 0.0001     reward: 1.47\n",
      "epis: 530   score: 1.0   mem len: 97805   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.44\n",
      "epis: 531   score: 1.0   mem len: 97975   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.45\n",
      "epis: 532   score: 2.0   mem len: 98172   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.46\n",
      "epis: 533   score: 3.0   mem len: 98420   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.48\n",
      "epis: 534   score: 2.0   mem len: 98617   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.5\n",
      "epis: 535   score: 4.0   mem len: 98914   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.52\n",
      "epis: 536   score: 1.0   mem len: 99065   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 537   score: 5.0   mem len: 99391   epsilon: 1.0    steps: 326    lr: 0.0001     reward: 1.54\n",
      "epis: 538   score: 0.0   mem len: 99513   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 539   score: 4.0   mem len: 99806   epsilon: 1.0    steps: 293    lr: 0.0001     reward: 1.55\n",
      "epis: 540   score: 1.0   mem len: 99977   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/memory.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array(sample)\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent_double.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent_double.py:121: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  next_q_values[mask] = self.target_net(non_final_next_states).gather(1, next_action.unsqueeze(1)).cuda()[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 541   score: 0.0   mem len: 100100   epsilon: 0.9998    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 542   score: 1.0   mem len: 100251   epsilon: 0.9995    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 543   score: 0.0   mem len: 100374   epsilon: 0.9993    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 544   score: 1.0   mem len: 100525   epsilon: 0.999    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 545   score: 1.0   mem len: 100693   epsilon: 0.9986    steps: 168    lr: 0.0001     reward: 1.51\n",
      "epis: 546   score: 1.0   mem len: 100862   epsilon: 0.9983    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 547   score: 3.0   mem len: 101108   epsilon: 0.9978    steps: 246    lr: 0.0001     reward: 1.51\n",
      "epis: 548   score: 1.0   mem len: 101277   epsilon: 0.9975    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 549   score: 1.0   mem len: 101428   epsilon: 0.9972    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 550   score: 1.0   mem len: 101579   epsilon: 0.9969    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 551   score: 4.0   mem len: 101876   epsilon: 0.9963    steps: 297    lr: 0.0001     reward: 1.49\n",
      "epis: 552   score: 1.0   mem len: 102044   epsilon: 0.996    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 553   score: 1.0   mem len: 102214   epsilon: 0.9956    steps: 170    lr: 0.0001     reward: 1.5\n",
      "epis: 554   score: 0.0   mem len: 102337   epsilon: 0.9954    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 555   score: 5.0   mem len: 102664   epsilon: 0.9947    steps: 327    lr: 0.0001     reward: 1.51\n",
      "epis: 556   score: 2.0   mem len: 102863   epsilon: 0.9943    steps: 199    lr: 0.0001     reward: 1.49\n",
      "epis: 557   score: 0.0   mem len: 102986   epsilon: 0.9941    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 558   score: 2.0   mem len: 103184   epsilon: 0.9937    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 559   score: 3.0   mem len: 103431   epsilon: 0.9932    steps: 247    lr: 0.0001     reward: 1.47\n",
      "epis: 560   score: 0.0   mem len: 103554   epsilon: 0.993    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 561   score: 1.0   mem len: 103704   epsilon: 0.9927    steps: 150    lr: 0.0001     reward: 1.43\n",
      "epis: 562   score: 2.0   mem len: 103901   epsilon: 0.9923    steps: 197    lr: 0.0001     reward: 1.45\n",
      "epis: 563   score: 4.0   mem len: 104157   epsilon: 0.9918    steps: 256    lr: 0.0001     reward: 1.46\n",
      "epis: 564   score: 2.0   mem len: 104375   epsilon: 0.9913    steps: 218    lr: 0.0001     reward: 1.47\n",
      "epis: 565   score: 2.0   mem len: 104572   epsilon: 0.9909    steps: 197    lr: 0.0001     reward: 1.46\n",
      "epis: 566   score: 1.0   mem len: 104743   epsilon: 0.9906    steps: 171    lr: 0.0001     reward: 1.46\n",
      "epis: 567   score: 1.0   mem len: 104911   epsilon: 0.9903    steps: 168    lr: 0.0001     reward: 1.47\n",
      "epis: 568   score: 2.0   mem len: 105131   epsilon: 0.9898    steps: 220    lr: 0.0001     reward: 1.48\n",
      "epis: 569   score: 0.0   mem len: 105254   epsilon: 0.9896    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 570   score: 0.0   mem len: 105376   epsilon: 0.9894    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 571   score: 2.0   mem len: 105594   epsilon: 0.9889    steps: 218    lr: 0.0001     reward: 1.44\n",
      "epis: 572   score: 0.0   mem len: 105717   epsilon: 0.9887    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 573   score: 2.0   mem len: 105914   epsilon: 0.9883    steps: 197    lr: 0.0001     reward: 1.42\n",
      "epis: 574   score: 3.0   mem len: 106141   epsilon: 0.9878    steps: 227    lr: 0.0001     reward: 1.45\n",
      "epis: 575   score: 1.0   mem len: 106310   epsilon: 0.9875    steps: 169    lr: 0.0001     reward: 1.44\n",
      "epis: 576   score: 0.0   mem len: 106433   epsilon: 0.9873    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 577   score: 1.0   mem len: 106602   epsilon: 0.9869    steps: 169    lr: 0.0001     reward: 1.44\n",
      "epis: 578   score: 0.0   mem len: 106725   epsilon: 0.9867    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 579   score: 1.0   mem len: 106894   epsilon: 0.9863    steps: 169    lr: 0.0001     reward: 1.43\n",
      "epis: 580   score: 2.0   mem len: 107092   epsilon: 0.986    steps: 198    lr: 0.0001     reward: 1.42\n",
      "epis: 581   score: 2.0   mem len: 107310   epsilon: 0.9855    steps: 218    lr: 0.0001     reward: 1.37\n",
      "epis: 582   score: 2.0   mem len: 107508   epsilon: 0.9851    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 583   score: 1.0   mem len: 107659   epsilon: 0.9848    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 584   score: 3.0   mem len: 107906   epsilon: 0.9843    steps: 247    lr: 0.0001     reward: 1.38\n",
      "epis: 585   score: 2.0   mem len: 108103   epsilon: 0.984    steps: 197    lr: 0.0001     reward: 1.4\n",
      "epis: 586   score: 0.0   mem len: 108226   epsilon: 0.9837    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 587   score: 0.0   mem len: 108349   epsilon: 0.9835    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 588   score: 1.0   mem len: 108519   epsilon: 0.9831    steps: 170    lr: 0.0001     reward: 1.39\n",
      "epis: 589   score: 2.0   mem len: 108717   epsilon: 0.9827    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 590   score: 1.0   mem len: 108869   epsilon: 0.9824    steps: 152    lr: 0.0001     reward: 1.39\n",
      "epis: 591   score: 1.0   mem len: 109019   epsilon: 0.9821    steps: 150    lr: 0.0001     reward: 1.38\n",
      "epis: 592   score: 1.0   mem len: 109191   epsilon: 0.9818    steps: 172    lr: 0.0001     reward: 1.39\n",
      "epis: 593   score: 3.0   mem len: 109435   epsilon: 0.9813    steps: 244    lr: 0.0001     reward: 1.42\n",
      "epis: 594   score: 0.0   mem len: 109558   epsilon: 0.9811    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 595   score: 2.0   mem len: 109756   epsilon: 0.9807    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 596   score: 0.0   mem len: 109879   epsilon: 0.9804    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 597   score: 1.0   mem len: 110048   epsilon: 0.9801    steps: 169    lr: 0.0001     reward: 1.4\n",
      "epis: 598   score: 2.0   mem len: 110245   epsilon: 0.9797    steps: 197    lr: 0.0001     reward: 1.42\n",
      "epis: 599   score: 2.0   mem len: 110442   epsilon: 0.9793    steps: 197    lr: 0.0001     reward: 1.42\n",
      "epis: 600   score: 1.0   mem len: 110593   epsilon: 0.979    steps: 151    lr: 0.0001     reward: 1.42\n",
      "epis: 601   score: 0.0   mem len: 110716   epsilon: 0.9788    steps: 123    lr: 0.0001     reward: 1.38\n",
      "epis: 602   score: 1.0   mem len: 110886   epsilon: 0.9784    steps: 170    lr: 0.0001     reward: 1.39\n",
      "epis: 603   score: 0.0   mem len: 111009   epsilon: 0.9782    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 604   score: 0.0   mem len: 111131   epsilon: 0.978    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 605   score: 1.0   mem len: 111299   epsilon: 0.9776    steps: 168    lr: 0.0001     reward: 1.37\n",
      "epis: 606   score: 5.0   mem len: 111621   epsilon: 0.977    steps: 322    lr: 0.0001     reward: 1.4\n",
      "epis: 607   score: 5.0   mem len: 111935   epsilon: 0.9764    steps: 314    lr: 0.0001     reward: 1.44\n",
      "epis: 608   score: 2.0   mem len: 112153   epsilon: 0.9759    steps: 218    lr: 0.0001     reward: 1.46\n",
      "epis: 609   score: 1.0   mem len: 112324   epsilon: 0.9756    steps: 171    lr: 0.0001     reward: 1.45\n",
      "epis: 610   score: 1.0   mem len: 112493   epsilon: 0.9753    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 611   score: 0.0   mem len: 112615   epsilon: 0.975    steps: 122    lr: 0.0001     reward: 1.43\n",
      "epis: 612   score: 1.0   mem len: 112787   epsilon: 0.9747    steps: 172    lr: 0.0001     reward: 1.44\n",
      "epis: 613   score: 1.0   mem len: 112956   epsilon: 0.9743    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 614   score: 0.0   mem len: 113078   epsilon: 0.9741    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 615   score: 1.0   mem len: 113228   epsilon: 0.9738    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 616   score: 1.0   mem len: 113397   epsilon: 0.9735    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 617   score: 0.0   mem len: 113520   epsilon: 0.9732    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 618   score: 1.0   mem len: 113670   epsilon: 0.9729    steps: 150    lr: 0.0001     reward: 1.44\n",
      "epis: 619   score: 3.0   mem len: 113937   epsilon: 0.9724    steps: 267    lr: 0.0001     reward: 1.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 620   score: 3.0   mem len: 114181   epsilon: 0.9719    steps: 244    lr: 0.0001     reward: 1.47\n",
      "epis: 621   score: 0.0   mem len: 114304   epsilon: 0.9717    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 622   score: 0.0   mem len: 114427   epsilon: 0.9714    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 623   score: 3.0   mem len: 114677   epsilon: 0.9709    steps: 250    lr: 0.0001     reward: 1.47\n",
      "epis: 624   score: 4.0   mem len: 114977   epsilon: 0.9703    steps: 300    lr: 0.0001     reward: 1.51\n",
      "epis: 625   score: 2.0   mem len: 115178   epsilon: 0.9699    steps: 201    lr: 0.0001     reward: 1.52\n",
      "epis: 626   score: 0.0   mem len: 115301   epsilon: 0.9697    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 627   score: 0.0   mem len: 115424   epsilon: 0.9695    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 628   score: 1.0   mem len: 115594   epsilon: 0.9691    steps: 170    lr: 0.0001     reward: 1.46\n",
      "epis: 629   score: 4.0   mem len: 115888   epsilon: 0.9685    steps: 294    lr: 0.0001     reward: 1.47\n",
      "epis: 630   score: 1.0   mem len: 116038   epsilon: 0.9682    steps: 150    lr: 0.0001     reward: 1.47\n",
      "epis: 631   score: 2.0   mem len: 116259   epsilon: 0.9678    steps: 221    lr: 0.0001     reward: 1.48\n",
      "epis: 632   score: 1.0   mem len: 116428   epsilon: 0.9675    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 633   score: 2.0   mem len: 116647   epsilon: 0.967    steps: 219    lr: 0.0001     reward: 1.46\n",
      "epis: 634   score: 2.0   mem len: 116845   epsilon: 0.9666    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 635   score: 1.0   mem len: 117016   epsilon: 0.9663    steps: 171    lr: 0.0001     reward: 1.43\n",
      "epis: 636   score: 1.0   mem len: 117167   epsilon: 0.966    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 637   score: 1.0   mem len: 117337   epsilon: 0.9657    steps: 170    lr: 0.0001     reward: 1.39\n",
      "epis: 638   score: 1.0   mem len: 117505   epsilon: 0.9653    steps: 168    lr: 0.0001     reward: 1.4\n",
      "epis: 639   score: 0.0   mem len: 117628   epsilon: 0.9651    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 640   score: 0.0   mem len: 117751   epsilon: 0.9649    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 641   score: 1.0   mem len: 117921   epsilon: 0.9645    steps: 170    lr: 0.0001     reward: 1.36\n",
      "epis: 642   score: 0.0   mem len: 118043   epsilon: 0.9643    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 643   score: 0.0   mem len: 118166   epsilon: 0.964    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 644   score: 0.0   mem len: 118288   epsilon: 0.9638    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 645   score: 3.0   mem len: 118538   epsilon: 0.9633    steps: 250    lr: 0.0001     reward: 1.36\n",
      "epis: 646   score: 0.0   mem len: 118660   epsilon: 0.9631    steps: 122    lr: 0.0001     reward: 1.35\n",
      "epis: 647   score: 0.0   mem len: 118783   epsilon: 0.9628    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 648   score: 3.0   mem len: 119010   epsilon: 0.9624    steps: 227    lr: 0.0001     reward: 1.34\n",
      "epis: 649   score: 4.0   mem len: 119306   epsilon: 0.9618    steps: 296    lr: 0.0001     reward: 1.37\n",
      "epis: 650   score: 1.0   mem len: 119478   epsilon: 0.9614    steps: 172    lr: 0.0001     reward: 1.37\n",
      "epis: 651   score: 1.0   mem len: 119646   epsilon: 0.9611    steps: 168    lr: 0.0001     reward: 1.34\n",
      "epis: 652   score: 0.0   mem len: 119769   epsilon: 0.9609    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 653   score: 1.0   mem len: 119920   epsilon: 0.9606    steps: 151    lr: 0.0001     reward: 1.33\n",
      "epis: 654   score: 3.0   mem len: 120166   epsilon: 0.9601    steps: 246    lr: 0.0001     reward: 1.36\n",
      "epis: 655   score: 4.0   mem len: 120483   epsilon: 0.9594    steps: 317    lr: 0.0001     reward: 1.35\n",
      "epis: 656   score: 0.0   mem len: 120605   epsilon: 0.9592    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 657   score: 0.0   mem len: 120728   epsilon: 0.959    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 658   score: 2.0   mem len: 120909   epsilon: 0.9586    steps: 181    lr: 0.0001     reward: 1.33\n",
      "epis: 659   score: 0.0   mem len: 121031   epsilon: 0.9584    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 660   score: 1.0   mem len: 121181   epsilon: 0.9581    steps: 150    lr: 0.0001     reward: 1.31\n",
      "epis: 661   score: 2.0   mem len: 121399   epsilon: 0.9576    steps: 218    lr: 0.0001     reward: 1.32\n",
      "epis: 662   score: 2.0   mem len: 121599   epsilon: 0.9572    steps: 200    lr: 0.0001     reward: 1.32\n",
      "epis: 663   score: 0.0   mem len: 121721   epsilon: 0.957    steps: 122    lr: 0.0001     reward: 1.28\n",
      "epis: 664   score: 2.0   mem len: 121940   epsilon: 0.9566    steps: 219    lr: 0.0001     reward: 1.28\n",
      "epis: 665   score: 2.0   mem len: 122137   epsilon: 0.9562    steps: 197    lr: 0.0001     reward: 1.28\n",
      "epis: 666   score: 4.0   mem len: 122433   epsilon: 0.9556    steps: 296    lr: 0.0001     reward: 1.31\n",
      "epis: 667   score: 1.0   mem len: 122604   epsilon: 0.9552    steps: 171    lr: 0.0001     reward: 1.31\n",
      "epis: 668   score: 2.0   mem len: 122801   epsilon: 0.9549    steps: 197    lr: 0.0001     reward: 1.31\n",
      "epis: 669   score: 2.0   mem len: 122999   epsilon: 0.9545    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 670   score: 2.0   mem len: 123197   epsilon: 0.9541    steps: 198    lr: 0.0001     reward: 1.35\n",
      "epis: 671   score: 1.0   mem len: 123367   epsilon: 0.9537    steps: 170    lr: 0.0001     reward: 1.34\n",
      "epis: 672   score: 1.0   mem len: 123538   epsilon: 0.9534    steps: 171    lr: 0.0001     reward: 1.35\n",
      "epis: 673   score: 2.0   mem len: 123736   epsilon: 0.953    steps: 198    lr: 0.0001     reward: 1.35\n",
      "epis: 674   score: 3.0   mem len: 124001   epsilon: 0.9525    steps: 265    lr: 0.0001     reward: 1.35\n",
      "epis: 675   score: 3.0   mem len: 124246   epsilon: 0.952    steps: 245    lr: 0.0001     reward: 1.37\n",
      "epis: 676   score: 1.0   mem len: 124397   epsilon: 0.9517    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 677   score: 1.0   mem len: 124566   epsilon: 0.9514    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 678   score: 1.0   mem len: 124717   epsilon: 0.9511    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 679   score: 5.0   mem len: 125063   epsilon: 0.9504    steps: 346    lr: 0.0001     reward: 1.43\n",
      "epis: 680   score: 1.0   mem len: 125232   epsilon: 0.95    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 681   score: 2.0   mem len: 125449   epsilon: 0.9496    steps: 217    lr: 0.0001     reward: 1.42\n",
      "epis: 682   score: 1.0   mem len: 125600   epsilon: 0.9493    steps: 151    lr: 0.0001     reward: 1.41\n",
      "epis: 683   score: 2.0   mem len: 125818   epsilon: 0.9489    steps: 218    lr: 0.0001     reward: 1.42\n",
      "epis: 684   score: 4.0   mem len: 126108   epsilon: 0.9483    steps: 290    lr: 0.0001     reward: 1.43\n",
      "epis: 685   score: 2.0   mem len: 126306   epsilon: 0.9479    steps: 198    lr: 0.0001     reward: 1.43\n",
      "epis: 686   score: 2.0   mem len: 126524   epsilon: 0.9475    steps: 218    lr: 0.0001     reward: 1.45\n",
      "epis: 687   score: 1.0   mem len: 126692   epsilon: 0.9471    steps: 168    lr: 0.0001     reward: 1.46\n",
      "epis: 688   score: 3.0   mem len: 126919   epsilon: 0.9467    steps: 227    lr: 0.0001     reward: 1.48\n",
      "epis: 689   score: 2.0   mem len: 127136   epsilon: 0.9463    steps: 217    lr: 0.0001     reward: 1.48\n",
      "epis: 690   score: 2.0   mem len: 127334   epsilon: 0.9459    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 691   score: 2.0   mem len: 127532   epsilon: 0.9455    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 692   score: 3.0   mem len: 127778   epsilon: 0.945    steps: 246    lr: 0.0001     reward: 1.52\n",
      "epis: 693   score: 7.0   mem len: 128160   epsilon: 0.9442    steps: 382    lr: 0.0001     reward: 1.56\n",
      "epis: 694   score: 0.0   mem len: 128282   epsilon: 0.944    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 695   score: 1.0   mem len: 128451   epsilon: 0.9437    steps: 169    lr: 0.0001     reward: 1.55\n",
      "epis: 696   score: 2.0   mem len: 128633   epsilon: 0.9433    steps: 182    lr: 0.0001     reward: 1.57\n",
      "epis: 697   score: 0.0   mem len: 128756   epsilon: 0.9431    steps: 123    lr: 0.0001     reward: 1.56\n",
      "epis: 698   score: 2.0   mem len: 128956   epsilon: 0.9427    steps: 200    lr: 0.0001     reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 699   score: 0.0   mem len: 129078   epsilon: 0.9424    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 700   score: 2.0   mem len: 129277   epsilon: 0.942    steps: 199    lr: 0.0001     reward: 1.55\n",
      "epis: 701   score: 0.0   mem len: 129400   epsilon: 0.9418    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 702   score: 0.0   mem len: 129523   epsilon: 0.9415    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 703   score: 1.0   mem len: 129674   epsilon: 0.9412    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 704   score: 0.0   mem len: 129797   epsilon: 0.941    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 705   score: 1.0   mem len: 129948   epsilon: 0.9407    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 706   score: 1.0   mem len: 130118   epsilon: 0.9404    steps: 170    lr: 0.0001     reward: 1.51\n",
      "epis: 707   score: 1.0   mem len: 130289   epsilon: 0.94    steps: 171    lr: 0.0001     reward: 1.47\n",
      "epis: 708   score: 5.0   mem len: 130609   epsilon: 0.9394    steps: 320    lr: 0.0001     reward: 1.5\n",
      "epis: 709   score: 0.0   mem len: 130732   epsilon: 0.9391    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 710   score: 0.0   mem len: 130854   epsilon: 0.9389    steps: 122    lr: 0.0001     reward: 1.48\n",
      "epis: 711   score: 2.0   mem len: 131053   epsilon: 0.9385    steps: 199    lr: 0.0001     reward: 1.5\n",
      "epis: 712   score: 0.0   mem len: 131176   epsilon: 0.9383    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 713   score: 1.0   mem len: 131345   epsilon: 0.9379    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 714   score: 1.0   mem len: 131497   epsilon: 0.9376    steps: 152    lr: 0.0001     reward: 1.5\n",
      "epis: 715   score: 4.0   mem len: 131753   epsilon: 0.9371    steps: 256    lr: 0.0001     reward: 1.53\n",
      "epis: 716   score: 7.0   mem len: 132172   epsilon: 0.9363    steps: 419    lr: 0.0001     reward: 1.59\n",
      "epis: 717   score: 1.0   mem len: 132323   epsilon: 0.936    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 718   score: 0.0   mem len: 132446   epsilon: 0.9358    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 719   score: 2.0   mem len: 132644   epsilon: 0.9354    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 720   score: 0.0   mem len: 132767   epsilon: 0.9351    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 721   score: 3.0   mem len: 132992   epsilon: 0.9347    steps: 225    lr: 0.0001     reward: 1.58\n",
      "epis: 722   score: 1.0   mem len: 133143   epsilon: 0.9344    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 723   score: 1.0   mem len: 133294   epsilon: 0.9341    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 724   score: 3.0   mem len: 133543   epsilon: 0.9336    steps: 249    lr: 0.0001     reward: 1.56\n",
      "epis: 725   score: 1.0   mem len: 133711   epsilon: 0.9333    steps: 168    lr: 0.0001     reward: 1.55\n",
      "epis: 726   score: 1.0   mem len: 133880   epsilon: 0.9329    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 727   score: 2.0   mem len: 134078   epsilon: 0.9325    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 728   score: 2.0   mem len: 134276   epsilon: 0.9321    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 729   score: 0.0   mem len: 134399   epsilon: 0.9319    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 730   score: 4.0   mem len: 134674   epsilon: 0.9313    steps: 275    lr: 0.0001     reward: 1.58\n",
      "epis: 731   score: 2.0   mem len: 134857   epsilon: 0.931    steps: 183    lr: 0.0001     reward: 1.58\n",
      "epis: 732   score: 1.0   mem len: 135008   epsilon: 0.9307    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 733   score: 1.0   mem len: 135158   epsilon: 0.9304    steps: 150    lr: 0.0001     reward: 1.57\n",
      "epis: 734   score: 1.0   mem len: 135327   epsilon: 0.9301    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 735   score: 1.0   mem len: 135497   epsilon: 0.9297    steps: 170    lr: 0.0001     reward: 1.56\n",
      "epis: 736   score: 0.0   mem len: 135619   epsilon: 0.9295    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 737   score: 0.0   mem len: 135741   epsilon: 0.9292    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 738   score: 0.0   mem len: 135864   epsilon: 0.929    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 739   score: 1.0   mem len: 136033   epsilon: 0.9287    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 740   score: 4.0   mem len: 136328   epsilon: 0.9281    steps: 295    lr: 0.0001     reward: 1.58\n",
      "epis: 741   score: 2.0   mem len: 136526   epsilon: 0.9277    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 742   score: 0.0   mem len: 136648   epsilon: 0.9274    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 743   score: 2.0   mem len: 136845   epsilon: 0.927    steps: 197    lr: 0.0001     reward: 1.61\n",
      "epis: 744   score: 3.0   mem len: 137109   epsilon: 0.9265    steps: 264    lr: 0.0001     reward: 1.64\n",
      "epis: 745   score: 0.0   mem len: 137231   epsilon: 0.9263    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 746   score: 1.0   mem len: 137400   epsilon: 0.9259    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 747   score: 2.0   mem len: 137617   epsilon: 0.9255    steps: 217    lr: 0.0001     reward: 1.64\n",
      "epis: 748   score: 0.0   mem len: 137739   epsilon: 0.9253    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 749   score: 5.0   mem len: 138065   epsilon: 0.9246    steps: 326    lr: 0.0001     reward: 1.62\n",
      "epis: 750   score: 3.0   mem len: 138312   epsilon: 0.9241    steps: 247    lr: 0.0001     reward: 1.64\n",
      "epis: 751   score: 0.0   mem len: 138435   epsilon: 0.9239    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 752   score: 2.0   mem len: 138633   epsilon: 0.9235    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 753   score: 2.0   mem len: 138849   epsilon: 0.9231    steps: 216    lr: 0.0001     reward: 1.66\n",
      "epis: 754   score: 4.0   mem len: 139145   epsilon: 0.9225    steps: 296    lr: 0.0001     reward: 1.67\n",
      "epis: 755   score: 1.0   mem len: 139297   epsilon: 0.9222    steps: 152    lr: 0.0001     reward: 1.64\n",
      "epis: 756   score: 3.0   mem len: 139566   epsilon: 0.9217    steps: 269    lr: 0.0001     reward: 1.67\n",
      "epis: 757   score: 3.0   mem len: 139816   epsilon: 0.9212    steps: 250    lr: 0.0001     reward: 1.7\n",
      "epis: 758   score: 0.0   mem len: 139939   epsilon: 0.9209    steps: 123    lr: 0.0001     reward: 1.68\n",
      "epis: 759   score: 2.0   mem len: 140137   epsilon: 0.9205    steps: 198    lr: 0.0001     reward: 1.7\n",
      "epis: 760   score: 3.0   mem len: 140364   epsilon: 0.9201    steps: 227    lr: 0.0001     reward: 1.72\n",
      "epis: 761   score: 1.0   mem len: 140533   epsilon: 0.9197    steps: 169    lr: 0.0001     reward: 1.71\n",
      "epis: 762   score: 2.0   mem len: 140731   epsilon: 0.9194    steps: 198    lr: 0.0001     reward: 1.71\n",
      "epis: 763   score: 0.0   mem len: 140854   epsilon: 0.9191    steps: 123    lr: 0.0001     reward: 1.71\n",
      "epis: 764   score: 2.0   mem len: 141051   epsilon: 0.9187    steps: 197    lr: 0.0001     reward: 1.71\n",
      "epis: 765   score: 0.0   mem len: 141174   epsilon: 0.9185    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 766   score: 1.0   mem len: 141344   epsilon: 0.9181    steps: 170    lr: 0.0001     reward: 1.66\n",
      "epis: 767   score: 3.0   mem len: 141591   epsilon: 0.9176    steps: 247    lr: 0.0001     reward: 1.68\n",
      "epis: 768   score: 3.0   mem len: 141817   epsilon: 0.9172    steps: 226    lr: 0.0001     reward: 1.69\n",
      "epis: 769   score: 2.0   mem len: 142020   epsilon: 0.9168    steps: 203    lr: 0.0001     reward: 1.69\n",
      "epis: 770   score: 0.0   mem len: 142143   epsilon: 0.9166    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 771   score: 3.0   mem len: 142371   epsilon: 0.9161    steps: 228    lr: 0.0001     reward: 1.69\n",
      "epis: 772   score: 3.0   mem len: 142597   epsilon: 0.9157    steps: 226    lr: 0.0001     reward: 1.71\n",
      "epis: 773   score: 1.0   mem len: 142766   epsilon: 0.9153    steps: 169    lr: 0.0001     reward: 1.7\n",
      "epis: 774   score: 1.0   mem len: 142936   epsilon: 0.915    steps: 170    lr: 0.0001     reward: 1.68\n",
      "epis: 775   score: 0.0   mem len: 143059   epsilon: 0.9147    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 776   score: 1.0   mem len: 143210   epsilon: 0.9144    steps: 151    lr: 0.0001     reward: 1.65\n",
      "epis: 777   score: 1.0   mem len: 143380   epsilon: 0.9141    steps: 170    lr: 0.0001     reward: 1.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 778   score: 0.0   mem len: 143503   epsilon: 0.9139    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 779   score: 3.0   mem len: 143749   epsilon: 0.9134    steps: 246    lr: 0.0001     reward: 1.62\n",
      "epis: 780   score: 1.0   mem len: 143918   epsilon: 0.913    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 781   score: 2.0   mem len: 144136   epsilon: 0.9126    steps: 218    lr: 0.0001     reward: 1.62\n",
      "epis: 782   score: 1.0   mem len: 144287   epsilon: 0.9123    steps: 151    lr: 0.0001     reward: 1.62\n",
      "epis: 783   score: 3.0   mem len: 144533   epsilon: 0.9118    steps: 246    lr: 0.0001     reward: 1.63\n",
      "epis: 784   score: 3.0   mem len: 144760   epsilon: 0.9114    steps: 227    lr: 0.0001     reward: 1.62\n",
      "epis: 785   score: 4.0   mem len: 145055   epsilon: 0.9108    steps: 295    lr: 0.0001     reward: 1.64\n",
      "epis: 786   score: 3.0   mem len: 145300   epsilon: 0.9103    steps: 245    lr: 0.0001     reward: 1.65\n",
      "epis: 787   score: 0.0   mem len: 145422   epsilon: 0.9101    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 788   score: 0.0   mem len: 145544   epsilon: 0.9098    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 789   score: 0.0   mem len: 145666   epsilon: 0.9096    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 790   score: 3.0   mem len: 145911   epsilon: 0.9091    steps: 245    lr: 0.0001     reward: 1.6\n",
      "epis: 791   score: 8.0   mem len: 146255   epsilon: 0.9084    steps: 344    lr: 0.0001     reward: 1.66\n",
      "epis: 792   score: 1.0   mem len: 146424   epsilon: 0.9081    steps: 169    lr: 0.0001     reward: 1.64\n",
      "epis: 793   score: 0.0   mem len: 146546   epsilon: 0.9078    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 794   score: 3.0   mem len: 146792   epsilon: 0.9073    steps: 246    lr: 0.0001     reward: 1.6\n",
      "epis: 795   score: 2.0   mem len: 146991   epsilon: 0.907    steps: 199    lr: 0.0001     reward: 1.61\n",
      "epis: 796   score: 1.0   mem len: 147142   epsilon: 0.9067    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 797   score: 2.0   mem len: 147360   epsilon: 0.9062    steps: 218    lr: 0.0001     reward: 1.62\n",
      "epis: 798   score: 1.0   mem len: 147531   epsilon: 0.9059    steps: 171    lr: 0.0001     reward: 1.61\n",
      "epis: 799   score: 3.0   mem len: 147796   epsilon: 0.9054    steps: 265    lr: 0.0001     reward: 1.64\n",
      "epis: 800   score: 0.0   mem len: 147919   epsilon: 0.9051    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 801   score: 2.0   mem len: 148136   epsilon: 0.9047    steps: 217    lr: 0.0001     reward: 1.64\n",
      "epis: 802   score: 2.0   mem len: 148357   epsilon: 0.9043    steps: 221    lr: 0.0001     reward: 1.66\n",
      "epis: 803   score: 0.0   mem len: 148479   epsilon: 0.904    steps: 122    lr: 0.0001     reward: 1.65\n",
      "epis: 804   score: 1.0   mem len: 148651   epsilon: 0.9037    steps: 172    lr: 0.0001     reward: 1.66\n",
      "epis: 805   score: 0.0   mem len: 148774   epsilon: 0.9034    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 806   score: 2.0   mem len: 148993   epsilon: 0.903    steps: 219    lr: 0.0001     reward: 1.66\n",
      "epis: 807   score: 0.0   mem len: 149116   epsilon: 0.9027    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 808   score: 1.0   mem len: 149267   epsilon: 0.9024    steps: 151    lr: 0.0001     reward: 1.61\n",
      "epis: 809   score: 0.0   mem len: 149389   epsilon: 0.9022    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 810   score: 0.0   mem len: 149511   epsilon: 0.902    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 811   score: 0.0   mem len: 149633   epsilon: 0.9017    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 812   score: 2.0   mem len: 149830   epsilon: 0.9013    steps: 197    lr: 0.0001     reward: 1.61\n",
      "epis: 813   score: 2.0   mem len: 150027   epsilon: 0.9009    steps: 197    lr: 0.0001     reward: 1.62\n",
      "epis: 814   score: 3.0   mem len: 150293   epsilon: 0.9004    steps: 266    lr: 0.0001     reward: 1.64\n",
      "epis: 815   score: 4.0   mem len: 150588   epsilon: 0.8998    steps: 295    lr: 0.0001     reward: 1.64\n",
      "epis: 816   score: 0.0   mem len: 150710   epsilon: 0.8996    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 817   score: 1.0   mem len: 150879   epsilon: 0.8993    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 818   score: 2.0   mem len: 151077   epsilon: 0.8989    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 819   score: 0.0   mem len: 151200   epsilon: 0.8986    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 820   score: 0.0   mem len: 151323   epsilon: 0.8984    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 821   score: 1.0   mem len: 151491   epsilon: 0.898    steps: 168    lr: 0.0001     reward: 1.55\n",
      "epis: 822   score: 1.0   mem len: 151660   epsilon: 0.8977    steps: 169    lr: 0.0001     reward: 1.55\n",
      "epis: 823   score: 1.0   mem len: 151831   epsilon: 0.8974    steps: 171    lr: 0.0001     reward: 1.55\n",
      "epis: 824   score: 2.0   mem len: 152028   epsilon: 0.897    steps: 197    lr: 0.0001     reward: 1.54\n",
      "epis: 825   score: 2.0   mem len: 152229   epsilon: 0.8966    steps: 201    lr: 0.0001     reward: 1.55\n",
      "epis: 826   score: 2.0   mem len: 152427   epsilon: 0.8962    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 827   score: 3.0   mem len: 152695   epsilon: 0.8957    steps: 268    lr: 0.0001     reward: 1.57\n",
      "epis: 828   score: 2.0   mem len: 152893   epsilon: 0.8953    steps: 198    lr: 0.0001     reward: 1.57\n",
      "epis: 829   score: 3.0   mem len: 153121   epsilon: 0.8948    steps: 228    lr: 0.0001     reward: 1.6\n",
      "epis: 830   score: 0.0   mem len: 153243   epsilon: 0.8946    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 831   score: 0.0   mem len: 153365   epsilon: 0.8943    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 832   score: 1.0   mem len: 153535   epsilon: 0.894    steps: 170    lr: 0.0001     reward: 1.54\n",
      "epis: 833   score: 0.0   mem len: 153658   epsilon: 0.8938    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 834   score: 0.0   mem len: 153781   epsilon: 0.8935    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 835   score: 1.0   mem len: 153931   epsilon: 0.8932    steps: 150    lr: 0.0001     reward: 1.52\n",
      "epis: 836   score: 3.0   mem len: 154160   epsilon: 0.8928    steps: 229    lr: 0.0001     reward: 1.55\n",
      "epis: 837   score: 3.0   mem len: 154407   epsilon: 0.8923    steps: 247    lr: 0.0001     reward: 1.58\n",
      "epis: 838   score: 1.0   mem len: 154557   epsilon: 0.892    steps: 150    lr: 0.0001     reward: 1.59\n",
      "epis: 839   score: 1.0   mem len: 154707   epsilon: 0.8917    steps: 150    lr: 0.0001     reward: 1.59\n",
      "epis: 840   score: 0.0   mem len: 154830   epsilon: 0.8914    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 841   score: 4.0   mem len: 155105   epsilon: 0.8909    steps: 275    lr: 0.0001     reward: 1.57\n",
      "epis: 842   score: 1.0   mem len: 155256   epsilon: 0.8906    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 843   score: 1.0   mem len: 155427   epsilon: 0.8903    steps: 171    lr: 0.0001     reward: 1.57\n",
      "epis: 844   score: 3.0   mem len: 155671   epsilon: 0.8898    steps: 244    lr: 0.0001     reward: 1.57\n",
      "epis: 845   score: 2.0   mem len: 155868   epsilon: 0.8894    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 846   score: 1.0   mem len: 156036   epsilon: 0.889    steps: 168    lr: 0.0001     reward: 1.59\n",
      "epis: 847   score: 2.0   mem len: 156233   epsilon: 0.8887    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 848   score: 5.0   mem len: 156577   epsilon: 0.888    steps: 344    lr: 0.0001     reward: 1.64\n",
      "epis: 849   score: 2.0   mem len: 156777   epsilon: 0.8876    steps: 200    lr: 0.0001     reward: 1.61\n",
      "epis: 850   score: 2.0   mem len: 156974   epsilon: 0.8872    steps: 197    lr: 0.0001     reward: 1.6\n",
      "epis: 851   score: 0.0   mem len: 157096   epsilon: 0.8869    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 852   score: 3.0   mem len: 157327   epsilon: 0.8865    steps: 231    lr: 0.0001     reward: 1.61\n",
      "epis: 853   score: 3.0   mem len: 157570   epsilon: 0.886    steps: 243    lr: 0.0001     reward: 1.62\n",
      "epis: 854   score: 2.0   mem len: 157768   epsilon: 0.8856    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 855   score: 2.0   mem len: 157948   epsilon: 0.8853    steps: 180    lr: 0.0001     reward: 1.61\n",
      "epis: 856   score: 0.0   mem len: 158071   epsilon: 0.885    steps: 123    lr: 0.0001     reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 857   score: 0.0   mem len: 158194   epsilon: 0.8848    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 858   score: 0.0   mem len: 158316   epsilon: 0.8845    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 859   score: 0.0   mem len: 158439   epsilon: 0.8843    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 860   score: 3.0   mem len: 158684   epsilon: 0.8838    steps: 245    lr: 0.0001     reward: 1.53\n",
      "epis: 861   score: 2.0   mem len: 158866   epsilon: 0.8834    steps: 182    lr: 0.0001     reward: 1.54\n",
      "epis: 862   score: 3.0   mem len: 159115   epsilon: 0.883    steps: 249    lr: 0.0001     reward: 1.55\n",
      "epis: 863   score: 3.0   mem len: 159384   epsilon: 0.8824    steps: 269    lr: 0.0001     reward: 1.58\n",
      "epis: 864   score: 0.0   mem len: 159506   epsilon: 0.8822    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 865   score: 1.0   mem len: 159674   epsilon: 0.8818    steps: 168    lr: 0.0001     reward: 1.57\n",
      "epis: 866   score: 2.0   mem len: 159893   epsilon: 0.8814    steps: 219    lr: 0.0001     reward: 1.58\n",
      "epis: 867   score: 3.0   mem len: 160142   epsilon: 0.8809    steps: 249    lr: 0.0001     reward: 1.58\n",
      "epis: 868   score: 3.0   mem len: 160387   epsilon: 0.8804    steps: 245    lr: 0.0001     reward: 1.58\n",
      "epis: 869   score: 3.0   mem len: 160637   epsilon: 0.8799    steps: 250    lr: 0.0001     reward: 1.59\n",
      "epis: 870   score: 0.0   mem len: 160760   epsilon: 0.8797    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 871   score: 4.0   mem len: 161035   epsilon: 0.8791    steps: 275    lr: 0.0001     reward: 1.6\n",
      "epis: 872   score: 1.0   mem len: 161207   epsilon: 0.8788    steps: 172    lr: 0.0001     reward: 1.58\n",
      "epis: 873   score: 2.0   mem len: 161424   epsilon: 0.8784    steps: 217    lr: 0.0001     reward: 1.59\n",
      "epis: 874   score: 2.0   mem len: 161622   epsilon: 0.878    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 875   score: 3.0   mem len: 161869   epsilon: 0.8775    steps: 247    lr: 0.0001     reward: 1.63\n",
      "epis: 876   score: 1.0   mem len: 162038   epsilon: 0.8772    steps: 169    lr: 0.0001     reward: 1.63\n",
      "epis: 877   score: 1.0   mem len: 162188   epsilon: 0.8769    steps: 150    lr: 0.0001     reward: 1.63\n",
      "epis: 878   score: 1.0   mem len: 162340   epsilon: 0.8766    steps: 152    lr: 0.0001     reward: 1.64\n",
      "epis: 879   score: 0.0   mem len: 162463   epsilon: 0.8763    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 880   score: 0.0   mem len: 162586   epsilon: 0.8761    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 881   score: 4.0   mem len: 162860   epsilon: 0.8755    steps: 274    lr: 0.0001     reward: 1.62\n",
      "epis: 882   score: 1.0   mem len: 163030   epsilon: 0.8752    steps: 170    lr: 0.0001     reward: 1.62\n",
      "epis: 883   score: 4.0   mem len: 163305   epsilon: 0.8747    steps: 275    lr: 0.0001     reward: 1.63\n",
      "epis: 884   score: 3.0   mem len: 163531   epsilon: 0.8742    steps: 226    lr: 0.0001     reward: 1.63\n",
      "epis: 885   score: 0.0   mem len: 163654   epsilon: 0.874    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 886   score: 1.0   mem len: 163825   epsilon: 0.8736    steps: 171    lr: 0.0001     reward: 1.57\n",
      "epis: 887   score: 1.0   mem len: 163994   epsilon: 0.8733    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 888   score: 1.0   mem len: 164144   epsilon: 0.873    steps: 150    lr: 0.0001     reward: 1.59\n",
      "epis: 889   score: 2.0   mem len: 164342   epsilon: 0.8726    steps: 198    lr: 0.0001     reward: 1.61\n",
      "epis: 890   score: 4.0   mem len: 164635   epsilon: 0.872    steps: 293    lr: 0.0001     reward: 1.62\n",
      "epis: 891   score: 1.0   mem len: 164786   epsilon: 0.8717    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 892   score: 2.0   mem len: 164984   epsilon: 0.8713    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 893   score: 1.0   mem len: 165153   epsilon: 0.871    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 894   score: 1.0   mem len: 165304   epsilon: 0.8707    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 895   score: 2.0   mem len: 165485   epsilon: 0.8703    steps: 181    lr: 0.0001     reward: 1.55\n",
      "epis: 896   score: 3.0   mem len: 165750   epsilon: 0.8698    steps: 265    lr: 0.0001     reward: 1.57\n",
      "epis: 897   score: 3.0   mem len: 165976   epsilon: 0.8694    steps: 226    lr: 0.0001     reward: 1.58\n",
      "epis: 898   score: 6.0   mem len: 166330   epsilon: 0.8687    steps: 354    lr: 0.0001     reward: 1.63\n",
      "epis: 899   score: 4.0   mem len: 166606   epsilon: 0.8681    steps: 276    lr: 0.0001     reward: 1.64\n",
      "epis: 900   score: 2.0   mem len: 166808   epsilon: 0.8677    steps: 202    lr: 0.0001     reward: 1.66\n",
      "epis: 901   score: 3.0   mem len: 167057   epsilon: 0.8672    steps: 249    lr: 0.0001     reward: 1.67\n",
      "epis: 902   score: 3.0   mem len: 167285   epsilon: 0.8668    steps: 228    lr: 0.0001     reward: 1.68\n",
      "epis: 903   score: 4.0   mem len: 167584   epsilon: 0.8662    steps: 299    lr: 0.0001     reward: 1.72\n",
      "epis: 904   score: 3.0   mem len: 167852   epsilon: 0.8657    steps: 268    lr: 0.0001     reward: 1.74\n",
      "epis: 905   score: 2.0   mem len: 168070   epsilon: 0.8652    steps: 218    lr: 0.0001     reward: 1.76\n",
      "epis: 906   score: 3.0   mem len: 168298   epsilon: 0.8648    steps: 228    lr: 0.0001     reward: 1.77\n",
      "epis: 907   score: 2.0   mem len: 168498   epsilon: 0.8644    steps: 200    lr: 0.0001     reward: 1.79\n",
      "epis: 908   score: 6.0   mem len: 168845   epsilon: 0.8637    steps: 347    lr: 0.0001     reward: 1.84\n",
      "epis: 909   score: 3.0   mem len: 169057   epsilon: 0.8633    steps: 212    lr: 0.0001     reward: 1.87\n",
      "epis: 910   score: 6.0   mem len: 169405   epsilon: 0.8626    steps: 348    lr: 0.0001     reward: 1.93\n",
      "epis: 911   score: 2.0   mem len: 169603   epsilon: 0.8622    steps: 198    lr: 0.0001     reward: 1.95\n",
      "epis: 912   score: 1.0   mem len: 169772   epsilon: 0.8618    steps: 169    lr: 0.0001     reward: 1.94\n",
      "epis: 913   score: 1.0   mem len: 169940   epsilon: 0.8615    steps: 168    lr: 0.0001     reward: 1.93\n",
      "epis: 914   score: 5.0   mem len: 170274   epsilon: 0.8609    steps: 334    lr: 0.0001     reward: 1.95\n",
      "epis: 915   score: 0.0   mem len: 170397   epsilon: 0.8606    steps: 123    lr: 0.0001     reward: 1.91\n",
      "epis: 916   score: 1.0   mem len: 170549   epsilon: 0.8603    steps: 152    lr: 0.0001     reward: 1.92\n",
      "epis: 917   score: 1.0   mem len: 170718   epsilon: 0.86    steps: 169    lr: 0.0001     reward: 1.92\n",
      "epis: 918   score: 4.0   mem len: 171014   epsilon: 0.8594    steps: 296    lr: 0.0001     reward: 1.94\n",
      "epis: 919   score: 4.0   mem len: 171315   epsilon: 0.8588    steps: 301    lr: 0.0001     reward: 1.98\n",
      "epis: 920   score: 0.0   mem len: 171438   epsilon: 0.8586    steps: 123    lr: 0.0001     reward: 1.98\n",
      "epis: 921   score: 3.0   mem len: 171692   epsilon: 0.858    steps: 254    lr: 0.0001     reward: 2.0\n",
      "epis: 922   score: 2.0   mem len: 171889   epsilon: 0.8577    steps: 197    lr: 0.0001     reward: 2.01\n",
      "epis: 923   score: 3.0   mem len: 172140   epsilon: 0.8572    steps: 251    lr: 0.0001     reward: 2.03\n",
      "epis: 924   score: 4.0   mem len: 172428   epsilon: 0.8566    steps: 288    lr: 0.0001     reward: 2.05\n",
      "epis: 925   score: 0.0   mem len: 172551   epsilon: 0.8563    steps: 123    lr: 0.0001     reward: 2.03\n",
      "epis: 926   score: 4.0   mem len: 172849   epsilon: 0.8558    steps: 298    lr: 0.0001     reward: 2.05\n",
      "epis: 927   score: 1.0   mem len: 173018   epsilon: 0.8554    steps: 169    lr: 0.0001     reward: 2.03\n",
      "epis: 928   score: 1.0   mem len: 173188   epsilon: 0.8551    steps: 170    lr: 0.0001     reward: 2.02\n",
      "epis: 929   score: 1.0   mem len: 173339   epsilon: 0.8548    steps: 151    lr: 0.0001     reward: 2.0\n",
      "epis: 930   score: 2.0   mem len: 173521   epsilon: 0.8544    steps: 182    lr: 0.0001     reward: 2.02\n",
      "epis: 931   score: 2.0   mem len: 173719   epsilon: 0.854    steps: 198    lr: 0.0001     reward: 2.04\n",
      "epis: 932   score: 1.0   mem len: 173869   epsilon: 0.8537    steps: 150    lr: 0.0001     reward: 2.04\n",
      "epis: 933   score: 2.0   mem len: 174067   epsilon: 0.8533    steps: 198    lr: 0.0001     reward: 2.06\n",
      "epis: 934   score: 3.0   mem len: 174292   epsilon: 0.8529    steps: 225    lr: 0.0001     reward: 2.09\n",
      "epis: 935   score: 3.0   mem len: 174517   epsilon: 0.8525    steps: 225    lr: 0.0001     reward: 2.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 936   score: 2.0   mem len: 174699   epsilon: 0.8521    steps: 182    lr: 0.0001     reward: 2.1\n",
      "epis: 937   score: 3.0   mem len: 174946   epsilon: 0.8516    steps: 247    lr: 0.0001     reward: 2.1\n",
      "epis: 938   score: 2.0   mem len: 175147   epsilon: 0.8512    steps: 201    lr: 0.0001     reward: 2.11\n",
      "epis: 939   score: 3.0   mem len: 175391   epsilon: 0.8507    steps: 244    lr: 0.0001     reward: 2.13\n",
      "epis: 940   score: 7.0   mem len: 175801   epsilon: 0.8499    steps: 410    lr: 0.0001     reward: 2.2\n",
      "epis: 941   score: 5.0   mem len: 176114   epsilon: 0.8493    steps: 313    lr: 0.0001     reward: 2.21\n",
      "epis: 942   score: 2.0   mem len: 176336   epsilon: 0.8489    steps: 222    lr: 0.0001     reward: 2.22\n",
      "epis: 943   score: 0.0   mem len: 176459   epsilon: 0.8486    steps: 123    lr: 0.0001     reward: 2.21\n",
      "epis: 944   score: 2.0   mem len: 176639   epsilon: 0.8483    steps: 180    lr: 0.0001     reward: 2.2\n",
      "epis: 945   score: 3.0   mem len: 176883   epsilon: 0.8478    steps: 244    lr: 0.0001     reward: 2.21\n",
      "epis: 946   score: 3.0   mem len: 177112   epsilon: 0.8473    steps: 229    lr: 0.0001     reward: 2.23\n",
      "epis: 947   score: 2.0   mem len: 177328   epsilon: 0.8469    steps: 216    lr: 0.0001     reward: 2.23\n",
      "epis: 948   score: 0.0   mem len: 177451   epsilon: 0.8466    steps: 123    lr: 0.0001     reward: 2.18\n",
      "epis: 949   score: 2.0   mem len: 177651   epsilon: 0.8462    steps: 200    lr: 0.0001     reward: 2.18\n",
      "epis: 950   score: 0.0   mem len: 177774   epsilon: 0.846    steps: 123    lr: 0.0001     reward: 2.16\n",
      "epis: 951   score: 0.0   mem len: 177897   epsilon: 0.8458    steps: 123    lr: 0.0001     reward: 2.16\n",
      "epis: 952   score: 0.0   mem len: 178019   epsilon: 0.8455    steps: 122    lr: 0.0001     reward: 2.13\n",
      "epis: 953   score: 2.0   mem len: 178235   epsilon: 0.8451    steps: 216    lr: 0.0001     reward: 2.12\n",
      "epis: 954   score: 4.0   mem len: 178513   epsilon: 0.8445    steps: 278    lr: 0.0001     reward: 2.14\n",
      "epis: 955   score: 3.0   mem len: 178742   epsilon: 0.8441    steps: 229    lr: 0.0001     reward: 2.15\n",
      "epis: 956   score: 3.0   mem len: 179007   epsilon: 0.8436    steps: 265    lr: 0.0001     reward: 2.18\n",
      "epis: 957   score: 1.0   mem len: 179176   epsilon: 0.8432    steps: 169    lr: 0.0001     reward: 2.19\n",
      "epis: 958   score: 0.0   mem len: 179299   epsilon: 0.843    steps: 123    lr: 0.0001     reward: 2.19\n",
      "epis: 959   score: 2.0   mem len: 179497   epsilon: 0.8426    steps: 198    lr: 0.0001     reward: 2.21\n",
      "epis: 960   score: 4.0   mem len: 179795   epsilon: 0.842    steps: 298    lr: 0.0001     reward: 2.22\n",
      "epis: 961   score: 2.0   mem len: 179995   epsilon: 0.8416    steps: 200    lr: 0.0001     reward: 2.22\n",
      "epis: 962   score: 2.0   mem len: 180213   epsilon: 0.8412    steps: 218    lr: 0.0001     reward: 2.21\n",
      "epis: 963   score: 3.0   mem len: 180479   epsilon: 0.8406    steps: 266    lr: 0.0001     reward: 2.21\n",
      "epis: 964   score: 4.0   mem len: 180775   epsilon: 0.8401    steps: 296    lr: 0.0001     reward: 2.25\n",
      "epis: 965   score: 4.0   mem len: 181070   epsilon: 0.8395    steps: 295    lr: 0.0001     reward: 2.28\n",
      "epis: 966   score: 1.0   mem len: 181239   epsilon: 0.8391    steps: 169    lr: 0.0001     reward: 2.27\n",
      "epis: 967   score: 2.0   mem len: 181454   epsilon: 0.8387    steps: 215    lr: 0.0001     reward: 2.26\n",
      "epis: 968   score: 3.0   mem len: 181680   epsilon: 0.8383    steps: 226    lr: 0.0001     reward: 2.26\n",
      "epis: 969   score: 1.0   mem len: 181851   epsilon: 0.8379    steps: 171    lr: 0.0001     reward: 2.24\n",
      "epis: 970   score: 3.0   mem len: 182118   epsilon: 0.8374    steps: 267    lr: 0.0001     reward: 2.27\n",
      "epis: 971   score: 4.0   mem len: 182376   epsilon: 0.8369    steps: 258    lr: 0.0001     reward: 2.27\n",
      "epis: 972   score: 2.0   mem len: 182593   epsilon: 0.8365    steps: 217    lr: 0.0001     reward: 2.28\n",
      "epis: 973   score: 1.0   mem len: 182764   epsilon: 0.8361    steps: 171    lr: 0.0001     reward: 2.27\n",
      "epis: 974   score: 2.0   mem len: 182945   epsilon: 0.8358    steps: 181    lr: 0.0001     reward: 2.27\n",
      "epis: 975   score: 1.0   mem len: 183116   epsilon: 0.8354    steps: 171    lr: 0.0001     reward: 2.25\n",
      "epis: 976   score: 2.0   mem len: 183299   epsilon: 0.8351    steps: 183    lr: 0.0001     reward: 2.26\n",
      "epis: 977   score: 0.0   mem len: 183421   epsilon: 0.8348    steps: 122    lr: 0.0001     reward: 2.25\n",
      "epis: 978   score: 2.0   mem len: 183639   epsilon: 0.8344    steps: 218    lr: 0.0001     reward: 2.26\n",
      "epis: 979   score: 7.0   mem len: 184020   epsilon: 0.8336    steps: 381    lr: 0.0001     reward: 2.33\n",
      "epis: 980   score: 4.0   mem len: 184329   epsilon: 0.833    steps: 309    lr: 0.0001     reward: 2.37\n",
      "epis: 981   score: 2.0   mem len: 184510   epsilon: 0.8327    steps: 181    lr: 0.0001     reward: 2.35\n",
      "epis: 982   score: 1.0   mem len: 184660   epsilon: 0.8324    steps: 150    lr: 0.0001     reward: 2.35\n",
      "epis: 983   score: 7.0   mem len: 184981   epsilon: 0.8317    steps: 321    lr: 0.0001     reward: 2.38\n",
      "epis: 984   score: 5.0   mem len: 185313   epsilon: 0.8311    steps: 332    lr: 0.0001     reward: 2.4\n",
      "epis: 985   score: 4.0   mem len: 185588   epsilon: 0.8305    steps: 275    lr: 0.0001     reward: 2.44\n",
      "epis: 986   score: 0.0   mem len: 185710   epsilon: 0.8303    steps: 122    lr: 0.0001     reward: 2.43\n",
      "epis: 987   score: 4.0   mem len: 185986   epsilon: 0.8297    steps: 276    lr: 0.0001     reward: 2.46\n",
      "epis: 988   score: 4.0   mem len: 186225   epsilon: 0.8293    steps: 239    lr: 0.0001     reward: 2.49\n",
      "epis: 989   score: 0.0   mem len: 186348   epsilon: 0.829    steps: 123    lr: 0.0001     reward: 2.47\n",
      "epis: 990   score: 4.0   mem len: 186641   epsilon: 0.8284    steps: 293    lr: 0.0001     reward: 2.47\n",
      "epis: 991   score: 1.0   mem len: 186791   epsilon: 0.8282    steps: 150    lr: 0.0001     reward: 2.47\n",
      "epis: 992   score: 0.0   mem len: 186914   epsilon: 0.8279    steps: 123    lr: 0.0001     reward: 2.45\n",
      "epis: 993   score: 2.0   mem len: 187111   epsilon: 0.8275    steps: 197    lr: 0.0001     reward: 2.46\n",
      "epis: 994   score: 2.0   mem len: 187333   epsilon: 0.8271    steps: 222    lr: 0.0001     reward: 2.47\n",
      "epis: 995   score: 4.0   mem len: 187592   epsilon: 0.8266    steps: 259    lr: 0.0001     reward: 2.49\n",
      "epis: 996   score: 0.0   mem len: 187715   epsilon: 0.8263    steps: 123    lr: 0.0001     reward: 2.46\n",
      "epis: 997   score: 0.0   mem len: 187838   epsilon: 0.8261    steps: 123    lr: 0.0001     reward: 2.43\n",
      "epis: 998   score: 0.0   mem len: 187961   epsilon: 0.8258    steps: 123    lr: 0.0001     reward: 2.37\n",
      "epis: 999   score: 7.0   mem len: 188382   epsilon: 0.825    steps: 421    lr: 0.0001     reward: 2.4\n",
      "epis: 1000   score: 2.0   mem len: 188581   epsilon: 0.8246    steps: 199    lr: 0.0001     reward: 2.4\n",
      "epis: 1001   score: 2.0   mem len: 188781   epsilon: 0.8242    steps: 200    lr: 0.0001     reward: 2.39\n",
      "epis: 1002   score: 2.0   mem len: 188981   epsilon: 0.8238    steps: 200    lr: 0.0001     reward: 2.38\n",
      "epis: 1003   score: 1.0   mem len: 189150   epsilon: 0.8235    steps: 169    lr: 0.0001     reward: 2.35\n",
      "epis: 1004   score: 1.0   mem len: 189301   epsilon: 0.8232    steps: 151    lr: 0.0001     reward: 2.33\n",
      "epis: 1005   score: 3.0   mem len: 189546   epsilon: 0.8227    steps: 245    lr: 0.0001     reward: 2.34\n",
      "epis: 1006   score: 1.0   mem len: 189697   epsilon: 0.8224    steps: 151    lr: 0.0001     reward: 2.32\n",
      "epis: 1007   score: 2.0   mem len: 189896   epsilon: 0.822    steps: 199    lr: 0.0001     reward: 2.32\n",
      "epis: 1008   score: 3.0   mem len: 190125   epsilon: 0.8216    steps: 229    lr: 0.0001     reward: 2.29\n",
      "epis: 1009   score: 5.0   mem len: 190451   epsilon: 0.8209    steps: 326    lr: 0.0001     reward: 2.31\n",
      "epis: 1010   score: 3.0   mem len: 190696   epsilon: 0.8204    steps: 245    lr: 0.0001     reward: 2.28\n",
      "epis: 1011   score: 5.0   mem len: 191022   epsilon: 0.8198    steps: 326    lr: 0.0001     reward: 2.31\n",
      "epis: 1012   score: 3.0   mem len: 191269   epsilon: 0.8193    steps: 247    lr: 0.0001     reward: 2.33\n",
      "epis: 1013   score: 2.0   mem len: 191487   epsilon: 0.8189    steps: 218    lr: 0.0001     reward: 2.34\n",
      "epis: 1014   score: 4.0   mem len: 191783   epsilon: 0.8183    steps: 296    lr: 0.0001     reward: 2.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1015   score: 2.0   mem len: 191983   epsilon: 0.8179    steps: 200    lr: 0.0001     reward: 2.35\n",
      "epis: 1016   score: 1.0   mem len: 192153   epsilon: 0.8175    steps: 170    lr: 0.0001     reward: 2.35\n",
      "epis: 1017   score: 1.0   mem len: 192321   epsilon: 0.8172    steps: 168    lr: 0.0001     reward: 2.35\n",
      "epis: 1018   score: 4.0   mem len: 192631   epsilon: 0.8166    steps: 310    lr: 0.0001     reward: 2.35\n",
      "epis: 1019   score: 4.0   mem len: 192947   epsilon: 0.816    steps: 316    lr: 0.0001     reward: 2.35\n",
      "epis: 1020   score: 1.0   mem len: 193118   epsilon: 0.8156    steps: 171    lr: 0.0001     reward: 2.36\n",
      "epis: 1021   score: 3.0   mem len: 193345   epsilon: 0.8152    steps: 227    lr: 0.0001     reward: 2.36\n",
      "epis: 1022   score: 1.0   mem len: 193496   epsilon: 0.8149    steps: 151    lr: 0.0001     reward: 2.35\n",
      "epis: 1023   score: 2.0   mem len: 193694   epsilon: 0.8145    steps: 198    lr: 0.0001     reward: 2.34\n",
      "epis: 1024   score: 2.0   mem len: 193892   epsilon: 0.8141    steps: 198    lr: 0.0001     reward: 2.32\n",
      "epis: 1025   score: 1.0   mem len: 194043   epsilon: 0.8138    steps: 151    lr: 0.0001     reward: 2.33\n",
      "epis: 1026   score: 3.0   mem len: 194289   epsilon: 0.8133    steps: 246    lr: 0.0001     reward: 2.32\n",
      "epis: 1027   score: 2.0   mem len: 194488   epsilon: 0.8129    steps: 199    lr: 0.0001     reward: 2.33\n",
      "epis: 1028   score: 3.0   mem len: 194736   epsilon: 0.8124    steps: 248    lr: 0.0001     reward: 2.35\n",
      "epis: 1029   score: 2.0   mem len: 194934   epsilon: 0.812    steps: 198    lr: 0.0001     reward: 2.36\n",
      "epis: 1030   score: 2.0   mem len: 195131   epsilon: 0.8116    steps: 197    lr: 0.0001     reward: 2.36\n",
      "epis: 1031   score: 5.0   mem len: 195455   epsilon: 0.811    steps: 324    lr: 0.0001     reward: 2.39\n",
      "epis: 1032   score: 6.0   mem len: 195839   epsilon: 0.8102    steps: 384    lr: 0.0001     reward: 2.44\n",
      "epis: 1033   score: 1.0   mem len: 196008   epsilon: 0.8099    steps: 169    lr: 0.0001     reward: 2.43\n",
      "epis: 1034   score: 0.0   mem len: 196131   epsilon: 0.8097    steps: 123    lr: 0.0001     reward: 2.4\n",
      "epis: 1035   score: 2.0   mem len: 196331   epsilon: 0.8093    steps: 200    lr: 0.0001     reward: 2.39\n",
      "epis: 1036   score: 2.0   mem len: 196529   epsilon: 0.8089    steps: 198    lr: 0.0001     reward: 2.39\n",
      "epis: 1037   score: 1.0   mem len: 196698   epsilon: 0.8085    steps: 169    lr: 0.0001     reward: 2.37\n",
      "epis: 1038   score: 1.0   mem len: 196867   epsilon: 0.8082    steps: 169    lr: 0.0001     reward: 2.36\n",
      "epis: 1039   score: 4.0   mem len: 197162   epsilon: 0.8076    steps: 295    lr: 0.0001     reward: 2.37\n",
      "epis: 1040   score: 0.0   mem len: 197285   epsilon: 0.8074    steps: 123    lr: 0.0001     reward: 2.3\n",
      "epis: 1041   score: 1.0   mem len: 197456   epsilon: 0.807    steps: 171    lr: 0.0001     reward: 2.26\n",
      "epis: 1042   score: 1.0   mem len: 197606   epsilon: 0.8067    steps: 150    lr: 0.0001     reward: 2.25\n",
      "epis: 1043   score: 0.0   mem len: 197729   epsilon: 0.8065    steps: 123    lr: 0.0001     reward: 2.25\n",
      "epis: 1044   score: 4.0   mem len: 198024   epsilon: 0.8059    steps: 295    lr: 0.0001     reward: 2.27\n",
      "epis: 1045   score: 2.0   mem len: 198206   epsilon: 0.8056    steps: 182    lr: 0.0001     reward: 2.26\n",
      "epis: 1046   score: 2.0   mem len: 198423   epsilon: 0.8051    steps: 217    lr: 0.0001     reward: 2.25\n",
      "epis: 1047   score: 4.0   mem len: 198699   epsilon: 0.8046    steps: 276    lr: 0.0001     reward: 2.27\n",
      "epis: 1048   score: 2.0   mem len: 198900   epsilon: 0.8042    steps: 201    lr: 0.0001     reward: 2.29\n",
      "epis: 1049   score: 2.0   mem len: 199119   epsilon: 0.8037    steps: 219    lr: 0.0001     reward: 2.29\n",
      "epis: 1050   score: 4.0   mem len: 199436   epsilon: 0.8031    steps: 317    lr: 0.0001     reward: 2.33\n",
      "epis: 1051   score: 1.0   mem len: 199607   epsilon: 0.8028    steps: 171    lr: 0.0001     reward: 2.34\n",
      "epis: 1052   score: 2.0   mem len: 199822   epsilon: 0.8024    steps: 215    lr: 0.0001     reward: 2.36\n",
      "epis: 1053   score: 2.0   mem len: 200004   epsilon: 0.802    steps: 182    lr: 4e-05     reward: 2.36\n",
      "epis: 1054   score: 1.0   mem len: 200174   epsilon: 0.8017    steps: 170    lr: 4e-05     reward: 2.33\n",
      "epis: 1055   score: 4.0   mem len: 200414   epsilon: 0.8012    steps: 240    lr: 4e-05     reward: 2.34\n",
      "epis: 1056   score: 4.0   mem len: 200713   epsilon: 0.8006    steps: 299    lr: 4e-05     reward: 2.35\n",
      "epis: 1057   score: 2.0   mem len: 200932   epsilon: 0.8002    steps: 219    lr: 4e-05     reward: 2.36\n",
      "epis: 1058   score: 4.0   mem len: 201230   epsilon: 0.7996    steps: 298    lr: 4e-05     reward: 2.4\n",
      "epis: 1059   score: 1.0   mem len: 201380   epsilon: 0.7993    steps: 150    lr: 4e-05     reward: 2.39\n",
      "epis: 1060   score: 7.0   mem len: 201767   epsilon: 0.7985    steps: 387    lr: 4e-05     reward: 2.42\n",
      "epis: 1061   score: 4.0   mem len: 202060   epsilon: 0.7979    steps: 293    lr: 4e-05     reward: 2.44\n",
      "epis: 1062   score: 1.0   mem len: 202228   epsilon: 0.7976    steps: 168    lr: 4e-05     reward: 2.43\n",
      "epis: 1063   score: 2.0   mem len: 202426   epsilon: 0.7972    steps: 198    lr: 4e-05     reward: 2.42\n",
      "epis: 1064   score: 3.0   mem len: 202656   epsilon: 0.7967    steps: 230    lr: 4e-05     reward: 2.41\n",
      "epis: 1065   score: 1.0   mem len: 202825   epsilon: 0.7964    steps: 169    lr: 4e-05     reward: 2.38\n",
      "epis: 1066   score: 1.0   mem len: 202996   epsilon: 0.7961    steps: 171    lr: 4e-05     reward: 2.38\n",
      "epis: 1067   score: 9.0   mem len: 203324   epsilon: 0.7954    steps: 328    lr: 4e-05     reward: 2.45\n",
      "epis: 1068   score: 2.0   mem len: 203521   epsilon: 0.795    steps: 197    lr: 4e-05     reward: 2.44\n",
      "epis: 1069   score: 6.0   mem len: 203880   epsilon: 0.7943    steps: 359    lr: 4e-05     reward: 2.49\n",
      "epis: 1070   score: 3.0   mem len: 204147   epsilon: 0.7938    steps: 267    lr: 4e-05     reward: 2.49\n",
      "epis: 1071   score: 2.0   mem len: 204345   epsilon: 0.7934    steps: 198    lr: 4e-05     reward: 2.47\n",
      "epis: 1072   score: 2.0   mem len: 204561   epsilon: 0.793    steps: 216    lr: 4e-05     reward: 2.47\n",
      "epis: 1073   score: 1.0   mem len: 204712   epsilon: 0.7927    steps: 151    lr: 4e-05     reward: 2.47\n",
      "epis: 1074   score: 2.0   mem len: 204909   epsilon: 0.7923    steps: 197    lr: 4e-05     reward: 2.47\n",
      "epis: 1075   score: 3.0   mem len: 205156   epsilon: 0.7918    steps: 247    lr: 4e-05     reward: 2.49\n",
      "epis: 1076   score: 4.0   mem len: 205431   epsilon: 0.7912    steps: 275    lr: 4e-05     reward: 2.51\n",
      "epis: 1077   score: 3.0   mem len: 205660   epsilon: 0.7908    steps: 229    lr: 4e-05     reward: 2.54\n",
      "epis: 1078   score: 4.0   mem len: 205959   epsilon: 0.7902    steps: 299    lr: 4e-05     reward: 2.56\n",
      "epis: 1079   score: 1.0   mem len: 206128   epsilon: 0.7899    steps: 169    lr: 4e-05     reward: 2.5\n",
      "epis: 1080   score: 1.0   mem len: 206297   epsilon: 0.7895    steps: 169    lr: 4e-05     reward: 2.47\n",
      "epis: 1081   score: 3.0   mem len: 206522   epsilon: 0.7891    steps: 225    lr: 4e-05     reward: 2.48\n",
      "epis: 1082   score: 2.0   mem len: 206720   epsilon: 0.7887    steps: 198    lr: 4e-05     reward: 2.49\n",
      "epis: 1083   score: 2.0   mem len: 206918   epsilon: 0.7883    steps: 198    lr: 4e-05     reward: 2.44\n",
      "epis: 1084   score: 2.0   mem len: 207100   epsilon: 0.7879    steps: 182    lr: 4e-05     reward: 2.41\n",
      "epis: 1085   score: 4.0   mem len: 207379   epsilon: 0.7874    steps: 279    lr: 4e-05     reward: 2.41\n",
      "epis: 1086   score: 3.0   mem len: 207590   epsilon: 0.787    steps: 211    lr: 4e-05     reward: 2.44\n",
      "epis: 1087   score: 2.0   mem len: 207787   epsilon: 0.7866    steps: 197    lr: 4e-05     reward: 2.42\n",
      "epis: 1088   score: 3.0   mem len: 207997   epsilon: 0.7862    steps: 210    lr: 4e-05     reward: 2.41\n",
      "epis: 1089   score: 1.0   mem len: 208165   epsilon: 0.7858    steps: 168    lr: 4e-05     reward: 2.42\n",
      "epis: 1090   score: 7.0   mem len: 208559   epsilon: 0.7851    steps: 394    lr: 4e-05     reward: 2.45\n",
      "epis: 1091   score: 1.0   mem len: 208709   epsilon: 0.7848    steps: 150    lr: 4e-05     reward: 2.45\n",
      "epis: 1092   score: 2.0   mem len: 208927   epsilon: 0.7843    steps: 218    lr: 4e-05     reward: 2.47\n",
      "epis: 1093   score: 2.0   mem len: 209108   epsilon: 0.784    steps: 181    lr: 4e-05     reward: 2.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1094   score: 2.0   mem len: 209306   epsilon: 0.7836    steps: 198    lr: 4e-05     reward: 2.47\n",
      "epis: 1095   score: 1.0   mem len: 209457   epsilon: 0.7833    steps: 151    lr: 4e-05     reward: 2.44\n",
      "epis: 1096   score: 1.0   mem len: 209626   epsilon: 0.7829    steps: 169    lr: 4e-05     reward: 2.45\n",
      "epis: 1097   score: 7.0   mem len: 210039   epsilon: 0.7821    steps: 413    lr: 4e-05     reward: 2.52\n",
      "epis: 1098   score: 1.0   mem len: 210209   epsilon: 0.7818    steps: 170    lr: 4e-05     reward: 2.53\n",
      "epis: 1099   score: 3.0   mem len: 210439   epsilon: 0.7813    steps: 230    lr: 4e-05     reward: 2.49\n",
      "epis: 1100   score: 3.0   mem len: 210684   epsilon: 0.7808    steps: 245    lr: 4e-05     reward: 2.5\n",
      "epis: 1101   score: 1.0   mem len: 210852   epsilon: 0.7805    steps: 168    lr: 4e-05     reward: 2.49\n",
      "epis: 1102   score: 3.0   mem len: 211100   epsilon: 0.78    steps: 248    lr: 4e-05     reward: 2.5\n",
      "epis: 1103   score: 2.0   mem len: 211300   epsilon: 0.7796    steps: 200    lr: 4e-05     reward: 2.51\n",
      "epis: 1104   score: 3.0   mem len: 211531   epsilon: 0.7792    steps: 231    lr: 4e-05     reward: 2.53\n",
      "epis: 1105   score: 2.0   mem len: 211715   epsilon: 0.7788    steps: 184    lr: 4e-05     reward: 2.52\n",
      "epis: 1106   score: 1.0   mem len: 211866   epsilon: 0.7785    steps: 151    lr: 4e-05     reward: 2.52\n",
      "epis: 1107   score: 0.0   mem len: 211989   epsilon: 0.7783    steps: 123    lr: 4e-05     reward: 2.5\n",
      "epis: 1108   score: 2.0   mem len: 212169   epsilon: 0.7779    steps: 180    lr: 4e-05     reward: 2.49\n",
      "epis: 1109   score: 1.0   mem len: 212319   epsilon: 0.7776    steps: 150    lr: 4e-05     reward: 2.45\n",
      "epis: 1110   score: 3.0   mem len: 212544   epsilon: 0.7772    steps: 225    lr: 4e-05     reward: 2.45\n",
      "epis: 1111   score: 7.0   mem len: 212970   epsilon: 0.7763    steps: 426    lr: 4e-05     reward: 2.47\n",
      "epis: 1112   score: 4.0   mem len: 213266   epsilon: 0.7757    steps: 296    lr: 4e-05     reward: 2.48\n",
      "epis: 1113   score: 1.0   mem len: 213416   epsilon: 0.7754    steps: 150    lr: 4e-05     reward: 2.47\n",
      "epis: 1114   score: 2.0   mem len: 213613   epsilon: 0.775    steps: 197    lr: 4e-05     reward: 2.45\n",
      "epis: 1115   score: 9.0   mem len: 214058   epsilon: 0.7742    steps: 445    lr: 4e-05     reward: 2.52\n",
      "epis: 1116   score: 2.0   mem len: 214239   epsilon: 0.7738    steps: 181    lr: 4e-05     reward: 2.53\n",
      "epis: 1117   score: 2.0   mem len: 214437   epsilon: 0.7734    steps: 198    lr: 4e-05     reward: 2.54\n",
      "epis: 1118   score: 2.0   mem len: 214654   epsilon: 0.773    steps: 217    lr: 4e-05     reward: 2.52\n",
      "epis: 1119   score: 1.0   mem len: 214824   epsilon: 0.7726    steps: 170    lr: 4e-05     reward: 2.49\n",
      "epis: 1120   score: 4.0   mem len: 215098   epsilon: 0.7721    steps: 274    lr: 4e-05     reward: 2.52\n",
      "epis: 1121   score: 5.0   mem len: 215423   epsilon: 0.7715    steps: 325    lr: 4e-05     reward: 2.54\n",
      "epis: 1122   score: 5.0   mem len: 215771   epsilon: 0.7708    steps: 348    lr: 4e-05     reward: 2.58\n",
      "epis: 1123   score: 3.0   mem len: 216036   epsilon: 0.7702    steps: 265    lr: 4e-05     reward: 2.59\n",
      "epis: 1124   score: 0.0   mem len: 216159   epsilon: 0.77    steps: 123    lr: 4e-05     reward: 2.57\n",
      "epis: 1125   score: 1.0   mem len: 216309   epsilon: 0.7697    steps: 150    lr: 4e-05     reward: 2.57\n",
      "epis: 1126   score: 3.0   mem len: 216553   epsilon: 0.7692    steps: 244    lr: 4e-05     reward: 2.57\n",
      "epis: 1127   score: 3.0   mem len: 216799   epsilon: 0.7687    steps: 246    lr: 4e-05     reward: 2.58\n",
      "epis: 1128   score: 3.0   mem len: 217026   epsilon: 0.7683    steps: 227    lr: 4e-05     reward: 2.58\n",
      "epis: 1129   score: 5.0   mem len: 217373   epsilon: 0.7676    steps: 347    lr: 4e-05     reward: 2.61\n",
      "epis: 1130   score: 4.0   mem len: 217647   epsilon: 0.7671    steps: 274    lr: 4e-05     reward: 2.63\n",
      "epis: 1131   score: 1.0   mem len: 217818   epsilon: 0.7667    steps: 171    lr: 4e-05     reward: 2.59\n",
      "epis: 1132   score: 1.0   mem len: 217968   epsilon: 0.7664    steps: 150    lr: 4e-05     reward: 2.54\n",
      "epis: 1133   score: 3.0   mem len: 218194   epsilon: 0.766    steps: 226    lr: 4e-05     reward: 2.56\n",
      "epis: 1134   score: 5.0   mem len: 218499   epsilon: 0.7654    steps: 305    lr: 4e-05     reward: 2.61\n",
      "epis: 1135   score: 3.0   mem len: 218724   epsilon: 0.7649    steps: 225    lr: 4e-05     reward: 2.62\n",
      "epis: 1136   score: 2.0   mem len: 218922   epsilon: 0.7645    steps: 198    lr: 4e-05     reward: 2.62\n",
      "epis: 1137   score: 2.0   mem len: 219119   epsilon: 0.7641    steps: 197    lr: 4e-05     reward: 2.63\n",
      "epis: 1138   score: 1.0   mem len: 219270   epsilon: 0.7638    steps: 151    lr: 4e-05     reward: 2.63\n",
      "epis: 1139   score: 1.0   mem len: 219440   epsilon: 0.7635    steps: 170    lr: 4e-05     reward: 2.6\n",
      "epis: 1140   score: 3.0   mem len: 219666   epsilon: 0.7631    steps: 226    lr: 4e-05     reward: 2.63\n",
      "epis: 1141   score: 1.0   mem len: 219816   epsilon: 0.7628    steps: 150    lr: 4e-05     reward: 2.63\n",
      "epis: 1142   score: 2.0   mem len: 220016   epsilon: 0.7624    steps: 200    lr: 4e-05     reward: 2.64\n",
      "epis: 1143   score: 3.0   mem len: 220262   epsilon: 0.7619    steps: 246    lr: 4e-05     reward: 2.67\n",
      "epis: 1144   score: 1.0   mem len: 220413   epsilon: 0.7616    steps: 151    lr: 4e-05     reward: 2.64\n",
      "epis: 1145   score: 3.0   mem len: 220624   epsilon: 0.7612    steps: 211    lr: 4e-05     reward: 2.65\n",
      "epis: 1146   score: 1.0   mem len: 220795   epsilon: 0.7608    steps: 171    lr: 4e-05     reward: 2.64\n",
      "epis: 1147   score: 5.0   mem len: 221144   epsilon: 0.7601    steps: 349    lr: 4e-05     reward: 2.65\n",
      "epis: 1148   score: 4.0   mem len: 221418   epsilon: 0.7596    steps: 274    lr: 4e-05     reward: 2.67\n",
      "epis: 1149   score: 5.0   mem len: 221729   epsilon: 0.759    steps: 311    lr: 4e-05     reward: 2.7\n",
      "epis: 1150   score: 5.0   mem len: 222055   epsilon: 0.7583    steps: 326    lr: 4e-05     reward: 2.71\n",
      "epis: 1151   score: 1.0   mem len: 222227   epsilon: 0.758    steps: 172    lr: 4e-05     reward: 2.71\n",
      "epis: 1152   score: 2.0   mem len: 222446   epsilon: 0.7576    steps: 219    lr: 4e-05     reward: 2.71\n",
      "epis: 1153   score: 3.0   mem len: 222691   epsilon: 0.7571    steps: 245    lr: 4e-05     reward: 2.72\n",
      "epis: 1154   score: 0.0   mem len: 222814   epsilon: 0.7568    steps: 123    lr: 4e-05     reward: 2.71\n",
      "epis: 1155   score: 1.0   mem len: 222965   epsilon: 0.7565    steps: 151    lr: 4e-05     reward: 2.68\n",
      "epis: 1156   score: 3.0   mem len: 223211   epsilon: 0.756    steps: 246    lr: 4e-05     reward: 2.67\n",
      "epis: 1157   score: 2.0   mem len: 223426   epsilon: 0.7556    steps: 215    lr: 4e-05     reward: 2.67\n",
      "epis: 1158   score: 5.0   mem len: 223749   epsilon: 0.755    steps: 323    lr: 4e-05     reward: 2.68\n",
      "epis: 1159   score: 2.0   mem len: 223946   epsilon: 0.7546    steps: 197    lr: 4e-05     reward: 2.69\n",
      "epis: 1160   score: 4.0   mem len: 224220   epsilon: 0.754    steps: 274    lr: 4e-05     reward: 2.66\n",
      "epis: 1161   score: 5.0   mem len: 224545   epsilon: 0.7534    steps: 325    lr: 4e-05     reward: 2.67\n",
      "epis: 1162   score: 7.0   mem len: 224926   epsilon: 0.7526    steps: 381    lr: 4e-05     reward: 2.73\n",
      "epis: 1163   score: 3.0   mem len: 225192   epsilon: 0.7521    steps: 266    lr: 4e-05     reward: 2.74\n",
      "epis: 1164   score: 7.0   mem len: 225600   epsilon: 0.7513    steps: 408    lr: 4e-05     reward: 2.78\n",
      "epis: 1165   score: 4.0   mem len: 225877   epsilon: 0.7508    steps: 277    lr: 4e-05     reward: 2.81\n",
      "epis: 1166   score: 5.0   mem len: 226240   epsilon: 0.75    steps: 363    lr: 4e-05     reward: 2.85\n",
      "epis: 1167   score: 5.0   mem len: 226565   epsilon: 0.7494    steps: 325    lr: 4e-05     reward: 2.81\n",
      "epis: 1168   score: 2.0   mem len: 226765   epsilon: 0.749    steps: 200    lr: 4e-05     reward: 2.81\n",
      "epis: 1169   score: 2.0   mem len: 226982   epsilon: 0.7486    steps: 217    lr: 4e-05     reward: 2.77\n",
      "epis: 1170   score: 2.0   mem len: 227180   epsilon: 0.7482    steps: 198    lr: 4e-05     reward: 2.76\n",
      "epis: 1171   score: 3.0   mem len: 227410   epsilon: 0.7477    steps: 230    lr: 4e-05     reward: 2.77\n",
      "epis: 1172   score: 4.0   mem len: 227653   epsilon: 0.7472    steps: 243    lr: 4e-05     reward: 2.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1173   score: 4.0   mem len: 227927   epsilon: 0.7467    steps: 274    lr: 4e-05     reward: 2.82\n",
      "epis: 1174   score: 2.0   mem len: 228126   epsilon: 0.7463    steps: 199    lr: 4e-05     reward: 2.82\n",
      "epis: 1175   score: 5.0   mem len: 228472   epsilon: 0.7456    steps: 346    lr: 4e-05     reward: 2.84\n",
      "epis: 1176   score: 3.0   mem len: 228716   epsilon: 0.7451    steps: 244    lr: 4e-05     reward: 2.83\n",
      "epis: 1177   score: 4.0   mem len: 229012   epsilon: 0.7446    steps: 296    lr: 4e-05     reward: 2.84\n",
      "epis: 1178   score: 5.0   mem len: 229354   epsilon: 0.7439    steps: 342    lr: 4e-05     reward: 2.85\n",
      "epis: 1179   score: 4.0   mem len: 229630   epsilon: 0.7433    steps: 276    lr: 4e-05     reward: 2.88\n",
      "epis: 1180   score: 3.0   mem len: 229856   epsilon: 0.7429    steps: 226    lr: 4e-05     reward: 2.9\n",
      "epis: 1181   score: 4.0   mem len: 230154   epsilon: 0.7423    steps: 298    lr: 4e-05     reward: 2.91\n",
      "epis: 1182   score: 3.0   mem len: 230400   epsilon: 0.7418    steps: 246    lr: 4e-05     reward: 2.92\n",
      "epis: 1183   score: 6.0   mem len: 230793   epsilon: 0.741    steps: 393    lr: 4e-05     reward: 2.96\n",
      "epis: 1184   score: 3.0   mem len: 231018   epsilon: 0.7406    steps: 225    lr: 4e-05     reward: 2.97\n"
     ]
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_double_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"epis:\", e, \"  score:\", score, \"  mem len:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 4), \"   steps:\", step,\n",
    "                  \"   lr:\", round(agent.optimizer.param_groups[0]['lr'], 7), \"    reward:\", round(np.mean(evaluation_reward), 2))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_double_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_double_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
