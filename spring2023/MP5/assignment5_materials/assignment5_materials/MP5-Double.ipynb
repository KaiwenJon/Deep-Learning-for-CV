{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84156/1886764261.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "/tmp/ipykernel_84156/1886764261.py:20: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 0   score: 0.0   mem len: 122   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 0.0\n",
      "epis: 1   score: 2.0   mem len: 342   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.0\n",
      "epis: 2   score: 1.0   mem len: 493   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.0\n",
      "epis: 3   score: 4.0   mem len: 768   epsilon: 1.0    steps: 275    lr: 0.0001     reward: 1.75\n",
      "epis: 4   score: 3.0   mem len: 1017   epsilon: 1.0    steps: 249    lr: 0.0001     reward: 2.0\n",
      "epis: 5   score: 2.0   mem len: 1215   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 2.0\n",
      "epis: 6   score: 4.0   mem len: 1474   epsilon: 1.0    steps: 259    lr: 0.0001     reward: 2.29\n",
      "epis: 7   score: 0.0   mem len: 1597   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 2.0\n",
      "epis: 8   score: 0.0   mem len: 1720   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.78\n",
      "epis: 9   score: 5.0   mem len: 2046   epsilon: 1.0    steps: 326    lr: 0.0001     reward: 2.1\n",
      "epis: 10   score: 3.0   mem len: 2293   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 2.18\n",
      "epis: 11   score: 0.0   mem len: 2416   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 2.0\n",
      "epis: 12   score: 6.0   mem len: 2751   epsilon: 1.0    steps: 335    lr: 0.0001     reward: 2.31\n",
      "epis: 13   score: 2.0   mem len: 2970   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 2.29\n",
      "epis: 14   score: 2.0   mem len: 3168   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 2.27\n",
      "epis: 15   score: 1.0   mem len: 3339   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 2.19\n",
      "epis: 16   score: 0.0   mem len: 3461   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 2.06\n",
      "epis: 17   score: 2.0   mem len: 3658   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 2.06\n",
      "epis: 18   score: 1.0   mem len: 3829   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 2.0\n",
      "epis: 19   score: 2.0   mem len: 4046   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 2.0\n",
      "epis: 20   score: 0.0   mem len: 4168   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.9\n",
      "epis: 21   score: 1.0   mem len: 4320   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.86\n",
      "epis: 22   score: 2.0   mem len: 4518   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.87\n",
      "epis: 23   score: 2.0   mem len: 4716   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.88\n",
      "epis: 24   score: 0.0   mem len: 4839   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.8\n",
      "epis: 25   score: 0.0   mem len: 4961   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.73\n",
      "epis: 26   score: 0.0   mem len: 5084   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 27   score: 1.0   mem len: 5235   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.64\n",
      "epis: 28   score: 4.0   mem len: 5509   epsilon: 1.0    steps: 274    lr: 0.0001     reward: 1.72\n",
      "epis: 29   score: 0.0   mem len: 5632   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 30   score: 7.0   mem len: 5952   epsilon: 1.0    steps: 320    lr: 0.0001     reward: 1.84\n",
      "epis: 31   score: 3.0   mem len: 6198   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.88\n",
      "epis: 32   score: 0.0   mem len: 6321   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.82\n",
      "epis: 33   score: 4.0   mem len: 6613   epsilon: 1.0    steps: 292    lr: 0.0001     reward: 1.88\n",
      "epis: 34   score: 0.0   mem len: 6736   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.83\n",
      "epis: 35   score: 3.0   mem len: 7003   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.86\n",
      "epis: 36   score: 2.0   mem len: 7201   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.86\n",
      "epis: 37   score: 2.0   mem len: 7399   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.87\n",
      "epis: 38   score: 2.0   mem len: 7597   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.87\n",
      "epis: 39   score: 2.0   mem len: 7795   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.88\n",
      "epis: 40   score: 0.0   mem len: 7918   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.83\n",
      "epis: 41   score: 1.0   mem len: 8089   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.81\n",
      "epis: 42   score: 0.0   mem len: 8211   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.77\n",
      "epis: 43   score: 2.0   mem len: 8409   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.77\n",
      "epis: 44   score: 2.0   mem len: 8589   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.78\n",
      "epis: 45   score: 1.0   mem len: 8760   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.76\n",
      "epis: 46   score: 0.0   mem len: 8883   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.72\n",
      "epis: 47   score: 0.0   mem len: 9006   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 48   score: 1.0   mem len: 9157   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.67\n",
      "epis: 49   score: 0.0   mem len: 9279   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 50   score: 1.0   mem len: 9447   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.63\n",
      "epis: 51   score: 1.0   mem len: 9617   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.62\n",
      "epis: 52   score: 1.0   mem len: 9786   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.6\n",
      "epis: 53   score: 2.0   mem len: 10004   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.61\n",
      "epis: 54   score: 1.0   mem len: 10173   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.6\n",
      "epis: 55   score: 2.0   mem len: 10371   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.61\n",
      "epis: 56   score: 2.0   mem len: 10552   epsilon: 1.0    steps: 181    lr: 0.0001     reward: 1.61\n",
      "epis: 57   score: 0.0   mem len: 10675   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 58   score: 2.0   mem len: 10892   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.59\n",
      "epis: 59   score: 4.0   mem len: 11178   epsilon: 1.0    steps: 286    lr: 0.0001     reward: 1.63\n",
      "epis: 60   score: 1.0   mem len: 11349   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.62\n",
      "epis: 61   score: 4.0   mem len: 11609   epsilon: 1.0    steps: 260    lr: 0.0001     reward: 1.66\n",
      "epis: 62   score: 2.0   mem len: 11791   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.67\n",
      "epis: 63   score: 0.0   mem len: 11914   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 64   score: 0.0   mem len: 12036   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 65   score: 0.0   mem len: 12159   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 66   score: 2.0   mem len: 12378   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.6\n",
      "epis: 67   score: 0.0   mem len: 12501   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 68   score: 1.0   mem len: 12673   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 69   score: 0.0   mem len: 12796   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 70   score: 3.0   mem len: 13046   epsilon: 1.0    steps: 250    lr: 0.0001     reward: 1.56\n",
      "epis: 71   score: 2.0   mem len: 13244   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.57\n",
      "epis: 72   score: 1.0   mem len: 13415   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.56\n",
      "epis: 73   score: 0.0   mem len: 13538   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 74   score: 1.0   mem len: 13710   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.53\n",
      "epis: 75   score: 0.0   mem len: 13833   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 76   score: 2.0   mem len: 14049   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.52\n",
      "epis: 77   score: 2.0   mem len: 14248   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.53\n",
      "epis: 78   score: 0.0   mem len: 14370   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 79   score: 0.0   mem len: 14493   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 80   score: 1.0   mem len: 14644   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.48\n",
      "epis: 81   score: 0.0   mem len: 14767   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 82   score: 4.0   mem len: 15084   epsilon: 1.0    steps: 317    lr: 0.0001     reward: 1.49\n",
      "epis: 83   score: 0.0   mem len: 15207   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 84   score: 4.0   mem len: 15504   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.51\n",
      "epis: 85   score: 7.0   mem len: 15964   epsilon: 1.0    steps: 460    lr: 0.0001     reward: 1.57\n",
      "epis: 86   score: 4.0   mem len: 16226   epsilon: 1.0    steps: 262    lr: 0.0001     reward: 1.6\n",
      "epis: 87   score: 1.0   mem len: 16378   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.59\n",
      "epis: 88   score: 2.0   mem len: 16594   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.6\n",
      "epis: 89   score: 1.0   mem len: 16766   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.59\n",
      "epis: 90   score: 1.0   mem len: 16916   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.58\n",
      "epis: 91   score: 2.0   mem len: 17113   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 92   score: 1.0   mem len: 17282   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 93   score: 2.0   mem len: 17480   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 94   score: 3.0   mem len: 17747   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.6\n",
      "epis: 95   score: 0.0   mem len: 17870   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 96   score: 1.0   mem len: 18039   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 97   score: 0.0   mem len: 18161   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 98   score: 1.0   mem len: 18312   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 99   score: 2.0   mem len: 18509   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.56\n",
      "epis: 100   score: 0.0   mem len: 18631   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 101   score: 3.0   mem len: 18858   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.57\n",
      "epis: 102   score: 3.0   mem len: 19086   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.59\n",
      "epis: 103   score: 1.0   mem len: 19237   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 104   score: 2.0   mem len: 19438   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.55\n",
      "epis: 105   score: 3.0   mem len: 19705   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.56\n",
      "epis: 106   score: 3.0   mem len: 19951   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.55\n",
      "epis: 107   score: 3.0   mem len: 20197   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.58\n",
      "epis: 108   score: 5.0   mem len: 20500   epsilon: 1.0    steps: 303    lr: 0.0001     reward: 1.63\n",
      "epis: 109   score: 0.0   mem len: 20623   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 110   score: 0.0   mem len: 20746   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 111   score: 0.0   mem len: 20868   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 112   score: 3.0   mem len: 21114   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.52\n",
      "epis: 113   score: 0.0   mem len: 21237   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 114   score: 2.0   mem len: 21454   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.5\n",
      "epis: 115   score: 1.0   mem len: 21604   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 116   score: 0.0   mem len: 21726   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 117   score: 1.0   mem len: 21895   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 118   score: 5.0   mem len: 22221   epsilon: 1.0    steps: 326    lr: 0.0001     reward: 1.53\n",
      "epis: 119   score: 2.0   mem len: 22419   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.53\n",
      "epis: 120   score: 3.0   mem len: 22684   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.56\n",
      "epis: 121   score: 1.0   mem len: 22852   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.56\n",
      "epis: 122   score: 3.0   mem len: 23099   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.57\n",
      "epis: 123   score: 7.0   mem len: 23362   epsilon: 1.0    steps: 263    lr: 0.0001     reward: 1.62\n",
      "epis: 124   score: 1.0   mem len: 23513   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.63\n",
      "epis: 125   score: 2.0   mem len: 23711   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 126   score: 1.0   mem len: 23879   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.66\n",
      "epis: 127   score: 0.0   mem len: 24002   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 128   score: 0.0   mem len: 24125   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 129   score: 2.0   mem len: 24323   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.63\n",
      "epis: 130   score: 4.0   mem len: 24598   epsilon: 1.0    steps: 275    lr: 0.0001     reward: 1.6\n",
      "epis: 131   score: 2.0   mem len: 24795   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 132   score: 0.0   mem len: 24918   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 133   score: 1.0   mem len: 25069   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 134   score: 1.0   mem len: 25241   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.57\n",
      "epis: 135   score: 0.0   mem len: 25364   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 136   score: 0.0   mem len: 25487   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 137   score: 0.0   mem len: 25610   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 138   score: 1.0   mem len: 25761   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.49\n",
      "epis: 139   score: 1.0   mem len: 25930   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 140   score: 2.0   mem len: 26151   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.5\n",
      "epis: 141   score: 0.0   mem len: 26273   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 142   score: 1.0   mem len: 26443   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.5\n",
      "epis: 143   score: 3.0   mem len: 26690   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.51\n",
      "epis: 144   score: 2.0   mem len: 26888   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 145   score: 3.0   mem len: 27136   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.53\n",
      "epis: 146   score: 0.0   mem len: 27259   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 147   score: 2.0   mem len: 27478   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.55\n",
      "epis: 148   score: 0.0   mem len: 27601   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 149   score: 2.0   mem len: 27799   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 150   score: 4.0   mem len: 28095   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.59\n",
      "epis: 151   score: 0.0   mem len: 28218   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 152   score: 3.0   mem len: 28485   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.6\n",
      "epis: 153   score: 2.0   mem len: 28682   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.6\n",
      "epis: 154   score: 1.0   mem len: 28833   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 155   score: 0.0   mem len: 28956   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 156   score: 2.0   mem len: 29154   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 157   score: 1.0   mem len: 29323   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.59\n",
      "epis: 158   score: 1.0   mem len: 29494   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.58\n",
      "epis: 159   score: 3.0   mem len: 29740   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.57\n",
      "epis: 160   score: 2.0   mem len: 29938   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 161   score: 1.0   mem len: 30089   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.55\n",
      "epis: 162   score: 4.0   mem len: 30363   epsilon: 1.0    steps: 274    lr: 0.0001     reward: 1.57\n",
      "epis: 163   score: 2.0   mem len: 30560   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 164   score: 1.0   mem len: 30713   epsilon: 1.0    steps: 153    lr: 0.0001     reward: 1.6\n",
      "epis: 165   score: 7.0   mem len: 31084   epsilon: 1.0    steps: 371    lr: 0.0001     reward: 1.67\n",
      "epis: 166   score: 3.0   mem len: 31350   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 167   score: 0.0   mem len: 31472   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 168   score: 0.0   mem len: 31594   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.67\n",
      "epis: 169   score: 4.0   mem len: 31890   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.71\n",
      "epis: 170   score: 0.0   mem len: 32012   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 171   score: 2.0   mem len: 32209   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.68\n",
      "epis: 172   score: 3.0   mem len: 32479   epsilon: 1.0    steps: 270    lr: 0.0001     reward: 1.7\n",
      "epis: 173   score: 0.0   mem len: 32602   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.7\n",
      "epis: 174   score: 1.0   mem len: 32754   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.7\n",
      "epis: 175   score: 2.0   mem len: 32956   epsilon: 1.0    steps: 202    lr: 0.0001     reward: 1.72\n",
      "epis: 176   score: 1.0   mem len: 33124   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.71\n",
      "epis: 177   score: 1.0   mem len: 33296   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.7\n",
      "epis: 178   score: 1.0   mem len: 33447   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.71\n",
      "epis: 179   score: 2.0   mem len: 33646   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.73\n",
      "epis: 180   score: 2.0   mem len: 33843   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.74\n",
      "epis: 181   score: 0.0   mem len: 33965   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.74\n",
      "epis: 182   score: 0.0   mem len: 34087   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.7\n",
      "epis: 183   score: 3.0   mem len: 34353   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.73\n",
      "epis: 184   score: 0.0   mem len: 34476   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 185   score: 0.0   mem len: 34598   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 186   score: 2.0   mem len: 34817   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.6\n",
      "epis: 187   score: 3.0   mem len: 35043   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.62\n",
      "epis: 188   score: 2.0   mem len: 35240   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.62\n",
      "epis: 189   score: 0.0   mem len: 35363   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 190   score: 2.0   mem len: 35563   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.62\n",
      "epis: 191   score: 1.0   mem len: 35713   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.61\n",
      "epis: 192   score: 2.0   mem len: 35895   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.62\n",
      "epis: 193   score: 1.0   mem len: 36067   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.61\n",
      "epis: 194   score: 3.0   mem len: 36296   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.61\n",
      "epis: 195   score: 2.0   mem len: 36513   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.63\n",
      "epis: 196   score: 3.0   mem len: 36745   epsilon: 1.0    steps: 232    lr: 0.0001     reward: 1.65\n",
      "epis: 197   score: 2.0   mem len: 36963   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.67\n",
      "epis: 198   score: 2.0   mem len: 37180   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.68\n",
      "epis: 199   score: 1.0   mem len: 37331   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.67\n",
      "epis: 200   score: 0.0   mem len: 37454   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.67\n",
      "epis: 201   score: 3.0   mem len: 37718   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.67\n",
      "epis: 202   score: 0.0   mem len: 37840   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.64\n",
      "epis: 203   score: 1.0   mem len: 38012   epsilon: 1.0    steps: 172    lr: 0.0001     reward: 1.64\n",
      "epis: 204   score: 1.0   mem len: 38163   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.63\n",
      "epis: 205   score: 2.0   mem len: 38383   epsilon: 1.0    steps: 220    lr: 0.0001     reward: 1.62\n",
      "epis: 206   score: 1.0   mem len: 38551   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.6\n",
      "epis: 207   score: 1.0   mem len: 38703   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.58\n",
      "epis: 208   score: 1.0   mem len: 38874   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.54\n",
      "epis: 209   score: 2.0   mem len: 39072   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 210   score: 1.0   mem len: 39242   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.57\n",
      "epis: 211   score: 5.0   mem len: 39586   epsilon: 1.0    steps: 344    lr: 0.0001     reward: 1.62\n",
      "epis: 212   score: 0.0   mem len: 39709   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 213   score: 3.0   mem len: 39922   epsilon: 1.0    steps: 213    lr: 0.0001     reward: 1.62\n",
      "epis: 214   score: 2.0   mem len: 40141   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.62\n",
      "epis: 215   score: 3.0   mem len: 40405   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.64\n",
      "epis: 216   score: 0.0   mem len: 40528   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.64\n",
      "epis: 217   score: 2.0   mem len: 40726   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 218   score: 2.0   mem len: 40924   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 219   score: 0.0   mem len: 41047   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 220   score: 0.0   mem len: 41170   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 221   score: 3.0   mem len: 41399   epsilon: 1.0    steps: 229    lr: 0.0001     reward: 1.59\n",
      "epis: 222   score: 0.0   mem len: 41521   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 223   score: 2.0   mem len: 41742   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.51\n",
      "epis: 224   score: 4.0   mem len: 42038   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.54\n",
      "epis: 225   score: 1.0   mem len: 42208   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.53\n",
      "epis: 226   score: 0.0   mem len: 42331   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 227   score: 1.0   mem len: 42500   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 228   score: 5.0   mem len: 42822   epsilon: 1.0    steps: 322    lr: 0.0001     reward: 1.58\n",
      "epis: 229   score: 1.0   mem len: 42991   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 230   score: 3.0   mem len: 43257   epsilon: 1.0    steps: 266    lr: 0.0001     reward: 1.56\n",
      "epis: 231   score: 2.0   mem len: 43454   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.56\n",
      "epis: 232   score: 1.0   mem len: 43605   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 233   score: 2.0   mem len: 43805   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.58\n",
      "epis: 234   score: 0.0   mem len: 43928   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 235   score: 2.0   mem len: 44149   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.59\n",
      "epis: 236   score: 3.0   mem len: 44375   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.62\n",
      "epis: 237   score: 1.0   mem len: 44526   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.63\n",
      "epis: 238   score: 2.0   mem len: 44726   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.64\n",
      "epis: 239   score: 1.0   mem len: 44896   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.64\n",
      "epis: 240   score: 0.0   mem len: 45019   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 241   score: 0.0   mem len: 45141   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 242   score: 1.0   mem len: 45310   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 243   score: 5.0   mem len: 45658   epsilon: 1.0    steps: 348    lr: 0.0001     reward: 1.64\n",
      "epis: 244   score: 0.0   mem len: 45781   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 245   score: 0.0   mem len: 45904   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 246   score: 0.0   mem len: 46026   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 247   score: 2.0   mem len: 46243   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.59\n",
      "epis: 248   score: 2.0   mem len: 46425   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.61\n",
      "epis: 249   score: 3.0   mem len: 46651   epsilon: 1.0    steps: 226    lr: 0.0001     reward: 1.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 250   score: 0.0   mem len: 46774   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 251   score: 3.0   mem len: 47021   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.61\n",
      "epis: 252   score: 2.0   mem len: 47240   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.6\n",
      "epis: 253   score: 1.0   mem len: 47391   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 254   score: 3.0   mem len: 47656   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.61\n",
      "epis: 255   score: 0.0   mem len: 47778   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.61\n",
      "epis: 256   score: 2.0   mem len: 47995   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.61\n",
      "epis: 257   score: 1.0   mem len: 48146   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.61\n",
      "epis: 258   score: 0.0   mem len: 48268   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 259   score: 3.0   mem len: 48496   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.6\n",
      "epis: 260   score: 2.0   mem len: 48678   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.6\n",
      "epis: 261   score: 2.0   mem len: 48876   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.61\n",
      "epis: 262   score: 0.0   mem len: 48998   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.57\n",
      "epis: 263   score: 1.0   mem len: 49148   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.56\n",
      "epis: 264   score: 2.0   mem len: 49369   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.57\n",
      "epis: 265   score: 2.0   mem len: 49567   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 266   score: 2.0   mem len: 49749   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.51\n",
      "epis: 267   score: 0.0   mem len: 49872   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 268   score: 5.0   mem len: 50218   epsilon: 1.0    steps: 346    lr: 0.0001     reward: 1.56\n",
      "epis: 269   score: 0.0   mem len: 50341   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 270   score: 0.0   mem len: 50464   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 271   score: 0.0   mem len: 50587   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 272   score: 1.0   mem len: 50756   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 273   score: 2.0   mem len: 50954   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 274   score: 1.0   mem len: 51123   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 275   score: 2.0   mem len: 51341   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.5\n",
      "epis: 276   score: 1.0   mem len: 51491   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 277   score: 2.0   mem len: 51688   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.51\n",
      "epis: 278   score: 3.0   mem len: 51956   epsilon: 1.0    steps: 268    lr: 0.0001     reward: 1.53\n",
      "epis: 279   score: 0.0   mem len: 52078   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 280   score: 0.0   mem len: 52201   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 281   score: 0.0   mem len: 52324   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 282   score: 2.0   mem len: 52540   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.51\n",
      "epis: 283   score: 1.0   mem len: 52708   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.49\n",
      "epis: 284   score: 0.0   mem len: 52831   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 285   score: 1.0   mem len: 52999   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 286   score: 2.0   mem len: 53197   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 287   score: 2.0   mem len: 53396   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.49\n",
      "epis: 288   score: 0.0   mem len: 53519   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 289   score: 1.0   mem len: 53689   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.48\n",
      "epis: 290   score: 7.0   mem len: 54009   epsilon: 1.0    steps: 320    lr: 0.0001     reward: 1.53\n",
      "epis: 291   score: 0.0   mem len: 54132   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 292   score: 3.0   mem len: 54357   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.53\n",
      "epis: 293   score: 0.0   mem len: 54480   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 294   score: 1.0   mem len: 54630   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.5\n",
      "epis: 295   score: 2.0   mem len: 54851   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.5\n",
      "epis: 296   score: 1.0   mem len: 55021   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.48\n",
      "epis: 297   score: 2.0   mem len: 55202   epsilon: 1.0    steps: 181    lr: 0.0001     reward: 1.48\n",
      "epis: 298   score: 2.0   mem len: 55400   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 299   score: 2.0   mem len: 55598   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 300   score: 0.0   mem len: 55720   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 301   score: 1.0   mem len: 55889   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 302   score: 3.0   mem len: 56114   epsilon: 1.0    steps: 225    lr: 0.0001     reward: 1.5\n",
      "epis: 303   score: 2.0   mem len: 56311   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.51\n",
      "epis: 304   score: 1.0   mem len: 56462   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 305   score: 0.0   mem len: 56585   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 306   score: 1.0   mem len: 56754   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 307   score: 4.0   mem len: 57050   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.52\n",
      "epis: 308   score: 2.0   mem len: 57248   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.53\n",
      "epis: 309   score: 2.0   mem len: 57466   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.53\n",
      "epis: 310   score: 2.0   mem len: 57664   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 311   score: 1.0   mem len: 57833   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 312   score: 0.0   mem len: 57956   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 313   score: 0.0   mem len: 58079   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 314   score: 0.0   mem len: 58201   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.45\n",
      "epis: 315   score: 2.0   mem len: 58398   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.44\n",
      "epis: 316   score: 0.0   mem len: 58521   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 317   score: 1.0   mem len: 58673   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.43\n",
      "epis: 318   score: 4.0   mem len: 58952   epsilon: 1.0    steps: 279    lr: 0.0001     reward: 1.45\n",
      "epis: 319   score: 3.0   mem len: 59198   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.48\n",
      "epis: 320   score: 0.0   mem len: 59321   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 321   score: 0.0   mem len: 59444   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 322   score: 2.0   mem len: 59642   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 323   score: 2.0   mem len: 59860   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.47\n",
      "epis: 324   score: 1.0   mem len: 60010   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.44\n",
      "epis: 325   score: 2.0   mem len: 60227   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.45\n",
      "epis: 326   score: 0.0   mem len: 60350   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 327   score: 4.0   mem len: 60625   epsilon: 1.0    steps: 275    lr: 0.0001     reward: 1.48\n",
      "epis: 328   score: 3.0   mem len: 60870   epsilon: 1.0    steps: 245    lr: 0.0001     reward: 1.46\n",
      "epis: 329   score: 2.0   mem len: 61068   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 330   score: 1.0   mem len: 61239   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.45\n",
      "epis: 331   score: 1.0   mem len: 61410   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.44\n",
      "epis: 332   score: 2.0   mem len: 61607   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 333   score: 3.0   mem len: 61840   epsilon: 1.0    steps: 233    lr: 0.0001     reward: 1.46\n",
      "epis: 334   score: 2.0   mem len: 62038   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 335   score: 4.0   mem len: 62354   epsilon: 1.0    steps: 316    lr: 0.0001     reward: 1.5\n",
      "epis: 336   score: 2.0   mem len: 62553   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.49\n",
      "epis: 337   score: 0.0   mem len: 62676   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 338   score: 0.0   mem len: 62799   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 339   score: 0.0   mem len: 62922   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 340   score: 1.0   mem len: 63073   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.46\n",
      "epis: 341   score: 3.0   mem len: 63303   epsilon: 1.0    steps: 230    lr: 0.0001     reward: 1.49\n",
      "epis: 342   score: 2.0   mem len: 63500   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.5\n",
      "epis: 343   score: 0.0   mem len: 63623   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 344   score: 2.0   mem len: 63805   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.47\n",
      "epis: 345   score: 2.0   mem len: 64022   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.49\n",
      "epis: 346   score: 0.0   mem len: 64145   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 347   score: 1.0   mem len: 64315   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.48\n",
      "epis: 348   score: 1.0   mem len: 64466   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.47\n",
      "epis: 349   score: 5.0   mem len: 64813   epsilon: 1.0    steps: 347    lr: 0.0001     reward: 1.49\n",
      "epis: 350   score: 1.0   mem len: 64982   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 351   score: 0.0   mem len: 65105   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 352   score: 1.0   mem len: 65255   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.46\n",
      "epis: 353   score: 6.0   mem len: 65628   epsilon: 1.0    steps: 373    lr: 0.0001     reward: 1.51\n",
      "epis: 354   score: 1.0   mem len: 65799   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.49\n",
      "epis: 355   score: 2.0   mem len: 65997   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 356   score: 1.0   mem len: 66165   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.5\n",
      "epis: 357   score: 2.0   mem len: 66363   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 358   score: 2.0   mem len: 66584   epsilon: 1.0    steps: 221    lr: 0.0001     reward: 1.53\n",
      "epis: 359   score: 0.0   mem len: 66707   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 360   score: 1.0   mem len: 66877   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.49\n",
      "epis: 361   score: 2.0   mem len: 67095   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.49\n",
      "epis: 362   score: 2.0   mem len: 67275   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.51\n",
      "epis: 363   score: 1.0   mem len: 67426   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.51\n",
      "epis: 364   score: 4.0   mem len: 67744   epsilon: 1.0    steps: 318    lr: 0.0001     reward: 1.53\n",
      "epis: 365   score: 1.0   mem len: 67895   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 366   score: 3.0   mem len: 68164   epsilon: 1.0    steps: 269    lr: 0.0001     reward: 1.53\n",
      "epis: 367   score: 2.0   mem len: 68363   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.55\n",
      "epis: 368   score: 0.0   mem len: 68485   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 369   score: 2.0   mem len: 68702   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.52\n",
      "epis: 370   score: 1.0   mem len: 68871   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 371   score: 2.0   mem len: 69069   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 372   score: 5.0   mem len: 69438   epsilon: 1.0    steps: 369    lr: 0.0001     reward: 1.59\n",
      "epis: 373   score: 1.0   mem len: 69607   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.58\n",
      "epis: 374   score: 0.0   mem len: 69730   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 375   score: 2.0   mem len: 69952   epsilon: 1.0    steps: 222    lr: 0.0001     reward: 1.57\n",
      "epis: 376   score: 2.0   mem len: 70149   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.58\n",
      "epis: 377   score: 1.0   mem len: 70318   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.57\n",
      "epis: 378   score: 6.0   mem len: 70698   epsilon: 1.0    steps: 380    lr: 0.0001     reward: 1.6\n",
      "epis: 379   score: 0.0   mem len: 70821   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 380   score: 2.0   mem len: 71022   epsilon: 1.0    steps: 201    lr: 0.0001     reward: 1.62\n",
      "epis: 381   score: 0.0   mem len: 71144   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 382   score: 1.0   mem len: 71294   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.61\n",
      "epis: 383   score: 3.0   mem len: 71540   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.63\n",
      "epis: 384   score: 0.0   mem len: 71662   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 385   score: 0.0   mem len: 71785   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 386   score: 2.0   mem len: 72003   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.62\n",
      "epis: 387   score: 3.0   mem len: 72230   epsilon: 1.0    steps: 227    lr: 0.0001     reward: 1.63\n",
      "epis: 388   score: 1.0   mem len: 72399   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.64\n",
      "epis: 389   score: 2.0   mem len: 72597   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.65\n",
      "epis: 390   score: 0.0   mem len: 72720   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 391   score: 0.0   mem len: 72843   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 392   score: 5.0   mem len: 73213   epsilon: 1.0    steps: 370    lr: 0.0001     reward: 1.6\n",
      "epis: 393   score: 2.0   mem len: 73432   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.62\n",
      "epis: 394   score: 1.0   mem len: 73601   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 395   score: 0.0   mem len: 73723   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 396   score: 2.0   mem len: 73941   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.61\n",
      "epis: 397   score: 2.0   mem len: 74141   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.61\n",
      "epis: 398   score: 0.0   mem len: 74263   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.59\n",
      "epis: 399   score: 3.0   mem len: 74507   epsilon: 1.0    steps: 244    lr: 0.0001     reward: 1.6\n",
      "epis: 400   score: 1.0   mem len: 74676   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 401   score: 3.0   mem len: 74940   epsilon: 1.0    steps: 264    lr: 0.0001     reward: 1.63\n",
      "epis: 402   score: 2.0   mem len: 75155   epsilon: 1.0    steps: 215    lr: 0.0001     reward: 1.62\n",
      "epis: 403   score: 0.0   mem len: 75278   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 404   score: 0.0   mem len: 75401   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 405   score: 2.0   mem len: 75598   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.61\n",
      "epis: 406   score: 3.0   mem len: 75826   epsilon: 1.0    steps: 228    lr: 0.0001     reward: 1.63\n",
      "epis: 407   score: 4.0   mem len: 76122   epsilon: 1.0    steps: 296    lr: 0.0001     reward: 1.63\n",
      "epis: 408   score: 2.0   mem len: 76304   epsilon: 1.0    steps: 182    lr: 0.0001     reward: 1.63\n",
      "epis: 409   score: 4.0   mem len: 76599   epsilon: 1.0    steps: 295    lr: 0.0001     reward: 1.65\n",
      "epis: 410   score: 2.0   mem len: 76817   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.65\n",
      "epis: 411   score: 1.0   mem len: 76986   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.65\n",
      "epis: 412   score: 1.0   mem len: 77155   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.66\n",
      "epis: 413   score: 2.0   mem len: 77353   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.68\n",
      "epis: 414   score: 0.0   mem len: 77475   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 415   score: 3.0   mem len: 77740   epsilon: 1.0    steps: 265    lr: 0.0001     reward: 1.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 416   score: 1.0   mem len: 77909   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.7\n",
      "epis: 417   score: 0.0   mem len: 78032   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.69\n",
      "epis: 418   score: 1.0   mem len: 78183   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.66\n",
      "epis: 419   score: 0.0   mem len: 78306   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 420   score: 0.0   mem len: 78429   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 421   score: 1.0   mem len: 78598   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.64\n",
      "epis: 422   score: 0.0   mem len: 78720   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 423   score: 1.0   mem len: 78888   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.61\n",
      "epis: 424   score: 0.0   mem len: 79010   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 425   score: 2.0   mem len: 79207   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.6\n",
      "epis: 426   score: 2.0   mem len: 79387   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.62\n",
      "epis: 427   score: 1.0   mem len: 79538   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 428   score: 1.0   mem len: 79689   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 429   score: 0.0   mem len: 79812   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 430   score: 1.0   mem len: 79982   epsilon: 1.0    steps: 170    lr: 0.0001     reward: 1.55\n",
      "epis: 431   score: 0.0   mem len: 80105   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 432   score: 1.0   mem len: 80274   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.53\n",
      "epis: 433   score: 0.0   mem len: 80397   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 434   score: 2.0   mem len: 80616   epsilon: 1.0    steps: 219    lr: 0.0001     reward: 1.5\n",
      "epis: 435   score: 2.0   mem len: 80816   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.48\n",
      "epis: 436   score: 2.0   mem len: 81034   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.48\n",
      "epis: 437   score: 2.0   mem len: 81250   epsilon: 1.0    steps: 216    lr: 0.0001     reward: 1.5\n",
      "epis: 438   score: 1.0   mem len: 81421   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.51\n",
      "epis: 439   score: 2.0   mem len: 81619   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.53\n",
      "epis: 440   score: 3.0   mem len: 81866   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.55\n",
      "epis: 441   score: 3.0   mem len: 82096   epsilon: 1.0    steps: 230    lr: 0.0001     reward: 1.55\n",
      "epis: 442   score: 0.0   mem len: 82219   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 443   score: 0.0   mem len: 82341   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 444   score: 0.0   mem len: 82463   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 445   score: 2.0   mem len: 82680   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.51\n",
      "epis: 446   score: 3.0   mem len: 82927   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.54\n",
      "epis: 447   score: 2.0   mem len: 83125   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 448   score: 2.0   mem len: 83324   epsilon: 1.0    steps: 199    lr: 0.0001     reward: 1.56\n",
      "epis: 449   score: 0.0   mem len: 83446   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 450   score: 2.0   mem len: 83643   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.52\n",
      "epis: 451   score: 8.0   mem len: 83989   epsilon: 1.0    steps: 346    lr: 0.0001     reward: 1.6\n",
      "epis: 452   score: 0.0   mem len: 84112   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 453   score: 1.0   mem len: 84281   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 454   score: 1.0   mem len: 84433   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.54\n",
      "epis: 455   score: 3.0   mem len: 84700   epsilon: 1.0    steps: 267    lr: 0.0001     reward: 1.55\n",
      "epis: 456   score: 3.0   mem len: 84951   epsilon: 1.0    steps: 251    lr: 0.0001     reward: 1.57\n",
      "epis: 457   score: 0.0   mem len: 85073   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 458   score: 1.0   mem len: 85242   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 459   score: 0.0   mem len: 85364   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 460   score: 1.0   mem len: 85533   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 461   score: 2.0   mem len: 85733   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.54\n",
      "epis: 462   score: 1.0   mem len: 85885   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.53\n",
      "epis: 463   score: 1.0   mem len: 86053   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.53\n",
      "epis: 464   score: 3.0   mem len: 86267   epsilon: 1.0    steps: 214    lr: 0.0001     reward: 1.52\n",
      "epis: 465   score: 2.0   mem len: 86464   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.53\n",
      "epis: 466   score: 0.0   mem len: 86587   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 467   score: 1.0   mem len: 86758   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.49\n",
      "epis: 468   score: 3.0   mem len: 86989   epsilon: 1.0    steps: 231    lr: 0.0001     reward: 1.52\n",
      "epis: 469   score: 0.0   mem len: 87112   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 470   score: 3.0   mem len: 87358   epsilon: 1.0    steps: 246    lr: 0.0001     reward: 1.52\n",
      "epis: 471   score: 0.0   mem len: 87481   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 472   score: 1.0   mem len: 87650   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.46\n",
      "epis: 473   score: 0.0   mem len: 87773   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 474   score: 2.0   mem len: 87991   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.47\n",
      "epis: 475   score: 1.0   mem len: 88143   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.46\n",
      "epis: 476   score: 1.0   mem len: 88312   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.45\n",
      "epis: 477   score: 1.0   mem len: 88462   epsilon: 1.0    steps: 150    lr: 0.0001     reward: 1.45\n",
      "epis: 478   score: 0.0   mem len: 88585   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 479   score: 1.0   mem len: 88736   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.4\n",
      "epis: 480   score: 0.0   mem len: 88858   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 481   score: 2.0   mem len: 89056   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.4\n",
      "epis: 482   score: 0.0   mem len: 89178   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.39\n",
      "epis: 483   score: 1.0   mem len: 89349   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.37\n",
      "epis: 484   score: 1.0   mem len: 89500   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 485   score: 4.0   mem len: 89794   epsilon: 1.0    steps: 294    lr: 0.0001     reward: 1.42\n",
      "epis: 486   score: 0.0   mem len: 89916   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 487   score: 5.0   mem len: 90243   epsilon: 1.0    steps: 327    lr: 0.0001     reward: 1.42\n",
      "epis: 488   score: 1.0   mem len: 90412   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.42\n",
      "epis: 489   score: 0.0   mem len: 90534   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 490   score: 0.0   mem len: 90657   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 491   score: 2.0   mem len: 90836   epsilon: 1.0    steps: 179    lr: 0.0001     reward: 1.42\n",
      "epis: 492   score: 0.0   mem len: 90959   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 493   score: 0.0   mem len: 91082   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.35\n",
      "epis: 494   score: 0.0   mem len: 91205   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 495   score: 0.0   mem len: 91328   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 496   score: 0.0   mem len: 91450   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 497   score: 4.0   mem len: 91763   epsilon: 1.0    steps: 313    lr: 0.0001     reward: 1.34\n",
      "epis: 498   score: 2.0   mem len: 91960   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 499   score: 0.0   mem len: 92083   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 500   score: 0.0   mem len: 92206   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 501   score: 0.0   mem len: 92329   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 502   score: 1.0   mem len: 92480   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.28\n",
      "epis: 503   score: 3.0   mem len: 92727   epsilon: 1.0    steps: 247    lr: 0.0001     reward: 1.31\n",
      "epis: 504   score: 1.0   mem len: 92895   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.32\n",
      "epis: 505   score: 2.0   mem len: 93093   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.32\n",
      "epis: 506   score: 1.0   mem len: 93262   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.3\n",
      "epis: 507   score: 1.0   mem len: 93413   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.27\n",
      "epis: 508   score: 1.0   mem len: 93565   epsilon: 1.0    steps: 152    lr: 0.0001     reward: 1.26\n",
      "epis: 509   score: 1.0   mem len: 93716   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.23\n",
      "epis: 510   score: 0.0   mem len: 93839   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 511   score: 2.0   mem len: 94037   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.22\n",
      "epis: 512   score: 0.0   mem len: 94160   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.21\n",
      "epis: 513   score: 1.0   mem len: 94311   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.2\n",
      "epis: 514   score: 2.0   mem len: 94529   epsilon: 1.0    steps: 218    lr: 0.0001     reward: 1.22\n",
      "epis: 515   score: 1.0   mem len: 94698   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.2\n",
      "epis: 516   score: 0.0   mem len: 94821   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.19\n",
      "epis: 517   score: 3.0   mem len: 95064   epsilon: 1.0    steps: 243    lr: 0.0001     reward: 1.22\n",
      "epis: 518   score: 2.0   mem len: 95261   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.23\n",
      "epis: 519   score: 4.0   mem len: 95577   epsilon: 1.0    steps: 316    lr: 0.0001     reward: 1.27\n",
      "epis: 520   score: 4.0   mem len: 95855   epsilon: 1.0    steps: 278    lr: 0.0001     reward: 1.31\n",
      "epis: 521   score: 2.0   mem len: 96053   epsilon: 1.0    steps: 198    lr: 0.0001     reward: 1.32\n",
      "epis: 522   score: 0.0   mem len: 96176   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 523   score: 2.0   mem len: 96373   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.33\n",
      "epis: 524   score: 2.0   mem len: 96595   epsilon: 1.0    steps: 222    lr: 0.0001     reward: 1.35\n",
      "epis: 525   score: 1.0   mem len: 96746   epsilon: 1.0    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 526   score: 3.0   mem len: 96994   epsilon: 1.0    steps: 248    lr: 0.0001     reward: 1.35\n",
      "epis: 527   score: 0.0   mem len: 97117   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 528   score: 2.0   mem len: 97317   epsilon: 1.0    steps: 200    lr: 0.0001     reward: 1.35\n",
      "epis: 529   score: 2.0   mem len: 97514   epsilon: 1.0    steps: 197    lr: 0.0001     reward: 1.37\n",
      "epis: 530   score: 2.0   mem len: 97731   epsilon: 1.0    steps: 217    lr: 0.0001     reward: 1.38\n",
      "epis: 531   score: 1.0   mem len: 97900   epsilon: 1.0    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 532   score: 0.0   mem len: 98022   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 533   score: 0.0   mem len: 98144   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 534   score: 0.0   mem len: 98266   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 535   score: 0.0   mem len: 98389   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 536   score: 0.0   mem len: 98512   epsilon: 1.0    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 537   score: 2.0   mem len: 98692   epsilon: 1.0    steps: 180    lr: 0.0001     reward: 1.32\n",
      "epis: 538   score: 1.0   mem len: 98863   epsilon: 1.0    steps: 171    lr: 0.0001     reward: 1.32\n",
      "epis: 539   score: 4.0   mem len: 99160   epsilon: 1.0    steps: 297    lr: 0.0001     reward: 1.34\n",
      "epis: 540   score: 0.0   mem len: 99282   epsilon: 1.0    steps: 122    lr: 0.0001     reward: 1.31\n",
      "epis: 541   score: 3.0   mem len: 99512   epsilon: 1.0    steps: 230    lr: 0.0001     reward: 1.31\n",
      "epis: 542   score: 3.0   mem len: 99780   epsilon: 1.0    steps: 268    lr: 0.0001     reward: 1.34\n",
      "epis: 543   score: 1.0   mem len: 99948   epsilon: 1.0    steps: 168    lr: 0.0001     reward: 1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/memory.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array(sample)\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent_double.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n",
      "/home/kaiwenjon/Documents/Spring2023/Deep-Learning-for-CV/spring2023/MP5/assignment5_materials/assignment5_materials/agent_double.py:109: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525551200/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  next_states_values[mask] = self.target_net(non_final_next_states).max(1)[0].cuda()[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 544   score: 2.0   mem len: 100145   epsilon: 0.9997    steps: 197    lr: 0.0001     reward: 1.37\n",
      "epis: 545   score: 2.0   mem len: 100345   epsilon: 0.9993    steps: 200    lr: 0.0001     reward: 1.37\n",
      "epis: 546   score: 0.0   mem len: 100468   epsilon: 0.9991    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 547   score: 3.0   mem len: 100696   epsilon: 0.9986    steps: 228    lr: 0.0001     reward: 1.35\n",
      "epis: 548   score: 0.0   mem len: 100819   epsilon: 0.9984    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 549   score: 2.0   mem len: 101017   epsilon: 0.998    steps: 198    lr: 0.0001     reward: 1.35\n",
      "epis: 550   score: 0.0   mem len: 101140   epsilon: 0.9977    steps: 123    lr: 0.0001     reward: 1.33\n",
      "epis: 551   score: 5.0   mem len: 101484   epsilon: 0.9971    steps: 344    lr: 0.0001     reward: 1.3\n",
      "epis: 552   score: 0.0   mem len: 101606   epsilon: 0.9968    steps: 122    lr: 0.0001     reward: 1.3\n",
      "epis: 553   score: 3.0   mem len: 101856   epsilon: 0.9963    steps: 250    lr: 0.0001     reward: 1.32\n",
      "epis: 554   score: 1.0   mem len: 102024   epsilon: 0.996    steps: 168    lr: 0.0001     reward: 1.32\n",
      "epis: 555   score: 4.0   mem len: 102344   epsilon: 0.9954    steps: 320    lr: 0.0001     reward: 1.33\n",
      "epis: 556   score: 2.0   mem len: 102560   epsilon: 0.9949    steps: 216    lr: 0.0001     reward: 1.32\n",
      "epis: 557   score: 0.0   mem len: 102683   epsilon: 0.9947    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 558   score: 1.0   mem len: 102854   epsilon: 0.9943    steps: 171    lr: 0.0001     reward: 1.32\n",
      "epis: 559   score: 2.0   mem len: 103052   epsilon: 0.994    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 560   score: 1.0   mem len: 103221   epsilon: 0.9936    steps: 169    lr: 0.0001     reward: 1.34\n",
      "epis: 561   score: 2.0   mem len: 103418   epsilon: 0.9932    steps: 197    lr: 0.0001     reward: 1.34\n",
      "epis: 562   score: 2.0   mem len: 103639   epsilon: 0.9928    steps: 221    lr: 0.0001     reward: 1.35\n",
      "epis: 563   score: 0.0   mem len: 103762   epsilon: 0.9925    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 564   score: 3.0   mem len: 104011   epsilon: 0.9921    steps: 249    lr: 0.0001     reward: 1.34\n",
      "epis: 565   score: 2.0   mem len: 104209   epsilon: 0.9917    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 566   score: 2.0   mem len: 104427   epsilon: 0.9912    steps: 218    lr: 0.0001     reward: 1.36\n",
      "epis: 567   score: 2.0   mem len: 104625   epsilon: 0.9908    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 568   score: 1.0   mem len: 104776   epsilon: 0.9905    steps: 151    lr: 0.0001     reward: 1.35\n",
      "epis: 569   score: 1.0   mem len: 104945   epsilon: 0.9902    steps: 169    lr: 0.0001     reward: 1.36\n",
      "epis: 570   score: 2.0   mem len: 105165   epsilon: 0.9898    steps: 220    lr: 0.0001     reward: 1.35\n",
      "epis: 571   score: 2.0   mem len: 105382   epsilon: 0.9893    steps: 217    lr: 0.0001     reward: 1.37\n",
      "epis: 572   score: 2.0   mem len: 105600   epsilon: 0.9889    steps: 218    lr: 0.0001     reward: 1.38\n",
      "epis: 573   score: 1.0   mem len: 105753   epsilon: 0.9886    steps: 153    lr: 0.0001     reward: 1.39\n",
      "epis: 574   score: 2.0   mem len: 105968   epsilon: 0.9882    steps: 215    lr: 0.0001     reward: 1.39\n",
      "epis: 575   score: 1.0   mem len: 106119   epsilon: 0.9879    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 576   score: 0.0   mem len: 106241   epsilon: 0.9876    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 577   score: 5.0   mem len: 106582   epsilon: 0.987    steps: 341    lr: 0.0001     reward: 1.42\n",
      "epis: 578   score: 0.0   mem len: 106705   epsilon: 0.9867    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 579   score: 0.0   mem len: 106828   epsilon: 0.9865    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 580   score: 2.0   mem len: 107026   epsilon: 0.9861    steps: 198    lr: 0.0001     reward: 1.43\n",
      "epis: 581   score: 0.0   mem len: 107148   epsilon: 0.9858    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 582   score: 6.0   mem len: 107541   epsilon: 0.9851    steps: 393    lr: 0.0001     reward: 1.47\n",
      "epis: 583   score: 5.0   mem len: 107858   epsilon: 0.9844    steps: 317    lr: 0.0001     reward: 1.51\n",
      "epis: 584   score: 1.0   mem len: 108028   epsilon: 0.9841    steps: 170    lr: 0.0001     reward: 1.51\n",
      "epis: 585   score: 0.0   mem len: 108150   epsilon: 0.9839    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 586   score: 0.0   mem len: 108273   epsilon: 0.9836    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 587   score: 3.0   mem len: 108502   epsilon: 0.9832    steps: 229    lr: 0.0001     reward: 1.45\n",
      "epis: 588   score: 2.0   mem len: 108700   epsilon: 0.9828    steps: 198    lr: 0.0001     reward: 1.46\n",
      "epis: 589   score: 0.0   mem len: 108822   epsilon: 0.9825    steps: 122    lr: 0.0001     reward: 1.46\n",
      "epis: 590   score: 1.0   mem len: 108991   epsilon: 0.9822    steps: 169    lr: 0.0001     reward: 1.47\n",
      "epis: 591   score: 1.0   mem len: 109159   epsilon: 0.9819    steps: 168    lr: 0.0001     reward: 1.46\n",
      "epis: 592   score: 1.0   mem len: 109309   epsilon: 0.9816    steps: 150    lr: 0.0001     reward: 1.47\n",
      "epis: 593   score: 5.0   mem len: 109631   epsilon: 0.9809    steps: 322    lr: 0.0001     reward: 1.52\n",
      "epis: 594   score: 3.0   mem len: 109856   epsilon: 0.9805    steps: 225    lr: 0.0001     reward: 1.55\n",
      "epis: 595   score: 0.0   mem len: 109978   epsilon: 0.9802    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 596   score: 0.0   mem len: 110101   epsilon: 0.98    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 597   score: 0.0   mem len: 110223   epsilon: 0.9798    steps: 122    lr: 0.0001     reward: 1.51\n",
      "epis: 598   score: 3.0   mem len: 110469   epsilon: 0.9793    steps: 246    lr: 0.0001     reward: 1.52\n",
      "epis: 599   score: 0.0   mem len: 110591   epsilon: 0.979    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 600   score: 3.0   mem len: 110857   epsilon: 0.9785    steps: 266    lr: 0.0001     reward: 1.55\n",
      "epis: 601   score: 0.0   mem len: 110980   epsilon: 0.9783    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 602   score: 3.0   mem len: 111244   epsilon: 0.9777    steps: 264    lr: 0.0001     reward: 1.57\n",
      "epis: 603   score: 0.0   mem len: 111367   epsilon: 0.9775    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 604   score: 0.0   mem len: 111490   epsilon: 0.9772    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 605   score: 2.0   mem len: 111710   epsilon: 0.9768    steps: 220    lr: 0.0001     reward: 1.53\n",
      "epis: 606   score: 1.0   mem len: 111861   epsilon: 0.9765    steps: 151    lr: 0.0001     reward: 1.53\n",
      "epis: 607   score: 0.0   mem len: 111984   epsilon: 0.9763    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 608   score: 1.0   mem len: 112156   epsilon: 0.9759    steps: 172    lr: 0.0001     reward: 1.52\n",
      "epis: 609   score: 2.0   mem len: 112354   epsilon: 0.9755    steps: 198    lr: 0.0001     reward: 1.53\n",
      "epis: 610   score: 2.0   mem len: 112551   epsilon: 0.9751    steps: 197    lr: 0.0001     reward: 1.55\n",
      "epis: 611   score: 1.0   mem len: 112719   epsilon: 0.9748    steps: 168    lr: 0.0001     reward: 1.54\n",
      "epis: 612   score: 1.0   mem len: 112888   epsilon: 0.9745    steps: 169    lr: 0.0001     reward: 1.55\n",
      "epis: 613   score: 2.0   mem len: 113106   epsilon: 0.974    steps: 218    lr: 0.0001     reward: 1.56\n",
      "epis: 614   score: 3.0   mem len: 113357   epsilon: 0.9736    steps: 251    lr: 0.0001     reward: 1.57\n",
      "epis: 615   score: 2.0   mem len: 113555   epsilon: 0.9732    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 616   score: 0.0   mem len: 113677   epsilon: 0.9729    steps: 122    lr: 0.0001     reward: 1.58\n",
      "epis: 617   score: 2.0   mem len: 113895   epsilon: 0.9725    steps: 218    lr: 0.0001     reward: 1.57\n",
      "epis: 618   score: 0.0   mem len: 114017   epsilon: 0.9722    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 619   score: 2.0   mem len: 114215   epsilon: 0.9719    steps: 198    lr: 0.0001     reward: 1.53\n",
      "epis: 620   score: 1.0   mem len: 114366   epsilon: 0.9716    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 621   score: 1.0   mem len: 114534   epsilon: 0.9712    steps: 168    lr: 0.0001     reward: 1.49\n",
      "epis: 622   score: 2.0   mem len: 114753   epsilon: 0.9708    steps: 219    lr: 0.0001     reward: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 623   score: 5.0   mem len: 115045   epsilon: 0.9702    steps: 292    lr: 0.0001     reward: 1.54\n",
      "epis: 624   score: 0.0   mem len: 115167   epsilon: 0.97    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 625   score: 1.0   mem len: 115318   epsilon: 0.9697    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 626   score: 2.0   mem len: 115516   epsilon: 0.9693    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 627   score: 1.0   mem len: 115688   epsilon: 0.9689    steps: 172    lr: 0.0001     reward: 1.52\n",
      "epis: 628   score: 2.0   mem len: 115886   epsilon: 0.9685    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 629   score: 1.0   mem len: 116055   epsilon: 0.9682    steps: 169    lr: 0.0001     reward: 1.51\n",
      "epis: 630   score: 0.0   mem len: 116178   epsilon: 0.968    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 631   score: 3.0   mem len: 116439   epsilon: 0.9674    steps: 261    lr: 0.0001     reward: 1.51\n",
      "epis: 632   score: 2.0   mem len: 116643   epsilon: 0.967    steps: 204    lr: 0.0001     reward: 1.53\n",
      "epis: 633   score: 2.0   mem len: 116841   epsilon: 0.9667    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 634   score: 3.0   mem len: 117104   epsilon: 0.9661    steps: 263    lr: 0.0001     reward: 1.58\n",
      "epis: 635   score: 2.0   mem len: 117302   epsilon: 0.9657    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 636   score: 0.0   mem len: 117424   epsilon: 0.9655    steps: 122    lr: 0.0001     reward: 1.6\n",
      "epis: 637   score: 0.0   mem len: 117547   epsilon: 0.9653    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 638   score: 3.0   mem len: 117812   epsilon: 0.9647    steps: 265    lr: 0.0001     reward: 1.6\n",
      "epis: 639   score: 0.0   mem len: 117934   epsilon: 0.9645    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 640   score: 2.0   mem len: 118132   epsilon: 0.9641    steps: 198    lr: 0.0001     reward: 1.58\n",
      "epis: 641   score: 2.0   mem len: 118330   epsilon: 0.9637    steps: 198    lr: 0.0001     reward: 1.57\n",
      "epis: 642   score: 0.0   mem len: 118452   epsilon: 0.9635    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 643   score: 2.0   mem len: 118650   epsilon: 0.9631    steps: 198    lr: 0.0001     reward: 1.55\n",
      "epis: 644   score: 3.0   mem len: 118920   epsilon: 0.9625    steps: 270    lr: 0.0001     reward: 1.56\n",
      "epis: 645   score: 0.0   mem len: 119042   epsilon: 0.9623    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 646   score: 0.0   mem len: 119165   epsilon: 0.9621    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 647   score: 3.0   mem len: 119417   epsilon: 0.9616    steps: 252    lr: 0.0001     reward: 1.54\n",
      "epis: 648   score: 2.0   mem len: 119635   epsilon: 0.9611    steps: 218    lr: 0.0001     reward: 1.56\n",
      "epis: 649   score: 1.0   mem len: 119787   epsilon: 0.9608    steps: 152    lr: 0.0001     reward: 1.55\n",
      "epis: 650   score: 4.0   mem len: 120101   epsilon: 0.9602    steps: 314    lr: 0.0001     reward: 1.59\n",
      "epis: 651   score: 0.0   mem len: 120224   epsilon: 0.96    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 652   score: 3.0   mem len: 120488   epsilon: 0.9594    steps: 264    lr: 0.0001     reward: 1.57\n",
      "epis: 653   score: 1.0   mem len: 120659   epsilon: 0.9591    steps: 171    lr: 0.0001     reward: 1.55\n",
      "epis: 654   score: 0.0   mem len: 120782   epsilon: 0.9588    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 655   score: 3.0   mem len: 121046   epsilon: 0.9583    steps: 264    lr: 0.0001     reward: 1.53\n",
      "epis: 656   score: 1.0   mem len: 121196   epsilon: 0.958    steps: 150    lr: 0.0001     reward: 1.52\n",
      "epis: 657   score: 0.0   mem len: 121319   epsilon: 0.9578    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 658   score: 3.0   mem len: 121566   epsilon: 0.9573    steps: 247    lr: 0.0001     reward: 1.54\n",
      "epis: 659   score: 2.0   mem len: 121764   epsilon: 0.9569    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 660   score: 0.0   mem len: 121887   epsilon: 0.9567    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 661   score: 1.0   mem len: 122038   epsilon: 0.9564    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 662   score: 3.0   mem len: 122305   epsilon: 0.9558    steps: 267    lr: 0.0001     reward: 1.53\n",
      "epis: 663   score: 0.0   mem len: 122427   epsilon: 0.9556    steps: 122    lr: 0.0001     reward: 1.53\n",
      "epis: 664   score: 0.0   mem len: 122550   epsilon: 0.9553    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 665   score: 0.0   mem len: 122673   epsilon: 0.9551    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 666   score: 0.0   mem len: 122796   epsilon: 0.9549    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 667   score: 3.0   mem len: 123040   epsilon: 0.9544    steps: 244    lr: 0.0001     reward: 1.47\n",
      "epis: 668   score: 1.0   mem len: 123208   epsilon: 0.954    steps: 168    lr: 0.0001     reward: 1.47\n",
      "epis: 669   score: 0.0   mem len: 123330   epsilon: 0.9538    steps: 122    lr: 0.0001     reward: 1.46\n",
      "epis: 670   score: 1.0   mem len: 123500   epsilon: 0.9535    steps: 170    lr: 0.0001     reward: 1.45\n",
      "epis: 671   score: 1.0   mem len: 123668   epsilon: 0.9531    steps: 168    lr: 0.0001     reward: 1.44\n",
      "epis: 672   score: 0.0   mem len: 123791   epsilon: 0.9529    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 673   score: 0.0   mem len: 123913   epsilon: 0.9527    steps: 122    lr: 0.0001     reward: 1.41\n",
      "epis: 674   score: 1.0   mem len: 124063   epsilon: 0.9524    steps: 150    lr: 0.0001     reward: 1.4\n",
      "epis: 675   score: 0.0   mem len: 124186   epsilon: 0.9521    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 676   score: 2.0   mem len: 124384   epsilon: 0.9517    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 677   score: 1.0   mem len: 124535   epsilon: 0.9514    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 678   score: 1.0   mem len: 124706   epsilon: 0.9511    steps: 171    lr: 0.0001     reward: 1.38\n",
      "epis: 679   score: 1.0   mem len: 124857   epsilon: 0.9508    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 680   score: 0.0   mem len: 124980   epsilon: 0.9505    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 681   score: 2.0   mem len: 125198   epsilon: 0.9501    steps: 218    lr: 0.0001     reward: 1.39\n",
      "epis: 682   score: 3.0   mem len: 125443   epsilon: 0.9496    steps: 245    lr: 0.0001     reward: 1.36\n",
      "epis: 683   score: 1.0   mem len: 125614   epsilon: 0.9493    steps: 171    lr: 0.0001     reward: 1.32\n",
      "epis: 684   score: 0.0   mem len: 125737   epsilon: 0.949    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 685   score: 2.0   mem len: 125935   epsilon: 0.9486    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 686   score: 6.0   mem len: 126301   epsilon: 0.9479    steps: 366    lr: 0.0001     reward: 1.39\n",
      "epis: 687   score: 2.0   mem len: 126517   epsilon: 0.9475    steps: 216    lr: 0.0001     reward: 1.38\n",
      "epis: 688   score: 1.0   mem len: 126668   epsilon: 0.9472    steps: 151    lr: 0.0001     reward: 1.37\n",
      "epis: 689   score: 0.0   mem len: 126790   epsilon: 0.947    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 690   score: 0.0   mem len: 126912   epsilon: 0.9467    steps: 122    lr: 0.0001     reward: 1.36\n",
      "epis: 691   score: 1.0   mem len: 127064   epsilon: 0.9464    steps: 152    lr: 0.0001     reward: 1.36\n",
      "epis: 692   score: 2.0   mem len: 127262   epsilon: 0.946    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 693   score: 2.0   mem len: 127478   epsilon: 0.9456    steps: 216    lr: 0.0001     reward: 1.34\n",
      "epis: 694   score: 2.0   mem len: 127676   epsilon: 0.9452    steps: 198    lr: 0.0001     reward: 1.33\n",
      "epis: 695   score: 1.0   mem len: 127827   epsilon: 0.9449    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 696   score: 0.0   mem len: 127949   epsilon: 0.9447    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 697   score: 0.0   mem len: 128072   epsilon: 0.9444    steps: 123    lr: 0.0001     reward: 1.34\n",
      "epis: 698   score: 2.0   mem len: 128290   epsilon: 0.944    steps: 218    lr: 0.0001     reward: 1.33\n",
      "epis: 699   score: 1.0   mem len: 128460   epsilon: 0.9436    steps: 170    lr: 0.0001     reward: 1.34\n",
      "epis: 700   score: 1.0   mem len: 128611   epsilon: 0.9433    steps: 151    lr: 0.0001     reward: 1.32\n",
      "epis: 701   score: 2.0   mem len: 128809   epsilon: 0.943    steps: 198    lr: 0.0001     reward: 1.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 702   score: 1.0   mem len: 128977   epsilon: 0.9426    steps: 168    lr: 0.0001     reward: 1.32\n",
      "epis: 703   score: 0.0   mem len: 129100   epsilon: 0.9424    steps: 123    lr: 0.0001     reward: 1.32\n",
      "epis: 704   score: 2.0   mem len: 129298   epsilon: 0.942    steps: 198    lr: 0.0001     reward: 1.34\n",
      "epis: 705   score: 0.0   mem len: 129420   epsilon: 0.9417    steps: 122    lr: 0.0001     reward: 1.32\n",
      "epis: 706   score: 2.0   mem len: 129638   epsilon: 0.9413    steps: 218    lr: 0.0001     reward: 1.33\n",
      "epis: 707   score: 1.0   mem len: 129789   epsilon: 0.941    steps: 151    lr: 0.0001     reward: 1.34\n",
      "epis: 708   score: 0.0   mem len: 129911   epsilon: 0.9408    steps: 122    lr: 0.0001     reward: 1.33\n",
      "epis: 709   score: 2.0   mem len: 130128   epsilon: 0.9403    steps: 217    lr: 0.0001     reward: 1.33\n",
      "epis: 710   score: 3.0   mem len: 130398   epsilon: 0.9398    steps: 270    lr: 0.0001     reward: 1.34\n",
      "epis: 711   score: 4.0   mem len: 130641   epsilon: 0.9393    steps: 243    lr: 0.0001     reward: 1.37\n",
      "epis: 712   score: 2.0   mem len: 130839   epsilon: 0.9389    steps: 198    lr: 0.0001     reward: 1.38\n",
      "epis: 713   score: 1.0   mem len: 131007   epsilon: 0.9386    steps: 168    lr: 0.0001     reward: 1.37\n",
      "epis: 714   score: 4.0   mem len: 131265   epsilon: 0.9381    steps: 258    lr: 0.0001     reward: 1.38\n",
      "epis: 715   score: 2.0   mem len: 131484   epsilon: 0.9377    steps: 219    lr: 0.0001     reward: 1.38\n",
      "epis: 716   score: 1.0   mem len: 131653   epsilon: 0.9373    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 717   score: 1.0   mem len: 131822   epsilon: 0.937    steps: 169    lr: 0.0001     reward: 1.38\n",
      "epis: 718   score: 0.0   mem len: 131944   epsilon: 0.9367    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 719   score: 3.0   mem len: 132170   epsilon: 0.9363    steps: 226    lr: 0.0001     reward: 1.39\n",
      "epis: 720   score: 2.0   mem len: 132385   epsilon: 0.9359    steps: 215    lr: 0.0001     reward: 1.4\n",
      "epis: 721   score: 0.0   mem len: 132508   epsilon: 0.9356    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 722   score: 0.0   mem len: 132630   epsilon: 0.9354    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 723   score: 2.0   mem len: 132847   epsilon: 0.935    steps: 217    lr: 0.0001     reward: 1.34\n",
      "epis: 724   score: 1.0   mem len: 133016   epsilon: 0.9346    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 725   score: 0.0   mem len: 133138   epsilon: 0.9344    steps: 122    lr: 0.0001     reward: 1.34\n",
      "epis: 726   score: 1.0   mem len: 133308   epsilon: 0.934    steps: 170    lr: 0.0001     reward: 1.33\n",
      "epis: 727   score: 1.0   mem len: 133477   epsilon: 0.9337    steps: 169    lr: 0.0001     reward: 1.33\n",
      "epis: 728   score: 1.0   mem len: 133646   epsilon: 0.9334    steps: 169    lr: 0.0001     reward: 1.32\n",
      "epis: 729   score: 0.0   mem len: 133769   epsilon: 0.9331    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 730   score: 0.0   mem len: 133892   epsilon: 0.9329    steps: 123    lr: 0.0001     reward: 1.31\n",
      "epis: 731   score: 3.0   mem len: 134140   epsilon: 0.9324    steps: 248    lr: 0.0001     reward: 1.31\n",
      "epis: 732   score: 0.0   mem len: 134263   epsilon: 0.9322    steps: 123    lr: 0.0001     reward: 1.29\n",
      "epis: 733   score: 4.0   mem len: 134538   epsilon: 0.9316    steps: 275    lr: 0.0001     reward: 1.31\n",
      "epis: 734   score: 2.0   mem len: 134754   epsilon: 0.9312    steps: 216    lr: 0.0001     reward: 1.3\n",
      "epis: 735   score: 2.0   mem len: 134952   epsilon: 0.9308    steps: 198    lr: 0.0001     reward: 1.3\n",
      "epis: 736   score: 2.0   mem len: 135173   epsilon: 0.9304    steps: 221    lr: 0.0001     reward: 1.32\n",
      "epis: 737   score: 2.0   mem len: 135392   epsilon: 0.9299    steps: 219    lr: 0.0001     reward: 1.34\n",
      "epis: 738   score: 3.0   mem len: 135659   epsilon: 0.9294    steps: 267    lr: 0.0001     reward: 1.34\n",
      "epis: 739   score: 2.0   mem len: 135857   epsilon: 0.929    steps: 198    lr: 0.0001     reward: 1.36\n",
      "epis: 740   score: 2.0   mem len: 136039   epsilon: 0.9286    steps: 182    lr: 0.0001     reward: 1.36\n",
      "epis: 741   score: 5.0   mem len: 136363   epsilon: 0.928    steps: 324    lr: 0.0001     reward: 1.39\n",
      "epis: 742   score: 0.0   mem len: 136486   epsilon: 0.9278    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 743   score: 2.0   mem len: 136701   epsilon: 0.9273    steps: 215    lr: 0.0001     reward: 1.39\n",
      "epis: 744   score: 0.0   mem len: 136824   epsilon: 0.9271    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 745   score: 5.0   mem len: 137149   epsilon: 0.9264    steps: 325    lr: 0.0001     reward: 1.41\n",
      "epis: 746   score: 5.0   mem len: 137493   epsilon: 0.9258    steps: 344    lr: 0.0001     reward: 1.46\n",
      "epis: 747   score: 2.0   mem len: 137710   epsilon: 0.9253    steps: 217    lr: 0.0001     reward: 1.45\n",
      "epis: 748   score: 5.0   mem len: 138040   epsilon: 0.9247    steps: 330    lr: 0.0001     reward: 1.48\n",
      "epis: 749   score: 1.0   mem len: 138209   epsilon: 0.9243    steps: 169    lr: 0.0001     reward: 1.48\n",
      "epis: 750   score: 0.0   mem len: 138332   epsilon: 0.9241    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 751   score: 0.0   mem len: 138455   epsilon: 0.9239    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 752   score: 0.0   mem len: 138578   epsilon: 0.9236    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 753   score: 0.0   mem len: 138701   epsilon: 0.9234    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 754   score: 0.0   mem len: 138824   epsilon: 0.9231    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 755   score: 1.0   mem len: 138992   epsilon: 0.9228    steps: 168    lr: 0.0001     reward: 1.38\n",
      "epis: 756   score: 2.0   mem len: 139209   epsilon: 0.9224    steps: 217    lr: 0.0001     reward: 1.39\n",
      "epis: 757   score: 0.0   mem len: 139332   epsilon: 0.9221    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 758   score: 2.0   mem len: 139551   epsilon: 0.9217    steps: 219    lr: 0.0001     reward: 1.38\n",
      "epis: 759   score: 0.0   mem len: 139674   epsilon: 0.9214    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 760   score: 0.0   mem len: 139797   epsilon: 0.9212    steps: 123    lr: 0.0001     reward: 1.36\n",
      "epis: 761   score: 5.0   mem len: 140102   epsilon: 0.9206    steps: 305    lr: 0.0001     reward: 1.4\n",
      "epis: 762   score: 2.0   mem len: 140299   epsilon: 0.9202    steps: 197    lr: 0.0001     reward: 1.39\n",
      "epis: 763   score: 2.0   mem len: 140497   epsilon: 0.9198    steps: 198    lr: 0.0001     reward: 1.41\n",
      "epis: 764   score: 1.0   mem len: 140668   epsilon: 0.9195    steps: 171    lr: 0.0001     reward: 1.42\n",
      "epis: 765   score: 0.0   mem len: 140791   epsilon: 0.9192    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 766   score: 2.0   mem len: 141009   epsilon: 0.9188    steps: 218    lr: 0.0001     reward: 1.44\n",
      "epis: 767   score: 3.0   mem len: 141276   epsilon: 0.9183    steps: 267    lr: 0.0001     reward: 1.44\n",
      "epis: 768   score: 2.0   mem len: 141474   epsilon: 0.9179    steps: 198    lr: 0.0001     reward: 1.45\n",
      "epis: 769   score: 0.0   mem len: 141597   epsilon: 0.9176    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 770   score: 3.0   mem len: 141841   epsilon: 0.9172    steps: 244    lr: 0.0001     reward: 1.47\n",
      "epis: 771   score: 2.0   mem len: 142038   epsilon: 0.9168    steps: 197    lr: 0.0001     reward: 1.48\n",
      "epis: 772   score: 0.0   mem len: 142161   epsilon: 0.9165    steps: 123    lr: 0.0001     reward: 1.48\n",
      "epis: 773   score: 3.0   mem len: 142408   epsilon: 0.916    steps: 247    lr: 0.0001     reward: 1.51\n",
      "epis: 774   score: 0.0   mem len: 142530   epsilon: 0.9158    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 775   score: 0.0   mem len: 142652   epsilon: 0.9155    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 776   score: 2.0   mem len: 142850   epsilon: 0.9152    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 777   score: 0.0   mem len: 142973   epsilon: 0.9149    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 778   score: 3.0   mem len: 143216   epsilon: 0.9144    steps: 243    lr: 0.0001     reward: 1.51\n",
      "epis: 779   score: 3.0   mem len: 143480   epsilon: 0.9139    steps: 264    lr: 0.0001     reward: 1.53\n",
      "epis: 780   score: 0.0   mem len: 143602   epsilon: 0.9137    steps: 122    lr: 0.0001     reward: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 781   score: 2.0   mem len: 143821   epsilon: 0.9132    steps: 219    lr: 0.0001     reward: 1.53\n",
      "epis: 782   score: 3.0   mem len: 144068   epsilon: 0.9127    steps: 247    lr: 0.0001     reward: 1.53\n",
      "epis: 783   score: 2.0   mem len: 144266   epsilon: 0.9124    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 784   score: 3.0   mem len: 144533   epsilon: 0.9118    steps: 267    lr: 0.0001     reward: 1.57\n",
      "epis: 785   score: 2.0   mem len: 144713   epsilon: 0.9115    steps: 180    lr: 0.0001     reward: 1.57\n",
      "epis: 786   score: 2.0   mem len: 144910   epsilon: 0.9111    steps: 197    lr: 0.0001     reward: 1.53\n",
      "epis: 787   score: 4.0   mem len: 145222   epsilon: 0.9105    steps: 312    lr: 0.0001     reward: 1.55\n",
      "epis: 788   score: 0.0   mem len: 145345   epsilon: 0.9102    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 789   score: 3.0   mem len: 145593   epsilon: 0.9097    steps: 248    lr: 0.0001     reward: 1.57\n",
      "epis: 790   score: 1.0   mem len: 145744   epsilon: 0.9094    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 791   score: 2.0   mem len: 145945   epsilon: 0.909    steps: 201    lr: 0.0001     reward: 1.59\n",
      "epis: 792   score: 2.0   mem len: 146143   epsilon: 0.9086    steps: 198    lr: 0.0001     reward: 1.59\n",
      "epis: 793   score: 0.0   mem len: 146266   epsilon: 0.9084    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 794   score: 0.0   mem len: 146388   epsilon: 0.9081    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 795   score: 0.0   mem len: 146510   epsilon: 0.9079    steps: 122    lr: 0.0001     reward: 1.54\n",
      "epis: 796   score: 2.0   mem len: 146690   epsilon: 0.9076    steps: 180    lr: 0.0001     reward: 1.56\n",
      "epis: 797   score: 0.0   mem len: 146812   epsilon: 0.9073    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 798   score: 0.0   mem len: 146935   epsilon: 0.9071    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 799   score: 2.0   mem len: 147116   epsilon: 0.9067    steps: 181    lr: 0.0001     reward: 1.55\n",
      "epis: 800   score: 1.0   mem len: 147286   epsilon: 0.9064    steps: 170    lr: 0.0001     reward: 1.55\n",
      "epis: 801   score: 0.0   mem len: 147409   epsilon: 0.9061    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 802   score: 0.0   mem len: 147531   epsilon: 0.9059    steps: 122    lr: 0.0001     reward: 1.52\n",
      "epis: 803   score: 1.0   mem len: 147699   epsilon: 0.9056    steps: 168    lr: 0.0001     reward: 1.53\n",
      "epis: 804   score: 3.0   mem len: 147943   epsilon: 0.9051    steps: 244    lr: 0.0001     reward: 1.54\n",
      "epis: 805   score: 0.0   mem len: 148066   epsilon: 0.9048    steps: 123    lr: 0.0001     reward: 1.54\n",
      "epis: 806   score: 2.0   mem len: 148284   epsilon: 0.9044    steps: 218    lr: 0.0001     reward: 1.54\n",
      "epis: 807   score: 2.0   mem len: 148485   epsilon: 0.904    steps: 201    lr: 0.0001     reward: 1.55\n",
      "epis: 808   score: 2.0   mem len: 148703   epsilon: 0.9036    steps: 218    lr: 0.0001     reward: 1.57\n",
      "epis: 809   score: 4.0   mem len: 149021   epsilon: 0.9029    steps: 318    lr: 0.0001     reward: 1.59\n",
      "epis: 810   score: 1.0   mem len: 149172   epsilon: 0.9026    steps: 151    lr: 0.0001     reward: 1.57\n",
      "epis: 811   score: 0.0   mem len: 149295   epsilon: 0.9024    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 812   score: 1.0   mem len: 149464   epsilon: 0.9021    steps: 169    lr: 0.0001     reward: 1.52\n",
      "epis: 813   score: 1.0   mem len: 149633   epsilon: 0.9017    steps: 169    lr: 0.0001     reward: 1.52\n",
      "epis: 814   score: 3.0   mem len: 149876   epsilon: 0.9012    steps: 243    lr: 0.0001     reward: 1.51\n",
      "epis: 815   score: 4.0   mem len: 150186   epsilon: 0.9006    steps: 310    lr: 0.0001     reward: 1.53\n",
      "epis: 816   score: 2.0   mem len: 150384   epsilon: 0.9002    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 817   score: 0.0   mem len: 150507   epsilon: 0.9    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 818   score: 1.0   mem len: 150676   epsilon: 0.8997    steps: 169    lr: 0.0001     reward: 1.54\n",
      "epis: 819   score: 2.0   mem len: 150855   epsilon: 0.8993    steps: 179    lr: 0.0001     reward: 1.53\n",
      "epis: 820   score: 4.0   mem len: 151130   epsilon: 0.8988    steps: 275    lr: 0.0001     reward: 1.55\n",
      "epis: 821   score: 0.0   mem len: 151253   epsilon: 0.8985    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 822   score: 1.0   mem len: 151422   epsilon: 0.8982    steps: 169    lr: 0.0001     reward: 1.56\n",
      "epis: 823   score: 5.0   mem len: 151718   epsilon: 0.8976    steps: 296    lr: 0.0001     reward: 1.59\n",
      "epis: 824   score: 1.0   mem len: 151869   epsilon: 0.8973    steps: 151    lr: 0.0001     reward: 1.59\n",
      "epis: 825   score: 1.0   mem len: 152019   epsilon: 0.897    steps: 150    lr: 0.0001     reward: 1.6\n",
      "epis: 826   score: 1.0   mem len: 152170   epsilon: 0.8967    steps: 151    lr: 0.0001     reward: 1.6\n",
      "epis: 827   score: 3.0   mem len: 152397   epsilon: 0.8963    steps: 227    lr: 0.0001     reward: 1.62\n",
      "epis: 828   score: 2.0   mem len: 152615   epsilon: 0.8958    steps: 218    lr: 0.0001     reward: 1.63\n",
      "epis: 829   score: 0.0   mem len: 152737   epsilon: 0.8956    steps: 122    lr: 0.0001     reward: 1.63\n",
      "epis: 830   score: 0.0   mem len: 152860   epsilon: 0.8953    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 831   score: 1.0   mem len: 153010   epsilon: 0.895    steps: 150    lr: 0.0001     reward: 1.61\n",
      "epis: 832   score: 2.0   mem len: 153225   epsilon: 0.8946    steps: 215    lr: 0.0001     reward: 1.63\n",
      "epis: 833   score: 4.0   mem len: 153520   epsilon: 0.894    steps: 295    lr: 0.0001     reward: 1.63\n",
      "epis: 834   score: 1.0   mem len: 153671   epsilon: 0.8937    steps: 151    lr: 0.0001     reward: 1.62\n",
      "epis: 835   score: 2.0   mem len: 153869   epsilon: 0.8933    steps: 198    lr: 0.0001     reward: 1.62\n",
      "epis: 836   score: 2.0   mem len: 154085   epsilon: 0.8929    steps: 216    lr: 0.0001     reward: 1.62\n",
      "epis: 837   score: 3.0   mem len: 154355   epsilon: 0.8924    steps: 270    lr: 0.0001     reward: 1.63\n",
      "epis: 838   score: 3.0   mem len: 154627   epsilon: 0.8918    steps: 272    lr: 0.0001     reward: 1.63\n",
      "epis: 839   score: 2.0   mem len: 154825   epsilon: 0.8914    steps: 198    lr: 0.0001     reward: 1.63\n",
      "epis: 840   score: 1.0   mem len: 154997   epsilon: 0.8911    steps: 172    lr: 0.0001     reward: 1.62\n",
      "epis: 841   score: 2.0   mem len: 155219   epsilon: 0.8907    steps: 222    lr: 0.0001     reward: 1.59\n",
      "epis: 842   score: 1.0   mem len: 155390   epsilon: 0.8903    steps: 171    lr: 0.0001     reward: 1.6\n",
      "epis: 843   score: 2.0   mem len: 155588   epsilon: 0.8899    steps: 198    lr: 0.0001     reward: 1.6\n",
      "epis: 844   score: 3.0   mem len: 155814   epsilon: 0.8895    steps: 226    lr: 0.0001     reward: 1.63\n",
      "epis: 845   score: 3.0   mem len: 156060   epsilon: 0.889    steps: 246    lr: 0.0001     reward: 1.61\n",
      "epis: 846   score: 3.0   mem len: 156305   epsilon: 0.8885    steps: 245    lr: 0.0001     reward: 1.59\n",
      "epis: 847   score: 0.0   mem len: 156428   epsilon: 0.8883    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 848   score: 2.0   mem len: 156626   epsilon: 0.8879    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 849   score: 2.0   mem len: 156825   epsilon: 0.8875    steps: 199    lr: 0.0001     reward: 1.55\n",
      "epis: 850   score: 3.0   mem len: 157054   epsilon: 0.887    steps: 229    lr: 0.0001     reward: 1.58\n",
      "epis: 851   score: 0.0   mem len: 157177   epsilon: 0.8868    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 852   score: 2.0   mem len: 157377   epsilon: 0.8864    steps: 200    lr: 0.0001     reward: 1.6\n",
      "epis: 853   score: 2.0   mem len: 157596   epsilon: 0.886    steps: 219    lr: 0.0001     reward: 1.62\n",
      "epis: 854   score: 1.0   mem len: 157746   epsilon: 0.8857    steps: 150    lr: 0.0001     reward: 1.63\n",
      "epis: 855   score: 1.0   mem len: 157915   epsilon: 0.8853    steps: 169    lr: 0.0001     reward: 1.63\n",
      "epis: 856   score: 1.0   mem len: 158066   epsilon: 0.885    steps: 151    lr: 0.0001     reward: 1.62\n",
      "epis: 857   score: 0.0   mem len: 158189   epsilon: 0.8848    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 858   score: 2.0   mem len: 158408   epsilon: 0.8844    steps: 219    lr: 0.0001     reward: 1.62\n",
      "epis: 859   score: 2.0   mem len: 158606   epsilon: 0.884    steps: 198    lr: 0.0001     reward: 1.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 860   score: 1.0   mem len: 158757   epsilon: 0.8837    steps: 151    lr: 0.0001     reward: 1.65\n",
      "epis: 861   score: 1.0   mem len: 158928   epsilon: 0.8833    steps: 171    lr: 0.0001     reward: 1.61\n",
      "epis: 862   score: 2.0   mem len: 159128   epsilon: 0.8829    steps: 200    lr: 0.0001     reward: 1.61\n",
      "epis: 863   score: 0.0   mem len: 159251   epsilon: 0.8827    steps: 123    lr: 0.0001     reward: 1.59\n",
      "epis: 864   score: 4.0   mem len: 159524   epsilon: 0.8821    steps: 273    lr: 0.0001     reward: 1.62\n",
      "epis: 865   score: 2.0   mem len: 159742   epsilon: 0.8817    steps: 218    lr: 0.0001     reward: 1.64\n",
      "epis: 866   score: 0.0   mem len: 159864   epsilon: 0.8815    steps: 122    lr: 0.0001     reward: 1.62\n",
      "epis: 867   score: 1.0   mem len: 160034   epsilon: 0.8811    steps: 170    lr: 0.0001     reward: 1.6\n",
      "epis: 868   score: 7.0   mem len: 160354   epsilon: 0.8805    steps: 320    lr: 0.0001     reward: 1.65\n",
      "epis: 869   score: 0.0   mem len: 160477   epsilon: 0.8803    steps: 123    lr: 0.0001     reward: 1.65\n",
      "epis: 870   score: 0.0   mem len: 160600   epsilon: 0.88    steps: 123    lr: 0.0001     reward: 1.62\n",
      "epis: 871   score: 1.0   mem len: 160751   epsilon: 0.8797    steps: 151    lr: 0.0001     reward: 1.61\n",
      "epis: 872   score: 1.0   mem len: 160920   epsilon: 0.8794    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 873   score: 1.0   mem len: 161089   epsilon: 0.879    steps: 169    lr: 0.0001     reward: 1.6\n",
      "epis: 874   score: 2.0   mem len: 161288   epsilon: 0.8786    steps: 199    lr: 0.0001     reward: 1.62\n",
      "epis: 875   score: 1.0   mem len: 161439   epsilon: 0.8783    steps: 151    lr: 0.0001     reward: 1.63\n",
      "epis: 876   score: 1.0   mem len: 161590   epsilon: 0.878    steps: 151    lr: 0.0001     reward: 1.62\n",
      "epis: 877   score: 3.0   mem len: 161856   epsilon: 0.8775    steps: 266    lr: 0.0001     reward: 1.65\n",
      "epis: 878   score: 2.0   mem len: 162056   epsilon: 0.8771    steps: 200    lr: 0.0001     reward: 1.64\n",
      "epis: 879   score: 0.0   mem len: 162179   epsilon: 0.8769    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 880   score: 1.0   mem len: 162348   epsilon: 0.8765    steps: 169    lr: 0.0001     reward: 1.62\n",
      "epis: 881   score: 0.0   mem len: 162471   epsilon: 0.8763    steps: 123    lr: 0.0001     reward: 1.6\n",
      "epis: 882   score: 0.0   mem len: 162594   epsilon: 0.8761    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 883   score: 4.0   mem len: 162849   epsilon: 0.8756    steps: 255    lr: 0.0001     reward: 1.59\n",
      "epis: 884   score: 3.0   mem len: 163093   epsilon: 0.8751    steps: 244    lr: 0.0001     reward: 1.59\n",
      "epis: 885   score: 0.0   mem len: 163216   epsilon: 0.8748    steps: 123    lr: 0.0001     reward: 1.57\n",
      "epis: 886   score: 0.0   mem len: 163339   epsilon: 0.8746    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 887   score: 4.0   mem len: 163599   epsilon: 0.8741    steps: 260    lr: 0.0001     reward: 1.55\n",
      "epis: 888   score: 0.0   mem len: 163721   epsilon: 0.8738    steps: 122    lr: 0.0001     reward: 1.55\n",
      "epis: 889   score: 1.0   mem len: 163873   epsilon: 0.8735    steps: 152    lr: 0.0001     reward: 1.53\n",
      "epis: 890   score: 0.0   mem len: 163996   epsilon: 0.8733    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 891   score: 0.0   mem len: 164119   epsilon: 0.873    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 892   score: 1.0   mem len: 164288   epsilon: 0.8727    steps: 169    lr: 0.0001     reward: 1.49\n",
      "epis: 893   score: 0.0   mem len: 164410   epsilon: 0.8725    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 894   score: 3.0   mem len: 164659   epsilon: 0.872    steps: 249    lr: 0.0001     reward: 1.52\n",
      "epis: 895   score: 2.0   mem len: 164856   epsilon: 0.8716    steps: 197    lr: 0.0001     reward: 1.54\n",
      "epis: 896   score: 2.0   mem len: 165054   epsilon: 0.8712    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 897   score: 2.0   mem len: 165251   epsilon: 0.8708    steps: 197    lr: 0.0001     reward: 1.56\n",
      "epis: 898   score: 3.0   mem len: 165481   epsilon: 0.8703    steps: 230    lr: 0.0001     reward: 1.59\n",
      "epis: 899   score: 2.0   mem len: 165678   epsilon: 0.87    steps: 197    lr: 0.0001     reward: 1.59\n",
      "epis: 900   score: 2.0   mem len: 165860   epsilon: 0.8696    steps: 182    lr: 0.0001     reward: 1.6\n",
      "epis: 901   score: 7.0   mem len: 166242   epsilon: 0.8688    steps: 382    lr: 0.0001     reward: 1.67\n",
      "epis: 902   score: 2.0   mem len: 166462   epsilon: 0.8684    steps: 220    lr: 0.0001     reward: 1.69\n",
      "epis: 903   score: 0.0   mem len: 166584   epsilon: 0.8682    steps: 122    lr: 0.0001     reward: 1.68\n",
      "epis: 904   score: 4.0   mem len: 166840   epsilon: 0.8677    steps: 256    lr: 0.0001     reward: 1.69\n",
      "epis: 905   score: 2.0   mem len: 167057   epsilon: 0.8672    steps: 217    lr: 0.0001     reward: 1.71\n",
      "epis: 906   score: 0.0   mem len: 167179   epsilon: 0.867    steps: 122    lr: 0.0001     reward: 1.69\n",
      "epis: 907   score: 1.0   mem len: 167329   epsilon: 0.8667    steps: 150    lr: 0.0001     reward: 1.68\n",
      "epis: 908   score: 3.0   mem len: 167575   epsilon: 0.8662    steps: 246    lr: 0.0001     reward: 1.69\n",
      "epis: 909   score: 2.0   mem len: 167791   epsilon: 0.8658    steps: 216    lr: 0.0001     reward: 1.67\n",
      "epis: 910   score: 1.0   mem len: 167960   epsilon: 0.8654    steps: 169    lr: 0.0001     reward: 1.67\n",
      "epis: 911   score: 1.0   mem len: 168131   epsilon: 0.8651    steps: 171    lr: 0.0001     reward: 1.68\n",
      "epis: 912   score: 1.0   mem len: 168282   epsilon: 0.8648    steps: 151    lr: 0.0001     reward: 1.68\n",
      "epis: 913   score: 2.0   mem len: 168497   epsilon: 0.8644    steps: 215    lr: 0.0001     reward: 1.69\n",
      "epis: 914   score: 1.0   mem len: 168669   epsilon: 0.864    steps: 172    lr: 0.0001     reward: 1.67\n",
      "epis: 915   score: 0.0   mem len: 168792   epsilon: 0.8638    steps: 123    lr: 0.0001     reward: 1.63\n",
      "epis: 916   score: 0.0   mem len: 168915   epsilon: 0.8635    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 917   score: 0.0   mem len: 169038   epsilon: 0.8633    steps: 123    lr: 0.0001     reward: 1.61\n",
      "epis: 918   score: 1.0   mem len: 169189   epsilon: 0.863    steps: 151    lr: 0.0001     reward: 1.61\n",
      "epis: 919   score: 2.0   mem len: 169407   epsilon: 0.8626    steps: 218    lr: 0.0001     reward: 1.61\n",
      "epis: 920   score: 1.0   mem len: 169558   epsilon: 0.8623    steps: 151    lr: 0.0001     reward: 1.58\n",
      "epis: 921   score: 1.0   mem len: 169708   epsilon: 0.862    steps: 150    lr: 0.0001     reward: 1.59\n",
      "epis: 922   score: 0.0   mem len: 169831   epsilon: 0.8617    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 923   score: 2.0   mem len: 170031   epsilon: 0.8613    steps: 200    lr: 0.0001     reward: 1.55\n",
      "epis: 924   score: 3.0   mem len: 170295   epsilon: 0.8608    steps: 264    lr: 0.0001     reward: 1.57\n",
      "epis: 925   score: 2.0   mem len: 170494   epsilon: 0.8604    steps: 199    lr: 0.0001     reward: 1.58\n",
      "epis: 926   score: 1.0   mem len: 170662   epsilon: 0.8601    steps: 168    lr: 0.0001     reward: 1.58\n",
      "epis: 927   score: 3.0   mem len: 170873   epsilon: 0.8597    steps: 211    lr: 0.0001     reward: 1.58\n",
      "epis: 928   score: 1.0   mem len: 171023   epsilon: 0.8594    steps: 150    lr: 0.0001     reward: 1.57\n",
      "epis: 929   score: 1.0   mem len: 171193   epsilon: 0.859    steps: 170    lr: 0.0001     reward: 1.58\n",
      "epis: 930   score: 2.0   mem len: 171374   epsilon: 0.8587    steps: 181    lr: 0.0001     reward: 1.6\n",
      "epis: 931   score: 3.0   mem len: 171622   epsilon: 0.8582    steps: 248    lr: 0.0001     reward: 1.62\n",
      "epis: 932   score: 2.0   mem len: 171838   epsilon: 0.8578    steps: 216    lr: 0.0001     reward: 1.62\n",
      "epis: 933   score: 2.0   mem len: 172018   epsilon: 0.8574    steps: 180    lr: 0.0001     reward: 1.6\n",
      "epis: 934   score: 3.0   mem len: 172282   epsilon: 0.8569    steps: 264    lr: 0.0001     reward: 1.62\n",
      "epis: 935   score: 1.0   mem len: 172451   epsilon: 0.8565    steps: 169    lr: 0.0001     reward: 1.61\n",
      "epis: 936   score: 1.0   mem len: 172623   epsilon: 0.8562    steps: 172    lr: 0.0001     reward: 1.6\n",
      "epis: 937   score: 1.0   mem len: 172775   epsilon: 0.8559    steps: 152    lr: 0.0001     reward: 1.58\n",
      "epis: 938   score: 0.0   mem len: 172897   epsilon: 0.8557    steps: 122    lr: 0.0001     reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 939   score: 2.0   mem len: 173094   epsilon: 0.8553    steps: 197    lr: 0.0001     reward: 1.55\n",
      "epis: 940   score: 2.0   mem len: 173276   epsilon: 0.8549    steps: 182    lr: 0.0001     reward: 1.56\n",
      "epis: 941   score: 2.0   mem len: 173492   epsilon: 0.8545    steps: 216    lr: 0.0001     reward: 1.56\n",
      "epis: 942   score: 5.0   mem len: 173789   epsilon: 0.8539    steps: 297    lr: 0.0001     reward: 1.6\n",
      "epis: 943   score: 0.0   mem len: 173912   epsilon: 0.8537    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 944   score: 1.0   mem len: 174063   epsilon: 0.8534    steps: 151    lr: 0.0001     reward: 1.56\n",
      "epis: 945   score: 3.0   mem len: 174327   epsilon: 0.8528    steps: 264    lr: 0.0001     reward: 1.56\n",
      "epis: 946   score: 0.0   mem len: 174450   epsilon: 0.8526    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 947   score: 4.0   mem len: 174723   epsilon: 0.852    steps: 273    lr: 0.0001     reward: 1.57\n",
      "epis: 948   score: 5.0   mem len: 175050   epsilon: 0.8514    steps: 327    lr: 0.0001     reward: 1.6\n",
      "epis: 949   score: 0.0   mem len: 175173   epsilon: 0.8512    steps: 123    lr: 0.0001     reward: 1.58\n",
      "epis: 950   score: 0.0   mem len: 175296   epsilon: 0.8509    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 951   score: 0.0   mem len: 175419   epsilon: 0.8507    steps: 123    lr: 0.0001     reward: 1.55\n",
      "epis: 952   score: 0.0   mem len: 175542   epsilon: 0.8504    steps: 123    lr: 0.0001     reward: 1.53\n",
      "epis: 953   score: 0.0   mem len: 175665   epsilon: 0.8502    steps: 123    lr: 0.0001     reward: 1.51\n",
      "epis: 954   score: 2.0   mem len: 175863   epsilon: 0.8498    steps: 198    lr: 0.0001     reward: 1.52\n",
      "epis: 955   score: 2.0   mem len: 176060   epsilon: 0.8494    steps: 197    lr: 0.0001     reward: 1.53\n",
      "epis: 956   score: 0.0   mem len: 176183   epsilon: 0.8492    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 957   score: 2.0   mem len: 176381   epsilon: 0.8488    steps: 198    lr: 0.0001     reward: 1.54\n",
      "epis: 958   score: 0.0   mem len: 176504   epsilon: 0.8485    steps: 123    lr: 0.0001     reward: 1.52\n",
      "epis: 959   score: 0.0   mem len: 176626   epsilon: 0.8483    steps: 122    lr: 0.0001     reward: 1.5\n",
      "epis: 960   score: 0.0   mem len: 176749   epsilon: 0.848    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 961   score: 1.0   mem len: 176900   epsilon: 0.8477    steps: 151    lr: 0.0001     reward: 1.49\n",
      "epis: 962   score: 0.0   mem len: 177022   epsilon: 0.8475    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 963   score: 0.0   mem len: 177145   epsilon: 0.8473    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 964   score: 0.0   mem len: 177268   epsilon: 0.847    steps: 123    lr: 0.0001     reward: 1.43\n",
      "epis: 965   score: 1.0   mem len: 177420   epsilon: 0.8467    steps: 152    lr: 0.0001     reward: 1.42\n",
      "epis: 966   score: 1.0   mem len: 177571   epsilon: 0.8464    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 967   score: 1.0   mem len: 177722   epsilon: 0.8461    steps: 151    lr: 0.0001     reward: 1.43\n",
      "epis: 968   score: 2.0   mem len: 177940   epsilon: 0.8457    steps: 218    lr: 0.0001     reward: 1.38\n",
      "epis: 969   score: 4.0   mem len: 178199   epsilon: 0.8452    steps: 259    lr: 0.0001     reward: 1.42\n",
      "epis: 970   score: 4.0   mem len: 178496   epsilon: 0.8446    steps: 297    lr: 0.0001     reward: 1.46\n",
      "epis: 971   score: 0.0   mem len: 178619   epsilon: 0.8443    steps: 123    lr: 0.0001     reward: 1.45\n",
      "epis: 972   score: 2.0   mem len: 178801   epsilon: 0.844    steps: 182    lr: 0.0001     reward: 1.46\n",
      "epis: 973   score: 2.0   mem len: 178999   epsilon: 0.8436    steps: 198    lr: 0.0001     reward: 1.47\n",
      "epis: 974   score: 3.0   mem len: 179266   epsilon: 0.8431    steps: 267    lr: 0.0001     reward: 1.48\n",
      "epis: 975   score: 3.0   mem len: 179512   epsilon: 0.8426    steps: 246    lr: 0.0001     reward: 1.5\n",
      "epis: 976   score: 1.0   mem len: 179663   epsilon: 0.8423    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 977   score: 2.0   mem len: 179865   epsilon: 0.8419    steps: 202    lr: 0.0001     reward: 1.49\n",
      "epis: 978   score: 3.0   mem len: 180094   epsilon: 0.8414    steps: 229    lr: 0.0001     reward: 1.5\n",
      "epis: 979   score: 0.0   mem len: 180217   epsilon: 0.8412    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 980   score: 0.0   mem len: 180339   epsilon: 0.8409    steps: 122    lr: 0.0001     reward: 1.49\n",
      "epis: 981   score: 2.0   mem len: 180537   epsilon: 0.8405    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 982   score: 1.0   mem len: 180688   epsilon: 0.8402    steps: 151    lr: 0.0001     reward: 1.52\n",
      "epis: 983   score: 1.0   mem len: 180856   epsilon: 0.8399    steps: 168    lr: 0.0001     reward: 1.49\n",
      "epis: 984   score: 2.0   mem len: 181054   epsilon: 0.8395    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 985   score: 1.0   mem len: 181224   epsilon: 0.8392    steps: 170    lr: 0.0001     reward: 1.49\n",
      "epis: 986   score: 2.0   mem len: 181423   epsilon: 0.8388    steps: 199    lr: 0.0001     reward: 1.51\n",
      "epis: 987   score: 2.0   mem len: 181640   epsilon: 0.8384    steps: 217    lr: 0.0001     reward: 1.49\n",
      "epis: 988   score: 0.0   mem len: 181763   epsilon: 0.8381    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 989   score: 2.0   mem len: 181983   epsilon: 0.8377    steps: 220    lr: 0.0001     reward: 1.5\n",
      "epis: 990   score: 0.0   mem len: 182106   epsilon: 0.8374    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 991   score: 0.0   mem len: 182229   epsilon: 0.8372    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 992   score: 1.0   mem len: 182400   epsilon: 0.8368    steps: 171    lr: 0.0001     reward: 1.5\n",
      "epis: 993   score: 2.0   mem len: 182580   epsilon: 0.8365    steps: 180    lr: 0.0001     reward: 1.52\n",
      "epis: 994   score: 1.0   mem len: 182749   epsilon: 0.8362    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 995   score: 1.0   mem len: 182900   epsilon: 0.8359    steps: 151    lr: 0.0001     reward: 1.49\n",
      "epis: 996   score: 0.0   mem len: 183023   epsilon: 0.8356    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 997   score: 2.0   mem len: 183241   epsilon: 0.8352    steps: 218    lr: 0.0001     reward: 1.47\n",
      "epis: 998   score: 4.0   mem len: 183516   epsilon: 0.8346    steps: 275    lr: 0.0001     reward: 1.48\n",
      "epis: 999   score: 3.0   mem len: 183785   epsilon: 0.8341    steps: 269    lr: 0.0001     reward: 1.49\n",
      "epis: 1000   score: 0.0   mem len: 183908   epsilon: 0.8339    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 1001   score: 0.0   mem len: 184030   epsilon: 0.8336    steps: 122    lr: 0.0001     reward: 1.4\n",
      "epis: 1002   score: 1.0   mem len: 184202   epsilon: 0.8333    steps: 172    lr: 0.0001     reward: 1.39\n",
      "epis: 1003   score: 5.0   mem len: 184550   epsilon: 0.8326    steps: 348    lr: 0.0001     reward: 1.44\n",
      "epis: 1004   score: 0.0   mem len: 184673   epsilon: 0.8323    steps: 123    lr: 0.0001     reward: 1.4\n",
      "epis: 1005   score: 3.0   mem len: 184923   epsilon: 0.8319    steps: 250    lr: 0.0001     reward: 1.41\n",
      "epis: 1006   score: 0.0   mem len: 185046   epsilon: 0.8316    steps: 123    lr: 0.0001     reward: 1.41\n",
      "epis: 1007   score: 1.0   mem len: 185197   epsilon: 0.8313    steps: 151    lr: 0.0001     reward: 1.41\n",
      "epis: 1008   score: 1.0   mem len: 185348   epsilon: 0.831    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 1009   score: 1.0   mem len: 185516   epsilon: 0.8307    steps: 168    lr: 0.0001     reward: 1.38\n",
      "epis: 1010   score: 2.0   mem len: 185714   epsilon: 0.8303    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 1011   score: 1.0   mem len: 185865   epsilon: 0.83    steps: 151    lr: 0.0001     reward: 1.39\n",
      "epis: 1012   score: 1.0   mem len: 186035   epsilon: 0.8296    steps: 170    lr: 0.0001     reward: 1.39\n",
      "epis: 1013   score: 1.0   mem len: 186186   epsilon: 0.8293    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 1014   score: 1.0   mem len: 186337   epsilon: 0.8291    steps: 151    lr: 0.0001     reward: 1.38\n",
      "epis: 1015   score: 1.0   mem len: 186506   epsilon: 0.8287    steps: 169    lr: 0.0001     reward: 1.39\n",
      "epis: 1016   score: 2.0   mem len: 186706   epsilon: 0.8283    steps: 200    lr: 0.0001     reward: 1.41\n",
      "epis: 1017   score: 1.0   mem len: 186857   epsilon: 0.828    steps: 151    lr: 0.0001     reward: 1.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1018   score: 4.0   mem len: 187115   epsilon: 0.8275    steps: 258    lr: 0.0001     reward: 1.45\n",
      "epis: 1019   score: 3.0   mem len: 187361   epsilon: 0.827    steps: 246    lr: 0.0001     reward: 1.46\n",
      "epis: 1020   score: 5.0   mem len: 187651   epsilon: 0.8264    steps: 290    lr: 0.0001     reward: 1.5\n",
      "epis: 1021   score: 1.0   mem len: 187802   epsilon: 0.8262    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 1022   score: 0.0   mem len: 187925   epsilon: 0.8259    steps: 123    lr: 0.0001     reward: 1.5\n",
      "epis: 1023   score: 2.0   mem len: 188123   epsilon: 0.8255    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 1024   score: 2.0   mem len: 188303   epsilon: 0.8252    steps: 180    lr: 0.0001     reward: 1.49\n",
      "epis: 1025   score: 2.0   mem len: 188501   epsilon: 0.8248    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 1026   score: 2.0   mem len: 188699   epsilon: 0.8244    steps: 198    lr: 0.0001     reward: 1.5\n",
      "epis: 1027   score: 0.0   mem len: 188821   epsilon: 0.8241    steps: 122    lr: 0.0001     reward: 1.47\n",
      "epis: 1028   score: 1.0   mem len: 188992   epsilon: 0.8238    steps: 171    lr: 0.0001     reward: 1.47\n",
      "epis: 1029   score: 0.0   mem len: 189115   epsilon: 0.8236    steps: 123    lr: 0.0001     reward: 1.46\n",
      "epis: 1030   score: 0.0   mem len: 189238   epsilon: 0.8233    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 1031   score: 3.0   mem len: 189482   epsilon: 0.8228    steps: 244    lr: 0.0001     reward: 1.44\n",
      "epis: 1032   score: 0.0   mem len: 189604   epsilon: 0.8226    steps: 122    lr: 0.0001     reward: 1.42\n",
      "epis: 1033   score: 1.0   mem len: 189755   epsilon: 0.8223    steps: 151    lr: 0.0001     reward: 1.41\n",
      "epis: 1034   score: 3.0   mem len: 190006   epsilon: 0.8218    steps: 251    lr: 0.0001     reward: 1.41\n",
      "epis: 1035   score: 3.0   mem len: 190234   epsilon: 0.8213    steps: 228    lr: 0.0001     reward: 1.43\n",
      "epis: 1036   score: 3.0   mem len: 190460   epsilon: 0.8209    steps: 226    lr: 0.0001     reward: 1.45\n",
      "epis: 1037   score: 3.0   mem len: 190690   epsilon: 0.8204    steps: 230    lr: 0.0001     reward: 1.47\n",
      "epis: 1038   score: 0.0   mem len: 190813   epsilon: 0.8202    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 1039   score: 1.0   mem len: 190985   epsilon: 0.8198    steps: 172    lr: 0.0001     reward: 1.46\n",
      "epis: 1040   score: 0.0   mem len: 191108   epsilon: 0.8196    steps: 123    lr: 0.0001     reward: 1.44\n",
      "epis: 1041   score: 0.0   mem len: 191231   epsilon: 0.8194    steps: 123    lr: 0.0001     reward: 1.42\n",
      "epis: 1042   score: 0.0   mem len: 191353   epsilon: 0.8191    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 1043   score: 0.0   mem len: 191475   epsilon: 0.8189    steps: 122    lr: 0.0001     reward: 1.37\n",
      "epis: 1044   score: 1.0   mem len: 191644   epsilon: 0.8185    steps: 169    lr: 0.0001     reward: 1.37\n",
      "epis: 1045   score: 1.0   mem len: 191813   epsilon: 0.8182    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 1046   score: 2.0   mem len: 192013   epsilon: 0.8178    steps: 200    lr: 0.0001     reward: 1.37\n",
      "epis: 1047   score: 3.0   mem len: 192281   epsilon: 0.8173    steps: 268    lr: 0.0001     reward: 1.36\n",
      "epis: 1048   score: 3.0   mem len: 192507   epsilon: 0.8168    steps: 226    lr: 0.0001     reward: 1.34\n",
      "epis: 1049   score: 1.0   mem len: 192676   epsilon: 0.8165    steps: 169    lr: 0.0001     reward: 1.35\n",
      "epis: 1050   score: 2.0   mem len: 192874   epsilon: 0.8161    steps: 198    lr: 0.0001     reward: 1.37\n",
      "epis: 1051   score: 0.0   mem len: 192997   epsilon: 0.8159    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 1052   score: 2.0   mem len: 193195   epsilon: 0.8155    steps: 198    lr: 0.0001     reward: 1.39\n",
      "epis: 1053   score: 0.0   mem len: 193318   epsilon: 0.8152    steps: 123    lr: 0.0001     reward: 1.39\n",
      "epis: 1054   score: 0.0   mem len: 193441   epsilon: 0.815    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 1055   score: 2.0   mem len: 193660   epsilon: 0.8146    steps: 219    lr: 0.0001     reward: 1.37\n",
      "epis: 1056   score: 0.0   mem len: 193783   epsilon: 0.8143    steps: 123    lr: 0.0001     reward: 1.37\n",
      "epis: 1057   score: 3.0   mem len: 194050   epsilon: 0.8138    steps: 267    lr: 0.0001     reward: 1.38\n",
      "epis: 1058   score: 0.0   mem len: 194172   epsilon: 0.8135    steps: 122    lr: 0.0001     reward: 1.38\n",
      "epis: 1059   score: 1.0   mem len: 194344   epsilon: 0.8132    steps: 172    lr: 0.0001     reward: 1.39\n",
      "epis: 1060   score: 4.0   mem len: 194660   epsilon: 0.8126    steps: 316    lr: 0.0001     reward: 1.43\n",
      "epis: 1061   score: 2.0   mem len: 194840   epsilon: 0.8122    steps: 180    lr: 0.0001     reward: 1.44\n",
      "epis: 1062   score: 2.0   mem len: 195041   epsilon: 0.8118    steps: 201    lr: 0.0001     reward: 1.46\n",
      "epis: 1063   score: 2.0   mem len: 195239   epsilon: 0.8114    steps: 198    lr: 0.0001     reward: 1.48\n",
      "epis: 1064   score: 2.0   mem len: 195461   epsilon: 0.811    steps: 222    lr: 0.0001     reward: 1.5\n",
      "epis: 1065   score: 1.0   mem len: 195630   epsilon: 0.8107    steps: 169    lr: 0.0001     reward: 1.5\n",
      "epis: 1066   score: 0.0   mem len: 195753   epsilon: 0.8104    steps: 123    lr: 0.0001     reward: 1.49\n",
      "epis: 1067   score: 3.0   mem len: 196000   epsilon: 0.8099    steps: 247    lr: 0.0001     reward: 1.51\n",
      "epis: 1068   score: 1.0   mem len: 196151   epsilon: 0.8096    steps: 151    lr: 0.0001     reward: 1.5\n",
      "epis: 1069   score: 5.0   mem len: 196517   epsilon: 0.8089    steps: 366    lr: 0.0001     reward: 1.51\n",
      "epis: 1070   score: 0.0   mem len: 196640   epsilon: 0.8087    steps: 123    lr: 0.0001     reward: 1.47\n",
      "epis: 1071   score: 2.0   mem len: 196837   epsilon: 0.8083    steps: 197    lr: 0.0001     reward: 1.49\n",
      "epis: 1072   score: 2.0   mem len: 197055   epsilon: 0.8078    steps: 218    lr: 0.0001     reward: 1.49\n",
      "epis: 1073   score: 2.0   mem len: 197253   epsilon: 0.8074    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 1074   score: 1.0   mem len: 197404   epsilon: 0.8071    steps: 151    lr: 0.0001     reward: 1.47\n",
      "epis: 1075   score: 1.0   mem len: 197555   epsilon: 0.8068    steps: 151    lr: 0.0001     reward: 1.45\n",
      "epis: 1076   score: 3.0   mem len: 197783   epsilon: 0.8064    steps: 228    lr: 0.0001     reward: 1.47\n",
      "epis: 1077   score: 3.0   mem len: 198030   epsilon: 0.8059    steps: 247    lr: 0.0001     reward: 1.48\n",
      "epis: 1078   score: 2.0   mem len: 198227   epsilon: 0.8055    steps: 197    lr: 0.0001     reward: 1.47\n",
      "epis: 1079   score: 2.0   mem len: 198425   epsilon: 0.8051    steps: 198    lr: 0.0001     reward: 1.49\n",
      "epis: 1080   score: 2.0   mem len: 198645   epsilon: 0.8047    steps: 220    lr: 0.0001     reward: 1.51\n",
      "epis: 1081   score: 2.0   mem len: 198843   epsilon: 0.8043    steps: 198    lr: 0.0001     reward: 1.51\n",
      "epis: 1082   score: 2.0   mem len: 199040   epsilon: 0.8039    steps: 197    lr: 0.0001     reward: 1.52\n",
      "epis: 1083   score: 6.0   mem len: 199432   epsilon: 0.8031    steps: 392    lr: 0.0001     reward: 1.57\n",
      "epis: 1084   score: 2.0   mem len: 199630   epsilon: 0.8027    steps: 198    lr: 0.0001     reward: 1.57\n",
      "epis: 1085   score: 0.0   mem len: 199752   epsilon: 0.8025    steps: 122    lr: 0.0001     reward: 1.56\n",
      "epis: 1086   score: 2.0   mem len: 199950   epsilon: 0.8021    steps: 198    lr: 0.0001     reward: 1.56\n",
      "epis: 1087   score: 2.0   mem len: 200148   epsilon: 0.8017    steps: 198    lr: 4e-05     reward: 1.56\n",
      "epis: 1088   score: 2.0   mem len: 200345   epsilon: 0.8013    steps: 197    lr: 4e-05     reward: 1.58\n",
      "epis: 1089   score: 2.0   mem len: 200543   epsilon: 0.8009    steps: 198    lr: 4e-05     reward: 1.58\n",
      "epis: 1090   score: 2.0   mem len: 200741   epsilon: 0.8005    steps: 198    lr: 4e-05     reward: 1.6\n",
      "epis: 1091   score: 2.0   mem len: 200938   epsilon: 0.8001    steps: 197    lr: 4e-05     reward: 1.62\n",
      "epis: 1092   score: 0.0   mem len: 201061   epsilon: 0.7999    steps: 123    lr: 4e-05     reward: 1.61\n",
      "epis: 1093   score: 2.0   mem len: 201259   epsilon: 0.7995    steps: 198    lr: 4e-05     reward: 1.61\n",
      "epis: 1094   score: 2.0   mem len: 201459   epsilon: 0.7991    steps: 200    lr: 4e-05     reward: 1.62\n",
      "epis: 1095   score: 3.0   mem len: 201707   epsilon: 0.7986    steps: 248    lr: 4e-05     reward: 1.64\n",
      "epis: 1096   score: 2.0   mem len: 201928   epsilon: 0.7982    steps: 221    lr: 4e-05     reward: 1.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epis: 1097   score: 3.0   mem len: 202156   epsilon: 0.7977    steps: 228    lr: 4e-05     reward: 1.67\n",
      "epis: 1098   score: 3.0   mem len: 202425   epsilon: 0.7972    steps: 269    lr: 4e-05     reward: 1.66\n",
      "epis: 1099   score: 3.0   mem len: 202672   epsilon: 0.7967    steps: 247    lr: 4e-05     reward: 1.66\n",
      "epis: 1100   score: 2.0   mem len: 202870   epsilon: 0.7963    steps: 198    lr: 4e-05     reward: 1.68\n",
      "epis: 1101   score: 2.0   mem len: 203068   epsilon: 0.7959    steps: 198    lr: 4e-05     reward: 1.7\n",
      "epis: 1102   score: 3.0   mem len: 203293   epsilon: 0.7955    steps: 225    lr: 4e-05     reward: 1.72\n",
      "epis: 1103   score: 2.0   mem len: 203515   epsilon: 0.795    steps: 222    lr: 4e-05     reward: 1.69\n",
      "epis: 1104   score: 2.0   mem len: 203712   epsilon: 0.7946    steps: 197    lr: 4e-05     reward: 1.71\n",
      "epis: 1105   score: 2.0   mem len: 203910   epsilon: 0.7943    steps: 198    lr: 4e-05     reward: 1.7\n",
      "epis: 1106   score: 4.0   mem len: 204227   epsilon: 0.7936    steps: 317    lr: 4e-05     reward: 1.74\n",
      "epis: 1107   score: 2.0   mem len: 204425   epsilon: 0.7932    steps: 198    lr: 4e-05     reward: 1.75\n",
      "epis: 1108   score: 2.0   mem len: 204623   epsilon: 0.7928    steps: 198    lr: 4e-05     reward: 1.76\n",
      "epis: 1109   score: 2.0   mem len: 204821   epsilon: 0.7925    steps: 198    lr: 4e-05     reward: 1.77\n",
      "epis: 1110   score: 0.0   mem len: 204944   epsilon: 0.7922    steps: 123    lr: 4e-05     reward: 1.75\n",
      "epis: 1111   score: 3.0   mem len: 205190   epsilon: 0.7917    steps: 246    lr: 4e-05     reward: 1.77\n",
      "epis: 1112   score: 2.0   mem len: 205388   epsilon: 0.7913    steps: 198    lr: 4e-05     reward: 1.78\n",
      "epis: 1113   score: 4.0   mem len: 205705   epsilon: 0.7907    steps: 317    lr: 4e-05     reward: 1.81\n",
      "epis: 1114   score: 2.0   mem len: 205903   epsilon: 0.7903    steps: 198    lr: 4e-05     reward: 1.82\n",
      "epis: 1115   score: 2.0   mem len: 206101   epsilon: 0.7899    steps: 198    lr: 4e-05     reward: 1.83\n",
      "epis: 1116   score: 2.0   mem len: 206298   epsilon: 0.7895    steps: 197    lr: 4e-05     reward: 1.83\n",
      "epis: 1117   score: 2.0   mem len: 206496   epsilon: 0.7891    steps: 198    lr: 4e-05     reward: 1.84\n",
      "epis: 1118   score: 2.0   mem len: 206693   epsilon: 0.7887    steps: 197    lr: 4e-05     reward: 1.82\n",
      "epis: 1119   score: 2.0   mem len: 206890   epsilon: 0.7884    steps: 197    lr: 4e-05     reward: 1.81\n",
      "epis: 1120   score: 2.0   mem len: 207088   epsilon: 0.788    steps: 198    lr: 4e-05     reward: 1.78\n",
      "epis: 1121   score: 2.0   mem len: 207286   epsilon: 0.7876    steps: 198    lr: 4e-05     reward: 1.79\n",
      "epis: 1122   score: 2.0   mem len: 207484   epsilon: 0.7872    steps: 198    lr: 4e-05     reward: 1.81\n",
      "epis: 1123   score: 2.0   mem len: 207682   epsilon: 0.7868    steps: 198    lr: 4e-05     reward: 1.81\n",
      "epis: 1124   score: 2.0   mem len: 207879   epsilon: 0.7864    steps: 197    lr: 4e-05     reward: 1.81\n",
      "epis: 1125   score: 2.0   mem len: 208077   epsilon: 0.786    steps: 198    lr: 4e-05     reward: 1.81\n",
      "epis: 1126   score: 2.0   mem len: 208275   epsilon: 0.7856    steps: 198    lr: 4e-05     reward: 1.81\n",
      "epis: 1127   score: 2.0   mem len: 208494   epsilon: 0.7852    steps: 219    lr: 4e-05     reward: 1.83\n",
      "epis: 1128   score: 2.0   mem len: 208692   epsilon: 0.7848    steps: 198    lr: 4e-05     reward: 1.84\n",
      "epis: 1129   score: 2.0   mem len: 208890   epsilon: 0.7844    steps: 198    lr: 4e-05     reward: 1.86\n",
      "epis: 1130   score: 2.0   mem len: 209088   epsilon: 0.784    steps: 198    lr: 4e-05     reward: 1.88\n",
      "epis: 1131   score: 2.0   mem len: 209285   epsilon: 0.7836    steps: 197    lr: 4e-05     reward: 1.87\n",
      "epis: 1132   score: 2.0   mem len: 209501   epsilon: 0.7832    steps: 216    lr: 4e-05     reward: 1.89\n",
      "epis: 1133   score: 2.0   mem len: 209699   epsilon: 0.7828    steps: 198    lr: 4e-05     reward: 1.9\n",
      "epis: 1134   score: 5.0   mem len: 210035   epsilon: 0.7821    steps: 336    lr: 4e-05     reward: 1.92\n",
      "epis: 1135   score: 2.0   mem len: 210233   epsilon: 0.7817    steps: 198    lr: 4e-05     reward: 1.91\n",
      "epis: 1136   score: 2.0   mem len: 210430   epsilon: 0.7813    steps: 197    lr: 4e-05     reward: 1.9\n",
      "epis: 1137   score: 2.0   mem len: 210628   epsilon: 0.781    steps: 198    lr: 4e-05     reward: 1.89\n",
      "epis: 1138   score: 2.0   mem len: 210825   epsilon: 0.7806    steps: 197    lr: 4e-05     reward: 1.91\n",
      "epis: 1139   score: 4.0   mem len: 211120   epsilon: 0.78    steps: 295    lr: 4e-05     reward: 1.94\n",
      "epis: 1140   score: 2.0   mem len: 211318   epsilon: 0.7796    steps: 198    lr: 4e-05     reward: 1.96\n",
      "epis: 1141   score: 2.0   mem len: 211515   epsilon: 0.7792    steps: 197    lr: 4e-05     reward: 1.98\n",
      "epis: 1142   score: 2.0   mem len: 211712   epsilon: 0.7788    steps: 197    lr: 4e-05     reward: 2.0\n",
      "epis: 1143   score: 2.0   mem len: 211910   epsilon: 0.7784    steps: 198    lr: 4e-05     reward: 2.02\n",
      "epis: 1144   score: 4.0   mem len: 212207   epsilon: 0.7778    steps: 297    lr: 4e-05     reward: 2.05\n",
      "epis: 1145   score: 2.0   mem len: 212405   epsilon: 0.7774    steps: 198    lr: 4e-05     reward: 2.06\n",
      "epis: 1146   score: 2.0   mem len: 212602   epsilon: 0.777    steps: 197    lr: 4e-05     reward: 2.06\n",
      "epis: 1147   score: 2.0   mem len: 212799   epsilon: 0.7767    steps: 197    lr: 4e-05     reward: 2.05\n",
      "epis: 1148   score: 2.0   mem len: 212996   epsilon: 0.7763    steps: 197    lr: 4e-05     reward: 2.04\n",
      "epis: 1149   score: 3.0   mem len: 213242   epsilon: 0.7758    steps: 246    lr: 4e-05     reward: 2.06\n",
      "epis: 1150   score: 2.0   mem len: 213439   epsilon: 0.7754    steps: 197    lr: 4e-05     reward: 2.06\n",
      "epis: 1151   score: 2.0   mem len: 213637   epsilon: 0.775    steps: 198    lr: 4e-05     reward: 2.08\n",
      "epis: 1152   score: 2.0   mem len: 213835   epsilon: 0.7746    steps: 198    lr: 4e-05     reward: 2.08\n",
      "epis: 1153   score: 2.0   mem len: 214033   epsilon: 0.7742    steps: 198    lr: 4e-05     reward: 2.1\n",
      "epis: 1154   score: 2.0   mem len: 214234   epsilon: 0.7738    steps: 201    lr: 4e-05     reward: 2.12\n",
      "epis: 1155   score: 2.0   mem len: 214431   epsilon: 0.7734    steps: 197    lr: 4e-05     reward: 2.12\n",
      "epis: 1156   score: 2.0   mem len: 214628   epsilon: 0.773    steps: 197    lr: 4e-05     reward: 2.14\n",
      "epis: 1157   score: 2.0   mem len: 214826   epsilon: 0.7726    steps: 198    lr: 4e-05     reward: 2.13\n",
      "epis: 1158   score: 3.0   mem len: 215074   epsilon: 0.7722    steps: 248    lr: 4e-05     reward: 2.16\n",
      "epis: 1159   score: 0.0   mem len: 215197   epsilon: 0.7719    steps: 123    lr: 4e-05     reward: 2.15\n",
      "epis: 1160   score: 2.0   mem len: 215395   epsilon: 0.7715    steps: 198    lr: 4e-05     reward: 2.13\n",
      "epis: 1161   score: 2.0   mem len: 215593   epsilon: 0.7711    steps: 198    lr: 4e-05     reward: 2.13\n",
      "epis: 1162   score: 5.0   mem len: 215919   epsilon: 0.7705    steps: 326    lr: 4e-05     reward: 2.16\n",
      "epis: 1163   score: 2.0   mem len: 216117   epsilon: 0.7701    steps: 198    lr: 4e-05     reward: 2.16\n"
     ]
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_double_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"epis:\", e, \"  score:\", score, \"  mem len:\",\n",
    "                  len(agent.memory), \"  epsilon:\", round(agent.epsilon, 4), \"   steps:\", step,\n",
    "                  \"   lr:\", round(agent.optimizer.param_groups[0]['lr'], 7), \"    reward:\", round(np.mean(evaluation_reward), 2))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_double_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_double_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
